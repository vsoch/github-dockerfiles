
    
    A n d r o M D A  -  3.1


    AndroMDA is an open-source MDA framework distributed under the BSD license.
    Go to http://www.andromda.org/ for more information.

    The project located in this directory has been generated by Cyril Pommier
    using the andromdapp:generate Maven plugin.

    You should at least be running Maven 1.0.2 if you want to build your
    project without any Maven-related problems, below is a summary of what has
    been generated and a list of example goals to call from the command line.

    The generated project structure is well-tailored for use in the development
    of J2EE projects. The build process itself makes use of Maven, dependencies
    and often-used goals have been added for your convenience.

    Custom configuration can be done by updating the project.properties files

    /Chado J2EE project
         |
         |    The root of the project contains a few files that control the overall
         |    build process and common properties
         |
         |-- project.xml
         |        contains information about this project, you may add more information
         |        as long as you do not violate the Maven POM schema, see
         |        http://maven.apache.org/reference/project-descriptor.html
         |-- maven.xml
         |        this file defines the goals that can be called from the root of the
         |        project, most of the time just calling goals in submodules, see below
         |        for examples
         |-- project.properties
         |        common properties that might be changed to configure the project
         |-- build.properties
         |        properties related to versions of external dependencies such as
         |        libraries, also defines project identifier properties; usually
         |        this file is not edited (except perhaps when upgrading to
         |        another AndroMDA version)
         |
         +-- /mda
         |     |
         |     |    The MDA module is the heart of this project, this is where
         |     |    AndroMDA is configured to generate the files needed to
         |     |    assemble the application
         |     |
         |     +-- project.xml
         |     |        contains the AndroMDA dependencies
         |     +-- project.properties
         |     |        specific AndroMDA properties, can be configured here,
         |     |        such as toggling the validation errors on/off
         |     +-- maven.xml
         |     |        no need to edit this file
         |     +-- /src
         |     |        additional sources such as merge-mappings can be
         |     |        placed here, check out the /uml directory, it contains
         |     |        the UML model from which AndroMDA will generate code
         |     +-- /conf/andromda.xml
         |              configures AndroMDA and its components, most
         |              importantly the cartridges which are listed in
         |              their own namespace; global settings are done in the
         |              'default' namespace
         |     
         +-- /common
         |     |
         |     |    The COMMON module collects those resources and classes
         |     |    that are shared between the other modules.
         |     |
         |     +-- project.xml
         |     |        lists common dependencies
         |     +-- /target
         |              shared resources and java classes are generated here,
         |              such as value objects and exceptions
         |
         +-- /core
         |     |
         |     |    The CORE module collects those resources and classes
         |     |    that use the Spring framework, optionally making
         |     |    use of Hibernate and/or EJB under the hood.
         |     |
         |     +-- project.xml
         |     |        lists Spring dependencies
         |     +-- project.properties
         |     |        specific Spring build properties,
         |     |        no need to edit these
         |     +-- maven.xml
         |     |        defines goals for the generation of the database
         |     |        specific DDL code for the creation/deletion of the
         |     |        schema (using Hibernate's SchemaExport)
         |     +-- /src
         |     |        Spring classes that need manual implementation are
         |     |        generated here, they will not be overwritten upon
         |     |        regeneration; this includes the service, DAO and
         |     |        entity implementations
         |     +-- /target
         |              the Spring resources and classes here will be
         |              overwriten each time AndroMDA generates new code
         |              using the Spring cartridge; this includes both
         |              the Hibernate entities and the correponding
         |              *.hbm.xml mapping files as well as the service
         |              and DAO base classes. You'll also find the DDL 
         |              for creating and dropping your schema within this 
         |              directory.
         |
         +-- /web
         |     |
         |     |    The WEB module collects all resources and classes
         |     |    that make up the presentation layer, as well as 
         |     |    bundling all other modules to create a deployable war.
         |     |
         |     +-- project.xml
         |     |        lists WebApp dependencies
         |     +-- project.properties
         |     |        specific Spring build properties,
         |     |        you can set precompileJsps=true to enable the
         |     |        automatic precompilation of JSPs
         |     +-- maven.xml
         |     |        defines specific goals related to JSP precompilation,
         |     |        uncomment the war:init preGoal to enable the copy-over
         |     |        feature
         |     +-- /src
         |     |        controller implementations and editable resource bundles
         |     |        will be generated here,
         |     |        you might consider putting your own JSPs here to
         |     |        be copied over the generated ones when bundling the
         |     |        .war file
         |     +-- /target
         |              a deployable war is bundled here
         |



    In order to succesfully build your project you will need to know
    how to invoke the build process for the existing modules, here's a
    list of examples:

        %> maven install

            simply builds all modules

        %> maven deploy

            collects all artifacts and builds a deployable .war which is then
            deployed, you will need to have built the other modules before

        %> maven clean install deploy

            deletes all generated files, rebuilds and deploys; files in /src
            directories are *not* deleted

        %> maven core

            only build the core module

        %> maven web

            only build the web module

        %> maven nuke

            cleans out all /target directories and removes all Java classes with names
            ending with 'Impl' from the source directories (from the common, core and
            web modules); this goal asks for confirmation, but be careful calling it
            anyway as you will lose your manually edited files

        %> maven create-schema

            invokes the generated DDL code and subsequently tells the database
            to create the schema for the entities

        %> maven drop-schema

            invokes the generated DDL code and subsequently tells the database
            to drop the schema for the entities

        %> maven start-andromda-server

            starts the AndroMDA server, with this server running you will be able
            to significantly speedup the generation process although it will require
            you to use another console while it is running

        %> maven stop-andromda-server

            stops the AndroMDA server

        %> maven mda

            runs AndroMDA on your model and thereby generating files in
            the subdirectories of the existing modules

        %> maven mda -Dfilter=java,hibernate

            runs AndroMDA, but this time only using the Java and Hibernate
            cartridges (in that order)

        %> maven mda -Dfilter=~java,hibernate

            runs AndroMDA, but this time using all cartridges *except*
            the Java and Hibernate cartridges

        %> maven mda -Dfilter=java core web deploy

            runs AndroMDA using only the Java cartridge, rebuilds the core
            module and then the web module and deploys afterwards


    Please note that you may use the '-o' flag at any time to avoid
    having Maven download the dependencies, only build online when Maven
    complains about missing dependencies.

    Each module will install its artifact into the local Maven repository,
    this might be a directory looking like this:

        (Windows)
        C:\Documents and Settings\Cyril Pommier\.maven\repository\chado\

        (*nix)
        /home/Cyril Pommier/.maven/repository/chado/

    For questions or feature requests please use our forum:
    http://forum.andromda.org/


    Good luck!


        -- The AndroMDA Team


How to use XORT and related API

1. add the env variable CodeBase to  where you gunzip the package to env,
i.e if you check out from cvs into this:/home/scott/schema/XMLTools/XORT so I will add the
following line to .cshrc file:

  setenv CodeBase /home/scott/schema/XMLTools

or for bash-likes:

  export CodeBase=/home/scott/schema/XMLTools


2. create the ddl.properties (this basically converts the ddl file which you create 
database into some property file used by XORT modules, for chado, this will be the ddl file:
 $CodeBase/XORT/Config/idb-full.ddl)

$CodeBase/XORT/bin/ddl_properties_creator.pl `-d <ddl file> [-v <view file> -h]`

It will create property file: $CodeBase/XORT/Config/ddl.properties

Notice: 
  If you have some view/functions which are specific to your project, and you want the 
XORT modules can RECOGNIZE(if you have view, but just want to ignore it, then -v is optional),
  e.g you also want to dump view into ChadoXML, or want to load some xml file with view 
data inside, or you want to diff two chado xml file without view data,   you need to MAKE 
one create TABLE statement for each view/function in another file(see sample file
$CodeBase/XORT/Config/function_view.sql & function_view.ddl), this will be
 argument -v to ddl_properties_creator.pl

examples: 
we have this view  for FlyBase project:

create or replace view prediction_evidence(prediction_evidence_id, feature_id, 
evidence_id, analysis_id) as 
select anchor.feature_id||':'||evloc.feature_id||':'||af.analysis_id, 
anchor.feature_id, evloc.feature_id, af.analysis_id
from featureloc anchor, featureloc evloc, analysisfeature af
where anchor.srcfeature_id=evloc.srcfeature_id 
and evloc.feature_id = af.feature_id 
and ((evloc.fmin>=anchor.fmin and evloc.fmax<=anchor.fmax) or
 (evloc.fmin<=anchor.fmin and evloc.fmax>=anchor.fmax)  or
 (evloc.fmin<=anchor.fmax and evloc.fmax>=anchor.fmax) or 
(evloc.fmin<=anchor.fmin and evloc.fmax>=anchor.fmin));

if we want to dump/load/diff chado xml with prediction_evidence using XORT,
 I will create this view.ddl
create table prediction_evidence (
   prediction_evidence_id varchar(50) not null,
   primary key (prediction_evidence_id), 
   feature_id int not null,
   foreign key (feature_id) references feature (feature_id),
   evidence_id int not null,
   foreign key (evidence_id) references feature (feature_id),
   analysis_id int not null,
   foreign key (analysis_id) references analysis (analysis_id),
   unique (feature_id, evidence_id, analysis_id)
);

Then XORT tools will treat this view:prediction_evidence like any other tables.

3. Create database properties for each database you intend to use, and put
those file in CodeBase/XORT/Config, name format: database_alias.properties,
i.e, for alias 'fg8' , the file name will be fg8.properties
CodeBase/XORT/Config/fg8.properties

it contains all necessary information for database connection, e.g fg8.properties contains:

db_type=postgres
db=chado_gadfly8
host=gonzalez.flybase.harvard.edu
port=5432
user=zhou
password=*******

4. now you are ready to give a try:
  $CodeBase/XORT/bin/dump.pl
  $CodeBase/XORT/bin/loader.pl
  $CodeBase/XORT/bin/validator.pl
  $CodeBase/XORT/bin/XORTDiff.pl


More on specific modules:
5. Loader module
   it will load chado xml into database. 
   Norally, before loading, you may want to check whether this xml will cause any 
   problem for loading or whether this is valid ChadoXML file. run  CodeBase/XORT/validator.pl
   will help you debug corrupt data. It will catch 99% errors. The result will be written to
   log file CodeBase/XORT/Log/validator_input_loadfile_name.log
  execute $CodeBase/XORT/bin/loader.pl  to try this module

6. Dump module,
      This module will dump objects from database into ChadoXML file. 

 6.1. The behavor of dumping can be guided by a dumpspec file, eg. we may nest
    all exons within transcripts, and all transcripts within gene, or we also want 
    to dump some evidnece to specific gene modelz(see $CodeBase/XORT/Config/dumpspec_gene.xml
    to see what dumpspec file look like, create one for your own purpose). we already
    created a lots of template dumpspec for different purpose, all those still to current
    chado schema. if you use the same schema, most likely you can re-use those template.
    Especially for apollo-chado round tripping.

 6.2. sometime, we may prefer use some global ID to refer a object, ig. CG# for a object in 
    FlyBase project, this is Project specific strategy, then you can edit this property file
    for your own purpose:   $CodeBase/XORT/Config/config_accession.xml

 6.3. If you try to use 'local_id' mechanism to represent an object, the dumper
   will use this format to represent it: 'table_name'.'_'.'primary_key', it will
   garantee the uniqueness. However, it is not necessary at all to use this
   format for loader, as long as its uniqueness, loader can handle it. 
   execute $CodeBase/XORT/bin/dump.pl  to try this module
  

7. XORTTreeDiff module: this module will compare two ChadoXML(essential can
   work for any XML as long as follow similar rules as ChadoXML) files. 
  Significant different from other generic tree comparison tools is that, 
  this is SCHEMA-AWARE comparison, that means, it only compare two objects
  that have same identity(here means same set of unique key col(s)). 
  There are three level of comparison: ref/all/custom
  for custom level, edit CodeBase/XORT/Config/XORTTreeDiffConfig.xml 
  for your project purpose.
   execute $CodeBase/XORT/bin/XORTDiff.pl  to try this module

8. whenever you change the DB schema(DDL file change), you need to run 
  $CodeBase/XORT/bin/ddl_properties_creator.pl `-d <ddl file> [-v <view file> -h]`
  to synchronize the ddl property file with database schema. 

9. Specially for the apollo-chado round-trip 
   there are a few function and views, which are NOT part to generic chado schema,
     but it is NECESSARY for for apollo-chado round-trip if you want those evidence
    data to support gene model data. 
   9.1 add those views function into property file:
   $CodeBase/XORT/bin/ddl_properties_creator.pl -d $CodeBase/XORT/Config/idb-full.ddl
      -v $CodeBase/XORT/Config/function_view.ddl

  9.2 create views in the database by run the following file inside database shell:
  $CodeBase/XORT/Config/function_view.sql

  9.3 dump chado into ChadoXML with this specific dumpspec file and other options
     $CodeBase/XORT/bin/dump.pl -f chado_xml_file_name -d database_alias -g $CodeBase/XORT/Config/dumpspec_scaffold_for_apollo_NE.xml -f no_local_id -s module  
      for not include 'evidence data' or use following to include 'evidence' data  

     $CodeBase/XORT/bin/dump.pl -f chado_xml_file_name -d database_alias -g $CodeBase/XORT/Config/dumpspec_scaffold_for_apollo.xml -f no_local_id -s module  



1. chado to apollo 
  first set $CodeBase, where $CodeBase is directory which XORT and 
  GameChadoConv live in. you can either add  following line to .cshrc file:
  setenv CodeBase /home/pinglei/schema/XMLTools

  or for bash-likes: 
  export CodeBase=/home/pinglei/schema/XMLTools

  then run 
  $CodeBase/XORT/bin/Chado2GAME.pl

2. game to chado
 first, do samething to set $CodeBase

 then run:
 $CodeBase/XORT/bin/GAME2Chado.pl





  Notice:
3. first following the direction of $CodeBase/XORT/Doc/readme_xort to 
   make sure you setup XORT correctly. 

4. current version not evidence data will not be included when convert 
   from chadoXML to game, we are working on those now.

5. here we couple the chado dumper with GameChadoConv into Chado2GAME.pl, 
   but you have the choice to de-couple those two.
  $CodeBase/XORT/bin/dump.pl   to dump chado into chadoxml(See 
   $CodeBase/XORT/Doc/readme_xort #9 for special requirement)
  
  to convert chado xml into game xml:
  java classpath $CodeBase/GameChadoConv/classes CTG -D....


6. also we couple the GameChadoConv with loader to loader GAME xml into chado, 
   you can divide it into two steps:
   first convert GAME xml into chado xml
   java classpath $CodeBase/GameChadoConv/classes apollo_file_name 
       chado_file_name [-g]  
  
   then call loader to load chado xml into chado:
   $CodeBase/XORT/bin/loader.pl


7. The currently apollo-chado round trip stick to chado schema version 1.0.1. 
    Once there are new schema version, we will update code or setup to 
     synchronize it with database schema.

8. It will dump data from Chado db into a intermediate chadoXML file in
   $CodeBase/XORT/Log/, so from time to time, you may want to clean this 
   directory.Java Tools

A set of tools and API to manipulate data stored in chado.

The src folder contains the code generation pipeline to generate the hibernate mapping and the upcomming buisness layer.
The dist folder contains hibernate mapings for the versions 0.003 and 0.5 of the chado schema.   _______________________________________________________________
  |                                                             |
  |                   AndroMDA for Chado                        |
  |_____________________________________________________________|


Author : Cyril Pommier INRA URGI 
         cyril.pommier@versailles.inra.fr
         

1/ Description

   This maven project allows to generate an UML model (xmi)  from a chado database, and then an hibernate mapping from that model.
 Future version will hopefully allow to generate also a DAO/Service (buisness) layer for Chado. 
 See readme.AndroMDA.txt for more details o this technology.


2/ Prerequiste

  Maven 1.0.2 (NOT 1.1) : this will take care of all the other dependencies.
              create a build.properties file in your home directory and insert the following in this file : 


maven.repo.remote = file:${user.home}/.maven/repository, \
                    http://repo1.maven.org/maven, \
                    http://team.andromda.org/maven


3/ Use

  3.1/UML Model generation

   If you are using another RDBMS than postgreSQL, you must edit hibernate.db.dialect and sql.mappings in the file project.properties.

   Cd to mda. Edit the file project.properties and modify the following properties : 
      dbname=
      maven.andromda.schema2xmi.user=cpommier
      maven.andromda.schema2xmi.password=
      maven.andromda.schema2xmi.driverClass=org.postgresql.Driver
      maven.andromda.schema2xmi.connectionUrl=
      #Modify this parameter if you are using something else than PostgreSQL
      maven.andromda.schema2xmi.mappingsUri=file:./PgMapping.xml

    To launch the model generation simply type : 
      maven chado:schema2xmi


    If you want to have a look at the model, you can use Magic Draw Community Edition version 9.5 
    (IMPORTANT, this has been only tested with UML 1.4, don't try to use this with outputs from a newer version of Magic Draw).


   3.2/ Hibernate Layer generation
  Cd to the project home and issue the following command : 
     maven  mda  -Dfilter=hibernate 
   If you want to check, the sources have been generated in core/target/src

   3.3/Package a jar archive.
  Type maven common core. 
  The jar archive should have been generated in core/target/ as chado-core-0.003.jar. 
  If you want to change the version number, edit application.version in ${basedir}/build.properties
________________________________________________

   Basic Test
________________________________________________




A simple program that will simply connect to a database, load the mapping and then navigate every objects to check everything is all right.

This also contains an example of a criteria query and a simpe navigation of the results.

1/ Set Up
Create a lib directory and copy  or link the following jars in it. Most of them are included in the hibernate 3 distribution, at the exception of chado-core-0.xxx.jar which is the hibernate mapping and postgresql.jar for the jdbc database driver.
        <pathelement location="lib/hibernate3.jar"/>
        <pathelement location="lib/chado-core-0.003.jar"/>
        <pathelement location="lib/c3p0-0.9.0.jar"/>
        <pathelement location="lib/dom4j-1.6.1.jar"/>
        <pathelement location="lib/ehcache-1.1.jar"/>
        <pathelement location="lib/cglib-2.1.3.jar"/>
        <pathelement location="lib/commons-collections-2.1.1.jar"/>
        <pathelement location="lib/commons-logging-1.0.4.jar"/>
        <pathelement location="lib/postgresql.jar"/>
        <pathelement location="lib/asm.jar"/>
        <pathelement location="lib/jta.jar"/>
        <pathelement location="lib/antlr-2.7.6rc1.jar"/>


Edit resources/hibernate.cfg.xml with your connexion parameters. Run with ant basicTestThis directory is for our experiments with Chado to Wiki, Wiki to Chado, and eventually a round trip.

Goal 1:
Query chado for one gene
Make a wiki page with one table containing gene name, description, synonyms

Components needed
Chado to intermediate text - Eric via ModWare
intermediate text to wiki page - JimThis directory contains hackathon related items from FlyBase

Directory layout-

dir/file                 description
==========================================================================
src/perl                 Perl modules used by FlyBase for writing ChadoXML

src/dump_specs           XORT dumpspecs that are used for generating FlyBase
                         web reports.

src/dump_specs/features  XORT dumpspecs for dumping ChadoXML that is used for
                         generating GFF and FASTA files.

loading_ontologies.txt   Standard operating procedure for loading *.obo files.Progress



20070420

Changed WikiBotData and WikiBoxRow to encapsulate data in $self->data hash, instead of in random object attributes.
---
Tendency to put functionality back into things!
See overview.
Can assign a DB object to a Data object.  The data object then "knows" how to load and save itself.
WikiBoxRow objects can self-load from the db if they have a $self->row_id
WikiBoxRow objects now know how to set row_data from their own attributes based on headings.

20070409
Restarted coding with a model closer to reading from free chapter in Perl Data Munging
Separate input/output/munging...in theory!!
This directory contains the makefile, which
produces 2 java .class files in the 'classes'
subdirectory.  These files are GTC.class, for
Game To Chado conversion, and CTG.class for
Chado To Game conversion.

The file 'testScript' provides a roundtrip
example of running the two programs.

The first input is the file to be read, and the
second file is the output.

GTC recognizes '-' parameters.
'-a' means convert 'all', both features and
	computational analyses. This is the default.
'-g' convert gene features, but ignore computational
	analyses
'-c' convert computational analyses, but not gene
	features
For all parameters, non annotation sequences are
converted.

CTG recognizes the '-D' parameter.
CTG '-D' is followed by a comma delimited list
of START,END,NAME, where START is the start
bound of NAME with respect to the ARM coordinates
in the Chado file, and the END with respect to
the ARM. 


This directory contains the makefile, which produces two
java .class files in the 'classes' subdirectory.  These
files are GTC.class, for Game To Chado conversion, and
CTG.class for Chado To Game conversion.

For both programs, the first input is the name of the file
to be read, and the second input is the name of the file
to be written.

GTC recognizes '-' parameters.
'-a' means convert 'all', both features and
	computational analyses. This is the default.
'-c' convert computational analyses, but not gene
	features
'-g' convert gene features, but ignore computational
	analyses
'-t' transactional - only convert those genes which
	are mentioned in a 'changed_gene' element

For all parameters except '-t', non annotation sequences
are converted.

CTG recognizes the '-D' parameter.
CTG '-D' is followed by a comma delimited list
of START,END,NAME, where START is the start
bound of NAME with respect to the ARM coordinates
in the Chado file, and the END with respect to
the ARM.  This is in the case where '_appdata'
elements are not used. 

The code needs alot of cleanup and work, as alot of
functionality was added ad hoc.  Currently, the
writing of features/annotations is stable for the needs
of our chado database loader/writer.  Computational analysis
data input/output is still under development.


# Chado

Chado is a modular schema for handling all kinds of biological
data.  It is intended to be used as both a primary datastore schema as
well as a warehouse-style schema.

Chado was originally conceived as the next generation Flybase
database, combining the sequence annotation database gadfly with the
Harvard and Cambridge databases.  We have avoided organism or
project specificity in the schema, and we hope it will be of use to
other projects.

The modules currently in chado are:

Module                     | Purpose
-------------------------- | -----------------------
Audit                      | database audits
Companalysis               | data from computational analysis
Contact                    | people and groups
Controlled Vocabulary (cv) | controlled vocabularies and ontologies
Expression                 | summarized RNA and protein expresssion
General                    | identifiers
Genetic                    | genetic data and genotypes
Library                    | descriptions of molecular libraries
Mage                       | microarray data
Map                        | maps without sequence
Organism                   | species
Phenotype                  | phenotypic data
Phylogeny                  | phylogenetic trees
Publication (pub)          | publications and references
Sequence                   | sequences and sequence features
Stock                      | specimens and biological collections

For documentation on the various modules, please see the [GMOD Wiki](http://gmod.org/wiki/Category:Chado_Modules).

Other modules are possible; the existing modules cover a very large
variety of use cases.

Chado has a fairly abstract schema, and ontologies and controlled
vocabularies (CVs) are utilised where their use is favourable to
relational modeling.  In particular, the sequence ontology (SO) is vital to
the sequence module.

Some (but not all) of the use cases we have discussed are:

- Central dogma genome annotations
- Genes that break the central dogma (of which there are many
  Annotated in fly, including polycistronic transcripts, transplicing,
  selenocysteine readthroughs, rna editing, ....)
- Sequence variation data, including SNPs, transposable element
  insertions, indels, ... how this relates to phenotypes, how these
  effect the central dogma....
- Non-wildtype data, including representing a wildtype transcriptome
  and proteome on a non wildtype genome; implicit and explicit central
  dogma examples for mutant strains
- Complex phenotypic data
- Ontologies structured as graphs; querying over graph ontologies
  non-recursively by pre-computing the closure
- Sequence ontology
- Comparative data
- Genetic interactions
- Transgene constructs, complex genetic experiments and their results

The core schema is DBMS independent.  The SQL table create files can
be found in the chado/modules directory.  The main Chado developers
are currently using PostgreSQL.


## Installation

For manual installation instructions, please read the included
[INSTALL.Chado](./INSTALL.Chado.md) document for instructions on how to install
the Chado schema.

Alternatively [Chado schema dumps](https://github.com/erasche/chado-schema-builder/releases) and [docker
images](https://github.com/erasche/docker-chado) are available if you are just
looking to experiment around with Chado.

## Support

Please see our website for more information on Chado and the GMOD project: http://www.gmod.org/
You can send questions to the Chado mailing list: gmod-schema@lists.sourceforge.net



## Authors

Chris Mungall, David Emmert and the GMOD team
October 2, 2003
Using Apollo with Chado is now documented as part of the Apollo project's documentation:

http://genomearchitect.readthedocs.io/en/latest/ChadoExport.html
This is mostly an experimental code area; will move out of test into src/perl
when it's more stable.

cjm@fruitfly.org
Perl Tools for Chado

Note

These tools are unsupported - I really just wrote them for my own use,
feel free to download and play with them, just don't base any critical
code on them, because they are liable to change at any time. 

Modules Required

In addition to some standard CPAN modules, you will need the following:
XML::NestArray

This is a module I wrote for dealing with nested array / tree
datastructures that uses XML. It also has a native indented text
format, that I find preferable for viewing/editing in emacs. 
This will eventually be uploaded to CPAN, but for now you can get it
from the BDGP CVS Repository, check out the project
scratch/xml-nestarray. 

Bio::XML

This is a module for dealing with biological data (such as features)
as nested arrays/xml. This may eventually go into bioperl, but it is
different from the bioperl approach, as it is emphatically not object
oriented. The core of the code is just a series of transforms in
Bio::XML::Sequence::Transform; for instance, inferring UTR from exons;
inferring splice sites; inferring coordinates of composite features
(eg transcripts) from the coordinates of leaf features (eg exons);
coordinate transformations.

In addition to being a relational database, Chado will also need a
Domain Logic Layer. This is one example of how that layer may
look. Other alternatives include SQL (views and procedures), XSLT and
an Object Oriented layer. My personal views are fairly anti-OO these
days, but that's just me. 

Bio::XML is checked into the chado repository, right here, see
chado/src/test/perl. 

Tools

After all that, there's really just one tool so far:

fgraph

This draws a network graph diagram showing Chado data. Right now it
works if you feed it XML::NestArray style indented text or XML. The
XML it expects will probably differ from official Chado XML, it uses a
schema transformation I find useful. Eventually it should all be in
sync.
 
For an example of the graph images produced, see Sequence Module examples 
or Comp Analysis Module examples

This tool requires GraphViz from CPAN.

Chris Mungall Last modified: Tue Nov 12 18:15:31 PST 2002
Chado-XML
=========

Chado-XML is a direct mapping of the Chado relational schema into
XML. Currently the only tool for performing this mapping is XML::XORT,
which can dump or save Chado-XML to and from a chado db.

Contents:
--------

chado-xml/

        README
        xsl/                    -- useful transforms
        dtd/                    -- DTDs/XSDs defining the xml model
        examples/          -- example XML files

The current documentation can be found at gmod.org.

See:

        SourceForge: http://sourceforge.net/projects/asciidoc/
        Main website: http://www.methods.co.nz/asciidoc/

Macros
------

The basic chado-xml expansion can be extremely verbose - this is
because chado-xml uses the unique keys from the chado db, yet it does
not database internal foreign keys.

Macros can be used to capture repeated nodes in the xml and give them
XML IDs that are valid within a particular document.

See also
--------

gmod.org/XORT
chado-xml/xsl
------------

XSLT programs for transforming Chado-XML - either into other formats,
or other Chado-XML forms (eg with or without macros).

See: http://www.w3.org/Style/XSL


Running an XSLT transformation
-------------------------

You can use any xslt processor you like with these; both Xalan and
xsltproc can be run from the command line. For integrating with an
application, you can use Xalan (java) or Lib::XSLT (perl).

Xalan can be downloaded from http://xml.apache.org/xalan-j

xsltproc is part of libxslt, available from http://xmlsoft.org/XSLT/

To run on the command line with xsltproc:

 $ cd gmod/schema/chado/chado-xml
 $ xsltproc xsl/chado-insert-macros.xsl examples/CG10833.expanded.chado-xml

The sample database that used to be in this directory has been moved to its
own cvs repository in the gmod cvs, in a repository called 'sample_dbs'.

Scott Cain
1/9/07

This directory is for BDGP extensions to Chado

Feel free to use any of these extensions - however, be aware that they
are experimental and may be liable to change. Currently these
extensions are not a supported part of chado.

The BDGP extensions currently comprise of

* BDGP Views

  bdgp-views.sql

* Sequence Ontology Layer

  so-views.sql                  -- portable SO view layer


Example data for phylogeny module in chado-xml

Use a generic loader to put this in db; either XML::XORT or DBIx::DBStag

If your test db does not have organisms loaded, first load org.chado-xml
INTRODUCTION

Chado is a relational database schema for managing genomic and genetic 
organism data. To maintain genomic data in chado, all sequence features
and their relationship are stored in 2 tables: feature and
feature_relationship. The intrinsic type of a feature is stored in the
feature table where the feature type is defined in the sequence
ontology (SO). Parent to child relationships are stored in the
feature_relationship table, with a relationship type defined in SO. 

By examining all feature types and relationships of every feature in a
chado database one can determine how SO is instantiated (SOI) for this
particular database instance. SOI can be created using a server-side
function, but it could also be created using a database trigger. Once
the SOI has been created, an SQL query can be written in a uniform way.
Of particular interest is that the query can be made much more efficient
in a RDBMS that supports server-side programming and sub-query. 

Traditionally, to retrieve relationship between features, a table
joining is used. This kind of SQL query may result in many duplicate
values coming in from database server. For example, a parent can appear
as many times as the number of its children multiplied by the number of
their children and so on. Using SOI, an SQL query can written in such a
way that each distinct feature appears only once with feature's values
plus its parent ID if any and the feature depth in a SOI sub-tree of
interest. With the depth value, the parent feature is guaranteed to come
in before its children so that placing children into their parent is a
linear search. Two SOI modules have been implemented in perl:
SOI::Adapter.pm and SOI::Feature.pm. SOI modules are lightweight, e.g.
Feature.pm has about 300 lines of code. Using these 2 modules, any data
model can be constructed from a chado database. An external SQL template
can be used to construct a feature tree as long as the SQL conforms to
the following: 1) each feature has parent ID (excluding the top node
feature), 2) each feature has the tree depth value. Since constructing
the feature relationship is no longer hard-coded into the SQL query,
data model growth will have no exponential effect on code base and
retrieval performance. Performance data from various queries show SOI
modules have excellent performance compared to architectures that rely
upon table joining for feature retrieval.


THE soi PROJECT

Currently the soi project has modules supporting only postgres chado database,
soi modules should work in other dbms as long as soi (ontology) can be
created.  Some templates depend on server side functions (see templates
and fx directories) 

contact: sshu@fruitfly.org


CONTENTS

readme: doc for soi project

Directories:

SOI: perl SOI modules (Adapter, Feature, Outputter)

scripts: perl scripts using SOI module to query chado db and use results
    for some purpose, say dump xml

cgi: script for web server, currently we have only get_xml.pl that serves
    out GAME xml for apollo

fx: plpgsql functions on which some templates depend
[deprecated, mv to cv or sequence module function dir]

templates: soi templates SOI::Adapter can take and construct feature tree


FEATURES

Pros:
    fast
    lightweight
    seamlessly extendable
    flexible

Cons:
    depends on soi created in a chado database
    depends on server side programming (hence plpgsql functions above)
    SQL is a bit harder to write? see templates for example
    (see ABSTRACT for more)


INSTALLATION

To create soi in a chado database:

1) load plpgsql functions in fx directory to the database (not just for
   creating soi)
2) select * from create_soi() in psql shell (need to change unique
   constraint on cvtermpath in current FlyBase chado dump, see
   gmod/schema/chado/modules/sequence/bdgp/bdgp-index.pl)

General instruction to write SQL for SOI::Adapter (see templates for example):

1) use union to join top feature select with child feature select
2) top feature parent_id has to be null and soi tree depth has to be 1
3) child feature depth has to be maximum from its parent in the soi tree
   so a child feature is guaranteed to come in after its parent feature
   from server
4) each feature appears only once in the result set

here is whole soi tree SQL for a type:

(select c.name, c.cvterm_id, 1 as depth
FROM cvterm c, cv
WHERE c.cv_id = cv.cv_id and c.name IN ($top_feature_type) and cv.name = 'so')
UNION
(select c.name, c.cvterm_id, max(pathdistance+1) as depth
FROM cvterm c, cvtermpath path, cvterm p, cv
WHERE c.cvterm_id = subject_id and p.cvterm_id = object_id
and path.cv_id =cv.cv_id and cv.name = 'soi'
and p.name in ($top_feature_type) group by c.name, c.cvterm_id)


here is child soi tree SQL for a type

select c.name, c.cvterm_id, max(pathdistance+1) as depth
FROM cvterm c, cvtermpath path, cvterm p, cv
WHERE c.cvterm_id = subject_id and p.cvterm_id = object_id
and path.cv_id =cv.cv_id and cv.name = 'soi'
and p.name in ($top_feature_type) group by c.name, c.cvterm_id

performance issue:

postgres v7.4 is better than v7.3
removing chromosome_arm residues form db speeds up query of
    golden_path_region (Dros specific?)
create cluster index on featureloc for speed (see
    chado/modules/sequence/bdgp/bdgp-index.pl)

Ontologies of feature properties to use for chado. The point of these
controlled vocabularies is to try to capture the terms you would
typically find as tags in a GFF file or GenBank entry. As such these
are not as coherent or uniform as the known OBO ontologies.

These may move to the SO CVS repository:

feature_property.obo
genbank_feature_property.obo

# $Id: README,v 1.5 2007-04-13 02:13:42 briano Exp $

chaos-xml/README

Refer to the INSTALL file for installation instructions.

                               Chaos-XML Library

Introduction

   The Chaos-XML Library contains software and specifications for the
   Chaos XML format. Chaos XML is for representing sequences and sequence
   features.

   Chaos is a mapping of the chado relational schema into a hierarchical
   (ie XML) model. Chaos XML can be used in conjunction with a Chado
   database, or it can be used entirely independently of Chado or any
   database. Of course, the Chado database may be used entirely
   independently of Chaos XML.

   This directory contains perl scripts, modules and XSL transforms for
   extracting chaos from various datasources, and for manipulating and
   transforming chaos documents.

   Chaos-XML is the annotation format for CGL.

Status

   This software and data format will be officially released on Jun 1
   2005. Until then that time it is to be considered alpha software.

Downloading and Installation

   You can either download the latest stable release, or fetch the
   bleeding-edge latest version from CVS.

Download stable release

   This is recommended for most users.

   The current latest release is Bio-Chaos-0.01. Download this and
   follow the instructions in the INSTALL file.

Obtain from CVS

   See http://wiki.gmod.org/index.php/Chaos_XML for details. Chaos-XML
	is part of the gmod-schema project, see gmod/schema/chado/chaos-xml

Motivation

   The chaos format arose out of a need for an annotation data exchange
   format within the BDGP. We needed an XML format that would be
   compatible with the richness of annotation data that could be
   represented in a Chado database. We also need a format that we (BDGP)
   had direct control over, so that we could make minor modifications as
   required.

   We created chaos-xml as an almost direct mapping of a pre-1.0 Chado
   database. The idea was that there would be no *semantic* mapping
   required between chado and chaos, purely a *syntactic* XML to
   relational mapping. A summary of the differences between Chado and
   Chaos are listed further on in this document.

   At around the same time the main chado software development team at
   FlyBase devised the official Chado XML format. Chado XML and Chaos XML
   are semantically very similar, but they are different in how the XML
   to relational database mapping is performed. Chado XML is also
   considerably more verbose than Chaos XML. This is because Chaos uses
   some denormalisations of the Chado model, explained below. In our view
   these two formats are complementary. Conversions between the formats
   should be trivial [TODO: XSL mapping].

The Chaos Model

   For full documentation on the Chaos XML datamodel see the dtd
   directory. Elements in Chaos XML will generally have an equivalent
   table or column in the Chado relational schema. Thus the Chado
   documentation should also serve as documentation for the Chaos XML
   format.

   Briefly, Chaos (and Chado) are generic ontology-typed feature graph
   formats. This is similar to, but richer than, the GFF3 format.

   The central concept in Chaos/Chado is a "feature". A feature can
   represent any genomic or sequence entity that is typed by the
   Sequence Ontology (SO).

   Features are interconnected in a feature graph using the
   feature_relationship element. This is to indicate which exons and
   proteins belong to which transcript, which transcripts belong to which
   gene.

   The location of a feature, relative to another feature, is described
   by the featureloc element. All locations are interbase (ie counting
   from 0, not 1. It is the gaps between bases that are counted, not the
   bases themselves). In contrast to chado, which uses fmin/fmax to
   indicate the left and right coordinates, chaos use nbeg/nend to
   indicate the five prime (natural start) and three prime (natural end)
   coordinates.

   For more details, see the dtd/ directory

Differences between Chaos XML and the Chado Relational Database Schema

   Chaos-XML is based on the Chado relational model. For a full
   explanation of the meaning of the elements in Chaos-XML, please refer
   to the Chado documentation at wiki.gmod.org.

   For a full explanation of the differences, see http://gmod.org//Chaos_XML

Library Contents

   The Chaos-XML Library consists of specifications and software for
   dealing with Chaos-XML files.

DTD

   The DTD specification can be found in dtd/chaos.dtd

   Soon there will also be specifications as XML Schema and/or Relax-NG.

XSL Stylesheets

   XSL transformations can be found in the xsl/ directory.

Example Chaos XML

   Example Chaos-XML can be found in the sample-data/ directory.

 Scripts

   The scripts are in the bin/ directory. You need to install the
   perl chaos library before running these scripts.

 Chaos Perl Module

   You can browse the perl modules in the lib/ directory. To install,
   download the chaos-xml library and follow the instructions in the
   INSTALL file.

Converting between Chaos-XML and Chado-XML

   There are XSLT stylesheets defined for mapping between these two
   similar formats, see the xsl/ directory.

   If you are not familiar with XSLT, you can use these scripts, part of
   this distribution (see the bin/ directory):

     * cx-chaos2chadoxml.pl
     * cx-chadoxml2chaos.pl

Future Extensions

   As new modules are added to chado (for example, the genetics module
   and the phylogeny module), corresponding chaos-xml DTDs will be
   generated.

Questions and Further info

   For general info on the chado schema, see http://gmod.org/Chado

   Send questions/comments to this mail list:
   
   http://lists.sourceforge.net/lists/listinfo/gmod-schema


   Chris Mungall

chaos-xml/xsl

CHAOS XSL TRANSFORMS
====================

Chris Mungall and Brad Marshall
BDGP 
2004

This directory contains XSL stylesheets for querying, transforming and
exporting data from Chaos XML files. For example chaos-xml files, see
the ../example-xml/ directory

There are a number of tools available for running XSL stylesheets. We
use xsltproc [see http://xmlsoft.org/XSLT/xsltproc2.html]

For example

  xsltproc xsl/chaos-to-dbxref-tbl.xsl example-xml/AE003734.chaos.xml

CHADO STAG TEMPLATES
====================

[note: this part of chado is NOT YET FULLY SUPPORTED]

This directory contains SQL Templates for querying chado in stag
format. The templates can either be browsed as a resource in their own
right, or they can be used in conjunction with the DBStag library.

These templates let you query chado via

  WWW
  Command Line
  API

And retrieve XML or auto-objects representing nested relations from a
set of templated queries.

For example

  chado-pcgenemodel -d chado gene_name=CG1234
  chado-genemodel -d chado gene_name=CR15821
  chado-protein -d chado CG1234-PA
  chado-mrna -d chado CG1234-RA
  chado-feature-by-direct-cvterm -d chado cvterm_name='chaperone activity'

PREREQUISITES
=============

You need a local chado database, AND you need to define the BDGP views
over them; see 

  chado/modules/sequence/bdgp/bdgp-views.sql

  You also need

  Data::Stag
  DBIx::DBStag (version 0.02 or higher)

UNERSTANDING STAG SQL TEMPLATES
===============================

A stag SQL template is fairly simple - the first part is basically an
SQL query with placeholder variables, and the second part is optional
metadata.

A placeholder variable (typically in the WHERE part of the query) looks like this

  [ feature.name => &name& ]

This means the template user can optionally fill in a value for &name&
- this can be a string match (in which case LIKE will be used) or an
exact match.

DBStag utilises one extension to SQL - the "USE NESTING" clause.

This is for constructing hierarchical XML from the query results -
DBStag will decompose the results of any SQL query back into the
original relations

For full documentation on templates, install DBIx::DBStag and read the
pod docs for DBIx::DBStag::SQLTemplate

USING STAG TEMPLATES
====================

Stag templates can either be used on the command line or via a WWW
interface. Both give the option of returning either row data or
hierarchical data (eg XML).

The WWW interface to chado via stag templates may or may not be running here:

  http://www.godatabase.org/cgi-bin/ubiq/ubiq.cgi

Select one of the chado databases and you will be presented with a
list of possible templates; select one and then fill in the
placeholder variables, or your own SQL.

Alternatively, you can install DBIx::DBStag (use at least version
0.02) from CPAN or from cvs (http://stag.sf.net), then do this:

  setenv DBSTAG_TEMPLATE_DIRS "$HOME/gmod/schema/chado/stag-templates"

(or wherever this directory resides)

You can use a template by specifying its name after the slash with the
following script:

  selectall_xml.pl -d chado /basic-feature CG1234

or you can bind specific placeholder variables like this:

  selectall_xml.pl -d chado /basic-feature name=CG1234

You can also generate an individual script for every templates; just type

  make -f make-publish PUBTMPLDIR=/my/template/dir TEMPLATEBINDIR=/my/bin

(substituting appropriately)

then you can just run the templates; try

  basic-feature -h

Have fun! More documentation coming soon

Chris Mungall BDGP
cjm@fruitfly.org
This package contains utility files for the community annotation system.
Currently, this consists of CGI files for making GAME or Chado XML files
for use by a WebStart Apollo application and a CGI for users to upload
their saved GAME or Chado XML files.

In order to work properly, this set of tools requires that several things
be installed already:

  A Chado database populated with data
  GBrowse (preferable 1.69 or better)
  Apollo (both a compiled and signed webstart as well as the compiled
     application on the server)


To install, run

  perl Build.PL
  ./Build
  sudo ./Build install

Future additions to this package include tools to fascilitate the 
Chado/MediaWiki roundtrip.


EXAMPLE GBROWSE CONFIG SECTION FOR MAKING A POPUP BALLOON WITH EDIT LINK

[DETAIL SELECT MENU]
width = 250
html  = <table style="width:100%">
         <tr>
           <th style="background:lightgrey;cell-padding:5">
             SELECTION
             <span style="right:0px;position:absolute;color:blue;cursor:pointer" 
                   onclick="SelectArea.prototype.cancelRubber()">
               [X]
             </span>
           </th>
         </tr>
         <tr>
           <td>
             <span style="color:blue;cursor:pointer" onclick="SelectArea.prototype.clearAndSubmit()">
              Zoom in
             </span>
           </td>
         </tr>
         <tr>
           <td>
             <span style="color:blue;cursor:pointer" onclick="SelectArea.prototype.clearAndRecenter()">
               Recenter on this region
             </span>
           </td>
         </tr>
         <tr>
           <td onmouseup="SelectArea.prototype.cancelRubber()">
             <a href="/cgi-bin/apollo_request_region.pl?selection=SELECTION">
              Open this section in Apollo
             </a>
           </td>
         </tr>
       </table>

