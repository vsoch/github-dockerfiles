README

Lucene provides support for segmenting Lao, Myanmar and Khmer into syllables with solr.ICUTokenizerFactory in the analysis-extras contrib module.
To use this tokenizer, see solr/contrib/analysis-extras/README.txt for instructions on which jars you need to add to your SOLR_HOME/lib
README

Lucene provides support for segmenting Lao, Myanmar and Khmer into syllables with solr.ICUTokenizerFactory in the analysis-extras contrib module.
To use this tokenizer, see solr/contrib/analysis-extras/README.txt for instructions on which jars you need to add to your SOLR_HOME/lib
README

Lucene provides support for segmenting Lao, Myanmar and Khmer into syllables with solr.ICUTokenizerFactory in the analysis-extras contrib module.
To use this tokenizer, see solr/contrib/analysis-extras/README.txt for instructions on which jars you need to add to your SOLR_HOME/lib
README

Lucene provides support for segmenting Lao, Myanmar and Khmer into syllables with solr.ICUTokenizerFactory in the analysis-extras contrib module.
To use this tokenizer, see solr/contrib/analysis-extras/README.txt for instructions on which jars you need to add to your SOLR_HOME/lib
README

Lucene provides support for segmenting Lao, Myanmar and Khmer into syllables with solr.ICUTokenizerFactory in the analysis-extras contrib module.
To use this tokenizer, see solr/contrib/analysis-extras/README.txt for instructions on which jars you need to add to your SOLR_HOME/lib
README

Lucene provides support for segmenting Lao, Myanmar and Khmer into syllables with solr.ICUTokenizerFactory in the analysis-extras contrib module.
To use this tokenizer, see solr/contrib/analysis-extras/README.txt for instructions on which jars you need to add to your SOLR_HOME/lib
### ElasticSearch and Kibi Docker

see also https://hub.docker.com/_/elasticsearch/

```
git pull zorino/docker-es-kibi

docker run -d --name es-kibi -v $PWD/data:/usr/share/elasticsearch/data -p 9200:9200 -p 5601:5601 zorino/docker-es-kibi
```

ElasticSearch is now running on port 9200 and Kibi on port 5601

This folder contains images used in the repository root readme file. 
# Navitia's dev image
Docker image used to build and run navitia's test

This image should be used only for dev purpose

# build

```
docker build -t navitia-local-builder .
```

Note: you can decide the number of thread you want the compilation to use with the env var NB_THREAD.

by default the compilation uses all it can

# run

the navitia dir needs to be given as a volume and mounted in build/navitia

We also need to give the docker socket to run docker tests inside the docker

```
docker run -v {you_path_to_navitia}:/work/navitia  -v /var/run/docker.sock:/var/run/docker.sock navitia-local-builder  
```   

If you plan to do it several times, you might want to cache the compilation

```
docker run -v {your_path_to_navitia}:/work/navitia -v ${pwd}/docker_build_dir:/work/build -v /var/run/docker.sock:/var/run/docker.sock navitia-local-builder  
```   

# misc

Note: don't forget to update the submodules in you local navitia sourcesThis project aim at building a set of docker images for navitia.
The end result will be three images for jormungandr, kraken and tyr

We use a temporary docker image to build the other images because we want to reduce the time to build and the size of the images.

First step, build the builder:
```
    docker build -t navitia-builder .
```

It's possible to compile navitia when building the image, it's mostly useful when testing,
it allow to not compile at each run so it's faster to run.
```
    docker build -t navitia-builder --build-arg BUILD=1 .
```

Then use to build the images, you need to mount your docker's socket into the builder.
It will compile navitia from scratch and create the images
```
    docker run -v /var/run/docker.sock:/var/run/docker.sock navitia-builder
```

This should take a while.

If you want to build the docker on your own sources you can mount your navitia sources. 
You also need to add the `n` flag to the builder for the script not to update the sources

```
docker run -v /var/run/docker.sock:/var/run/docker.sock -v {your_path_to_navitia}:/build/navitia navitia-builder -n
```

those images are available on dockerhub and are used in the docker compose


If you want to use them without the docker compose, you will need some configuration files and to mount them
in the different containers. You will find example for each file in the docker compose

There is a few external dependancies that are require for navitia to work:

    - rabbitmq
    - postgresql
    - redis

So first we will start our dependencies, we need one postgres database for jormungandr/tyr and one postgis for our instance
```
docker run --name postgres_jormungandr -e POSTGRES_PASSWORD=navitia -e POSTGRES_USER=jormungandr -d postgres
docker run --name postgres_default -e POSTGRES_PASSWORD=navitia -e POSTGRES_USER=default -d mdillon/postgis
docker run --name rabbitmq rabbitmq:management
```
Docker compose file for artemis

the instances are listed in artemis_instances_list.yml

the file docker-artemis-instance.yml has been generated using j2cli (cf. general readme)

`j2 docker-instances.jinja2 artemis/artemis_instances_list.yml > artemis/docker-artemis-instance.yml`


To start artemis:

`docker-compose -f docker-compose.yml -f artemis/docker-artemis-instance.yml up`

# TODO
for the moment we just have the artemis platform with this, we'll need to add an artemis container and change artemis:
 - to be able to launch the binarization from another container (either by using an http call to tyr or by running `docker exec tyr_container manage.py load_data`)
 - to find a way around the `service kraken_toto start|stop` (I think we can just change artemis to work if the kraken is already started)
 - to find a way around the `service apache2 start|stop` (likewise, I think we can make artemis work with the jormungandr already started)
# Running with local python sources

You can run jormungandr with your own sources

Note: if you changed any cpp or protobuff files, you will need to rebuild all the navitia's images (cf the readme in the builder directory)

```
NAVITIA_PATH={your_own_navitia_path} docker-compose -f docker-compose.yml -f docker-compose-local-jormun.yml up
```
# Træfik Web UI

Access to Træfik Web UI, ex: http://localhost:8080

## Interface

Træfik Web UI provide 2 types of informations:
- Providers with their backends and frontends information.
- Health of the web server.

## How to build (for backends developer)

Use the make file :

```shell
make build           # Generate Docker image
make generate-webui  # Generate static contents in `traefik/static/` folder.
```

## How to build (only for frontends developer)

- prerequisite: [Node 4+](https://nodejs.org) [yarn](https://yarnpkg.com/)

  Note: In case of conflict with the Apache Hadoop Yarn Command Line Interface, use the `yarnpkg`
  alias.

- Go to the directory `webui`

- To install dependencies, execute the following commands:
  - `yarn install`

- Build static Web UI, execute the following command:
  - `yarn run build`

- Static contents are build in the directory `static`

**Don't change manually the files in the directory `static`**

- The build allow to:
  - optimize all JavaScript
  - optimize all CSS
  - add vendor prefixes to CSS (cross-bowser support)
  - add a hash in the file names to prevent browser cache problems
  - all images will be optimized at build
  - bundle JavaScript in one file


## How to edit (only for frontends developer)

**Don't change manually the files in the directory `static`**

- Go to the directory `webui`
- Edit files in `webui/src`

- Run in development mode :
  - `yarn run serve`

- Træfik API connections are defined in:
  - `webui/src/app/core/health.resource.js`
  - `webui/src/app/core/providers.resource.js`

- The pages contents are in the directory `webui/src/app/sections`.


## Libraries

- [Node](https://nodejs.org)
- [Yarn](https://yarnpkg.com/)
- [Generator FountainJS](https://github.com/FountainJS/generator-fountain-webapp)
- [Webpack](https://github.com/webpack/webpack)
- [AngularJS](https://docs.angularjs.org/api)
- [UI Router](https://github.com/angular-ui/ui-router)
  - [UI Router - Documentation](https://github.com/angular-ui/ui-router/wiki)
- [Bootstrap](http://getbootstrap.com)
- [Angular Bootstrap](https://angular-ui.github.io/bootstrap)
- [D3](http://d3js.org)
  - [D3 - Documentation](https://github.com/mbostock/d3/wiki)
- [NVD3](http://nvd3.org)
- [Angular nvD3](http://krispo.github.io/angular-nvd3)


---------------------------------
Copyright (c) Henrik Ravn 2004

Use, modification and distribution are subject to the Boost Software License, Version 1.0.This directory contains an update to the ZLib interface unit,
distributed by Borland as a Delphi supplemental component.

The original ZLib unit is Copyright (c) 1997,99 Borland Corp.,
and is based on zlib version 1.0.4.  There are a series of bugs
and security problems associated with that old zlib version, and============

The zlibpas interface is:
  Copyright (C) 1995-2003 Jean-loup Gailly and Mark Adler.
  Copyright (C) 1998 by Bob Dellaca.
  Copyright (C) 2003 by Cosmin Truta.

The example program is:
  Copyright (C) 1995-2003 by Jean-loup Gailly.
  Copyright (C) 1998,1999,2000 by Jacques Nomssi Nzali.
  Copyright (C) 2003 by Cosmin Truta.

  This software is provided 'as-is', without any express or impliedThis directory contains a .Net wrapper class library for the ZLib1.dll

The wrapper includes support for inflating/deflating memory buffers,
.Net streaming wrappers for the gz streams part of zlib, and wrappers
for the checksum parts of zlib. See DotZLib/UnitTests.cs for examples.

Directory structure:
--------------------

LICENSE_1_0.txt       - License file.
readme.txt            - This file.
DotZLib.chm           - Class library documentation
DotZLib.build         - NAnt build file
DotZLib.sln           - Microsoft Visual Studio 2003 solution file

DotZLib\*.cs          - Source files for the class library

Unit tests:
-----------
The file DotZLib/UnitTests.cs contains unit tests for use with NUnit 2.1 or higher.
To include unit tests in the build, define nunit before building.


Build instructions:
-------------------

1. Using Visual Studio.Net 2003:
   Open DotZLib.sln in VS.Net and build from there. Output file (DotZLib.dll)
   will be found ./DotZLib/bin/release or ./DotZLib/bin/debug, depending on
   you are building the release or debug version of the library. Check
   DotZLib/UnitTests.cs for instructions on how to include unit tests in the
   build.

2. Using NAnt:
   Open a command prompt with access to the build environment and run nant
   in the same directory as the DotZLib.build file.
   You can define 2 properties on the nant command-line to control the build:
   debug={true|false} to toggle between release/debug builds (default=true).
   nunit={true|false} to include or esclude unit tests (default=true).
   Also the target clean will remove binaries.
   Output file (DotZLib.dll) will be found in either ./DotZLib/bin/release
   or ./DotZLib/bin/debug, depending on whether you are building the release
   or debug version of the library.

   Examples:
     nant -D:debug=false -D:nunit=false
       will build a release mode version of the library without unit tests.
     nant
       will build a debug version of the library with unit tests
     nant clean
       will remove all previously built files.


---------------------------------
Copyright (c) Henrik Ravn 2004

Use, modification and distribution are subject to the Boost Software License, Version 1.0.
(See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
---
name: BVLC AlexNet Model
caffemodel: bvlc_alexnet.caffemodel
caffemodel_url: http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel
license: unrestricted
sha1: 9116a64c0fbe4459d18f4bb6b56d647b63920377
caffe_commit: 709dc15af4a06bebda027c1eb2b3f3e3375d5077
---

This model is a replication of the model described in the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) publication.

Differences:
- not training with the relighting data-augmentation;
- initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss).

The bundled model is the iteration 360,000 snapshot.
The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948.
This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop.
(Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.)

This model was trained by Evan Shelhamer @shelhamer

## License

This model is released for unrestricted use.
---
name: BVLC GoogleNet Model
caffemodel: bvlc_googlenet.caffemodel
caffemodel_url: http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel
license: unrestricted
sha1: 405fc5acd08a3bb12de8ee5e23a96bec22f08204
caffe_commit: bc614d1bd91896e3faceaf40b23b72dab47d44f5
gist_id: 866e2aa1fd707b89b913
---

This model is a replication of the model described in the [GoogleNet](http://arxiv.org/abs/1409.4842) publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model.

Differences:
- not training with the relighting data-augmentation;
- not training with the scale or aspect-ratio data-augmentation;
- uses "xavier" to initialize the weights instead of "gaussian";
- quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs);

The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt

This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop.
(Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.)

Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c:
 - Average Forward pass: 562.841 ms.
 - Average Backward pass: 1123.84 ms.
 - Average Forward-Backward: 1688.8 ms.

This model was trained by Sergio Guadarrama @sguada

## License

This model is released for unrestricted use.
# docker-lemp

## Images
based on official:
- nginx:mainline-alpine
- php:fpm-alpine
- mariadb:10.1

nginx offical repository is often slow with update  
to have the last version, build your own image with a dockerfile  
simply copy the original dockerfile and change the version :D  

in the dokcer-compose.yml, under nginx sections, add:
```
build: .
dockerfile: nginx-1.11.3-alpine.dockerfile
```

## Config
```
cp mariadb.config.env.sample mariadb.config.env
```
```
cp php.config.env.sample php.config.env
```
then, change default value ...
# Official Jenkins Docker image

The Jenkins Continuous Integration and Delivery server.

This is a fully functional Jenkins server, based on the Long Term Support release.
[http://jenkins.io/](http://jenkins.io/).

For weekly releases check out [`jenkinsci/jenkins`](https://hub.docker.com/r/jenkinsci/jenkins/)


<img src="http://jenkins-ci.org/sites/default/files/jenkins_logo.png"/>


# Usage

```
docker run -p 8080:8080 -p 50000:50000 jenkins
```

NOTE: read below the _build executors_ part for the role of the `50000` port mapping.

This will store the workspace in /var/jenkins_home. All Jenkins data lives in there - including plugins and configuration.
You will probably want to make that an explicit volume so you can manage it and attach to another container for upgrades :

```
docker run -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home jenkins
```

this will automatically create a 'jenkins_home' volume on docker host, that will survive container stop/restart/deletion. 

Avoid using a bind mount from a folder on host into `/var/jenkins_home`, as this might result in file permission issue. If you _really_ need to bind mount jenkins_home, ensure that directory on host is accessible by the jenkins user in container (jenkins user - uid 1000) or use `-u some_other_user` parameter with `docker run`.

## Backing up data

If you bind mount in a volume - you can simply back up that directory
(which is jenkins_home) at any time.

This is highly recommended. Treat the jenkins_home directory as you would a database - in Docker you would generally put a database on a volume.

If your volume is inside a container - you can use ```docker cp $ID:/var/jenkins_home``` command to extract the data, or other options to find where the volume data is.
Note that some symlinks on some OSes may be converted to copies (this can confuse jenkins with lastStableBuild links etc)

For more info check Docker docs section on [Managing data in containers](https://docs.docker.com/engine/tutorials/dockervolumes/)

# Setting the number of executors

You can specify and set the number of executors of your Jenkins master instance using a groovy script. By default its set to 2 executors, but you can extend the image and change it to your desired number of executors :

`executors.groovy`
```
import jenkins.model.*
Jenkins.instance.setNumExecutors(5)
```

and `Dockerfile`

```
FROM jenkins
COPY executors.groovy /usr/share/jenkins/ref/init.groovy.d/executors.groovy
```


# Attaching build executors

You can run builds on the master out of the box.

But if you want to attach build slave servers **through JNLP (Java Web Start)**: make sure you map the port: ```-p 50000:50000``` - which will be used when you connect a slave agent.

If you are only using [SSH slaves](https://wiki.jenkins-ci.org/display/JENKINS/SSH+Slaves+plugin), then you do **NOT** need to put that port mapping.

# Passing JVM parameters

You might need to customize the JVM running Jenkins, typically to pass system properties or tweak heap memory settings. Use JAVA_OPTS environment
variable for this purpose :

```
docker run --name myjenkins -p 8080:8080 -p 50000:50000 --env JAVA_OPTS=-Dhudson.footerURL=http://mycompany.com jenkins
```

# Configuring logging

Jenkins logging can be configured through a properties file and `java.util.logging.config.file` Java property.
For example:

```
mkdir data
cat > data/log.properties <<EOF
handlers=java.util.logging.ConsoleHandler
jenkins.level=FINEST
java.util.logging.ConsoleHandler.level=FINEST
EOF
docker run --name myjenkins -p 8080:8080 -p 50000:50000 --env JAVA_OPTS="-Djava.util.logging.config.file=/var/jenkins_home/log.properties" -v `pwd`/data:/var/jenkins_home jenkins
```

# Configuring reverse proxy
If you want to install Jenkins behind a reverse proxy with prefix, example: mysite.com/jenkins, you need to add environnement variable `JENKINS_OPTS="--prefix=/jenkins"` and then follow the below procedures to configure your reverse proxy, which will depend if you have Apache ou Nginx:
- [Apache](https://wiki.jenkins-ci.org/display/JENKINS/Running+Jenkins+behind+Apache)
- [Nginx](https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+behind+an+NGinX+reverse+proxy)

# Passing Jenkins launcher parameters

Argument you pass to docker running the jenkins image are passed to jenkins launcher, so you can run for sample :
```
docker run jenkins --version
```
This will dump Jenkins version, just like when you run jenkins as an executable war.

You also can define jenkins arguments as `JENKINS_OPTS`. This is usefull to define a set of arguments to pass to jenkins launcher as you
define a derived jenkins image based on the official one with some customized settings. The following sample Dockerfile uses this option
to force use of HTTPS with a certificate included in the image

```
FROM jenkins:1.565.3

COPY https.pem /var/lib/jenkins/cert
COPY https.key /var/lib/jenkins/pk
ENV JENKINS_OPTS --httpPort=-1 --httpsPort=8083 --httpsCertificate=/var/lib/jenkins/cert --httpsPrivateKey=/var/lib/jenkins/pk
EXPOSE 8083
```

You can also change the default slave agent port for jenkins by defining `JENKINS_SLAVE_AGENT_PORT` in a sample Dockerfile.

```
FROM jenkins:1.565.3
ENV JENKINS_SLAVE_AGENT_PORT 50001
```
or as a parameter to docker,
```
docker run --name myjenkins -p 8080:8080 -p 50001:50001 --env JENKINS_SLAVE_AGENT_PORT=50001 jenkins
```

# Installing more tools

You can run your container as root - and install via apt-get, install as part of build steps via jenkins tool installers, or you can create your own Dockerfile to customise, for example:

```
FROM jenkins
# if we want to install via apt
USER root
RUN apt-get update && apt-get install -y ruby make more-thing-here
# drop back to the regular jenkins user - good practice
USER jenkins
```

In such a derived image, you can customize your jenkins instance with hook scripts or additional plugins.
For this purpose, use `/usr/share/jenkins/ref` as a place to define the default JENKINS_HOME content you
wish the target installation to look like :

```
FROM jenkins
COPY custom.groovy /usr/share/jenkins/ref/init.groovy.d/custom.groovy
```

## Preinstalling plugins

You can rely on the `install-plugins.sh` script to pass a set of plugins to download with their dependencies.
Use plugin artifact ID, whithout `-plugin` extension, and append the version if needed separated by `:`.
Dependencies that are already included in the Jenkins war will only be downloaded if their required version is newer than the one included.

```
FROM jenkins
RUN /usr/local/bin/install-plugins.sh docker-slaves github-branch-source:1.8
```

When jenkins container starts, it will check `JENKINS_HOME` has this reference content, and copy them
there if required. It will not override such files, so if you upgraded some plugins from UI they won't
be reverted on next start.

In case you *do* want to override, append '.override' to the name of the reference file. E.g. a file named
`/usr/share/jenkins/ref/config.xml.override` will overwrite an existing `config.xml` file in JENKINS_HOME.

Also see [JENKINS-24986](https://issues.jenkins-ci.org/browse/JENKINS-24986)


Here is an example to get the list of plugins from an existing server:

```
JENKINS_HOST=username:password@myhost.com:port
curl -sSL "http://$JENKINS_HOST/pluginManager/api/xml?depth=1&xpath=/*/*/shortName|/*/*/version&wrapper=plugins" | perl -pe 's/.*?<shortName>([\w-]+).*?<version>([^<]+)()(<\/\w+>)+/\1 \2\n/g'|sed 's/ /:/'
```

Example Output:

```
cucumber-testresult-plugin:0.8.2
pam-auth:1.1
matrix-project:1.4.1
script-security:1.13
...
```

For 2.x-derived images, you may also want to

    RUN echo 2.0 > /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state

to indicate that this Jenkins installation is fully configured.
Otherwise a banner will appear prompting the user to install additional plugins,
which may be inappropriate.

# Upgrading

All the data needed is in the /var/jenkins_home directory - so depending on how you manage that - depends on how you upgrade. Generally - you can copy it out - and then "docker pull" the image again - and you will have the latest LTS - you can then start up with -v pointing to that data (/var/jenkins_home) and everything will be as you left it.

As always - please ensure that you know how to drive docker - especially volume handling!

## Upgrading plugins

By default, plugins will be upgraded if they haven't been upgraded manually and if the version from the docker image is newer than the version in the container. Versions installed by the docker image are tracked through a marker file.

The default behaviour when upgrading from a docker image that didn't write marker files is to leave existing plugins in place. If you want to upgrade existing plugins without marker you may run the docker image with `-e TRY_UPGRADE_IF_NO_MARKER=true`. Then plugins will be upgraded if the version provided by the docker image is newer.

# Building

Build with the usual

    docker build -t jenkins .

Tests are written using [bats](https://github.com/sstephenson/bats) under the `tests` dir

    bats tests

Bats can be easily installed with `brew install bats` on OS X

# Questions?

Jump on irc.freenode.net and the #jenkins room. Ask!
<p align="center">
    <img src="https://s19.postimg.org/jblfytw9f/laradock-logo.jpg" alt="Laradock Logo"/>
</p>

<p align="center">A Docker PHP development environment that facilitates running PHP Apps on Docker</p>

<p align="center">
   <a href="https://travis-ci.org/laradock/laradock"><img src="https://travis-ci.org/laradock/laradock.svg?branch=master" alt="Build status"></a>
   <a href="https://github.com/laradock/laradock/stargazers"><img src="https://img.shields.io/github/stars/laradock/laradock.svg" alt="GitHub stars"></a>
   <a href="https://github.com/laradock/laradock/network"><img src="https://img.shields.io/github/forks/laradock/laradock.svg" alt="GitHub forks"></a>
   <a href="https://github.com/laradock/laradock/issues"><img src="https://img.shields.io/github/issues/laradock/laradock.svg" alt="GitHub issues"></a>
   <a href="https://raw.githubusercontent.com/laradock/laradock/master/LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="GitHub license"></a>
    <a href="http://laradock.io/contributing"><img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat" alt="contributions welcome"></a>
</p>

<h4 align="center" style="color:#7d58c2">Use Docker First And Learn About It Later</h4>

<p align="center">
    <a href="https://zalt.me"><img src="http://forthebadge.com/images/badges/built-by-developers.svg" alt="forthebadge" width="240" ></a>
</p>


---

<p align="center">
	<a href="http://laradock.io">
	   <img src="https://s19.postimg.org/ecnn9vdw3/Screen_Shot_2017-08-01_at_5.08.54_AM.png" width=300px" alt="Laradock Docs"/>
	</a>
</p>


## Credits

- [Mahmoud Zalt](https://github.com/Mahmoudz) @mahmoudz | [Twitter](https://twitter.com/Mahmoud_Zalt) | [Site](http://zalt.me)
- [Bo-Yi Wu](https://github.com/appleboy) @appleboy | [Twitter](https://twitter.com/appleboy)
- [Philippe Trépanier](https://github.com/philtrep) @philtrep
- [Mike Erickson](https://github.com/mikeerickson) @mikeerickson
- [Dwi Fahni Denni](https://github.com/zeroc0d3) @zeroc0d3
- [Thor Erik](https://github.com/thorerik) @thorerik
- [Winfried van Loon](https://github.com/winfried-van-loon) @winfried-van-loon
- [TJ Miller](https://github.com/sixlive) @sixlive
- [Yu-Lung Shao (Allen)](https://github.com/bestlong) @bestlong
- [Milan Urukalo](https://github.com/urukalo) @urukalo
- [Vince Chu](https://github.com/vwchu) @vwchu
- Join Us.

## License

[MIT License](https://github.com/laradock/laradock/blob/master/LICENSE)
# Laradock

[![forthebadge](http://forthebadge.com/images/badges/built-by-developers.svg)](http://zalt.me)

[![Gitter](https://badges.gitter.im/Laradock/laradock.svg)](https://gitter.im/Laradock/laradock?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

Laradock 能够帮你在 **Docker** 上快速搭建 **Laravel** 应用。

就像 Laravel Homestead 一样，但是 Docker 替换了 Vagrant。

> 先在使用 Laradock，然后再学习它们。

## 目录
- [Intro](#Intro)
	- [Features](#features)
	- [Supported Software's](#Supported-Containers)
	- [What is Docker](#what-is-docker)
	- [What is Laravel](#what-is-laravel)
	- [Why Docker not Vagrant](#why-docker-not-vagrant)
	- [Laradock VS Homestead](#laradock-vs-homestead)
- [Demo Video](#Demo)
- [Requirements](#Requirements)
- [Installation](#Installation)
- [Usage](#Usage)
- [Documentation](#Documentation)
	- [Docker](#Docker)
		- [List current running Containers](#List-current-running-Containers)
		- [Close all running Containers](#Close-all-running-Containers)
		- [Delete all existing Containers](#Delete-all-existing-Containers)
		- [Enter a Container (SSH into a running Container)](#Enter-Container)
		- [Edit default container configuration](#Edit-Container)
		- [Edit a Docker Image](#Edit-a-Docker-Image)
		- [Build/Re-build Containers](#Build-Re-build-Containers)
		- [Add more Software's (Docker Images)](#Add-Docker-Images)
		- [View the Log files](#View-the-Log-files)
	- [Laravel](#Laravel):
		- [Install Laravel from a Docker Container](#Install-Laravel)
		- [Run Artisan Commands](#Run-Artisan-Commands)
		- [Use Redis](#Use-Redis)
		- [Use Mongo](#Use-Mongo)
	- [PHP](#PHP)
		- [Install PHP Extensions](#Install-PHP-Extensions)
		- [Change the PHP-FPM Version](#Change-the-PHP-FPM-Version)
		- [Change the PHP-CLI Version](#Change-the-PHP-CLI-Version)
		- [Install xDebug](#Install-xDebug)
	- [Misc](#Misc)
		- [Use custom Domain](#Use-custom-Domain)
		- [Enable Global Composer Build Install](#Enable-Global-Composer-Build-Install)
		- [Install Prestissimo](#Install-Prestissimo)
		- [Install Node + NVM](#Install-Node)
		- [Debugging](#debugging)
		- [Upgrading Laradock](#upgrading-laradock)
- [Help & Questions](#Help)


<a name="Intro"></a>
## 介绍

Laradock 努力简化创建开发环境过程。
它包含预包装 Docker 镜像，提供你一个美妙的开发环境而不需要安装 PHP, NGINX, MySQL 和其他任何软件在你本地机器上。

**使用概览：**

让我们了解使用它安装 `NGINX`, `PHP`, `Composer`, `MySQL` 和 `Redis`，然后运行 `Laravel`

1. 将 Laradock 放到你的 Laravel 项目中：
```bash
git clone https://github.com/laradock/laradock.git
```

2. 进入 Laradock 目录
 ```bash
cp env-example .env
```

3. 运行这些容器。
```bash
docker-compose up -d nginx mysql redis
```

4. 打开你的Laravel 项目的 `.env` 文件，然后设置 `mysql` 的 `DB_HOST` 和  `redis` 的`REDIS_HOST`。

5. 打开浏览器，访问 localhost：

<a name="features"></a>
### 特点

- 在 PHP 版本：7.0，5.6.5.5...之中可以简单切换。
- 可选择你最喜欢的数据库引擎，比如：MySQL, Postgres, MariaDB...
- 可运行自己的软件组合，比如：Memcached, HHVM, Beanstalkd...
- 所有软件运行在不同的容器之中，比如：PHP-FPM, NGINX, PHP-CLI...
- 通过简单的编写 `Dockerfile` 容易定制任何容器。
- 所有镜像继承自一个官方基础镜像（Trusted base Images）
- 可预配置Laravel的Nginx环境
- 容易应用容器中的配置 配置文件（`Dockerfile`）
- 最新的 Docker Compose 版本（`docker-compose`）
- 所有的都是可视化和可编辑的
- 快速的镜像构建
- 每周都会有更新...

<a name="Supported-Containers"></a>
### 支持的软件 (容器)

- **数据库引擎:**
	- MySQL
	- PostgreSQL
	- MariaDB
	- MongoDB
	- Neo4j
- **缓存引擎:**
	- Redis
	- Memcached
- **PHP 服务器:**
	- NGINX
	- Apache2
	- Caddy
- **PHP 编译工具:**
	- PHP-FPM
	- HHVM
- **消息队列系统:**
	- Beanstalkd (+ Beanstalkd Console)
- **工具:**
	- Workspace (PHP7-CLI, Composer, Git, Node, Gulp, SQLite, Vim, Nano, cURL...)

>如果你找不到你需要的软件，构建它然后把它添加到这个列表。你的贡献是受欢迎的。

<a name="what-is-docker"></a>
### Docker 是什么?

[Docker](https://www.docker.com) 是一个开源项目,自动化部署应用程序软件的容器,在 Linux, Mac OS and Windows 提供一个额外的抽象层和自动化的[操作系统级的虚拟化](https://en.wikipedia.org/wiki/Operating-system-level_virtualization)

<a name="what-is-laravel"></a>
### Laravel 是什么?

额，这很认真的!!!

<a name="why-docker-not-vagrant"></a>
### 为什么使用 Docker 而不是 Vagrant!?

[Vagrant](https://www.vagrantup.com) 构建虚拟机需要几分钟然而 Docker 构建虚拟容器只需要几秒钟。
而不是提供一个完整的虚拟机,就像你用 Vagrant, Docker 为您提供**轻量级**虚拟容器,共享相同的内核和允许安全执行独立的进程。

除了速度, Docker 提供大量的 Vagrant 无法实现的功能。

最重要的是 Docker 可以运行在开发和生产(相同环境无处不在)。Vagrant 是专为开发,(所以在生产环境你必须每一次重建您的服务器)。

<a name="laradock-vs-homestead"></a>
### Laradock Homestead 对比

Laradock and [Homestead](https://laravel.com/docs/master/homestead) 给你一个完整的虚拟开发环境。(不需要安装和配置软件在你自己的每一个操作系统)。

Homestead 是一个工具,为你控制虚拟机(使用 Homestead 特殊命令)。Vagrant 可以管理你的管理虚容器。

运行一个虚拟容器比运行一整个虚拟机快多了 **Laradock 比 Homestead 快多了**

<a name="Demo"></a>
## 演示视频
还有什么比**演示视频**好：

- Laradock [v4.0](https://www.youtube.com/watch?v=TQii1jDa96Y)
- Laradock [v2.2](https://www.youtube.com/watch?v=-DamFMczwDA)
- Laradock [v0.3](https://www.youtube.com/watch?v=jGkyO6Is_aI)
- Laradock [v0.1](https://www.youtube.com/watch?v=3YQsHe6oF80)

<a name="Requirements"></a>
## 依赖

- [Git](https://git-scm.com/downloads)       
- [Docker](https://www.docker.com/products/docker/)

<a name="Installation"></a>
## 安装

1 - 克隆 `Laradock` 仓库:

**A)** 如果你已经有一个 Laravel 项目,克隆这个仓库在到 `Laravel` 根目录

```bash
git submodule add https://github.com/laradock/laradock.git
```

>如果你不是使用 Git 管理 Laravel 项目,您可以使用 `git clone` 而不是 `git submodule`。

**B)** 如果你没有一个 Laravel 项目,你想 Docker 安装 Laravel,克隆这个源在您的机器任何地方上:

```bash
git clone https://github.com/laradock/laradock.git
```

<a name="Usage"></a>
## 使用

**请在开始之前阅读:**
如果你正在使用 **Docker Toolbox** (VM)，选择以下任何一个方法：
- 更新到 Docker [Native](https://www.docker.com/products/docker) Mac/Windows 版本 (建议). 查看 [Upgrading Laradock](#upgrading-laradock)
- 使用 Laradock v3.* (访问 `Laradock-ToolBox` [分支](https://github.com/laradock/laradock/tree/Laradock-ToolBox)).
如果您使用的是 **Docker Native**(Mac / Windows 版本)甚至是 Linux 版本,通常可以继续阅读这个文档，Laradock v4 以上版本将仅支持 **Docker Native**。

1 - 运行容器: *(在运行 `docker-compose` 命令之前，确认你在 `laradock` 目录中*

**例子:** 运行 NGINX 和 MySQL:

```bash
docker-compose up -d  nginx mysql
```
你可以从以下列表选择你自己的容器组合：

`nginx`, `hhvm`, `php-fpm`, `mysql`, `redis`, `postgres`, `mariadb`, `neo4j`, `mongo`, `apache2`, `caddy`, `memcached`, `beanstalkd`, `beanstalkd-console`, `workspace`.

**说明**: `workspace` 和 `php-fpm` 将运行在大部分实例中, 所以不需要在 `up` 命令中加上它们.

2 - 进入 Workspace 容器, 执行像 (Artisan, Composer, PHPUnit, Gulp, ...)等命令

```bash
docker-compose exec workspace bash
```

增加 `--user=laradock` (例如 `docker-compose exec --user=laradock workspace bash`) 作为您的主机的用户创建的文件. (你可以从 `docker-compose.yml`修改 PUID (User id) 和 PGID (group id) 值 ).

3 - 编辑 Laravel 的配置.

如果你还没有安装 Laravel 项目，请查看 [How to Install Laravel in a Docker Container](#Install-Laravel).

打开 Laravel 的 `.env` 文件 然后 配置 你的 `mysql` 的 `DB_HOST`:

```env
DB_HOST=mysql
```

4 - 打开浏览器访问 localhost (`http://localhost/`).

**调试**: 如果你碰到任何问题，请查看 [调试](#debugging) 章节
如果你需要特别支持，请联系我，更多细节在[帮助 & 问题](#Help)章节

<a name="Documentation"></a>
## 文档

<a name="Docker"></a>
### [Docker]

<a name="List-current-running-Containers"></a>
### 列出正在运行的容器
```bash
docker ps
```

你也可以使用以下命令查看某项目的容器
```bash
docker-compose ps
```

<a name="Close-all-running-Containers"></a>
### 关闭所有容器
```bash
docker-compose stop
```

停止某个容器:

```bash
docker-compose stop {容器名称}
```

<a name="Delete-all-existing-Containers"></a>
### 删除所有容器
```bash
docker-compose down
```

小心这个命令,因为它也会删除你的数据容器。(如果你想保留你的数据你应该在上述命令后列出容器名称删除每个容器本身):*

<a name="Enter-Container"></a>
### 进入容器 (通过 SSH 进入一个运行中的容器)

1 - 首先使用 `docker ps` 命令查看正在运行的容器

2 - 进入某个容器使用:

```bash
docker-compose exec {container-name} bash
```

*例如: 进入 MySQL 容器*

```bash
docker-compose exec mysql bash
```

3 - 退出容器, 键入 `exit`.


<a name="Edit-Container"></a>
### 编辑默认容器配置
打开 `docker-compose.yml` 然后 按照你想的修改.

例如:

修改 MySQL 数据库名称:

```yml
  environment:
    MYSQL_DATABASE: laradock
```

修改 Redis 默认端口为 1111:

```yml
  ports:
    - "1111:6379"
```

<a name="Edit-a-Docker-Image"></a>
### 编辑 Docker 镜像

1 - 找到你想修改的镜像的 `Dockerfile` ,
<br>
例如： `mysql` 在 `mysql/Dockerfile`.

2 - 按你所要的编辑文件.

3 - 重新构建容器:

```bash
docker-compose build mysql
```

更多信息在容器重建中[点击这里](#Build-Re-build-Containers).

<a name="Build-Re-build-Containers"></a>
### 建立/重建容器

如果你做任何改变 `Dockerfile` 确保你运行这个命令,可以让所有修改更改生效:

```bash
docker-compose build
```

选择你可以指定哪个容器重建(而不是重建所有的容器):

```bash
docker-compose build {container-name}
```

如果你想重建整个容器，你可能需要使用 `--no-cache` 选项  (`docker-compose build --no-cache {container-name}`).

<a name="Add-Docker-Images"></a>
### 增加更多软件 (Docker 镜像)

为了增加镜像（软件）, 编辑 `docker-compose.yml` 添加容器细节， 你需要熟悉 [docker compose 文件语法](https://docs.docker.com/compose/compose-file/).

<a name="View-the-Log-files"></a>
### 查看日志文件
Nginx的日志在 `logs/nginx` 目录

然后查看其它容器日志(MySQL, PHP-FPM,...) 你可以运行:

```bash
docker logs {container-name}
```

<a name="Laravel"></a>
### [Laravel]

<a name="Install-Laravel"></a>
### 从 Docker 镜像安装 Laravel

1 - 首先你需要进入 Workspace 容器.

2 - 安装 Laravel.

例如 使用 Composer

```bash
composer create-project laravel/laravel my-cool-app "5.2.*"
```

> 我们建议使用 `composer create-project` 替换 Laravel 安装器去安装 Laravel.

关于更多 Laravel 安装内容请 [点击这儿](https://laravel.com/docs/master#installing-laravel).


3 - 编辑 `docker-compose.yml` 映射新的应用目录:
系统默认 Laradock 假定 Laravel 应用在 laradock 的父级目录中

更新 Laravel 应用在 `my-cool-app` 目录中, 我们需要用 `../my-cool-app/:/var/www`替换 `../:/var/www` , 如下:

```yaml
    application:
        build: ./application
        volumes:
            - ../my-cool-app/:/var/www
```

4 - 进入目录下继续工作..

```bash
cd my-cool-app
```

5 - 回到 Laradock 安装步骤,看看如何编辑 `.env` 的文件。

<a name="Run-Artisan-Commands"></a>
### 运行 Artisan 命令

你可以从 Workspace 容器运行 artisan 命令和其他终端命令

1 - 确认 Workspace 容器已经运行.

```bash
docker-compose up -d workspace // ..and all your other containers
```

2 - 找到 Workspace 容器名称:

```bash
docker-compose ps
```

3 - 进入 Workspace 容器:

```bash
docker-compose exec workspace bash
```

增加 `--user=laradock` (例如 `docker-compose exec --user=laradock workspace bash`) 作为您的主机的用户创建的文件.

4 - 运行任何你想的 :)

```bash
php artisan
```
```bash
composer update
```
```bash
phpunit
```

<a name="Use-Redis"></a>
### 使用 Redis
1 - 首先务必用 `docker-compose up` 命令运行 (`redis`) 容器.

```bash
docker-compose up -d redis
```

2 - 打开你的Laravel的 `.env` 文件 然后 配置 `redis` 的 `REDIS_HOST`

```env
REDIS_HOST=redis
```

如果在你的 `.env` 文件没有找到 `REDIS_HOST` 变量。打开数据库配置文件 `config/database.php` 然后用 `redis` 替换默认 IP `127.0.0.1`，例如：


```php
'redis' => [
    'cluster' => false,
    'default' => [
        'host'     => 'redis',
        'port'     => 6379,
        'database' => 0,
    ],
],
```

3 - 启用 Redis 缓存或者开启 Session 管理也在 `.env` 文件中用 `redis` 替换默认 `file` 设置 `CACHE_DRIVER` 和 `SESSION_DRIVER`

```env
CACHE_DRIVER=redis
SESSION_DRIVER=redis
```

4 - 最好务必通过 Composer 安装 `predis/predis` 包 `(~1.0)`:

```bash
composer require predis/predis:^1.0
```

5 - 你可以用以下代码在 Laravel 中手动测试：

```php
\Cache::store('redis')->put('Laradock', 'Awesome', 10);
```

<a name="Use-Mongo"></a>
### 使用 Mongo

1 - 首先在 Workspace 和 PHP-FPM 容器中安装 `mongo`:

    a) 打开 `docker-compose.yml` 文件
    b) 在 Workspace 容器中找到 `INSTALL_MONGO` 选项：
    c) 设置为 `true`
    d) 在 PHP-FPM 容器中找到 `INSTALL_MONGO`
    e) 设置为 `true`

相关配置项如下:

```yml
    workspace:
        build:
            context: ./workspace
            args:
                - INSTALL_MONGO=true
    ...
    php-fpm:
        build:
            context: ./php-fpm
            args:
                - INSTALL_MONGO=true
    ...
```

2 - 重建 `Workspace、PHP-FPM` 容器

```bash
docker-compose build workspace php-fpm
```

3 - 使用 `docker-compose up` 命令运行 MongoDB 容器 (`mongo`)

```bash
docker-compose up -d mongo
```

4 - 在 `config/database.php` 文件添加 MongoDB 的配置项:

```php
'connections' => [

    'mongodb' => [
        'driver'   => 'mongodb',
        'host'     => env('DB_HOST', 'localhost'),
        'port'     => env('DB_PORT', 27017),
        'database' => env('DB_DATABASE', 'database'),
        'username' => '',
        'password' => '',
        'options'  => [
            'database' => '',
        ]
    ],

	// ...

],
```

5 - 打开 Laravel 的 `.env` 文件然后更新以下字段:

- 设置 `DB_HOST` 为 `mongo` 的主机 IP.
- 设置 `DB_PORT` 为 `27017`.
- 设置 `DB_DATABASE` 为 `database`.


6 - 最后务必通过 Composer 安装 `jenssegers/mongodb` 包，添加服务提供者（Laravel Service Provider）


```bash
composer require jenssegers/mongodb
```

更多细节内容 [点击这儿](https://github.com/jenssegers/laravel-mongodb#installation).

7 - 测试:

- 首先让你的模型继承 Mongo 的 Eloquent Model. 查看 [文档](https://github.com/jenssegers/laravel-mongodb#eloquent).
- 进入 Workspace 容器.
- 迁移数据库 `php artisan migrate`.

<a name="PHP"></a>
### [PHP]

<a name="Install-PHP-Extensions"></a>
### 安装 PHP 拓展

安装 PHP 扩展之前,你必须决定你是否需要 `FPM` 或 `CLI`,因为他们安装在不同的容器上,如果你需要两者,则必须编辑两个容器。

PHP-FPM 拓展务必安装在 `php-fpm/Dockerfile-XX`. *(用你 PHP 版本号替换 XX)*.

PHP-CLI 拓展应该安装到 `workspace/Dockerfile`.

<a name="Change-the-PHP-FPM-Version"></a>
### 修改 PHP-FPM 版本
默认运行 **PHP-FPM 7.0** 版本.

>PHP-FPM 负责服务你的应用代码,如果你是计划运行您的应用程序在不同 PHP-FPM 版本上，则不需要更改 PHP-CLI 版本。

#### A) 切换版本 PHP `7.0` 到 PHP `5.6`

1 - 打开 `docker-compose.yml`。

2 - 在PHP容器的 `Dockerfile-70` 文件。

3 - 修改版本号, 用 `Dockerfile-56` 替换 `Dockerfile-70` , 例如:

```txt
php-fpm:
    build:
        context: ./php-fpm
        dockerfile: Dockerfile-70
```

4 - 最后重建PHP容器

```bash
docker-compose build php
```

> 更多关于PHP基础镜像, 请访问 [PHP Docker官方镜像](https://hub.docker.com/_/php/).


#### B) 切换版本 PHP `7.0` 或 `5.6` 到 PHP `5.5`
我们已不在本地支持 PHP5.5，但是你按照以下步骤获取：

1 - 克隆 `https://github.com/laradock/php-fpm`.

2 - 重命名 `Dockerfile-56` 为 `Dockerfile-55`.

3 - 编辑文件 `FROM php:5.6-fpm` 为 `FROM php:5.5-fpm`.

4 - 从 `Dockerfile-55` 构建镜像.

5 - 打开 `docker-compose.yml` 文件.

6 - 将 `php-fpm` 指向你的 `Dockerfile-55` 文件.


<a name="Change-the-PHP-CLI-Version"></a>
### 修改 PHP-CLI 版本
默认运行 **PHP-CLI 7.0** 版本

>说明: PHP-CLI 只用于执行 Artisan 和 Composer 命令，不服务于你的应用代码，这是 PHP-FPM 的工作，所以编辑 PHP-CLI 的版本不是很重要。
PHP-CLI 安装在 Workspace 容器，改变 PHP-CLI 版本你需要编辑 `workspace/Dockerfile`.
现在你必须手动修改 PHP-FPM 的 `Dockerfile` 或者创建一个新的。 (可以考虑贡献功能).

<a name="Install-xDebug"></a>
### 安装 xDebug

1 - 首先在 Workspace 和 PHP-FPM 容器安装 `xDebug`:

    a) 打开 `docker-compose.yml` 文件
    b) 在 Workspace 容器中找到 `INSTALL_XDEBUG` 选项
    c) 改为 `true`
    d) 在 PHP-FPM 容器中找到 `INSTALL_XDEBUG ` 选项
    e) 改为 `true`

例如:

```yml
    workspace:
        build:
            context: ./workspace
            args:
                - INSTALL_XDEBUG=true
    ...
    php-fpm:
        build:
            context: ./php-fpm
            args:
                - INSTALL_XDEBUG=true
    ...
```

2 - 重建容器 `docker-compose build workspace php-fpm`

<a name="Misc"></a>
### [Misc]

<a name="Use-custom-Domain"></a>
### 使用自定义域名 (替换 Docker 的 IP)

假定你的自定义域名是 `laravel.dev`

1 - 打开 `/etc/hosts` 文件添加以下内容，映射你的 localhost 地址 `127.0.0.1` 为 `laravel.dev` 域名
```bash
127.0.0.1    laravel.dev
```

2 - 打开你的浏览器访问 `{http://laravel.dev}`

你可以在 nginx 配置文件自定义服务器名称,如下:

```conf
server_name laravel.dev;
```

<a name="Enable-Global-Composer-Build-Install"></a>
### 安装全局 Composer 命令

为启用全局 Composer Install 在容器构建中允许你安装 composer 的依赖，然后构建完成后就是可用的。

1 - 打开 `docker-compose.yml` 文件

2 - 在 Workspace 容器找到 `COMPOSER_GLOBAL_INSTALL` 选项并设置为 `true`

例如:

```yml
    workspace:
        build:
            context: ./workspace
            args:
                - COMPOSER_GLOBAL_INSTALL=true
    ...
```
3 - 现在特价你的依赖关系到 `workspace/composer.json`

4 - 重建 Workspace 容器 `docker-compose build workspace`

<a name="Install-Prestissimo"></a>
### 安装 Prestissimo

[Prestissimo](https://github.com/hirak/prestissimo) 是一个平行安装功能的 composer 插件。

1 - 在安装期间，使全局 Composer Install 正在运行:

    点击这个 [启用全局 Composer 构建安装](#Enable-Global-Composer-Build-Install) 然后继续步骤1、2.

2 - 添加 prestissimo 依赖到 Composer:

a - 现在打开 `workspace/composer.json` 文件

b - 添加 `"hirak/prestissimo": "^0.3"` 依赖

c - 重建 Workspace 容器 `docker-compose build workspace`


<a name="Install-Node"></a>
### 安装 Node + NVM

在 Workspace 容器安装 NVM 和 NodeJS

1 - 打开 `docker-compose.yml` 文件

2 - 在 Workspace 容器找到 `INSTALL_NODE` 选项设为 `true`

例如:

```yml
    workspace:
        build:
            context: ./workspace
            args:
                - INSTALL_NODE=true
    ...
```

3 - 重建容器 `docker-compose build workspace`

<a name="debugging"></a>
### Debugging

*这里是你可能面临的常见问题列表,以及可能的解决方案.*

#### 看到空白页而不是 Laravel 的欢迎页面!

在 Laravel 根目录，运行下列命令:

```bash
sudo chmod -R 777 storage bootstrap/cache
```

#### 看到 "Welcome to nginx" 而不是 Laravel 应用!

在浏览器使用 `http://127.0.0.1` 替换 `http://localhost`.

#### 看到包含 `address already in use` 的错误

确保你想运行的服务端口(80, 3306, etc.)不是已经被其他程序使用，例如 `apache`/`httpd` 服务或其他安装的开发工具

<a name="upgrading-laradock"></a>
### Laradock 升级


从 Docker Toolbox (VirtualBox) 移动到 Docker Native (for Mac/Windows)，需要从 Laradock v3.* 升级到 v4.*:

1. 停止 Docker 虚拟机 `docker-machine stop {default}`
2. 安装 Docker [Mac](https://docs.docker.com/docker-for-mac/) 或 [Windows](https://docs.docker.com/docker-for-windows/).
3. 升级 Laradock 到 `v4.*.*` (`git pull origin master`)
4. 像之前一样使用 Laradock: `docker-compose up -d nginx mysql`.

**说明:** 如果你面临任何上面的问题的最后一步:重建你所有的容器
```bash
docker-compose build --no-cache
```
"警告：容器数据可能会丢失!"


## 贡献
这个小项目是由一个有一个全职工作和很多的职责的人建立的,所以如果你喜欢这个项目,并且发现它需要一个 bug 修复或支持或新软件或升级任何容器,或其他任何. . 你是非常欢迎，欢迎毫不不犹豫地贡献吧:)

#### 阅读我们的 [贡献说明](https://github.com/laradock/laradock/blob/master/CONTRIBUTING.md)

<a name="Help"></a>
## 帮助 & 问题

从聊天室 [Gitter](https://gitter.im/Laradock/laradock) 社区获取帮助和支持.

你也可以打开 Github 上的 [issue](https://github.com/laradock/laradock/issues) (将被贴上问题和答案) 或与大家讨论 [Gitter](https://gitter.im/Laradock/laradock).

Docker 或 Laravel 的特别帮助，你可以在 [Codementor.io](https://www.codementor.io/mahmoudz) 上直接和项目创始人在线沟通

## 关于作者

**创始人:**

- [Mahmoud Zalt](https://github.com/Mahmoudz)  (Twitter [@Mahmoud_Zalt](https://twitter.com/Mahmoud_Zalt))

**优秀的人:**

- [Contributors](https://github.com/laradock/laradock/graphs/contributors)
- [Supporters](https://github.com/laradock/laradock/issues?utf8=%E2%9C%93&q=)


## 许可证

[MIT License](https://github.com/laradock/laradock/blob/master/LICENSE) (MIT)
# Material Docs

A material design theme for [Hugo](https://gohugo.io).

[![Screenshot](https://raw.githubusercontent.com/digitalcraftsman/hugo-material-docs/master/static/images/screen.png)](https://digitalcraftsman.github.io/hugo-material-docs/)

## Quick start

Install with `git`:


    git clone https://github.com/digitalcraftsman/hugo-material-docs.git themes/hugo-material-docs


Next, take a look in the `exampleSite` folder at. This directory contains an example config file and the content for the demo. It serves as an example setup for your documentation. 

Copy at least the `config.toml` in the root directory of your website. Overwrite the existing config file if necessary. 

Hugo includes a development server, so you can view your changes as you go -
very handy. Spin it up with the following command:

``` sh
hugo server
```

Now you can go to [localhost:1313](http://localhost:1313) and the Material
theme should be visible. For detailed installation instructions visit the [demo](http://themes.gohugo.io/theme/material-docs/).

Noteworthy changes of this theme are listed in the [changelog](https://github.com/digitalcraftsman/hugo-material-docs/blob/master/CHANGELOG.md).

## Acknowledgements

A big thank you to [Martin Donath](https://github.com/squidfunk). He created the original [Material theme](https://github.com/squidfunk/mkdocs-material) for Hugo's companion [MkDocs](http://www.mkdocs.org/). This port wouldn't be possible without him.

Furthermore, thanks to [Steve Francia](https://gihub.com/spf13) for creating Hugo and the [awesome community](https://github.com/spf13/hugo/graphs/contributors) around the project.

## License

The theme is released under the MIT license. Read the [license](https://github.com/digitalcraftsman/hugo-material-docs/blob/master/LICENSE.md) for more information.

![Dockerfile](https://static.webdevops.io/dockerfile.svg)

Automated built and tested on [WebDevOps Build server](https://build.webdevops.io/) sponsored by [<img src="https://static.webdevops.io/sponsor-infogene.png" width="100">](http://infogene.eu/)

[![Docker layout](documentation/docs/resources/images/docker-image-layout.gv.png)](documentation/docs/resources/images/docker-image-layout.gv.png)

# Documentation

* [Documentation is available on readthedocs](https://dockerfile.readthedocs.io/en/latest/)


Dockerfile                                                | Description                                                                             | Depends on                                                                       |
--------------------------------------------------------- | --------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
[`bootstrap`](docker/base/README.md)                      | Distribution with ansible and some scripts                                              | official docker files                                                            |
[`base`](docker/base/README.md)                           | Base containers for WebDevOps service containers                                        | [`webdevops/bootstrap`](https://hub.docker.com/r/webdevops/bootstrap/)           |
[`base-app`](docker/base-app/README.md)                   | Base containers for WebDevOps application containers                                    | [`webdevops/base`](https://hub.docker.com/r/webdevops/base/)                     |
[`php`](docker/php/README.md)                             | PHP (cli and fpm) service containers                                                    | [`webdevops/base-app`](https://hub.docker.com/r/webdevops/base-app/)             |
[`php-apache`](docker/php-apache/README.md)               | PHP (cli and fpm) with Apache service containers                                        | [`webdevops/php`](https://hub.docker.com/r/webdevops/php/)                       |
[`php-nginx`](docker/php-nginx/README.md)                 | PHP (cli and fpm) with Nginx service containers                                         | [`webdevops/php`](https://hub.docker.com/r/webdevops/php/)                       |
[`hhvm`](docker/hhvm/README.md)                           | HHVM (cli and fcgi) service containers                                                  | [`webdevops/base-app`](https://hub.docker.com/r/webdevops/base-app/)             |
[`hhvm-apache`](docker/hhvm-apache/README.md)             | HHVM (cli and fcgi) with Apache service containers                                      | [`webdevops/hhvm`](https://hub.docker.com/r/webdevops/hhvm/)                     |
[`hhvm-nginx`](docker/hhvm-nginx/README.md)               | HHVM (cli and fcgi) with Nginx service containers                                       | [`webdevops/hhvm`](https://hub.docker.com/r/webdevops/hhvm/)                     |
[`vsftp`](docker/vsftp/README.md)                         | VSFTP (ftp service) service container                                                   | [`webdevops/base:latest`](https://hub.docker.com/r/webdevops/base/)              |
[`storage`](docker/storage/latest/README.md)              | Storage (noop) container                                                                | [`webdevops/base:latest`](https://hub.docker.com/r/webdevops/base/)              |
[`ssh`](docker/ssh/README.md)                             | SSH service container                                                                   | [`webdevops/base:latest`](https://hub.docker.com/r/webdevops/base/)              |
[`postfix`](docker/postfix/README.md)                     | Postfix service container                                                               | [`webdevops/base:latest`](https://hub.docker.com/r/webdevops/base/)              |
[`mail-sandbox`](docker/mail-sandbox/README.md)           | Mail catcher service container (catches all mails via SMTP and are accessable via IMAP) | [`webdevops/postfix:latest`](https://hub.docker.com/r/webdevops/postfix/)        |
[`samson-deployment`](docker/samson-deployment/README.md) | [Samson](https://github.com/webdevops/samson-deployment) based deployment service       | [`zendesk/samson`](https://hub.docker.com/r/zendesk/samson/)                     |
[`sphinx`](docker/sphinx/latest/README.md)                | Sphinx container                                                                        | [`webdevops/bootstrap:alpine-3`](https://hub.docker.com/r/webdevops/bootstrap/)  |
[`varnish`](docker/varnish/latest/README.md)              | Varnish container                                                                       | [`webdevops/base:alpine-3`](https://hub.docker.com/r/webdevops/bootstrap/)       |

# Building

Local building of containers can be done with `make` and `Makefile`:

Command                     | Description                                                                       
--------------------------- | ----------------------------------------------------------------------------------
`sudo make setup`           | To Install dependancies of build chain tools 
`make all`                  | Build all containers *fast mode* (parallel building, `FAST=1`)
`FAST=0 make all`           | Build all containers *slow mode* (serial building)
`DEBUG=1 make all`          | Show log of build process even if process is successfull
`FORCE=1 make all`          | Force container build (`docker build --no-cache ...`)
`WHITELIST="alpine-3 centos-7" make all`          | Build all container with tag alpine-3 or centos-7
<br>                        |
`make baselayout`           | Build and deploy baselayout.tar
`make provision`            | Deploy all configuration files from [_provisioning/](_provisioning/README.md)
`make dist-update`          | Update local distrubtion images (CentOS, Debian, Ubuntu)
`make full`                 | Run provision and build all images
<br>                        |
`make test`                 | Run testsuite (use currently available docker images on your docker host)
`make test-hub-images`      | Run testsuite but pull newest docker images from docker hub first
<br>                        |
`make push`                 | Run tests and rebuild them (use cache) and push them to docker hub
`make publish`              | Run `dist-update`, `all` with FORCE and `push`
<br>                        |
`make base`                 | Build all base containers
`make service`              | Build all service containers
`make php`                  | Build all php containers
`make hhvm`                 | Build all hhvm containers
`make nginx`                | Build all nginx containers
`make apache`               | Build all apache containers
`make webdevops/php-nginx`  | Build specific containers (as example)

# Provisioning

All `base` inherited containers provides an modular provisioning available as simple shell scripts and ansible roles.
See [docker/base/README.md](docker/base/README.md) for more informations.

The configuration and provisioning files are build from [_provisioning/](_provisioning/README.md) to get a consistent
configuraiton for all containers. This also should reduce copy&paste errors because the configuration will be deployed
automatically into containers on build process.

# Docker provisioning files

These are the base configuration files which will be deployed into the Dockerfile directories to simplify and 
minimize overhead for multiple docker containers (eg. multiple ubuntu and debian versions).
 
These files will be deployed based on [.bin/provision.sh](../.bin/provision.sh) rules. This script will just copy these
files into the configured (and filtered) docker directories.
# Ansible Docker container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Containers with Ansible pre-installed (see `webdevops/bootstrap`).
Ansible is installed via `pip` and each container will always contain the latest version of Ansible.

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/ansible:latest            | [![](https://badge.imagelayers.io/webdevops/ansible:latest.svg)](https://imagelayers.io/?images=webdevops/ansible:latest 'Get your own badge on imagelayers.io')
webdevops/ansible:ubuntu-12.04      | [![](https://badge.imagelayers.io/webdevops/ansible:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/ansible:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/ansible:ubuntu-14.04      | [![](https://badge.imagelayers.io/webdevops/ansible:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/ansible:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/ansible:ubuntu-15.04      | [![](https://badge.imagelayers.io/webdevops/ansible:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/ansible:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/ansible:ubuntu-15.10      | [![](https://badge.imagelayers.io/webdevops/ansible:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/ansible:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/ansible:centos-7          | [![](https://badge.imagelayers.io/webdevops/ansible:centos-7.svg)](https://imagelayers.io/?images=webdevops/ansible:centos-7 'Get your own badge on imagelayers.io')
webdevops/ansible:debian-7          | [![](https://badge.imagelayers.io/webdevops/ansible:debian-7.svg)](https://imagelayers.io/?images=webdevops/ansible:debian-7 'Get your own badge on imagelayers.io')
webdevops/ansible:debian-8          | [![](https://badge.imagelayers.io/webdevops/ansible:debian-8.svg)](https://imagelayers.io/?images=webdevops/ansible:debian-8 'Get your own badge on imagelayers.io')
# Samson deployment container layout

[Zendesk](https://www.zendesk.com/) Deployment web ui with Ansible Ansistrano, Capistrano and PHP Deployer support

[Samson Boilerplate is available at GitHub.](https://github.com/webdevops/samson-deployment)

Installed packages:
* [Samson deployment web ui](https://github.com/zendesk/samson)
* [Ansible](https://www.ansible.com/) with [Ansistrano](https://github.com/ansistrano)
* [Capistrano](http://capistranorb.com/)
* [PHP Deployer](http://deployer.org/)
* [Magallanes](http://magephp.com/)
* git
* rsync
* docker & docker-compose (as client)
* gulp, grunt, bower
* PHP cli & [composer](https://getcomposer.org/)
# PHP with Apache container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                                | Distribution name        | PHP Version                                                               
---------------------------------------- | -------------------------|---------------
`webdevops/php-apache:ubuntu-12.04`      | precise                  | PHP 5.3
`webdevops/php-apache:ubuntu-14.04`      | trusty (LTS)             | PHP 5.5
`webdevops/php-apache:ubuntu-15.04`      | vivid                    | PHP 5.6
`webdevops/php-apache:ubuntu-15.10`      | wily                     | PHP 5.6
`webdevops/php-apache:ubuntu-16.04`      | xenial (LTS)             | PHP 7.0
`webdevops/php-apache:debian-7`          | wheezy                   | PHP 5.4
`webdevops/php-apache:debian-8`          | jessie                   | PHP 5.6
`webdevops/php-apache:debian-8-php7`     | jessie with dotdeb       | PHP 7.x (via dotdeb)
`webdevops/php-apache:centos-7`          |                          | PHP 5.4

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd`         | Apache configuration
`/opt/docker/etc/httpd/ssl`     | Apache ssl configuration with example server.crt, server.csr, server.key

File                                                | Description
--------------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd/main.conf`                   | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/httpd/global.conf`                 | Global apache configuration options
`/opt/docker/etc/httpd/php.conf`                    | PHP configuration (connection to FPM)
`/opt/docker/etc/httpd/vhost.common.conf`           | Vhost common stuff (placeholder)
`/opt/docker/etc/httpd/vhost.conf`                  | Default vhost
`/opt/docker/etc/httpd/vhost.ssl.conf`              | Default ssl configuration for vhost
`/opt/docker/etc/php/fpm/php-fpm.conf`              | PHP FPM daemon configuration
`/opt/docker/etc/php/fpm/pool.d/application.conf`   | PHP FPM pool configuration

## Environment variables

Variable              | Description
--------------------- |  ------------------------------------------------------------------------------
`CLI_SCRIPT`          | Predefined CLI script for service
`APPLICATION_UID`     | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`     | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`   | Document root for Nginx
`WEB_DOCUMENT_INDEX`  | Document index (eg. `index.php`) for Nginx
`WEB_ALIAS_DOMAIN`    | Alias domains (eg. `*.vm`) for Nginx


## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/php-apache:latest         | [![](https://badge.imagelayers.io/webdevops/php-apache:latest.svg)](https://imagelayers.io/?images=webdevops/php-apache:latest 'Get your own badge on imagelayers.io')
webdevops/php-apache:ubuntu-14.04   | [![](https://badge.imagelayers.io/webdevops/php-apache:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/php-apache:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/php-apache:ubuntu-15.04   | [![](https://badge.imagelayers.io/webdevops/php-apache:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/php-apache:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/php-apache:ubuntu-15.10   | [![](https://badge.imagelayers.io/webdevops/php-apache:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/php-apache:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/php-apache:centos-7       | [![](https://badge.imagelayers.io/webdevops/php-apache:centos-7.svg)](https://imagelayers.io/?images=webdevops/php-apache:centos-7 'Get your own badge on imagelayers.io')
webdevops/php-apache:debian-8-php7  | [![](https://badge.imagelayers.io/webdevops/php-apache:debian-8-php-apache7.svg)](https://imagelayers.io/?images=webdevops/php-apache:debian-8-php-apache7 'Get your own badge on imagelayers.io')
webdevops/php-apache:debian-8       | [![](https://badge.imagelayers.io/webdevops/php-apache:debian-8.svg)](https://imagelayers.io/?images=webdevops/php-apache:debian-8 'Get your own badge on imagelayers.io')
webdevops/php-apache:debian-7       | [![](https://badge.imagelayers.io/webdevops/php-apache:debian-7.svg)](https://imagelayers.io/?images=webdevops/php-apache:debian-7 'Get your own badge on imagelayers.io')
# Storage container layout

_nothing special_

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/storage:latest            | [![](https://badge.imagelayers.io/webdevops/storage:latest.svg)](https://imagelayers.io/?images=webdevops/storage:latest 'Get your own badge on imagelayers.io')
# Nginx webserver Docker container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                           | Distribution name                                                                 
----------------------------------- | -------------------
`webdevops/nginx:ubuntu-12.04`      | precise            
`webdevops/nginx:ubuntu-14.04`      | trusty (LTS)       
`webdevops/nginx:ubuntu-15.04`      | vivid              
`webdevops/nginx:ubuntu-15.10`      | wily               
`webdevops/nginx:debian-7`          | wheezy            
`webdevops/nginx:debian-8`          | jessie            
`webdevops/nginx:centos-7`          |                    


## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`           | Predefined CLI script for service
`APPLICATION_UID`      | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`      | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`    | Document root for Nginx
`WEB_DOCUMENT_INDEX`   | Document index (eg. `index.php`) for Nginx
`WEB_ALIAS_DOMAIN`     | Alias domains (eg. `*.vm`) for Nginx

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx`         | Nginx configuration
`/opt/docker/etc/nginx/ssl`     | Nginx ssl configuration with example server.crt, server.csr, server.key

File                                          | Description
--------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx/main.conf`             | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/nginx/global.conf`           | Global nginx configuration options
`/opt/docker/etc/nginx/conf.d/*.conf`         | Global apache configuration directory (will be included)
`/opt/docker/etc/nginx/php.conf`              | PHP configuration (connection to FPM)
`/opt/docker/etc/httpd/vhost.common.d/*.conf` | Vhost common directory (will be included)
`/opt/docker/etc/nginx/vhost.conf`            | Default vhost
`/opt/docker/etc/nginx/vhost.ssl.conf`        | Default ssl configuration for vhost


## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/nginx:latest              | [![](https://badge.imagelayers.io/webdevops/nginx:latest.svg)](https://imagelayers.io/?images=webdevops/nginx:latest 'Get your own badge on imagelayers.io')
webdevops/nginx:ubuntu-14.04        | [![](https://badge.imagelayers.io/webdevops/nginx:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/nginx:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/nginx:ubuntu-15.04        | [![](https://badge.imagelayers.io/webdevops/nginx:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/nginx:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/nginx:ubuntu-15.10        | [![](https://badge.imagelayers.io/webdevops/nginx:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/nginx:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/nginx:centos-7            | [![](https://badge.imagelayers.io/webdevops/nginx:centos-7.svg)](https://imagelayers.io/?images=webdevops/nginx:centos-7 'Get your own badge on imagelayers.io')
webdevops/nginx:debian-7            | [![](https://badge.imagelayers.io/webdevops/nginx:debian-7.svg)](https://imagelayers.io/?images=webdevops/nginx:debian-7 'Get your own badge on imagelayers.io')
webdevops/nginx:debian-8            | [![](https://badge.imagelayers.io/webdevops/nginx:debian-8.svg)](https://imagelayers.io/?images=webdevops/nginx:debian-8 'Get your own badge on imagelayers.io')
# Apache webserver Docker container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`           | Predefined CLI script for service
`APPLICATION_UID`      | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`      | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`    | Document root for Apache HTTPD
`WEB_DOCUMENT_INDEX`   | Document index (eg. `index.php`) for Apache HTTPD
`WEB_ALIAS_DOMAIN`     | Alias domains (eg. `*.vm`) for Apache HTTPD

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd`         | Apache configuration
`/opt/docker/etc/httpd/ssl`     | Apache ssl configuration with example server.crt, server.csr, server.key

File                                          | Description
--------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd/main.conf`             | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/httpd/global.conf`           | Global apache configuration options
`/opt/docker/etc/httpd/conf.d/*.conf`         | Global apache configuration directory (will be included)
`/opt/docker/etc/httpd/php.conf`              | PHP configuration (connection to FPM)
`/opt/docker/etc/httpd/vhost.common.d/*.conf` | Vhost common directory (will be included)
`/opt/docker/etc/httpd/vhost.conf`            | Default vhost
`/opt/docker/etc/httpd/vhost.ssl.conf`        | Default ssl configuration for vhost


## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/apache:latest             | [![](https://badge.imagelayers.io/webdevops/apache:latest.svg)](https://imagelayers.io/?images=webdevops/apache:latest 'Get your own badge on imagelayers.io')
webdevops/apache:ubuntu-14.04       | [![](https://badge.imagelayers.io/webdevops/apache:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/apache:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/apache:ubuntu-15.04       | [![](https://badge.imagelayers.io/webdevops/apache:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/apache:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/apache:ubuntu-15.10       | [![](https://badge.imagelayers.io/webdevops/apache:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/apache:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/apache:centos-7           | [![](https://badge.imagelayers.io/webdevops/apache:centos-7.svg)](https://imagelayers.io/?images=webdevops/apache:centos-7 'Get your own badge on imagelayers.io')
webdevops/apache:debian-7           | [![](https://badge.imagelayers.io/webdevops/apache:debian-7.svg)](https://imagelayers.io/?images=webdevops/apache:debian-7 'Get your own badge on imagelayers.io')
webdevops/apache:debian-8           | [![](https://badge.imagelayers.io/webdevops/apache:debian-8.svg)](https://imagelayers.io/?images=webdevops/apache:debian-8 'Get your own badge on imagelayers.io')
# Base container layout

## Containers
Container                           | Distribution name                                                                 
----------------------------------- | -------------------------
`webdevops/base:ubuntu-12.04`       | precise                   
`webdevops/base:ubuntu-14.04`       | trusty (LTS)             
`webdevops/base:ubuntu-15.04`       | vivid                    
`webdevops/base:ubuntu-15.10`       | wily                     
`webdevops/base:debian-7`           | wheezy                   
`webdevops/base:debian-8`           | jessie                   
`webdevops/base:centos-7`           |                          

## Environment variables

Variable            | Description
------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`        | Predefined CLI script for service
`APPLICATION_UID`   | Application UID (Effective user ID)
`APPLICATION_GID`   | Application GID (Effective group ID)

## Filesystem layout

The whole docker directroy is deployed into `/opt/docker/`.


Directory                            | Description
------------------------------------ | ------------------------------------------------------------------------------
`/opt/docker/bin`                    | Script directory for various script eg. `entrypoint.sh`
`/opt/docker/bin/entrypoint.d`       | Entrypoint scripts
`/opt/docker/bin/service.d`          | Service (wrapper) scripts for supervisord
<br>                                 |
`/opt/docker/etc`                    | Configuration directory
`/opt/docker/etc/supervisor.d`       | Supervisor service configuration `*.conf` directory
<br>                                 |
`/opt/docker/provision`              | Ansible provisioning configuration directory
`/opt/docker/provision/roles`        | Ansible roles configuration directory
`/opt/docker/provision/bootstrap.d`  | Directory for bash `*.sh` scripts which will automatcally run by `bootstrrap.sh` (will be removed after run, for usage in `Dockerfile`)
`/opt/docker/provision/entrypoint.d` | Directory for bash `*.sh` scripts which will automatcally run by `entrypoint.sh`
`/opt/docker/provision/onbuild.d`    | Directory for bash `*.sh` scripts which will automatcally run by `onbuild` (`bootstrap.sh onbuild` must be called for execution with ONBUILD RUN) 
 

File                                         | Description
-------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/bin/config.sh`                  | Config for `entrypoint.sh` and other scripts (eg. `/opt/docker/bin/entrypoint.d`). All bash functions/variables can be used in custom scripts.
`/opt/docker/bin/entrypoint.sh`              | Main entrypoint for docker container
`/opt/docker/bin/logwatch.sh`                | Log reader for childen processes (can be used with named pipes)
`/opt/docker/bin/provision.sh`               | Ansible provision wrapper script
`/opt/docker/bin/control.sh`                 | Control script for container and provisioning registration handling
<br>                                         |
`/opt/docker/etc/supervisor.conf`            | Main supervisor configuration (will include other scripts in `/opt/docker/etc/supervisor.d/*.conf`)
`/opt/docker/etc/supervisor.d/cron.conf`     | Cron service script _(disabled by default)_
`/opt/docker/etc/supervisor.d/ssh.conf`      | SSH server service script _(disabled by default)_


## Ansible provisioning

Whole configuration will deployed in `/opt/docker/provision`.

Available tags:
- bootstrap (only run once)
- entrypoint (run at startup)

If there is no `playbook.yml` it will be created dynamically based on registred roles by `control.sh`.
`bootstrap` roles will only run once (at docker build) and not again on inherited containers.
`entrypoint` roles will run at each startup also on inherited containers.

To use the modular ansible provisioning you have to deploy your own role into `/opt/docker/provision/roles`, eg.:

Directory: `/opt/docker/provision/roles/yourrolename/`
Main task file: `/opt/docker/provision/roles/yourrolename/tasks/main.yml`

To register your role execute following script in your `Dockerfile`:

For `bootstrap` and `entrypoint` tag:
`RUN bash /opt/docker/bin/control.sh provision.role.bootstrap yourrolename`

For only `bootstrap` tag:
`RUN bash /opt/docker/bin/control.sh provision.role.bootstrap.bootstrap yourrolename`

For only `entrypoint` tag:
`RUN bash /opt/docker/bin/control.sh provision.role.bootstrap.entrypoint yourrolename`

## `entrypoint.sh`

CMD             | Description
--------------- | ------------------------------------------------------------------------------
supervisord     | Start supervisor and configured services
noop            | Endless noop loop (endless sleep)
root            | Root shell (external usage)
cli             | Run predefined `CLI_SCRIPT` (env variable) as `EFFECTIVE_USER` if defined
all other       | Run defined command as `EFFECTIVE_USER` if defined

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/base:latest               | [![](https://badge.imagelayers.io/webdevops/base:latest.svg)](https://imagelayers.io/?images=webdevops/base:latest 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-12.04         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-14.04         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-15.04         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-15.10         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/base:centos-7             | [![](https://badge.imagelayers.io/webdevops/base:centos-7.svg)](https://imagelayers.io/?images=webdevops/base:centos-7 'Get your own badge on imagelayers.io')
webdevops/base:debian-7             | [![](https://badge.imagelayers.io/webdevops/base:debian-7.svg)](https://imagelayers.io/?images=webdevops/base:debian-7 'Get your own badge on imagelayers.io')
webdevops/base:debian-8             | [![](https://badge.imagelayers.io/webdevops/base:debian-8.svg)](https://imagelayers.io/?images=webdevops/base:debian-8 'Get your own badge on imagelayers.io')
# TYPO3 container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Based on `webdevops/php-apache:ubuntu-14.04` with automatic TYPO3 installer

Install location is `/app/`.

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/typo3:latest              | [![](https://badge.imagelayers.io/webdevops/typo3:latest.svg)](https://imagelayers.io/?images=webdevops/typo3:latest 'Get your own badge on imagelayers.io')
# Nginx webserver Docker container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                           | Distribution name                                                                 
----------------------------------- | -------------------
`webdevops/nginx:ubuntu-12.04`      | precise            
`webdevops/nginx:ubuntu-14.04`      | trusty (LTS)       
`webdevops/nginx:ubuntu-15.04`      | vivid              
`webdevops/nginx:ubuntu-15.10`      | wily               
`webdevops/nginx:debian-7`          | wheezy            
`webdevops/nginx:debian-8`          | jessie            
`webdevops/nginx:centos-7`          |                    


## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`           | Predefined CLI script for service
`APPLICATION_UID`      | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`      | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`    | Document root for Nginx
`WEB_DOCUMENT_INDEX`   | Document index (eg. `index.php`) for Nginx
`WEB_ALIAS_DOMAIN`     | Alias domains (eg. `*.vm`) for Nginx

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx`         | Nginx configuration
`/opt/docker/etc/nginx/ssl`     | Nginx ssl configuration with example server.crt, server.csr, server.key

File                                          | Description
--------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx/main.conf`             | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/nginx/global.conf`           | Global nginx configuration options
`/opt/docker/etc/nginx/conf.d/*.conf`         | Global apache configuration directory (will be included)
`/opt/docker/etc/nginx/php.conf`              | PHP configuration (connection to FPM)
`/opt/docker/etc/httpd/vhost.common.d/*.conf` | Vhost common directory (will be included)
`/opt/docker/etc/nginx/vhost.conf`            | Default vhost
`/opt/docker/etc/nginx/vhost.ssl.conf`        | Default ssl configuration for vhost


## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/nginx:latest              | [![](https://badge.imagelayers.io/webdevops/nginx:latest.svg)](https://imagelayers.io/?images=webdevops/nginx:latest 'Get your own badge on imagelayers.io')
webdevops/nginx:ubuntu-14.04        | [![](https://badge.imagelayers.io/webdevops/nginx:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/nginx:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/nginx:ubuntu-15.04        | [![](https://badge.imagelayers.io/webdevops/nginx:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/nginx:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/nginx:ubuntu-15.10        | [![](https://badge.imagelayers.io/webdevops/nginx:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/nginx:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/nginx:centos-7            | [![](https://badge.imagelayers.io/webdevops/nginx:centos-7.svg)](https://imagelayers.io/?images=webdevops/nginx:centos-7 'Get your own badge on imagelayers.io')
webdevops/nginx:debian-7            | [![](https://badge.imagelayers.io/webdevops/nginx:debian-7.svg)](https://imagelayers.io/?images=webdevops/nginx:debian-7 'Get your own badge on imagelayers.io')
webdevops/nginx:debian-8            | [![](https://badge.imagelayers.io/webdevops/nginx:debian-8.svg)](https://imagelayers.io/?images=webdevops/nginx:debian-8 'Get your own badge on imagelayers.io')
# PHP with Apache container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                                | Distribution name        | PHP Version                                                               
---------------------------------------- | -------------------------|---------------
`webdevops/php-apache:ubuntu-12.04`      | precise                  | PHP 5.3
`webdevops/php-apache:ubuntu-14.04`      | trusty (LTS)             | PHP 5.5
`webdevops/php-apache:ubuntu-15.04`      | vivid                    | PHP 5.6
`webdevops/php-apache:ubuntu-15.10`      | wily                     | PHP 5.6
`webdevops/php-apache:ubuntu-16.04`      | xenial (LTS)             | PHP 7.0
`webdevops/php-apache:debian-7`          | wheezy                   | PHP 5.4
`webdevops/php-apache:debian-8`          | jessie                   | PHP 5.6
`webdevops/php-apache:debian-8-php7`     | jessie with dotdeb       | PHP 7.x (via dotdeb)
`webdevops/php-apache:centos-7`          |                          | PHP 5.4

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd`         | Apache configuration
`/opt/docker/etc/httpd/ssl`     | Apache ssl configuration with example server.crt, server.csr, server.key

File                                                | Description
--------------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd/main.conf`                   | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/httpd/global.conf`                 | Global apache configuration options
`/opt/docker/etc/httpd/php.conf`                    | PHP configuration (connection to FPM)
`/opt/docker/etc/httpd/vhost.common.conf`           | Vhost common stuff (placeholder)
`/opt/docker/etc/httpd/vhost.conf`                  | Default vhost
`/opt/docker/etc/httpd/vhost.ssl.conf`              | Default ssl configuration for vhost
`/opt/docker/etc/php/fpm/php-fpm.conf`              | PHP FPM daemon configuration
`/opt/docker/etc/php/fpm/pool.d/application.conf`   | PHP FPM pool configuration

## Environment variables

Variable              | Description
--------------------- |  ------------------------------------------------------------------------------
`CLI_SCRIPT`          | Predefined CLI script for service
`APPLICATION_UID`     | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`     | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`   | Document root for Apache
`WEB_DOCUMENT_INDEX`  | Document index (eg. `index.php`) for Apache
`WEB_ALIAS_DOMAIN`    | Alias domains (eg. `*.vm`) for Apache
`PHP_DEBUGGER`        | Either `xdebug`, `blackfire` or `none`. Default is `xdebug`.

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/php-apache:latest         | [![](https://badge.imagelayers.io/webdevops/php-apache:latest.svg)](https://imagelayers.io/?images=webdevops/php-apache:latest 'Get your own badge on imagelayers.io')
webdevops/php-apache:ubuntu-14.04   | [![](https://badge.imagelayers.io/webdevops/php-apache:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/php-apache:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/php-apache:ubuntu-15.04   | [![](https://badge.imagelayers.io/webdevops/php-apache:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/php-apache:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/php-apache:ubuntu-15.10   | [![](https://badge.imagelayers.io/webdevops/php-apache:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/php-apache:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/php-apache:centos-7       | [![](https://badge.imagelayers.io/webdevops/php-apache:centos-7.svg)](https://imagelayers.io/?images=webdevops/php-apache:centos-7 'Get your own badge on imagelayers.io')
webdevops/php-apache:debian-8-php7  | [![](https://badge.imagelayers.io/webdevops/php-apache:debian-8-php-apache7.svg)](https://imagelayers.io/?images=webdevops/php-apache:debian-8-php-apache7 'Get your own badge on imagelayers.io')
webdevops/php-apache:debian-8       | [![](https://badge.imagelayers.io/webdevops/php-apache:debian-8.svg)](https://imagelayers.io/?images=webdevops/php-apache:debian-8 'Get your own badge on imagelayers.io')
webdevops/php-apache:debian-7       | [![](https://badge.imagelayers.io/webdevops/php-apache:debian-7.svg)](https://imagelayers.io/?images=webdevops/php-apache:debian-7 'Get your own badge on imagelayers.io')
# SSH container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)


_nothing special_


## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/ssh:latest                | [![](https://badge.imagelayers.io/webdevops/ssh:latest.svg)](https://imagelayers.io/?images=webdevops/ssh:latest 'Get your own badge on imagelayers.io')
# PHP tools container for static analyse
Container                                 | Distribution name        | PHP Version                                                               
----------------------------------------- | -------------------------|---------------
`webdevops/php-audit:debian-7`            | wheezy                   | PHP 5.4
`webdevops/php-audit:debian-8`            | jessie                   | PHP 5.6
`webdevops/php-audit:debian-8-php7`       | jessie with dotdeb       | PHP 7.x (via dotdeb)
`webdevops/php-audit:debian-9`            |                          | PHP 5.x
`webdevops/php-audit:debian-9-php7`       |                          | PHP 7.x
`webdevops/php-audit:centos-7`            |                          | PHP 5.4
`webdevops/php-audit:centos-7-php56`      |                          | PHP 5.6 (via webtatic)
`webdevops/php-audit:centos-7-php7`       |                          | PHP 7.0 (via webtatic)
`webdevops/php-audit:alpine-3`            |                          | PHP 5.6 

## Available tools

- [PHPLoc](https://github.com/sebastianbergmann/phploc) 
- [PHP Depend](https://pdepend.org/) 
- [PHPUnit](https://phpunit.de/) 
- [PHP Mess Detector](https://phpmd.org/) 
- [PHP CodeSniffer](https://github.com/squizlabs/PHP_CodeSniffer) 
- [PHP Copy/Paste Detector](https://github.com/sebastianbergmann/phpcpd) 
- [PHP Dead Code Detector](https://github.com/sebastianbergmann/phpdcd) 
- [PHP Coding Standards Fixer](http://cs.sensiolabs.org/) 
- [SensioLabs DeprecationDetector](https://github.com/sensiolabs-de/deprecation-detector)
- [PHP 7 Compatibility Checker](https://github.com/sstalle/php7cc)

## Environment variables

Variable              | Description
--------------------- |  ------------------------------------------------------------------------------
`PHP_DEBUGGER`        | Either `xdebug`, `blackfire` or `none`. Default is `xdebug`.
# PHP with Nginx container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                               | Distribution name        | PHP Version
--------------------------------------- | ------------------------ | --------------
`webdevops/php-nginx:ubuntu-12.04`      | precise                  | PHP 5.3
`webdevops/php-nginx:ubuntu-14.04`      | trusty (LTS)             | PHP 5.5
`webdevops/php-nginx:ubuntu-15.04`      | vivid                    | PHP 5.6
`webdevops/php-nginx:ubuntu-15.10`      | wily                     | PHP 5.6
`webdevops/php-nginx:ubuntu-16.04`      | xenial (LTS)             | PHP 7.0
`webdevops/php-nginx:debian-7`          | wheezy                   | PHP 5.4
`webdevops/php-nginx:debian-8`          | jessie                   | PHP 5.6
`webdevops/php-nginx:debian-8-php7`     | jessie with dotdeb       | PHP 7.x (via dotdeb)
`webdevops/php-nginx:centos-7`          |                          | PHP 5.4


## Environment variables

Variable              | Description
--------------------- |  ------------------------------------------------------------------------------
`CLI_SCRIPT`          | Predefined CLI script for service
`APPLICATION_UID`     | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`     | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`   | Document root for Nginx
`WEB_DOCUMENT_INDEX`  | Document index (eg. `index.php`) for Nginx
`WEB_ALIAS_DOMAIN`    | Alias domains (eg. `*.vm`) for Nginx
`PHP_DEBUGGER`        | Either `xdebug`, `blackfire` or `none`. Default is `xdebug`.

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx`         | Nginx configuration
`/opt/docker/etc/nginx/ssl`     | Nginx ssl configuration with example server.crt, server.csr, server.key

File                                                | Description
--------------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx/main.conf`                   | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/nginx/global.conf`                 | Global nginx configuration options
`/opt/docker/etc/nginx/php.conf`                    | PHP configuration (connection to FPM)
`/opt/docker/etc/nginx/vhost.common.conf`           | Vhost common stuff (placeholder)
`/opt/docker/etc/nginx/vhost.conf`                  | Default vhost
`/opt/docker/etc/nginx/vhost.ssl.conf`              | Default ssl configuration for vhost
`/opt/docker/etc/php/fpm/php-fpm.conf`              | PHP FPM daemon configuration
`/opt/docker/etc/php/fpm/pool.d/application.conf`   | PHP FPM pool configuration

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/php-nginx:latest          | [![](https://badge.imagelayers.io/webdevops/php-nginx:latest.svg)](https://imagelayers.io/?images=webdevops/php-nginx:latest 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-12.04    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-14.04    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-15.04    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-15.10    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/php-nginx:centos-7        | [![](https://badge.imagelayers.io/webdevops/php-nginx:centos-7.svg)](https://imagelayers.io/?images=webdevops/php-nginx:centos-7 'Get your own badge on imagelayers.io')
webdevops/php-nginx:debian-8-php7   | [![](https://badge.imagelayers.io/webdevops/php-nginx:debian-8-php-apache7.svg)](https://imagelayers.io/?images=webdevops/php-nginx:debian-8-php-apache7 'Get your own badge on imagelayers.io')
webdevops/php-nginx:debian-8        | [![](https://badge.imagelayers.io/webdevops/php-nginx:debian-8.svg)](https://imagelayers.io/?images=webdevops/php-nginx:debian-8 'Get your own badge on imagelayers.io')
webdevops/php-nginx:debian-7        | [![](https://badge.imagelayers.io/webdevops/php-nginx:debian-7.svg)](https://imagelayers.io/?images=webdevops/php-nginx:debian-7 'Get your own badge on imagelayers.io')
# varnish container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Filesystem layout

The whole docker directroy is deployed into `/opt/docker/`.

File                                       | Description
------------------------------------------ | ------------------------------------------------------------------------------
`/opt/docker/bin/entrypoint.d/varnishd.sh` | Entrypoint cmd file for starting varnishd
`/opt/docker/etc/varnish/varnish.vcl`      | Default varnish configuration file (with `VARNISH_BACKEND_HOST` and `VARNISH_BACKEND_PORT` markers)


## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`VARNISH_PORT`         | Listening port of varnish
`VARNISH_CONFIG`       | Path to custom varnish configuration file (must be uploaded to image)
`VARNISH_STORAGE`      | Storage setting (default: `malloc,128m`)
`VARNISH_OPTS`         | Extra varnishd options
`VARNISH_BACKEND_HOST` | Host of backend server 
`VARNISH_BACKEND_PORT` | Port of backend server (default: `80`)

# Postfix container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Environment variables

Variable             | Description
-------------------- | ------------------------------------------------------------------------------
`POSTFIX_MYNETWORKS` | Postfix mynetwork setting
`POSTFIX_RELAYHOST`  | Postfix relayhost setting

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/postfix:latest            | [![](https://badge.imagelayers.io/webdevops/postfix:latest.svg)](https://imagelayers.io/?images=webdevops/postfix:latest 'Get your own badge on imagelayers.io')
# Piwik container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Based on `webdevops/php-nginx:ubuntu-14.04` with automatic Piwik installer

Install location is `/app/piwik`, crontask is automatically configured.

## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`PIWIK_URL`            | URL of piwik installation (requried for crontask)

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/piwik:latest              | [![](https://badge.imagelayers.io/webdevops/piwik:latest.svg)](https://imagelayers.io/?images=webdevops/piwik:latest 'Get your own badge on imagelayers.io')
# Postfix container layout

## Environment variables

Variable             | Description
-------------------- | ------------------------------------------------------------------------------
`POSTFIX_MYNETWORKS` | Postfix mynetwork setting
`POSTFIX_RELAYHOST`  | Postfix relayhost setting

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/postfix:latest            | [![](https://badge.imagelayers.io/webdevops/postfix:latest.svg)](https://imagelayers.io/?images=webdevops/postfix:latest 'Get your own badge on imagelayers.io')

## Example usage

Running a sphinx-autobuild server for Live preview.

```bash
docker run -t -i --rm -p 8080:8000 -v <yourDocsDirectory>:/opt/docs webdevops/sphinx sphinx-autobuild -H 0.0.0.0 /opt/docs html
```
# Bootstrap container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Based on official distribution containers with installed Ansible packages (via pip) and all locales.

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/bootstrap:latest          | [![](https://badge.imagelayers.io/webdevops/bootstrap:latest.svg)](https://imagelayers.io/?images=webdevops/bootstrap:latest 'Get your own badge on imagelayers.io')
webdevops/bootstrap:ubuntu-12.04    | [![](https://badge.imagelayers.io/webdevops/bootstrap:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/bootstrap:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/bootstrap:ubuntu-14.04    | [![](https://badge.imagelayers.io/webdevops/bootstrap:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/bootstrap:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/bootstrap:ubuntu-15.04    | [![](https://badge.imagelayers.io/webdevops/bootstrap:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/bootstrap:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/bootstrap:ubuntu-15.10    | [![](https://badge.imagelayers.io/webdevops/bootstrap:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/bootstrap:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/bootstrap:centos-7        | [![](https://badge.imagelayers.io/webdevops/bootstrap:centos-7.svg)](https://imagelayers.io/?images=webdevops/bootstrap:centos-7 'Get your own badge on imagelayers.io')
webdevops/bootstrap:debian-7        | [![](https://badge.imagelayers.io/webdevops/bootstrap:debian-7.svg)](https://imagelayers.io/?images=webdevops/bootstrap:debian-7 'Get your own badge on imagelayers.io')
webdevops/bootstrap:debian-8        | [![](https://badge.imagelayers.io/webdevops/bootstrap:debian-8.svg)](https://imagelayers.io/?images=webdevops/bootstrap:debian-8 'Get your own badge on imagelayers.io')
# Certbot container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`CERTBOT_EMAIL`        | Email of sysadmin
`CERTBOT_DOMAIN`       | Registered dns or public ip

## USAGE

To create or renew existing certificate
```bash
docker run -ti --rm \
           -v /etc/letsencrypt:/etc/letsencrypt \
           -v /your/document_root:/var/www \
        webdevops/certbot /usr/bin/certbot certonly \
	    --agree-tos \
	    --webroot \
	    -w /var/www
	    -d webdevops.io \
	    -m "webmaster@webdevops.io"
```
See [commandline options](https://certbot.eff.org/docs/using.html#command-line-options)

## Template a cronjob to reissue the certificate

Create a file **/etc/cron.monthly/reissue**
```bash
#!/bin/sh
set -euo pipefail
# Certificate reissue

docker run -ti --rm \
           -v /etc/letsencrypt:/etc/letsencrypt \
           -v /your/document_root:/var/www \
       webdevops/certbot /usr/bin/certbot renew

 ```
make file executable : chmod +x /etc/cron.monthly/reissue

see [Renewal](https://certbot.eff.org/docs/using.html#renewal)# Base container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Containers
Container                           | Distribution name                                                                 
----------------------------------- | -------------------------
`webdevops/base:ubuntu-12.04`       | precise                   
`webdevops/base:ubuntu-14.04`       | trusty (LTS)             
`webdevops/base:ubuntu-15.04`       | vivid                    
`webdevops/base:ubuntu-15.10`       | wily                     
`webdevops/base:debian-7`           | wheezy                   
`webdevops/base:debian-8`           | jessie                   
`webdevops/base:centos-7`           |                          

## Environment variables

Variable            | Description
------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`        | Predefined CLI script for service
`APPLICATION_UID`   | Application UID (Effective user ID)
`APPLICATION_GID`   | Application GID (Effective group ID)

## Filesystem layout

The whole docker directroy is deployed into `/opt/docker/`.


Directory                            | Description
------------------------------------ | ------------------------------------------------------------------------------
`/opt/docker/bin`                    | Script directory for various script eg. `entrypoint.sh`
`/opt/docker/bin/entrypoint.d`       | Entrypoint scripts
`/opt/docker/bin/service.d`          | Service (wrapper) scripts for supervisord
<br>                                 |
`/opt/docker/etc`                    | Configuration directory
`/opt/docker/etc/supervisor.d`       | Supervisor service configuration `*.conf` directory
<br>                                 |
`/opt/docker/provision`              | Ansible provisioning configuration directory
`/opt/docker/provision/roles`        | Ansible roles configuration directory
`/opt/docker/provision/bootstrap.d`  | Directory for bash `*.sh` scripts which will automatcally run by `bootstrrap.sh` (will be removed after run, for usage in `Dockerfile`)
`/opt/docker/provision/entrypoint.d` | Directory for bash `*.sh` scripts which will automatcally run by `entrypoint.sh`
`/opt/docker/provision/onbuild.d`    | Directory for bash `*.sh` scripts which will automatcally run by `onbuild` (`bootstrap.sh onbuild` must be called for execution with ONBUILD RUN) 
 

File                                         | Description
-------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/bin/config.sh`                  | Config for `entrypoint.sh` and other scripts (eg. `/opt/docker/bin/entrypoint.d`). All bash functions/variables can be used in custom scripts.
`/opt/docker/bin/entrypoint.sh`              | Main entrypoint for docker container
`/opt/docker/bin/logwatch.sh`                | Log reader for childen processes (can be used with named pipes)
`/opt/docker/bin/provision.sh`               | Ansible provision wrapper script
`/opt/docker/bin/control.sh`                 | Control script for container and provisioning registration handling
<br>                                         |
`/opt/docker/etc/supervisor.conf`            | Main supervisor configuration (will include other scripts in `/opt/docker/etc/supervisor.d/*.conf`)
`/opt/docker/etc/supervisor.d/cron.conf`     | Cron service script _(disabled by default)_
`/opt/docker/etc/supervisor.d/ssh.conf`      | SSH server service script _(disabled by default)_


## Ansible provisioning

Whole configuration will deployed in `/opt/docker/provision`.

Available tags:
- bootstrap (only run once)
- entrypoint (run at startup)

If there is no `playbook.yml` it will be created dynamically based on registred roles by `control.sh`.
`bootstrap` roles will only run once (at docker build) and not again on inherited containers.
`entrypoint` roles will run at each startup also on inherited containers.

To use the modular ansible provisioning you have to deploy your own role into `/opt/docker/provision/roles`, eg.:

Directory: `/opt/docker/provision/roles/yourrolename/`
Main task file: `/opt/docker/provision/roles/yourrolename/tasks/main.yml`

To register your role execute following script in your `Dockerfile`:

For `bootstrap` and `entrypoint` tag:
`RUN bash /opt/docker/bin/control.sh provision.role yourrolename`

For only `bootstrap` tag:
`RUN bash /opt/docker/bin/control.sh provision.role.bootstrap yourrolename`

For only `entrypoint` tag:
`RUN bash /opt/docker/bin/control.sh provision.role.entrypoint yourrolename`

## `entrypoint.sh`

CMD             | Description
--------------- | ------------------------------------------------------------------------------
supervisord     | Start supervisor and configured services
noop            | Endless noop loop (endless sleep)
root            | Root shell (external usage)
cli             | Run predefined `CLI_SCRIPT` (env variable) as `EFFECTIVE_USER` if defined
all other       | Run defined command as `EFFECTIVE_USER` if defined

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/base:latest               | [![](https://badge.imagelayers.io/webdevops/base:latest.svg)](https://imagelayers.io/?images=webdevops/base:latest 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-12.04         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-14.04         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-15.04         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/base:ubuntu-15.10         | [![](https://badge.imagelayers.io/webdevops/base:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/base:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/base:centos-7             | [![](https://badge.imagelayers.io/webdevops/base:centos-7.svg)](https://imagelayers.io/?images=webdevops/base:centos-7 'Get your own badge on imagelayers.io')
webdevops/base:debian-7             | [![](https://badge.imagelayers.io/webdevops/base:debian-7.svg)](https://imagelayers.io/?images=webdevops/base:debian-7 'Get your own badge on imagelayers.io')
webdevops/base:debian-8             | [![](https://badge.imagelayers.io/webdevops/base:debian-8.svg)](https://imagelayers.io/?images=webdevops/base:debian-8 'Get your own badge on imagelayers.io')
# Mail sandbox container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Mail sandbox which catches all mails and delivers them to a local user and is accessable via IMAP and via Web (roundcube)

## Environment variables

Variable             | Description
-------------------- | ------------------------------------------------------------------------------
`MAILBOX_USERNAME`   | Username for mailbox (Default `dev`)
`MAILBOX_PASSWORD`   | Password for mailbox (Default `dev`)

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/mail-sandbox:latest       | [![](https://badge.imagelayers.io/webdevops/mail-sandbox:latest.svg)](https://imagelayers.io/?images=webdevops/mail-sandbox:latest 'Get your own badge on imagelayers.io')
# Apache webserver Docker container

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Environment variables

Variable               | Description
---------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`           | Predefined CLI script for service
`APPLICATION_UID`      | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`      | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`    | Document root for Apache HTTPD
`WEB_DOCUMENT_INDEX`   | Document index (eg. `index.php`) for Apache HTTPD
`WEB_ALIAS_DOMAIN`     | Alias domains (eg. `*.vm`) for Apache HTTPD

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd`         | Apache configuration
`/opt/docker/etc/httpd/ssl`     | Apache ssl configuration with example server.crt, server.csr, server.key

File                                          | Description
--------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/httpd/main.conf`             | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/httpd/global.conf`           | Global apache configuration options
`/opt/docker/etc/httpd/conf.d/*.conf`         | Global apache configuration directory (will be included)
`/opt/docker/etc/httpd/php.conf`              | PHP configuration (connection to FPM)
`/opt/docker/etc/httpd/vhost.common.d/*.conf` | Vhost common directory (will be included)
`/opt/docker/etc/httpd/vhost.conf`            | Default vhost
`/opt/docker/etc/httpd/vhost.ssl.conf`        | Default ssl configuration for vhost


## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/apache:latest             | [![](https://badge.imagelayers.io/webdevops/apache:latest.svg)](https://imagelayers.io/?images=webdevops/apache:latest 'Get your own badge on imagelayers.io')
webdevops/apache:ubuntu-14.04       | [![](https://badge.imagelayers.io/webdevops/apache:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/apache:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/apache:ubuntu-15.04       | [![](https://badge.imagelayers.io/webdevops/apache:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/apache:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/apache:ubuntu-15.10       | [![](https://badge.imagelayers.io/webdevops/apache:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/apache:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/apache:centos-7           | [![](https://badge.imagelayers.io/webdevops/apache:centos-7.svg)](https://imagelayers.io/?images=webdevops/apache:centos-7 'Get your own badge on imagelayers.io')
webdevops/apache:debian-7           | [![](https://badge.imagelayers.io/webdevops/apache:debian-7.svg)](https://imagelayers.io/?images=webdevops/apache:debian-7 'Get your own badge on imagelayers.io')
webdevops/apache:debian-8           | [![](https://badge.imagelayers.io/webdevops/apache:debian-8.svg)](https://imagelayers.io/?images=webdevops/apache:debian-8 'Get your own badge on imagelayers.io')
# PHP container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                           | Distribution name        | PHP Version                                                               
----------------------------------- | -----------------------------------------
`webdevops/php:ubuntu-12.04`        | precise                  | PHP 5.3
`webdevops/php:ubuntu-14.04`        | trusty (LTS)             | PHP 5.5
`webdevops/php:ubuntu-15.04`        | vivid                    | PHP 5.6
`webdevops/php:ubuntu-15.10`        | wily                     | PHP 5.6
`webdevops/php:ubuntu-16.04`        | xenial (LTS)             | PHP 7.0
`webdevops/php:debian-7`            | wheezy                   | PHP 5.4
`webdevops/php:debian-8`            | jessie                   | PHP 5.6
`webdevops/php:debian-8-php7`       | jessie with dotdeb       | PHP 7.x (via dotdeb)
`webdevops/php:centos-7`            |                          | PHP 5.4


## Filesystem layout

The whole docker directroy is deployed into `/opt/docker/`.

File                                                   | Description
------------------------------------------------------ | ------------------------------------------------------------------------------
`/opt/docker/etc/php/fpm/php-fpm.conf`                 | FPM daemon configuration
`/opt/docker/etc/php/fpm/pool.d/application.conf`      | FPM pool configuration


## Environment variables

Variable            | Description
------------------- | ------------------------------------------------------------------------------
`CLI_SCRIPT`        | Predefined CLI script for service
`APPLICATION_UID`   | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`   | PHP-FPM GID (Effective group ID)

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/php:latest                | [![](https://badge.imagelayers.io/webdevops/php:latest.svg)](https://imagelayers.io/?images=webdevops/php:latest 'Get your own badge on imagelayers.io')
webdevops/php:ubuntu-12.04          | [![](https://badge.imagelayers.io/webdevops/php:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/php:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/php:ubuntu-14.04          | [![](https://badge.imagelayers.io/webdevops/php:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/php:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/php:ubuntu-15.04          | [![](https://badge.imagelayers.io/webdevops/php:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/php:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/php:ubuntu-15.10          | [![](https://badge.imagelayers.io/webdevops/php:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/php:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/php:centos-7              | [![](https://badge.imagelayers.io/webdevops/php:centos-7.svg)](https://imagelayers.io/?images=webdevops/php:centos-7 'Get your own badge on imagelayers.io')
webdevops/php:debian-8-php7         | [![](https://badge.imagelayers.io/webdevops/php:debian-8-php7.svg)](https://imagelayers.io/?images=webdevops/php:debian-8-php7 'Get your own badge on imagelayers.io')
webdevops/php:debian-8              | [![](https://badge.imagelayers.io/webdevops/php:debian-8.svg)](https://imagelayers.io/?images=webdevops/php:debian-8 'Get your own badge on imagelayers.io')
webdevops/php:debian-7              | [![](https://badge.imagelayers.io/webdevops/php:debian-7.svg)](https://imagelayers.io/?images=webdevops/php:debian-7 'Get your own badge on imagelayers.io')
# PHP with Nginx container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

Container                               | Distribution name        | PHP Version
--------------------------------------- | ------------------------ | --------------
`webdevops/php-nginx:ubuntu-12.04`      | precise                  | PHP 5.3
`webdevops/php-nginx:ubuntu-14.04`      | trusty (LTS)             | PHP 5.5
`webdevops/php-nginx:ubuntu-15.04`      | vivid                    | PHP 5.6
`webdevops/php-nginx:ubuntu-15.10`      | wily                     | PHP 5.6
`webdevops/php-nginx:ubuntu-16.04`      | xenial (LTS)             | PHP 7.0
`webdevops/php-nginx:debian-7`          | wheezy                   | PHP 5.4
`webdevops/php-nginx:debian-8`          | jessie                   | PHP 5.6
`webdevops/php-nginx:debian-8-php7`     | jessie with dotdeb       | PHP 7.x (via dotdeb)
`webdevops/php-nginx:centos-7`          |                          | PHP 5.4


## Environment variables

Variable              | Description
--------------------- |  ------------------------------------------------------------------------------
`CLI_SCRIPT`          | Predefined CLI script for service
`APPLICATION_UID`     | PHP-FPM UID (Effective user ID)
`APPLICATION_GID`     | PHP-FPM GID (Effective group ID)
`WEB_DOCUMENT_ROOT`   | Document root for Nginx
`WEB_DOCUMENT_INDEX`  | Document index (eg. `index.php`) for Nginx
`WEB_ALIAS_DOMAIN`    | Alias domains (eg. `*.vm`) for Nginx

## Filesystem layout

Directory                       | Description
------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx`         | Nginx configuration
`/opt/docker/etc/nginx/ssl`     | Nginx ssl configuration with example server.crt, server.csr, server.key

File                                                | Description
--------------------------------------------------- | ------------------------------------------------------------------------------
`/opt/docker/etc/nginx/main.conf`                   | Main include file (will include `global.conf`, `php.conf` and `vhost.conf`) 
`/opt/docker/etc/nginx/global.conf`                 | Global nginx configuration options
`/opt/docker/etc/nginx/php.conf`                    | PHP configuration (connection to FPM)
`/opt/docker/etc/nginx/vhost.common.conf`           | Vhost common stuff (placeholder)
`/opt/docker/etc/nginx/vhost.conf`                  | Default vhost
`/opt/docker/etc/nginx/vhost.ssl.conf`              | Default ssl configuration for vhost
`/opt/docker/etc/php/fpm/php-fpm.conf`              | PHP FPM daemon configuration
`/opt/docker/etc/php/fpm/pool.d/application.conf`   | PHP FPM pool configuration

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/php-nginx:latest          | [![](https://badge.imagelayers.io/webdevops/php-nginx:latest.svg)](https://imagelayers.io/?images=webdevops/php-nginx:latest 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-12.04    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-12.04.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-12.04 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-14.04    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-14.04.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-14.04 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-15.04    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-15.04.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-15.04 'Get your own badge on imagelayers.io')
webdevops/php-nginx:ubuntu-15.10    | [![](https://badge.imagelayers.io/webdevops/php-nginx:ubuntu-15.10.svg)](https://imagelayers.io/?images=webdevops/php-nginx:ubuntu-15.14 'Get your own badge on imagelayers.io')
webdevops/php-nginx:centos-7        | [![](https://badge.imagelayers.io/webdevops/php-nginx:centos-7.svg)](https://imagelayers.io/?images=webdevops/php-nginx:centos-7 'Get your own badge on imagelayers.io')
webdevops/php-nginx:debian-8-php7   | [![](https://badge.imagelayers.io/webdevops/php-nginx:debian-8-php-apache7.svg)](https://imagelayers.io/?images=webdevops/php-nginx:debian-8-php-apache7 'Get your own badge on imagelayers.io')
webdevops/php-nginx:debian-8        | [![](https://badge.imagelayers.io/webdevops/php-nginx:debian-8.svg)](https://imagelayers.io/?images=webdevops/php-nginx:debian-8 'Get your own badge on imagelayers.io')
webdevops/php-nginx:debian-7        | [![](https://badge.imagelayers.io/webdevops/php-nginx:debian-7.svg)](https://imagelayers.io/?images=webdevops/php-nginx:debian-7 'Get your own badge on imagelayers.io')
# FTP container layout

Automated build and tested by [WebDevOps Build Server](https://build.webdevops.io/)

## Environment variables

Variable         | Description
---------------- | ------------------------------------------------------------------------------
`FTP_USER`       | FTP account - username (default: application)
`FTP_PASSWORD`   | FTP account - password (default: application)
`FTP_UID`        | UID of FTP account
`FTP_GID`        | GID of FTP account
`FTP_PATH`       | FTP root path

## Container info

Image                               | Info                                                                       
----------------------------------- | ----------------------------------------------------------------------------------
webdevops/vsftp:latest              | [![](https://badge.imagelayers.io/webdevops/vsftp:latest.svg)](https://imagelayers.io/?images=webdevops/vsftp:latest 'Get your own badge on imagelayers.io')
# Docker ELK stack

[![Join the chat at https://gitter.im/deviantony/docker-elk](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/deviantony/docker-elk?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Run the latest version of the ELK (Elasticsearch, Logstash, Kibana) stack with Docker and Docker-compose.

It will give you the ability to analyze any data set by using the searching/aggregation capabilities of Elasticsearch and the visualization power of Kibana.

Based on the official images:

* [elasticsearch](https://github.com/elastic/elasticsearch-docker)
* [logstash](https://github.com/elastic/logstash-docker)
* [kibana](https://github.com/elastic/kibana-docker)

**Note**: Other branches in this project are available:

* ELK 5 with X-Pack support: https://github.com/deviantony/docker-elk/tree/x-pack
* ELK 5 in Vagrant: https://github.com/deviantony/docker-elk/tree/vagrant
* ELK 5 with Search Guard: https://github.com/deviantony/docker-elk/tree/searchguard

# Requirements

## Setup

1. Install [Docker](http://docker.io).
2. Install [Docker-compose](http://docs.docker.com/compose/install/) **version >= 1.6**.
3. Clone this repository

## Increase `vm.max_map_count` on your host

You need to increase the `vm.max_map_count` kernel setting on your Docker host.
To do this follow the recommended instructions from the Elastic documentation: [Install Elasticsearch with Docker](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-prod-mode)

## SELinux

On distributions which have SELinux enabled out-of-the-box you will need to either re-context the files or set SELinux into Permissive mode in order for docker-elk to start properly.
For example on Redhat and CentOS, the following will apply the proper context:

```bash
$ chcon -R system_u:object_r:admin_home_t:s0 docker-elk/
```

# Usage

Start the ELK stack using *docker-compose*:

```bash
$ docker-compose up
```

You can also choose to run it in background (detached mode):

```bash
$ docker-compose up -d
```

Now that the stack is running, you'll want to inject logs in it. The shipped logstash configuration allows you to send content via tcp:

```bash
$ nc localhost 5000 < /path/to/logfile.log
```

And then access Kibana UI by hitting [http://localhost:5601](http://localhost:5601) with a web browser.

*NOTE*: You'll need to inject data into logstash before being able to configure a logstash index pattern in Kibana. Then all you should have to do is to hit the create button.

Refer to [Connect Kibana with Elasticsearch](https://www.elastic.co/guide/en/kibana/current/connect-to-elasticsearch.html) for detailed instructions about the index pattern configuration.

By default, the stack exposes the following ports:
* 5000: Logstash TCP input.
* 9200: Elasticsearch HTTP
* 9300: Elasticsearch TCP transport
* 5601: Kibana

*WARNING*: If you're using *boot2docker*, you must access it via the *boot2docker* IP address instead of *localhost*.

*WARNING*: If you're using *Docker Toolbox*, you must access it via the *docker-machine* IP address instead of *localhost*.

# Configuration

*NOTE*: Configuration is not dynamically reloaded, you will need to restart the stack after any change in the configuration of a component.

## How can I tune Kibana configuration?

The Kibana default configuration is stored in `kibana/config/kibana.yml`.

## How can I tune Logstash configuration?

The logstash configuration is stored in `logstash/config/logstash.yml`.

It is also possible to map the entire `config` directory inside the container in the `docker-compose.yml`. Update the logstash container declaration to:

```yml
logstash:
  build: logstash/
  volumes:
    - ./logstash/pipeline:/usr/share/logstash/pipeline
    - ./logstash/config:/usr/share/logstash/config
  ports:
    - "5000:5000"
  networks:
    - docker_elk
  depends_on:
    - elasticsearch
```

In the above example the folder `logstash/config` is mapped onto the container `/usr/share/logstash/config` so you can create more than one file in that folder if you'd like to. However, you must be aware that config files will be read from the directory in alphabetical order, and that Logstash will be expecting a [`log4j2.properties`](https://github.com/elastic/logstash-docker/tree/master/build/logstash/config) file for its own logging.

## How can I specify the amount of memory used by Logstash?

The Logstash container use the *LS_HEAP_SIZE* environment variable to determine how much memory should be associated to the JVM heap memory (defaults to 500m).

If you want to override the default configuration, add the *LS_HEAP_SIZE* environment variable to the container in the `docker-compose.yml`:

```yml
logstash:
  build: logstash/
  volumes:
    - ./logstash/pipeline:/usr/share/logstash/pipeline
  ports:
    - "5000:5000"
  networks:
    - docker_elk
  depends_on:
    - elasticsearch
  environment:
    - LS_HEAP_SIZE=2048m
```

## How can I add Logstash plugins? ##

To add plugins to logstash you have to:

1. Add a RUN statement to the `logstash/Dockerfile` (ex. `RUN logstash-plugin install logstash-filter-json`)
2. Add the associated plugin code configuration to the `logstash/pipeline/logstash.conf` file

## How can I enable a remote JMX connection to Logstash?

As for the Java heap memory, another environment variable allows to specify JAVA_OPTS used by Logstash. You'll need to specify the appropriate options to enable JMX and map the JMX port on the docker host.

Update the container in the `docker-compose.yml` to add the *LS_JAVA_OPTS* environment variable with the following content (I've mapped the JMX service on the port 18080, you can change that), do not forget to update the *-Djava.rmi.server.hostname* option with the IP address of your Docker host (replace **DOCKER_HOST_IP**):

```yml
logstash:
  build: logstash/
  volumes:
    - ./logstash/pipeline:/usr/share/logstash/pipeline
  ports:
    - "5000:5000"
  networks:
    - docker_elk
  depends_on:
    - elasticsearch
  environment:
    - LS_JAVA_OPTS=-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=18080 -Dcom.sun.management.jmxremote.rmi.port=18080 -Djava.rmi.server.hostname=DOCKER_HOST_IP -Dcom.sun.management.jmxremote.local.only=false
```

## How can I tune Elasticsearch configuration?

The Elasticsearch container is using the [shipped configuration](https://github.com/elastic/elasticsearch-docker/blob/master/build/elasticsearch/elasticsearch.yml).

If you want to override the default configuration, create a file `elasticsearch/config/elasticsearch.yml` and add your configuration in it.

Then, you'll need to map your configuration file inside the container in the `docker-compose.yml`. Update the elasticsearch container declaration to:

```yml
elasticsearch:
  build: elasticsearch/
  ports:
    - "9200:9200"
    - "9300:9300"
  environment:
    ES_JAVA_OPTS: "-Xms1g -Xmx1g"
  networks:
    - docker_elk
  volumes:
    - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
```

You can also specify the options you want to override directly via environment variables:

```yml
elasticsearch:
  build: elasticsearch/
  ports:
    - "9200:9200"
    - "9300:9300"
  environment:
    ES_JAVA_OPTS: "-Xms1g -Xmx1g"
    network.host: "_non_loopback_"
    cluster.name: "my-cluster"
  networks:
    - docker_elk
```

## How can I scale up the Elasticsearch cluster?

Follow the instructions from the Wiki: [Scaling up Elasticsearch](https://github.com/deviantony/docker-elk/wiki/Elasticsearch-cluster)

# Storage

## How can I store Elasticsearch data?

The data stored in Elasticsearch will be persisted after container reboot but not after container removal.

In order to persist Elasticsearch data even after removing the Elasticsearch container, you'll have to mount a volume on your Docker host. Update the elasticsearch container declaration to:

```yml
elasticsearch:
  build: elasticsearch/
  ports:
    - "9200:9200"
    - "9300:9300"
  environment:
    ES_JAVA_OPTS: "-Xms1g -Xmx1g"
    network.host: "_non_loopback_"
    cluster.name: "my-cluster"
  networks:
    - docker_elk
  volumes:
    - /path/to/storage:/usr/share/elasticsearch/data
```

This will store elasticsearch data inside `/path/to/storage`.
# NGINX-Modular
dockerfiles
===========

A collection of delicious docker recipes.

[![](https://travis-ci.org/vimagick/dockerfiles.svg)](https://travis-ci.org/vimagick/dockerfiles)

- :octocat: <https://github.com/vimagick/dockerfiles>
- :beetle: <https://github.com/vimagick/dockerfiles/issues>
- :book: <https://github.com/vimagick/dockerfiles/wiki>
- :whale: <https://hub.docker.com/u/vimagick/> (x86)
- :whale: <https://hub.docker.com/u/easypi/> (arm)

## 友情赞助

[![](https://www.vultr.com/media/banner_1.png)](https://www.vultr.com/?ref=6821947)

## Todo

- [ ] caddy
- [ ] dsniff
- [ ] ettercap
- [ ] freegeoip
- [ ] freelan
- [ ] gitbook
- [ ] gitolite
- [ ] hashcat
- [ ] imagemagick
- [ ] ipset
- [ ] irc
- [ ] libreswan
- [ ] metasploit
- [ ] mitmproxy
- [ ] nagios
- [ ] nfs
- [ ] openldap
- [ ] openswan
- [ ] postfix
- [ ] pritunl
- [ ] pyinstaller
- [ ] rtmpdump
- [ ] sensu
- [ ] ssf
- [ ] tshark
- [ ] youtube-upload
- [ ] xl2tpd

## IoT

- [x] hass :+1:
- [x] hbdg :+1:
- [x] node-red :+1:
- [x] mosquitto :+1:

## Daemon

- [x] alpine-arm :+1:
- [x] aria2 :+1:
- [x] audiowaveform
- [x] cadvisor
- [x] casperjs :+1:
- [x] collectd
- [x] errbot :octocat:
- [x] freeradius
- [x] graphite
- [x] h2o
- [x] httpbin :+1:
- [x] hubot :octocat:
- [x] influxdb
- [x] luigi
- [x] mariadb
- [x] mariadb-arm
- [x] monit
- [x] moodle :beetle:
- [x] mosquitto
- [x] motion-arm :+1:
- [x] nginx
- [x] nifi
- [x] nullmailer
- [x] nullmailer-arm
- [x] openhab
- [x] openssh
- [x] ot-recorder
- [x] ot-recorder-arm
- [x] piknik
- [x] portia
- [x] pure-ftpd
- [x] redis-arm
- [x] rslsync
- [x] rsyncd
- [x] rsyslog
- [x] samba :+1:
- [x] samba-arm :+1:
- [x] scrapyd :+1:
- [x] statsd
- [x] swarm-arm
- [x] taskd
- [x] telegraf
- [x] tftpd
- [x] tmail :beetle:
- [x] urlwatch :beetle:
- [x] vnstat
- [x] vsftpd
- [x] webhook
- [x] webkit :beetle:

## Media

- [x] cmus
- [x] cmus-arm
- [x] ffmpeg
- [x] ffmpeg-arm
- [x] ffserver :beetle:
- [x] icecast
- [x] live555
- [x] minidlna
- [x] mpd
- [x] murmur
- [x] plex :moneybag:
- [x] red5 :+1:
- [x] red5-arm :construction:
- [x] rtmp-client-arm :+1:
- [x] rtmp-server
- [x] shairplay-arm
- [x] shoutcast
- [x] tesseract
- [x] vnc2flv
- [x] youtube-dl
- [x] youtube-worker :beetle:

## Web

- [x] discuz :cn:
- [x] django-cms
- [x] dokuwiki :+1:
- [x] dokuwiki-arm :+1:
- [x] glances
- [x] gogs-arm :cn:
- [x] grafana
- [x] hugo
- [x] hugo-arm
- [x] jamapi
- [x] jenkins-arm :beetle:
- [x] joomla
- [x] json-server
- [x] magento
- [x] mantisbt
- [x] mediagoblin
- [x] netdata
- [x] nginad
- [x] nodebb :+1:
- [x] opencart
- [x] openrefine
- [x] phpbb
- [x] phpmyadmin-arm
- [x] phpvirtualbox-arm
- [x] piwik
- [x] revive
- [x] zoneminder :+1:

## Security

- [x] bro
- [x] clamav
- [x] dsniff
- [x] ferm
- [x] grr
- [x] hydra
- [x] iptables
- [x] routersploit
- [x] snort :beetle:
- [x] sslsplit
- [x] webgoat

## Proxy

- [x] dante
- [x] delegated
- [x] fteproxy :+1:
- [x] fteproxy-arm :+1:
- [x] hans
- [x] haproxy-arm
- [x] kcptun :cn:
- [x] mysql-proxy
- [x] ngrok :+1:
- [x] obfsproxy
- [x] polipo
- [x] privoxy
- [x] privoxy-arm
- [x] proxyhub
- [x] shadowsocks
    - [x] shadowsocks
    - [x] shadowsocks-libev :+1:
    - [x] shadowsocks-arm
    - [x] shadowsocks-libev-arm :+1:
- [x] spiped
- [x] squid
- [x] stunnel
- [x] stunnel-arm
- [x] tinyproxy
- [x] tor
- [x] traefik-arm

## VPN

- [x] ocserv :+1:
- [x] openconnect
- [x] openconnect-arm
- [x] openvpn-arm
- [x] pptp
- [x] pptp-arm
- [x] pptpd
- [x] shadowvpn
- [x] strongswan :+1:
- [x] tinc :+1:
- [x] tinc-arm :+1:

## DNS

- [x] chinadns
- [x] dnscrypt
    - [x] dnscrypt-proxy
    - [x] dnscrypt-wrapper
- [x] dnsmasq
- [x] dnsmasq-arm
- [x] pdnsd

## 3rd-party

- [x] cachethq/docker
- [x] centurylink/watchtower
- [x] certbot
- [x] drone/drone
- [x] drupal
- [x] elk
- [x] ghost
- [x] gitlab/gitlab-ce
- [ ] gliderlabs/logspout
- [x] gliderlabs/registrator
- [ ] glot
    - [ ] bash
    - [ ] python
- [x] gogs :cn:
- [x] haproxy
- [x] indiehosters/nextcloud
- [x] jazzdd/phpvirtualbox
- [x] jenkins
- [x] jupyter/notebook
- [x] kylemanna/openvpn
- [x] mongo
- [x] neo4j
- [x] owncloud
- [x] phpmyadmin
- [x] portainer/portainer :+1:
- [x] postgres
- [x] registry
- [x] rocket.chat
- [x] scrapinghub/splash
- [ ] selenium
    - [ ] hub
    - [ ] node-firefox
    - [x] standalone-firefox
- [x] tutum/builder
- [x] wekanteam/wekan

## auto-completion

```bash
#!/bin/bash
#
# handy auto-completion for docker-exec
#

enter() {
  local name=${1:?}
  docker exec -it $name sh -c 'exec $(command -v bash || command -v sh)'
}

__enter() {
  local cur=${COMP_WORDS[COMP_CWORD]}
  for cid in $(docker ps -q)
  do
    local name=$(docker inspect -f '{{.Name}}' $cid)
    name=${name#/}
    if [[ $name = $cur* ]]
    then
      COMPREPLY+=("$name")
    fi
  done
}

complete -F __enter enter
```
magento
=======

[Magento][1] Community Edition is open source eCommerce software used to power your
online store and can be downloaded for free. Developers can modify the core
code and add features and functionality by installing extensions from the
Magento Connect marketplace.

## docker-compose.yml

```yaml
magento:
  image: vimagick/magento
  ports:
    - "8000:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=magento
  restart: always
```

[1]: https://magento.com/
jamapi
======

![](https://badge.imagelayers.io/vimagick/jamapi:latest.svg)

[jamapi][1] parses web pages using CSS query selectors.

[1]: http://www.jamapi.xyz
nginad
======

[NginAd][1] open source ad server & exchange with OpenRTB.

## docker-compose.yml

```
nginad:
  image: vimagick/nginad
  ports:
    - "80:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=nginad
  restart: always
```

## run

```
$ fig up -d

$ wget https://nginad.atlassian.net/wiki/download/attachments/1114149/nginad-1.6.sql
$ docker exec -i nginad_mysql_1 mysql -u root -proot nginad < nginad-1.6.sql

$ DOMAIN=nginad.foobar.site
$ docker exec nginad_nginad_1 sed -i "/adserver_domain/s/'.*'/'$DOMAIN'/" upload/public/ad/nginad.js
$ docker exec nginad_nginad_1 sed -i "s/server.nginad.com/$DOMAIN/" upload/config/autoload/delivery.local.php
$ docker exec nginad_nginad_1 sed -i "s/server.nginad.com/$DOMAIN/" upload/public/buytest.html

$ fig restart nginad
```

## test

Open test page in your browser: <http://nginad.foobar.site/buytest.html>

## admin

Now log into the Demand Customer and Publisher [dashboards][2] as admin and
start adding and configuring RTB campaigns.

- username: admin@localhost
- password: password

Log in as the default publisher account:

- username: sergey.page@blowmedianow.com
- password: password

Log in as the default demand customer account:

- username: larry.brin@suckmedianow.com
- password: password

[1]: https://nginad.atlassian.net/wiki/display/NGIN/nginad+Home
[2]: http://nginad.foobar.site/auth/login
ferm - for Easy Rule Making
===========================

![](https://badge.imagelayers.io/vimagick/ferm:latest.svg)

[`ferm`][1] is a frontend for iptables, providing a way to write manageable
rulesets without sacrificing flexibility.

## Tutorial

```
$ alias ferm='docker run -i --rm vimagick/ferm'

$ cat > iptables.rules <<_EOF_
chain INPUT {
    policy DROP;
    mod state  state (RELATED ESTABLISHED)  ACCEPT;
    proto tcp  dport (http ftp ssh)  ACCEPT;
}
_EOF_

$ ferm -h
Usage:
    ferm *options* *inputfiles*

Options:
     -n, --noexec      Do not execute the rules, just simulate
     -F, --flush       Flush all netfilter tables managed by ferm
     -l, --lines       Show all rules that were created
     -i, --interactive Interactive mode: revert if user does not confirm
     -t, --timeout s   Define interactive mode timeout in seconds
     --remote          Remote mode; ignore host specific configuration.
                       This implies --noexec and --lines.
     -V, --version     Show current version number
     -h, --help        Look at this text
     --slow            Slow mode, do not use iptables-restore
     --shell           Generate a shell script which calls iptables-restore
     --domain {ip|ip6} Handle only the specified domain
     --def '$name=v'   Override a variable

$ ferm < iptables.rules
# Generated by ferm 2.2 on Mon Jul  6 00:32:04 2015
*filter
:INPUT DROP [0:0]
-A INPUT --match state --state RELATED,ESTABLISHED --jump ACCEPT
-A INPUT --protocol tcp --dport http --jump ACCEPT
-A INPUT --protocol tcp --dport ftp --jump ACCEPT
-A INPUT --protocol tcp --dport ssh --jump ACCEPT
COMMIT

$ ferm --slow - < iptables.rules
iptables -t filter -P INPUT ACCEPT
iptables -t filter -F
iptables -t filter -X
iptables -t filter -P INPUT DROP
iptables -t filter -A INPUT --match state --state RELATED,ESTABLISHED --jump ACCEPT
iptables -t filter -A INPUT --protocol tcp --dport http --jump ACCEPT
iptables -t filter -A INPUT --protocol tcp --dport ftp --jump ACCEPT
iptables -t filter -A INPUT --protocol tcp --dport ssh --jump ACCEPT
```

[1]: http://ferm.foo-projects.org/
ffmpeg
======

![](https://badge.imagelayers.io/vimagick/ffmpeg:latest.svg)

[FFmpeg][1] is a complete, cross-platform solution to record, convert and stream audio and video.

## Tutorial

```bash
# Create an alias
$ alias ffmpeg='docker run --rm -u $(id -u):$(id -g) -v $PWD:/data vimagick/ffmpeg'

# Do video transcoding
$ ffmpeg -i video.mov video.mp4
```

## YouTube Live Streaming

```yaml
ffmpeg:
  image: vimagick/ffmpeg
  entrypoint: sh
  command:
    - -c
    - ffmpeg -f lavfi -i anullsrc -rtsp_transport udp -i $$RTMP_DEV -tune zerolatency -vcodec mpeg4 -pix_fmt + -c:v copy -an -f flv $$RTMP_URI
  environment:
    - RTMP_DEV=rtsp://10.50.254.197/live
    - RTMP_URI=rtmp://a.rtmp.youtube.com/live2/xxxx-xxxx-xxxx-xxxx
  restart: always
```

## Todo List

- [ ] Re-compile with fontconfig/freetype enabled to support [timestamp][2] overlay.

[1]: http://ffmpeg.org/
[2]: https://einar.slaskete.net/2011/09/05/adding-time-stamp-overlay-to-video-stream-using-ffmpeg/
murmur
======

[Mumble][1] is an open source, low-latency, high quality voice chat software
primarily intended for use while gaming.

## docker-compose.yml

```yaml
murmur:
  image: vimagick/murmur
  ports:
    - "64738:64738/tcp"
    - "64738:64738/udp"
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d
$ docker-compose logs
```

> Superuser password was generated at startup.

## Client Setup

- Android: <https://play.google.com/store/apps/details?id=com.morlunk.mumbleclient.free>
- IOS: <http://itunes.apple.com/us/app/mumble/id443472808?ls=1&mt=8>
- Windows: <https://github.com/mumble-voip/mumble/releases/download/1.2.17/mumble-1.2.17.msi>
- MacOSX: <https://github.com/mumble-voip/mumble/releases/download/1.2.17/Mumble-1.2.17.dmg>

[1]: https://wiki.mumble.info/wiki/Main_Page
motion-arm
==========

![](http://www.lavrsen.dk/foswiki/pub/Motion/WebPreferences/motion-trans.gif)

[Motion][1] is a program that monitors the video signal from one or more cameras
and is able to detect if a significant part of the picture has changed. Or in
other words, it can detect motion.

## docker-compose.yml

```yaml
motion:
  image: easypi/motion-arm
  ports:
    - "8080:8080"
    - "8081:8081"
  volumes:
    - ./motion.conf:/etc/motion/motion.conf
    - ./data:/var/lib/motion
    - /etc/localtime:/etc/localtime
  devices:
    - /dev/video0:/dev/video0
  restart: always
```

You can edit `motion.conf` to customize motion.

```ini
# set image width
width 640

# set image height
height 480

# set frame rate
framerate 5

# disable image output
output_pictures off

# disable video output
ffmpeg_output_movies off

# encode timelapse movie
ffmpeg_timelapse 60
```

Motion can work with Home-Assistant via [External Commands][2].

``yaml
binary_sensor:
  - platform: mqtt
    name: Motion
    state_topic: /pi/sensor/motion
    qos: 0
    payload_on: ON
    payload_off: OFF
    device_class: motion
```

```bash
# Command to be executed when a movie file (.mpg|.avi) is created. (default: none)
# To give the filename as an argument to a command append it with %f

# CURL
;on_movie_start curl -s -H 'X-HA-Access: ******' -H 'Content-Type: application/json' -d '{"state": "on", "attributes": {"friendly_name": "Motion", "device_class": "motion"}}' http://hass.easypi.pro:8123/api/states/binary_sensor.motion
# MQTT
;on_movie_start mosquitto_pub -h mqtt.easypi.pro -u username -P password -r -t /pi/sensor/motion -m ON

# Command to be executed when a movie file (.mpg|.avi) is closed. (default: none)
# To give the filename as an argument to a command append it with %f

# CURL
;on_movie_end curl -s -H 'X-HA-Access: ******' -H 'Content-Type: application/json' -d '{"state": "off", "attributes": {"friendly_name": "Motion", "device_class": "motion"}}' http://hass.easypi.pro:8123/api/states/binary_sensor.motion
# MQTT
;on_movie_end mosquitto_pub -h mqtt.easypi.pro -u username -P password -r -t /pi/sensor/motion -m OFF

```

Please read [this][3] to enable raspberry pi camera module.

```
####################
# /boot/config.txt #
####################

##Camera
gpu_mem=128
start_file=start_x.elf
fixup_file=fixup_x.dat
disable_camera_led=1
```

[1]: http://lavrsen.dk/foswiki/bin/view/Motion/WebHome
[2]: http://www.lavrsen.dk/foswiki/bin/view/Motion/ExternalCommands
[3]: https://wiki.archlinux.org/index.php/Raspberry_Pi#Raspberry_Pi_camera_module
oled
====

## Setup

```
OLED Pin | Name | Remarks | RPi Pin |  RPi Function
---------+------+---------+---------+--------------
1        | GND  | Ground  | P01-6   |  GND
2        | VCC  | +3.3V   | P01-1   |  3V3
3        | SCL  | Clock   | P01-5   |  GPIO 3 (SCL)
4        | SDA  | Data    | P01-3   |  GPIO 2 (SDA)
```

```bash
# hello world
$ docker-compose run --rm oled < demo.py
```

```
# list all examples
$ docker-compose run --rm --entrypoint ls oled

# clock
$ docker-compose run --rm oled clock.py

# pi logo
$ docker-compose run --rm oled pi_logo.py

# game of life
$ docker-compose run --rm game_of_life.py
```

## Todo

- [ ] Implement RESTful api as default CMD.
telegraf
========

[Telegraf][1] is an open source agent written in Go for collecting metrics and
data on the system it's running on or from other services. Telegraf writes data
it collects to InfluxDB in the correct format.

## docker-compose.yml

```yaml
telegraf:
  image: telegraf:alpine
  ports:
    - "8092:8092/udp"
    - "8094:8094/tcp"
  volumes:
    - ./telegraf.conf:/etc/telegraf/telegraf.conf
  restart: always
```

## up and running

```bash
$ cd ~/fig/telegraf/
$ docker-compose run --rm telegraf -sample > telegraf.conf
$ vim telegraf.conf
$ docker-compose up -d
```

[1]: https://github.com/influxdata/telegraf
opencart
========

[OpenCart][1] is designed feature rich, easy to use, search engine
friendly and with a visually appealing interface.

```
opencart:
  image: vimagick/opencart
  ports:
    - "8000:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=opencart
  restart: always
```

[1]: http://www.opencart.com/index.php
dnscrypt
========

## About

- `dnscrypt-wrapper` - A server-side dnscrypt proxy.
- `dnscrypt-proxy` - A protocol for securing communications between a client and a DNS resolver.

## Config

    wrapper:
      image: vimagick/dnscrypt-wrapper
      ports:
        - "443:443/udp"
        - "443:443/tcp"
      environment:
        - LISTEN_ADDR=0.0.0.0:443
        - RESOLVER_ADDR=8.8.8.8:53
        - PROVIDER_NAME=2.dnscrypt-cert.easypi.info
      restart: always

    proxy:
      image: vimagick/dnscrypt-proxy
      ports:
        - "53:53/udp"
        - "53:53/tcp"
      environment:
        - LISTEN_ADDR=0.0.0.0:443
        - RESOLVER_ADDR=1.2.3.4:443
        - PROVIDER_NAME=2.dnscrypt-cert.easypi.info
        - PROVIDER_KEY=4C29:9CEB:CF8D:4612:48A8:B2F2:3B6F:A046:EBF5:2F2B:6433:27C6:5F3A:88F5:495E:3075
      restart: always

> `RESOLVER_ADDR` is server public ip address.

## Server

    $ cd dnscrypt
    $ fig up -d wrapper
    $ docker exec -it dnscrypt_wrapper_1 cat README.txt
    Public key fingerprint: 4C29:9CEB:CF8D:4612:48A8:B2F2:3B6F:A046:EBF5:2F2B:6433:27C6:5F3A:88F5:495E:3075

## Client

    $ cd dnscrypt
    $ fig up -d proxy
    $ dig @127.0.0.1 www.google.com
    $ dig @127.0.0.1 www.youtube.com +tcp

## Note

You'd better to use `vimagick/dnscrypt-proxy` as backend of `dnsmasq` or `pdnsd` for better performance.

Please read [this](https://github.com/Cofyc/dnscrypt-wrapper) to re-generate keys!
https://github.com/vimagick/dockerfiles/tree/master/dnscrypt
https://github.com/vimagick/dockerfiles/tree/master/dnscrypt
fteproxy
========

[fteproxy][1] provides transport-layer protection to resist keyword filtering,
censorship and discriminatory routing policies.

Its job is to relay datastreams, such as web browsing traffic,
by encoding streams as messages that match a user-specified regular expression.

fteproxy is fast, free, open source, and cross platform. 
It works very well with [vimagick/openvpn][2](TCP mode).

In the following example, I will setup a server/client pair to connect www.google.com:80.

## Internet Censorship

### The problem

![before](https://fteproxy.org/images/withoutfte.png)

### The solution

![after](https://fteproxy.org/images/withfte.png)

## Create a docker-compose.yml

```yaml
server:
  image: vimagick/fteproxy
  ports:
    - "80"
  environment:
    - "MODE=server"
    - "SERVER_IP=0.0.0.0"
    - "SERVER_PORT=80"
    - "PROXY_IP=www.google.com"
    - "PROXY_PORT=80"
    - "KEY=CB2FBA2BC70490526E749E01BB050F6B555964290DFF58CF24785B4A093F7B18"

client:
  image: vimagick/fteproxy
  ports:
    - "9009:80"
  links:
    - server
  environment:
    - "MODE=client"
    - "SERVER_IP=server"
    - "SERVER_PORT=80"
    - "CLIENT_IP=0.0.0.0"
    - "CLIENT_PORT=80"
    - "KEY=CB2FBA2BC70490526E749E01BB050F6B555964290DFF58CF24785B4A093F7B18"
```

> To generate random key:  
> `xxd -u -p -c32 /dev/urandom | head -n1`

## Run fteproxy server/client

```bash
$ docker-compose up -d
Creating fteproxy_server_1...
Creating fteproxy_client_1...

$ docker-compose ps
Name                     Command               State           Ports
----------------------------------------------------------------------------------
fteproxy_client_1   /bin/sh -c /fteproxy/ftepr ...   Up      0.0.0.0:9009->80/tcp
fteproxy_server_1   /bin/sh -c /fteproxy/ftepr ...   Up      0.0.0.0:32768->80/tcp
```

## Test with curl

```html
$ curl http://localhost:9009/
<HTML><HEAD><meta http-equiv="content-type" content="text/html;charset=utf-8">
<TITLE>302 Moved</TITLE></HEAD><BODY>
<H1>302 Moved</H1>
The document has moved
<A HREF="http://www.google.com/">here</A>.
</BODY></HTML>
```

[1]: https://fteproxy.org/
[2]: https://hub.docker.com/r/vimagick/openvpn/
stunnel
=======

[Stunnel][1] is a proxy designed to add TLS encryption functionality to
existing clients and servers without any changes in the programs' code.

### Overview

domain | ip:port      | country | services
-------| ------------ | ------- | ------------------------------
master | 1.2.3.4:4911 | Japan   | openvpn-server, stunnel-server
bridge | 5.6.7.8:1194 | China   | stunnel-client
N/A    | 192.168/16   | China   | openvpn-client

### docker-compose.yml

```yaml
# In Japan
master:
  image: vimagick/stunnel
  ports:
    - "4911:4911"
  environment:
    - CLIENT=no
    - SERVICE=openvpn
    - ACCEPT=0.0.0.0:4911
    - CONNECT=server:1194
  external_links:
    - openvpn_server_1:server
  restart: always

# In China
bridge:
  image: vimagick/stunnel
  ports:
    - "1194:1194"
  environment:
    - CLIENT=yes
    - SERVICE=openvpn
    - ACCEPT=0.0.0.0:1194
    - CONNECT=server:4911
  extra_hosts:
    - server:1.2.3.4
  restart: always
```

### Server Setup (Cloud)

```bash
# master server (Japan)
docker-compose up -d master
```

### Client Setup (Cloud)

```bash
# bridge server (China)
docker-compose up -d bridge
```

### Client Setup (Local)

File: /etc/stunnel/stunnel.conf

```ini
foreground = yes
client = yes

[openvpn]
accept = 127.0.0.1:1194
connect = 1.2.3.4:4911
```

> Pro Tip: Running stunnel locally is faster.

### OpenVPN Setup (Partial)

```ini
# For Cloud Setup
...
remote 5.6.7.8 1194 tcp
route 192.168.0.0 255.255.0.0 net_gateway
...
```

```ini
# For Local Setup
...
remote 127.0.0.1 1194 tcp
route 1.2.3.4 255.255.255.255 net_gateway
route 192.168.0.0 255.255.0.0 net_gateway
....
```

[1]: https://www.stunnel.org/index.html
tor
===

![](https://badge.imagelayers.io/vimagick/tor:latest.svg)

[`Tor`][1] is free software and an open network that helps you defend against
traffic analysis, a form of network surveillance that threatens personal
freedom and privacy, confidential business activities and relationships, and
state security.

- Tor prevents people from learning your location or browsing habits.
- Tor is for web browsers, instant messaging clients, and more.
- Tor is free and open source for Windows, Mac, Linux/Unix, and Android

ServerTransportPlugin:

- [x] fte
- [x] meek
- [x] obfs3
- [x] obfs4

## docker-compose.yml

```
tor:
  image: vimagick/tor
  ports:
#   - "7002:7002"
    - "9001:9001"
# volumes:
#   - ./torrc:/etc/tor/torrc
#   - ./cert.pem:/var/lib/tor/cert.pem
#   - ./key.pem:/var/lib/tor/key.pem
  restart: always
```

> Default `torrc` is for `obfs4`.
> Uncomment lines to use `meek`.

## torrc (server)

```
BridgeRelay 1
ContactInfo noreply@easypi.info
DataDirectory /var/lib/tor
Exitpolicy reject *:*
Nickname easypi
ORPort 9001
PublishServerDescriptor 0
SocksPort 0
#ServerTransportPlugin fte exec /usr/bin/fteproxy --mode server --managed
#ServerTransportPlugin meek exec /usr/bin/meek-server --port 7002 --cert cert.pem --key key.pem
#ServerTransportPlugin obfs3 exec /usr/bin/obfsproxy managed
ServerTransportPlugin obfs4 exec /usr/bin/obfs4proxy
```

## torrc (client)

```
#Socks5Proxy 127.0.0.1:1080
UseBridges 1
#Bridge fte 1.2.3.4:9001 F24BF4DE74649E205A8A3621C84F97FF623B2083
#Bridge meek 1.2.3.4:9001 url=https://meek.easypi.info:7002/
#Bridge obfs3 1.2.3.4:9001 F24BF4DE74649E205A8A3621C84F97FF623B2083
Bridge obfs4 1.2.3.4:9001 F24BF4DE74649E205A8A3621C84F97FF623B2083
#ClientTransportPlugin fte exec /usr/local/bin/fteproxy
#ClientTransportPlugin meek exec /usr/local/bin/meek-client
#ClientTransportPlugin obfs3 exec /usr/local/bin/obfsproxy
ClientTransportPlugin obfs4 exec /usr/local/bin/obfs4proxy
```

> Please connect via `HTTPProxy`/`HTTPSProxy`/`Socks5Proxy` if you're blocked!

## server

```
$ openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days 365 -nodes -subj "/C=JP/ST=Tokyo/L=Heiwajima/O=DataGeek/OU=Org/CN=meek.easypi.info"
$ docker-compose up -d
$ docker-compose logs
$ docker-compose exec tor tor --quiet --list-fingerprint
easypi F24B F4DE 7464 9E20 5A8A 3621 C84F 97FF 623B 2083
```

## client

```
$ tor -f /etc/tor/torrc
$ curl -x socks5h://127.0.0.1:9050 ifconfig.ovh
```

## references

- https://www.torproject.org/projects/obfsproxy-debian-instructions.html.en
- https://blog.torproject.org/blog/how-use-%E2%80%9Cmeek%E2%80%9D-pluggable-transport
- https://fteproxy.org/help-server-with-tor
- https://github.com/Yawning/obfs4

[1]: https://www.torproject.org/
Tor on OpenWrt
==============

![](https://trac.torproject.org/images/tor-logo.png)

## Install Tor

```bash
$ opkg update
$ opkg install tor

$ vi /etc/tor/torrc
$ vi /etc/config/dhcp
$ vi /etc/firewall.user

$ /etc/init.d/tor restart
$ /etc/init.d/dnsmasq restart
$ /etc/init.d/firewall restart
```

## Config Tor

```
# /etc/tor/torrc

RunAsDaemon 0  
DataDirectory /var/lib/tor  
User tor  
#Socks5Proxy 127.0.0.1:1080
SocksPort 0.0.0.0:9050  
TransPort 0.0.0.0:9040  
DNSPort 0.0.0.0:9053  
AvoidDiskWrites 1 
```

## Socks5 Proxy (optional)

You may need to connect tor network via `Socks5Proxy`.
You can use [ss-local][1] to setup a local socks5 proxy.

```bash
$ wget https://github.com/vimagick/rpi-bin/raw/master/ipk/shadowsocks-libev_2.4.8-3_arm_cortex-a53_neon-vfpv4.ipk
$ opkg install shadowsocks-libev_2.4.8-3_arm_cortex-a53_neon-vfpv4.ipk
$ vi /etc/shadowsocks.json
$ vi /etc/rc.local
$ sh /etc/rc.local
```

```bash
# /etc/rc.local

ss-local -c /etc/shadowsocks.json -f /var/run/ss-local.pid

exit 0
```

```json
{
    "server": "1.2.3.4",
    "server_port": 8388,
    "local_address": "127.0.0.1",
    "local_port": 1080,
    "password": "******",
    "timeout": 60,
    "method": "chacha20",
    "auth": true
}
```

## Config DNS

You can config DNS via Luci.

```bash
# /etc/config/dhcp

config dnsmasq
       option noresolv '1'
       list server '127.0.0.1#9053'
```

## Config Firewall

```bash
# /etc/firewall.user

iptables -t nat -X TOR
iptables -t nat -N TOR
iptables -t nat -A TOR -d 0.0.0.0/8 -j RETURN
iptables -t nat -A TOR -d 10.0.0.0/8 -j RETURN
iptables -t nat -A TOR -d 127.0.0.0/8 -j RETURN
iptables -t nat -A TOR -d 169.254.0.0/16 -j RETURN
iptables -t nat -A TOR -d 172.16.0.0/12 -j RETURN
iptables -t nat -A TOR -d 192.168.0.0/16 -j RETURN
iptables -t nat -A TOR -d 224.0.0.0/4 -j RETURN
iptables -t nat -A TOR -d 240.0.0.0/4 -j RETURN
iptables -t nat -A TOR -p udp --dport 53 -j REDIRECT --to-ports 9053
iptables -t nat -A TOR -p tcp --syn -j REDIRECT --to-ports 9040
iptables -t nat -A PREROUTING -i br-lan -j TOR
```

## Debug Tor

```bash
$ /etc/init.d/tor stop
$ tor
Aug 16 11:49:43.222 [notice] Tor v0.2.7.6 running on Linux with Libevent 2.0.22-stable, OpenSSL 1.0.2h and Zlib 1.2.8.  
Aug 16 11:49:43.223 [notice] Tor can't help you if you use it wrong! Learn how to be safe at https://www.torproject.org/download/download#warning  
Aug 16 11:49:43.223 [notice] Read configuration file "/etc/tor/torrc".  
Aug 16 11:49:43.246 [warn] You specified a public address '0.0.0.0:9050' for SocksPort. Other people on the Internet might find your computer and use it as an open proxy. Please don't allow this unless you have a good reason.  
Aug 16 11:49:43.246 [warn] You specified a public address '0.0.0.0:9053' for DNSPort. Other people on the Internet might find your computer and use it as an open proxy. Please don't allow this unless you have a good reason.  
Aug 16 11:49:43.246 [warn] You specified a public address '0.0.0.0:9040' for TransPort. Other people on the Internet might find your computer and use it as an open proxy. Please don't allow this unless you have a good reason.  
Aug 16 11:49:43.251 [notice] Opening Socks listener on 0.0.0.0:9050  
Aug 16 11:49:43.251 [notice] Opening DNS listener on 0.0.0.0:9053  
Aug 16 11:49:43.251 [notice] Opening Transparent pf/netfilter listener on 0.0.0.0:9040  
Aug 16 11:49:44.000 [notice] Bootstrapped 0%: Starting  
Aug 16 11:49:46.000 [notice] Bootstrapped 5%: Connecting to directory server  
Aug 16 11:49:46.000 [notice] Bootstrapped 80%: Connecting to the Tor network  
Aug 16 11:49:46.000 [notice] Bootstrapped 85%: Finishing handshake with first hop  
Aug 16 11:49:53.000 [notice] Bootstrapped 90%: Establishing a Tor circuit  
Aug 16 11:49:58.000 [notice] Tor has successfully opened a circuit. Looks like client functionality is working.  
Aug 16 11:49:58.000 [notice] Bootstrapped 100%: Done  
^C
$ /etc/init.d/tor start
```

[1]: https://github.com/vimagick/rpi-bin/raw/master/ipk/shadowsocks-libev_2.4.8-3_arm_cortex-a53_neon-vfpv4.ipk
h2o
===

![](https://badge.imagelayers.io/vimagick/h2o:latest.svg)

[H2O][1] is a new generation HTTP server providing quicker response to users when
compared to older generation of web servers.

## Quick Start

```
$ mkdir html
$ echo 'hello world' > html/index.html
$ docker run -d -p 8080:80 -v `pwd`/html:/var/www/html vimagick/h2o
$ curl localhost:8080
```

[1]: https://h2o.examp1e.net/index.html
shairplay
=========

[Shairplay][1] is a free portable AirPlay server implementation similar to [ShairPort][2].
Currently only AirPort Express emulation is supported.

## docker-compose.yml

```yaml
shairplay:
  image: easypi/shairplay-arm
  command: shairplay --apname=EasyPi --hwaddr=01:45:89:ab:cd:ef
  volumes:
    - /var/run/dbus:/var/run/dbus
  devices:
    - /dev/snd
  net: host
  restart: unless-stopped
```

[1]: https://github.com/juhovh/shairplay
[2]: https://github.com/abrasive/shairport
rsyslog
=======

[RSYSLOG][1] is the rocket-fast system for log processing.

## docker-compose.yml

```yaml
rsyslog:
  image: vimagick/rsyslog
  ports:
    - "514:514/tcp"
    - "514:514/udp"
  volumes:
    - ./log:/var/log
  restart: always
```

## Up and Running

```bash
$ docker-compose up -d

$ docker-compose exec rsyslog sh
>>> pwd
/var/log
>>> touch maillog
>>> tail -f /var/log/maillog
2016-10-29T08:17:34+00:00 172.17.0.1 root: hello
2016-10-29T08:17:41+00:00 172.17.0.1 root: world
^C
>>> exit

$ logger -n localhost -p mail.debug hello
$ logger -n localhost -p mail.info world
```

[1]: http://www.rsyslog.com/
spiped
======

## docker-compose.yml

```yaml
keygen:
  image: spiped:alpine
  command: spiped-generate-key.sh
  volumes:
    - ./key:/spiped/key/spiped-keyfile

server:
  image: spiped:alpine
  command: -d -s 0.0.0.0:5353 -t 8.8.8.8:53
  ports:
    - "5353:5353"
  volumes:
    - ./key:/spiped/key:ro
  restart: always

client:
  image: spiped:alpine
  command: -e -s 0.0.0.0:5533 -t 1.2.3.4:5353
  ports:
    - "5533:5533"
  volumes:
    - ./key:/spiped/key:ro
  restart: always
```

## up and running

```bash
$ mkdir -p ~/fig/spiped
$ cd ~/fig/spiped
$ touch key
$ docker-compose run --rm keygen
$ docker-compose up -d server
$ docker-compose up -d client
$ dig -p 53 @8.8.8.8 +tcp www.google.com
$ dig -p 5533 @127.0.0.1 +tcp www.google.com
```

> Please run server and client on different machines.
chinadns + dnsmasq
==================

## About

- chinadns: Protect yourself against DNS poisoning in China.
- dnsmasq: A free software DNS forwarder and DHCP server for small networks.

## Fig

    chinadns:
      image: vimagick/chinadns
      ports:
        - "53:53/udp"
        - "53:53/tcp"
      restart: always

## Run

    fig up -d

## Test

    # UDP
    dig @127.0.0.1 www.google.com

    # TCP
    dig @127.0.0.1 www.youtube.com +tcp

moodle
======

[Moodle][1] is a learning platform designed to provide educators,
administrators and learners with a single robust, secure and integrated system
to create personalised learning environments.

## docker-compose.yml

```
moodle:
  image: vimagick/moodle
  ports:
    - "8000:80"
  links:
    - mysql
  volumes:
    - ./moodledata:/var/www/moodledata
  restart: always

mysql:
  image: mysql
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=moodle
  restart: always
```

## up and running

```
$ cd ~/fig/moodle/
$ mkdir -p moodledata
$ chmod 777 moodledata
$ docker-compose up -d
```

[1]: https://moodle.org/
gogs
====

[Gogs][1] (Go Git Service) is a painless self-hosted Git service.

## docker-compose.yml

```yaml
gogs:
  image: gogs/gogs
  ports:
    - "2222:22"
    - "3000:3000"
  volumes:
    - ./data:/data
  restart: always
```

## up and running

```
$ docker-compose up -d

$ docker-compose exec gogs vi /data/gogs/conf/app.ini

$ docker-compose restart

$ firefox http://localhost:3000/

$ tree -FL 3 ./data/git/
./data/git/
├── git -> /data/git
└── gogs-repositories/
    ├── root/
    ├── user1/
    │   ├── project1.git/
    │   └── project2.git/
    └── user2/
```

[1]: https://gogs.io/
gogs-arm
========

```yaml
Database Settings:
    Database Type: MySQL
    Host: mysql:3306
    User: root
    Password: root
    Database Name: gogs

Application General Settings:
    Application Name: "Gogs: Go Git Service"
    Repository Root Path: /data/git/gogs-repositories
    Run User: git
    Domain: 192.168.31.231
    SSH Port: 2222
    HTTP Port: 3000
    Application URL: http://192.168.31.231:3000/
    Log Path: /app/gogs/log

Optional Settings:
    Email Service Settings:
    Server and Other Services Settings:
    Admin Account Settings:
        Username: root
        Password: ******
        Confirm Password: ******
        Admin Email: root@easypi.info
```
luigi
=====

![](https://raw.githubusercontent.com/spotify/luigi/master/doc/luigi.png)

[Luigi][1] is a Python (2.7, 3.3, 3.4, 3.5) package that helps you build
complex pipelines of batch jobs. It handles dependency resolution, workflow
management, visualization, handling failures, command line integration, and
much more.

[1]: https://github.com/spotify/luigi
home-assistant
==============

![](https://home-assistant.io/demo/favicon-192x192.png)

[Home Assistant][1] is an open-source home automation platform running on
Python 3. Track and control all devices at home and automate control. Perfect
to run on a Raspberry Pi.

## directory tree

```
~/fig/hass/
├── data/
│   ├── automations.yaml
│   ├── configuration.yaml
│   ├── device_trackers.yaml
│   ├── groups.yaml
│   ├── known_devices.yaml
│   └── secrets.yaml
└── docker-compose.yml
```

Click [here][3] to get latitude and longitude.

## docker-compose.yml

Lightweight without bluetooth [trackers][2]

```yaml
hass:
  image: vimagick/hass
  volumes:
    - ./data:/etc/hass
  net: host
  restart: unless-stopped
```

Heavyweight with bluetooth [trackers][2]

```yaml
hass:
  image: vimagick/hass:debian
  volumes:
    - ./data:/etc/hass
  net: host
  privileged: yes
  restart: unless-stopped
```

## up and running

```bash
$ docker-compose up -d
$ docker-compose logs -f
$ curl http://localhost:8123
```

```bash
# Error during Bluetooth LE scan: Invalid device!
$ sudo hciconfig hci0 down
$ sudo hciconfig hci0 up
```

> More python packages will be installed during startup automatically.

## quick start

```yaml
# configuration.yaml
automation: !include automations.yaml
device_tracker: !include device_trackers.yaml
group: !include groups.yaml

mqtt:
  broker: iot.eclipse.org
  port: 8883
  certificate: auto
  username: !secret mqtt_username
  password: !secret mqtt_password
```

```yaml
# secrets.yaml
mqtt_username: YOUR_MQTT_USERNAME
http_password: YOUR_MQTT_PASSWORD
```

```yaml
# groups.yaml
default_view:
  view: yes
  entities:
    - group.living_room
    - group.bedroom
Living Room:
  view: no
  entities:
    - device_tracker.band
    - device_tracker.iphone
Bedroom:
  view: no
  entities:
    - device_tracker.android
```

```yaml
# device_trackers.yaml
- platform: bluetooth_le_tracker
  interval_seconds: 30
  consider_home: 120
  track_new_devices: no
- platform: ubus
  host: 192.168.1.1
  username: root
  password: ********
  track_new_devices: no
```

```yaml
# known_devices.yaml
mi_band_2:
  hide_if_away: false
  mac: BLE_D7:3D:97:88:88:88
  name: Band
  gravatar: vimagick@gmail.com
  picture:
  track: true
  vendor: unknown

kevin_iphone:
  hide_if_away: false
  mac: 48:A1:95:88:88:88
  name: iPhone
  picture: https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Apple_logo_black.svg/80px-Apple_logo_black.svg.png
  track: true
  vendor: Apple, Inc.

kevin_android:
  hide_if_away: false
  mac: 8C:70:5A:88:88:88
  name: Android
  picture: https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Android_robot_2014.svg/75px-Android_robot_2014.svg.png
  track: true
  vendor: unknown
```

## setup mosquitto

Use `vimagick/mosquitto:latest` with letsencrypt free certificates.

- https://github.com/vimagick/dockerfiles/tree/master/mosquitto
- https://mosquitto.org/man/mosquitto-conf-5.html
- https://github.com/vimagick/dockerfiles/tree/master/certbot

## setup ibeacon

Use `vimagick/hass:latest`, and disable bluetooth device tracker.

- https://joost.oostdijk.net/articles/playing_with_ble_on_an_onion_omega/
- https://home-assistant.io/blog/2016/04/30/ibeacons-part-1-making-presence-detection-work-better/
- https://home-assistant.io/blog/2016/05/26/ibeacons-how-to-track-things-that-cant-track-themselves-part-ii/
- http://owntracks.org/booklet/tech/json/
- https://play.google.com/store/apps/details?id=de.flurp.beaconscanner.app&hl=en

[1]: https://home-assistant.io/
[2]: https://home-assistant.io/components/device_tracker/
[3]: http://api.map.baidu.com/lbsapi/getpoint/index.html
drupal
======

```
drupal:
  image: drupal
  ports:
    - "8888:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  ports:
    - "127.0.0.1:3306:3306"
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=drupal
  restart: always
```

```
cd ~/fig/drupal/
docker-compose up -d mysql
sleep 30
docker-compose up -d drupal

docker cp drupal_mysql_1:/usr/bin/mysql /usr/local/bin/
mysql -h127.0.0.1 -P3306 -uroot -proot drupal

firefox http://127.0.0.1:8888
```
glances
=======

[Glances][1] an Eye on your system. A top/htop alternative.

## docker-compose.yml

```yaml
glances:
  image: vimagick/glances
  volumes:
    - ./data:/etc/glances
    - /var/run/docker.sock:/var/run/docker.sock:ro
  environment:
    - GLANCES_OPT=-w
  net: host
  pid: host
  restart: always
```

> Read [this][2] for all command line options.

## up and running

```bash
$ docker-compose up -d
$ curl http://localhost:61208
```

> You can put settings in [./data/glances.conf][3].

## screenshot

![](http://glances.readthedocs.io/en/stable/_images/screenshot-wide.png)

[1]: http://nicolargo.github.io/glances/
[2]: http://glances.readthedocs.io/en/stable/cmds.html
[3]: http://glances.readthedocs.io/en/stable/config.html
hcfg
====

See: https://home-assistant.io/docs/ecosystem/hass-configurator/

```yaml
panel_iframe:
  configurator:
    title: Configurator
    icon: mdi:wrench
    url: http://123.123.132.132:3218
```
tinyproxy
=========

[Tinyproxy][1] is a light-weight HTTP/HTTPS proxy daemon for POSIX operating systems.

## docker-compose.yml

```yaml
tinyproxy:
  image: vimagick/tinyproxy
  ports:
    - "8888:8888"
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d
```

## Client Setup

```bash
$ curl -x https://127.0.0.1:8888 https://ifconfig.co
```

[1]: https://tinyproxy.github.io/
casperjs
========

[CasperJS][1] is a navigation scripting & testing utility for PhantomJS and SlimerJS
written in Javascript.

## Quick Start

```
$ alias casperjs='docker run --rm -v `pwd`:/app vimagick/casperjs'
$ wget https://raw.githubusercontent.com/vimagick/dockerfiles/master/casperjs/sample.js
$ casperjs sample.js
CasperJS, a navigation scripting and testing utility for PhantomJS and SlimerJS
PhantomJS | PhantomJS
```

[1]: http://casperjs.org/
jenkins
=======

![](https://badge.imagelayers.io/jenkinsci/jenkins:latest.svg)

In a nutshell, [Jenkins][1] is the leading open source automation server. Built
with Java, it provides hundreds of plugins to support building, testing,
deploying and automation for virtually any project.

## docker-compose.yml

```yml
jenkins:
  image: jenkinsci/jenkins
  ports:
    - "8080:8080"
    - "50000:50000"
  volumes:
    - ./data:/var/jenkins_home
  restart: always
```

## up and running

```
$ cd ~/fig/jenkins
$ mkdir -p data
$ sudo chown 1000 data
$ docker-compose up -d
$ docker-compose exec jenkins bash
>>> cat ~/secrets/initialAdminPassword
******
>>> ssh-keygen
>>> cat ~/.ssh/id_rsa.pub
......
>>> exit
$ docker-compose exec --user root jenkins apk add -U git
$ firefox http://localhost:8080/
```

[1]: http://jenkins-ci.org/
jenkins-arm
===========

WARNING: TOO SLOW, DO NOT USE!
zoneminder
==========

![](https://badge.imagelayers.io/vimagick/zoneminder:latest.svg)

[ZoneMinder][1] is a full-featured, open source, state-of-the-art video
surveillance software system. Monitor your home, office, or wherever you want.
Using off the shelf hardware with any camera, you can design a system as large
or as small as you need.

## docker-compose.yml

```yaml
zoneminder:
  image: vimagick/zoneminder
  ports:
    - "127.0.0.1:8080:80"
  links:
    - mysql
  volumes:
    - ./fcgiwrap:/etc/default/fcgiwrap
  restart: always

mysql:
  image: mysql
  volumes:
    - ./mysql/my.cnf:/etc/mysql/conf.d/my.cnf
    - ./mysql:/docker-entrypoint-initdb.d
    - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime
  environment:
    - MYSQL_ROOT_PASSWORD=root
  restart: always
```

> - `timezone = Asia/Shanghai` was hard-coded in Dockerfile.
> - Make sure two containers have the same timezone.

## fcgiwrap

```bash
FCGI_CHILDREN=4
```

## /etc/nginx/sites-enabled/default

```
server {
    listen 80;
    server_name zm.easypi.info;
    location = / {
      return 301 /zm/;
    }
    location /zm/ {
        auth_basic "Restricted";
        auth_basic_user_file /etc/nginx/htpasswd;
        proxy_pass http://127.0.0.1:8080;
    }
}
```

## /etc/nginx/htpasswd

```
user:$apr1$zLX30Ahb$S0pZUiZW676E0gTplhpie0
```

## up and running

```bash
$ docker-compose up -d
```

- Open <http://zm.easypi.info/>
- Add New Monitor
  - Click Presets: 640x480, mpjpeg
  - Change Source
    - Remote Host Name: user:pass@x.x.x.x
    - Remote Host Port: 8080
    - Remote Host Path: /?action=stream
  - Save
- Run State: Stopped -> Running
- Click `Monitor-1`

> I'm running `mjpg-streamer` on OpenWrt.

## references

- <https://wiki.zoneminder.com/Ubuntu_Server_16.04_64-bit_with_Zoneminder_1.29.0_the_easy_way>
- <https://chiralsoftware.com/content/zoneminder-nginx-yes-it-works>
- <https://forums.zoneminder.com/viewtopic.php?p=55482>
- <https://dev.mysql.com/doc/refman/5.7/en/option-files.html>
- <http://www.htaccesstools.com/htpasswd-generator/>

[1]: https://www.zoneminder.com/
vsftpd
======

[vsftpd][1] is a GPL licensed FTP server for UNIX systems, including Linux.
It is secure and extremely fast. It is stable. Don't take my word for it, though.

## Directory Tree

```
~/fig/vsftpd/
├── docker-compose.yml
├── ftp/
│   └── README
├── pam.d/
│   └── vsftpd          => For Virutal User
└── vsftpd/
    ├── passwd          => For Virtual User
    ├── vsftpd.conf
    └── vsftpd.pem      => For SSL
```

## vsftpd/vsftpd.conf

```bash
# DEFAULT SETTINGS
allow_writeable_chroot=YES
anonymous_enable=YES
chroot_local_user=YES
connect_from_port_20=YES
dirmessage_enable=YES
ftpd_banner=Welcome to VSFTPD service.
listen=YES
local_enable=YES
no_anon_password=YES
pasv_addr_resolve=YES
pasv_address=my-ftp-server # <== PLEASE CHANGE THIS
pasv_enable=YES
pasv_max_port=30010
pasv_min_port=30000
port_enable=YES
seccomp_sandbox=NO
write_enable=YES
xferlog_enable=YES

# VIRTUAL USER SETTINGS
#guest_enable=YES
#guest_username=virtual
#local_root=/home/virtual/$USER
#pam_service_name=vsftpd
#user_sub_token=$USER
#virtual_use_local_privs=YES

# SSL SETTINGS
#force_local_data_ssl=YES
#force_local_logins_ssl=YES
#rsa_cert_file=/etc/vsftpd/vsftpd.pem
#rsa_private_key_file=/etc/vsftpd/vsftpd.pem
#ssl_enable=YES
```

> Please set `pasv_address` to your ftp server.

## pam.d/vsftpd

```
auth required pam_pwdfile.so pwdfile=/etc/vsftpd/passwd
account required pam_permit.so
```

## docker-compose.yml

```yaml
vsftpd:
  image: vimagick/vsftpd
  net: host
# ports:
#   - "20:20"
#   - "21:21"
#   - "30000-30010:30000-30010"
  volumes:
    - ./vsftpd:/etc/vsftpd
    - ./ftp:/var/lib/ftp
#   - ./pam.d/vsftpd:/etc/pam.d/vsftpd
#   - ./virtual:/home/virtual
  privileged: true
  restart: always
```

> You can use `ports` instead of `net: host`.
> Make sure these ports are allowed by firewall.

## Server

```bash
$ cd ~/fig/vsftpd/
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout vsftpd/vsftpd.pem -out vsftpd/vsftpd.pem
$ echo "tom:$(openssl passwd -1 uzia9Tu6)" >> vsftpd/passwd
$ echo "ftp's home" > ./ftp/README
$ docker-compose up -d
$ docker exec -it vsftpd_vsftpd_1 sh
>>>
>>> adduser kev
Changing password for kev
New password: ******
Retype password: ******
Password for kev changed by root
>>> echo "kev's home" > ~kev/README
>>>
>>> mkdir ~virtual/tom
>>> echo "tom's home" > ~virtual/tom/README
>>> chown -R virutal:virtual ~virtual
>>>
>>> exit
```

> I added a local user called `kev`, a virtual user called `tom` here.  
> You can edit [/etc/vsftpd/vsftpd.conf][2] to enable more [functions][3].

## Client

You can login as `kev`(local user), `tom`(virtual user) or `ftp`(anonymous user).

```bash
$ ftp my-ftp-server
Connected to my-ftp-server.
220 Welcome to VSFTPD service.
Name (my-ftp-server:kev): ftp
230 Login successful.
Remote system type is UNIX.
Using binary mode to transfer files.

ftp> verbose off
Verbose mode off.

ftp> ls
-rw-r--r--    1 0        0               0 Jan 31 15:06 README.md

ftp> get README.md
     0        0.00 KiB/s

ftp> !cat README.md

ftp> put README.md
Permission denied.

ftp> bye
```

Only local user or virtual user can upload file.

```bash
$ lftp
lftp :~> set ssl:verify-certificate no
lftp :~> open tom@my-ftp-server
Password: ******
lftp root@my-ftp-server:~> put README.md
lftp root@my-ftp-server:~> ls
-rw-------    1 0        0             337 Jan 31 16:26 README.md
lftp root@my-ftp-server:~> bye
```

[1]: https://security.appspot.com/vsftpd.html
[2]: http://vsftpd.beasts.org/vsftpd_conf.html
[3]: https://wiki.archlinux.org/index.php/Very_Secure_FTP_Daemon
[4]: https://github.com/tiwe-de/libpam-pwdfile
[5]: http://linux.die.net/man/8/pam_listfile
drone
=====

[Drone][1] is a Continuous Integration platform built on Docker, written in Go.

## github

```yaml
#
# Github » Settings » Applications » Developer applications » Register new application
#
Application name: drone
Homepage URL: http://drone.easypi.info/
Application description: Drone is a Continuous Integration platform built on Docker, written in Go
Authorization callback URL: http://drone.easypi.info/authorize
Client ID: ... (generated by github)
Client Secret: ... (generated by github)
```

## docker-compose.yml

```yaml
drone:
  image: drone/drone
  ports:
    - "8000:8000"
  volumes:
    - ./drone:/var/lib/drone
    - /var/run/docker.sock:/var/run/docker.sock
  environment:
    - REMOTE_DRIVER=github
    - REMOTE_CONFIG=https://github.com?client_id=...&client_secret=...
#   - REMOTE_DRIVER=gogs
#   - REMOTE_CONFIG=https://git.easypi.info/?open=false
    - DEBUG=false
  restart: always

wall:
  image: drone/drone-wall
  ports:
    - "8080:80"
  restart: always
```

> Drone will register gogs webhooks automatically, you don't need to do it manually.

## nginx/sites-enabled/drone

```
server {
    listen 80;
    server_name drone.easypi.info;
    location / {
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header Host $http_host;
        proxy_set_header Origin "";

        proxy_pass http://127.0.0.1:8000;
        proxy_redirect off;
        proxy_http_version 1.1;
        proxy_buffering off;

        chunked_transfer_encoding off;
    }
}

server {
    listen 80;
    server_name wall.easypi.info;
    location / {
        proxy_pass http://127.0.0.1:8080;
    }
}
```

## up and running

```bash
# server
$ cd ~/fig/drone/
$ docker-compose up -d
$ docker-compose logs

# client (login with remote driver credential)
$ firefox http://drone.easypi.info/

# dashboard
$ firefox http://wall.easypi.info/
```

[1]: http://readme.drone.io/usage/overview/
drone-arm
=========

```yaml
drone:
  image: armdrone/drone
  ports:
    - "8000:80"
  volumes:
    - ./data:/var/lib/drone
    - /var/run/docker.sock:/var/run/docker.sock
  environment:
    - REMOTE_DRIVER=gogs
    - REMOTE_CONFIG=http://git.easypi.info:3000/?open=false
    - PLUGIN_FILTER=armdrone/*
    - GIN_MODE=release
  extra_hosts:
    - git.easypi.info:192.168.31.231
  restart: always
```

## Deploy Key Setup

```yaml
drone:
    Settings > Public Key: ssh-rsa ...

gogs:
    Settings > Add Deploy Key:
        Title: drone
        Content: ssh-rsa ...
```

## Server Setup

```
$ cat /boot/cmdline.txt
$ sed -i '1s/$/ cgroup_enable=cpuset/' /boot/cmdline.txt
$ reboot
```

## Client Setup

```
# install drone-cli
$ curl http://downloads.drone.io/drone-cli/drone_darwin_amd64.tar.gz | tar zx -C /usr/local/bin/
$ export DRONE_SERVER=https://drone.easypi.info
$ export DRONE_TOKEN=xxxxxxxx

# create .drone.yml
$ git clone git@git.easypi.info:EasyPi/docker-shadowsocks-libev.git
$ cd docker-shadowsocks-libev
$ git remote add github git@github.com:EasyPi/docker-shadowsocks-libev.git
$ git remote add bitbucket git@bitbucket.org:EasyPi/docker-shadowsocks-libev.git
$ vi -p .drone.yml secrets.yml
$ echo 'secrets.yml' >> .gitignore
$ drone secure --repo EasyPi/docker-shadowsocks-libev --in secrets.yml
$ git add .gitignore .drone.yml .drone.sec
$ git commit -m 'Add .drone.yml'
$ git push -u origin master
$ git push -u github master
$ git push -u bitbucket master
```

> You need to re-generate `.drone.sec` after editing `.drone.yml`.

File: .drone.yml

```yaml
publish:
  docker:
#   environment:
#       - DOCKER_LAUNCH_DEBUG=true
    username: $$DOCKER_USER
    password: $$DOCKER_PASS
    email: $$DOCKER_EMAIL
    file: Dockerfile.arm
    repo: easypi/shadowsocks-libev-arm
    tag:
      - latest
      - 2.4.8
```

File: secrets.yml

```yaml
environment:
  DOCKER_USER: noreply
  DOCKER_PASS: ********
  DOCKER_EMAIL: noreply@easypi.info
```

## read more

- <http://readme.drone.io/setup/overview/>
- <http://readme.drone.io/plugins/>
- <http://readme.drone.io/devs/cli/>
- <https://github.com/drone-plugins/drone-docker/blob/master/DOCS.md>
- <https://gist.github.com/philipz/1e7a36560700fdc1ad63>
- <http://a.frtzlr.com/kubernetes-on-raspberry-pi-3-the-missing-troubleshooting-guide/>
grafana
=======

![](https://badge.imagelayers.io/vimagick/grafana:latest.svg)

[Grafana][1] is a leading open source application for visualizing large-scale
measurement data.

## docker-compose.yml

```yaml
grafana:
  image: grafana/grafana
  ports:
    - "3000:3000"
  volumes:
    - ./data:/var/lib/grafana
  environment:
    - GF_SERVER_ROOT_URL=http://grafana.example.com:3000/
    - GF_SECURITY_ADMIN_USER=admin
    - GF_SECURITY_ADMIN_PASSWORD=admin
    - GF_SMTP_ENABLED=true
    - GF_SMTP_HOST=smtp.gmail.com:587
    - GF_SMTP_USER=grafana@example.com
    - GF_SMTP_FROM_ADDRESS=grafana@example.com
    - GF_SMTP_PASSWORD=******
  restart: always
```

## up and running

```bash
$ mkdir data
$ chmod 777 data
$ docker-compose up -d
$ docker-compose exec grafana bash
>>> cat /etc/grafana/grafana.ini
>>> grafana-cli plugins install grafana-worldmap-panel
>>> exit
$ docker-compose restart
$ curl http://localhost:3000/
```

[1]: http://grafana.org/
django-cms
==========

[django CMS][1] is a modern web publishing platform built with Django, the web
application framework “for perfectionists with deadlines”.

### docker-compose.yml

```yaml
django-cms:
  image: vimagick/django-cms
  ports:
    - "8000:80"
  restart: always
```

### up and running

```bash
$ docker-compose up -d

$ docker-compose exec django-cms sh
/app # ./manage.py createsuperuser
Username (leave blank to use 'root'): admin
Email address: admin@easypi.info
Password: ******
Password (again): ******
Superuser created successfully.
/app # exit

$ firefox http://easypi.info:8000
```

[1]: https://www.django-cms.org/en/
twistd-mail
===========

An SMTP / POP3 email server plugin for twistd.

## docker-compose.yml

```
tmail:
  image: vimagick/tmail
  ports:
    - "25:25"
    - "110:110"
  volumes:
    - mail:/var/mail
  environment:
    - MAIL_NAME=domain.org
    - MAIL_PATH=/var/mail
    - MAIL_USER=user
    - MAIL_PASS=pass
  restart: always
```

> You can pass additional arguments via environment variable `MAIL_OPTS`:
>> `MAIL_OPTS=--user=admin=admin`

## server

```
$ docker-compose up -d
```

## client

- POP3: domain.org:110
- USER: user@domain.org
- PASS: pass

## todo

- [x] receive email via pop3
- [ ] send email via smtp

routersploit
============

The Router Exploitation Framework.
badvpn
======

## Build Binaries

```bash
apt-get install build-essential cmake
wget https://github.com/ambrop72/badvpn/archive/1.999.130.tar.gz
tar xzf 1.999.130.tar.gz
cd badvpn-1.999.130
mkdir build
cd build
cmake -DBUILD_NOTHING_BY_DEFAULT=1 -DBUILD_TUN2SOCKS=1 -DBUILD_UDPGW=1 ..
make install
ls /usr/local/bin/
```

## Server Setup

```bash
badvpn-udpgw --listen-addr 0.0.0.0:7300
```

## Client Setup

```bash
systemctl start shadowsocks-libev@jp

ip tuntap add dev tun0 mode tun user root
ip addr add 10.0.0.1/24 dev tun0
ip link set tun0 up

badvpn-tun2socks --tundev tun0 \
                 --netif-ipaddr 10.0.0.2 \
                 --netif-netmask 255.255.255.0 \
                 --socks-server-addr 127.0.0.1:1080 \
                 --udpgw-remote-server-addr 45.32.57.113:7300

ip route add 45.32.57.113 via 192.168.31.1
ip route add 0.0.0.0/1 via 10.0.0.2
ip route add 128.0.0.0/1 via 10.0.0.2
```

## Client Test

```bash
# ICMP (Failed)
ping ifconfig.co

# TCP/UDP (Success)
curl ifconfig.co
```

## References

- <https://code.google.com/archive/p/badvpn/wikis/tun2socks.wiki>
hubot
=====

![](https://badge.imagelayers.io/vimagick/hubot:latest.svg)

[Hubot][1] is a customizable, life embetterment robot commissioned by github.
Hubot's power comes through [scripts][2].

## docker-compse.yml

```yaml
hubot:
  image: vimagick/hubot
  ports:
    - "8080:8080"
  volumes:
    - ./data:/home/hubot/scripts
  environment:
    - HUBOT_SLACK_TOKEN=xoxb-xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxx
    - HUBOT_AUTH_ADMIN=UXXXXXXXX
  restart: always
```

- Click [this][3] to generate `HUBOT_SLACK_TOKEN`.
- Click [this][4] to get `HUBOT_AUTH_ADMIN`.

## up and running

```bash
$ docker-compose up -d
$ docker-compose exec hubot sh
>>> npm install --save hubot-auth
>>> vi external-scripts.json
>>> exit
$ vi data/example.coffee
$ docker-compose restart
```

```coffee
module.exports = (robot) ->

  robot.respond /who is @?([\w .\-]+)\?*$/i, (res) ->
    name = res.match[1].trim()
    users = robot.brain.usersForFuzzyName(name)
    for user in users
      delete user['slack']
      user = JSON.stringify user, null, 2
      res.send "```#{user}```"

  robot.respond /shell:?\s+(.+)/i, (res) ->
    cmd = res.match[1]
    user = robot.brain.userForName(res.message.user.name)
    if robot.auth.hasRole(user, ["ops"])
      cp = require "child_process"
      cp.exec cmd, (error, stdout, stderr) ->
        res.send "```#{stdout}```" if stdout
        res.send "```#{stderr}```" if stderr
    else
      res.reply "Access Denied!"
```

## chat-ops

```
kevin [5:25 PM] what roles do I have
hubot [5:25 PM] kevin has the following roles: admin.

kevin [5:24 PM] who is kevin
hubot [5:24 PM] { "id": "UXXXXXXXX", "name": "kevin" }

kevin [5:26 PM] kevin has ops role
hubot [5:26 PM] OK, kevin has the 'ops' role.

kevin [5:27 PM] shell date
hubot [5:27 PM] Sat Jan 21 10:08:28 UTC 2017

kevin [5:28 PM] kevin doesn't have ops role
hubot [5:28 PM] OK, kevin doesn't have the 'ops' role.

kevin [5:29 PM] shell date
hubot [5:29 PM] Access Denied!
```

[1]: https://hubot.github.com/
[2]: https://hubot.github.com/docs/scripting/
[3]: https://my.slack.com/services/new/hubot
[4]: https://api.slack.com/methods/users.list/test
openconnect
===========

![](https://badge.imagelayers.io/vimagick/openconnect:latest.svg)

[OpenConnect][1] is an SSL VPN client initially created to support Cisco's
AnyConnect SSL VPN. It has since been ported to support the Juniper SSL VPN
which is now known as Pulse Connect Secure.

## docker-compose.yml

```yaml
openconnect:
  image: vimagick/openconnect
  command: https://vpn.easypi.info:4443
  net: host
  volumes:
    - ./data:/etc/openconnect
  stop_signal: SIGINT
  privileged: yes
```

## up and running

```bash
$ cd ~/fig/openconnect/
$ tree
.
├── data/
│   ├── certs/
│   │   └── client.p12
│   └── openconnect.conf
├── docker-compose.yml
├── Dockerfile
└── README.md

$ cd ./data/certs/
$ openssl pkcs12 -in client.p12 -nodes -cacerts -out ca-cert.pem
$ openssl pkcs12 -in client.p12 -nodes -clcerts -out client-cert.pem
$ openssl pkcs12 -in client.p12 -nodes -nocerts -out client-key.pem

$ docker-compose up -d
$ docker-compose logs -f

$ ip link show
$ ip addr show
$ ip route show

$ curl ifconfig.co
$ curl ifconfig.ovh
$ curl ifconfig.me
```
[1]: http://www.infradead.org/openconnect/index.html
monit
=====

![](https://badge.imagelayers.io/vimagick/monit:latest.svg)

[Monit][1] is a utility for managing and monitoring processes, programs, files,
directories and filesystems on a Unix system.

## directory tree

```
~/fig/monit/
├── docker-compose.yml
└── data/
    ├── conf.d/
    │   ├── network.cfg
    │   ├── setting.cfg
    │   └── system.cfg
    └── monitrc
```

> Make sure config file endswith `.cfg`.

## docker-compose.yml

```yaml
monit:
  image: vimagick/monit
  volumes:
    - ./data:/etc/monit
  pid: host
  net: host
  restart: always
```

> The control file `/etc/monit/monitrc` must have permissions no more than
> `-rwx------ (0700)`

## Server Setup

```bash
$ cd ~/fig/monit/
$ docker-compose up -d
$ docker exec monit_monit_1 monit status
```

## Client Setup

```bash
$ firefox http://admin:monit@server:2812
```

[1]: https://www.mmonit.com/monit/documentation/monit.html
etcd-arm
========

![](https://badge.imagelayers.io/vimagick/etcd-arm:latest.svg)

![](https://coreos.com/assets/images/media/Fleet-Scheduling.png)

## Cross compilation for Raspberry Pi

```
git clone https://github.com/coreos/etcd.git
cd etcd
# ARMv6
GOOS=linux GOARCH=arm GOARM=6 ./build
scp bin/* pi1:/usr/local/bin
# ARMv7
GOOS=linux GOARCH=arm GOARM=7 ./build
scp bin/* pi2:/usr/local/bin
```
cowrie
======

![](https://badge.imagelayers.io/vimagick/cowrie:latest.svg)

[`Cowrie`][1] is a medium interaction SSH honeypot designed to log brute force attacks
and, most importantly, the entire shell interaction performed by the attacker.

`Cowrie` is directly based on [`Kippo`][2] by Upi Tamminen (desaster).

## docker-compose.yml

```
cowrie:
  image: vimagick/cowrie
  ports:
    - "2222:2222"
  volumes:
    - ./dl:/home/cowrie/dl
    - ./log:/home/cowrie/log
  restart: always
```

## server

```
$ cd ~/fig/cowrie
$ mkdir -p dl log/tty
$ chmod -R 777 dl log
$ tree -F
.
├── docker-compose.yml
├── dl/
└── log/
    └── tty/
$ docker-compose up -d
$ tail -f log/cowrie.log
```

## client

```
$ ssh -p 2222 root@server
```

> You can login as `root` with any password except `root` or `123456`.

[1]: https://github.com/micheloosterhof/cowrie
[2]: http://github.com/desaster/kippo/
live555
=======

![](https://badge.imagelayers.io/vimagick/live555:latest.svg)

The [LIVE555 Media Server][1] is a complete RTSP server application. It can
stream several kinds of media file (which must be stored in the current working
directory - i.e., the directory from which you launch the application - or a
subdirectory.)

## docker-compose.yml

```yaml
live555:
  image: vimagick/live555
  ports:
    - "8080:80"
    - "554:554"
  volumes:
    - ./data:/data
  restart: always
```

[1]: http://www.live555.com/mediaServer/
audiowaveform
=============

![](https://badge.imagelayers.io/vimagick/audiowaveform:latest.svg)

**audiowaveform** is a C++ command-line application that generates waveform data
from either MP3, WAV, or FLAC format audio files. Waveform data can be used to
produce a visual rendering of the audio, similar in appearance to audio editing
applications.

### Run

    $ docker pull vimagick/audiowaveform
    $ alias awf='docker run --rm -v `pwd`:/work -w /work vimagick/audiowaveform'
    $ awf -i input.mp3 -o output.png

## Command line options

**audiowaveform** accepts the following command-line options:

| Short           | Long                           | Description                                                                                                   |
| --------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------------- |
|                 | `--help`                       | Show help message                                                                                             |
| `-v`            | `--version`                    | Show version information                                                                                      |
| `-i <filename>` | `--input-filename <filename>`  | Input mono or stereo audio (.wav or .mp3) or waveform data (.dat) file name                                   |
| `-o <filename>` | `--output-filename <filename>` | Output waveform data (.dat or .json), audio (.wav), or PNG image (.png) file name                             |
| `-z <level>`    | `--zoom <zoom>`                | Zoom level (samples per pixel), default: 256. Not valid if `--end` or `--pixels-per-second` is also specified |
|                 | `--pixels-per-second <zoom>`   | Zoom level (pixels per second), default: 100. Not valid if `--end` or `--zoom` is also specified              |
| `-b <bits>`     | `--bits <bits>`                | Number of bits resolution when creating a waveform data file (either 8 or 16), default: 16                    |
| `-s <seconds>`  | `--start <seconds>`            | Start time (seconds), default: 0                                                                              |
| `-e <seconds>`  | `--end <seconds>`              | End time (seconds). Not valid if `--zoom` is also specified                                                   |
| `-w <width>`    | `--width <width>`              | Width of output image (pixels), default: 800                                                                  |
| `-h <height>`   | `--height <height>`            | Height of output image (pixels), default: 250                                                                 |
| `-c <scheme>`   | `--colors <scheme>`            | Color scheme of output image (either 'audition' or 'audacity'), default: audacity                             |
|                 | `--border-color <color>`       | Border color (in rrggbb\[aa\] hex format), default: set by `--colors` option                                  |
|                 | `--background-color <color>`   | Background color (in rrggbb\[aa\] hex format), default: set by `--colors` option                              |
|                 | `--waveform-color <color>`     | Waveform color (in rrggbb\[aa\] hex format), default: set by `--colors` option                                |
|                 | `--axis-label-color <color>`   | Axis label color (in rrggbb\[aa\] hex format), default: set by `--colors` option                              |
|                 | `--no-axis-labels`             | Render PNG images without axis labels                                                                         |
|                 | `--with-axis-labels`           | Render PNG images with axis labels (default)                                                                  |

rsyncd
======

![](https://badge.imagelayers.io/vimagick/rsyncd:latest.svg)

[rsync][1] is an open source utility that provides fast incremental file transfer.
[rsyncd][2] is rsync in daemon mode.

## docker-compose.yml

```yaml
rsyncd:
  image: vimagick/rsyncd
  ports:
    - "873:873"
  volumes:
#   - ./rsyncd.conf:/etc/rsyncd.conf
    - ./share:/share
  restart: always
```

> You can mount `rsyncd.conf` to override the default one.

## rsyncd.conf (default)

```
[global]
charset = utf-8
max connections = 8
reverse lookup = no

[share]
path = /share
read only = yes
#hosts allow = 192.168.0.0/16
#auth users = *
#secrets file = /etc/rsyncd.secrets
#strict modes = false
```

## server

```
docker-compose up -d
```

## client

```
rsync -avz easypi.info::share /path/to/folder
```

[1]: https://rsync.samba.org
[2]: https://download.samba.org/pub/rsync/rsyncd.conf.html
nullmailer
==========

![](https://badge.imagelayers.io/vimagick/nullmailer:latest.svg)

[nullmailer][1] is a simple relay-only mail transport agent.

## Directory Tree

```
~/fig/nullmailer/
├── Dockerfile
└── nullmailer/
    ├── adminaddr --> username@gmail.com
    └── remotes   --> smtp.gmail.com smtp --port=465 --auth-login --user=username --pass=****** --ssl
```

> Read [this][2] to config.

## docker-compose.yml

```yaml
nullmailer:
  image: vimagick/nullmailer
  cotainer_name: nullmailer
  volumes:
    - ./nullmailer:/etc/nullmailer
  restart: always
```

## Up and Running


```bash
# start service
$ cd ~/fig/nullmailer
$ docker-compose up -d

# send email (from gmail to yahoo)
$ alias sendmail='docker exec -i nullmailer sendmail -f username@gmail.com'
$ echo -e 'Subject: hello\r\n\r\nworld' | sendmail username@yahoo.com
```

[1]: http://untroubled.org/nullmailer/
[2]: http://raspberry.znix.com/2013/03/nullmailer-on-raspberry-pi.html
portainer
=========

[Portainer][1] is an open-source lightweight management UI which allows you to easily manage your Docker host or Swarm cluster.

## docker-compose.yml

```yaml
portainer:
  image: portainer/portainer
  ports:
    - "9000:9000"
  volumes:
    - ./data:/data
    - /var/run/docker.sock:/var/run/docker.sock
  restart: always
```

[1]: http://portainer.io/
nginx
=====

![](https://badge.imagelayers.io/vimagick/nginx:latest.svg)

[Nginx][1] is an open source reverse proxy server for HTTP, HTTPS, SMTP, POP3, and
IMAP protocols, as well as a load balancer, HTTP cache, and a web server
(origin server).

## Static Website

File: docker-compose.yml

```yaml
nginx:
  image: nginx:alpine
  ports:
    - "80:80"
  volumes:
    - ./data/default.conf:/etc/nginx/default.conf
    - ./data/html:/usr/share/nginx/html
  restart: always
```

## Reverse Proxy

File: docker-compose.yml

```yaml
nginx:
  image: nginx:alpine
  volumes:
    - ./data/default.conf:/etc/nginx/conf.d/default.conf
    - ./data/ssl:/etc/nginx/ssl
    - ./data/htpasswd:/etc/nginx/htpasswd
  net: host
  restart: always
```

> Password file can be generated by:
>> `echo "username:$(openssl passwd -apr1 password)" >> data/htpasswd`

File: nginx.conf

```nginx
user  nginx;
worker_processes  4;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
    include /etc/nginx/sites-enabled/*;
}
```

File: default

```nginx
server {
    listen 80 default;
    server_name _;
    return 301 http://blog.foobar.site/;
}

server {
    listen 80;
    server_name blog.foobar.site blog.easypi.info;
    location / {
        proxy_pass http://127.0.0.1:6109;
    }
}

server {
    listen 80;
    server_name wiki.foobar.site wiki.easypi.info;
    location / {
        auth_basic restricted;
        auth_basic_user_file /etc/nginx/htpasswd;
        proxy_pass http://127.0.0.1:8000;
    }
}

server {
    listen 80;
    server_name iot.foobar.site iot.easypi.info;
    location / {
        proxy_pass http://127.0.0.1:1880;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

File: [rtmp][1]

```nginx
rtmp {
    server {
        listen 1935;
        application live {
            live on;
        }
    }
}
```

[1]: http://nginx.org/
[2]: https://github.com/arut/nginx-rtmp-module/wiki/Directives
phpBB
=====

[phpBB][1] is a free flat-forum bulletin board software solution
that can be used to stay in touch with a group of people
or can power your entire website.

This docker image support mysqli/postgres/sqlite3.
If you choose sqlite3, please use `/var/www/store/phpbb.db` as dbpath.
And it has no volumes, please use phpBB control panel to backup database.

## docker-compose.yml

```yaml
phpbb:
  image: vimagick/phpbb
  ports:
    - "8000:80"
  volumes:
    - ./data:/var/www/store
  restart: always
```

# up and running

```bash
# create volume
$ mkdir -m 777 data

# run container
$ docker-compose up -d

# setup forum
$ firefox http://localhost:8000/install

# view database
$ sqlite3 data/phpbb.db
>>> .help
>>> .quit
```

[1]: https://www.phpbb.com/
salt-master
===========

![](https://badge.imagelayers.io/vimagick/salt-master:latest.svg)

Salt is a new approach to infrastructure management. Easy enough to get running
in minutes, scalable enough to manage tens of thousands of servers, and fast
enough to communicate with them in seconds.

Salt delivers a dynamic communication bus for infrastructures that can be used
for orchestration, remote execution, configuration management and much more.

This image contains `salt-master`/`salt-ssh` and other power tools:

- curl
- httpie
- ipython
- jq
- vim-tiny

## network topology

- master
    - coreos: 192.168.1.1
- minions
    - arch: 192.168.1.100
    - debian: 192.168.1.101
    - ubuntu: 192.168.1.102

## docker-compose.yml

```
master:
  image: vimagick/salt-master
  ports:
    - "4505:4505"
    - "4506:4506"
  restart: always
```

## salt-master

```
$ cd ~/fig/salt/
$ docker-compose up -d
$ alias salt='docker exec -it salt_master_1 bash'
```

## salt-minion

```
# create roster
$ cat > /etc/salt/roster << _EOF_
arch:
    host: 192.168.1.100
    user: root
    passwd: XcAreP36
debian:
    host: 192.168.1.101
    user: root
    passwd: 0Q4yEQw7
ubuntu:
    host: 192.168.1.102
    user: root
    passwd: pc2wW6Dv
_EOF_

# deploy master key
$ salt-ssh -i --passwd xxxxxx --key-deploy '*' test.ping

# install salt-minion
$ salt-ssh '*' -r 'curl -L https://bootstrap.saltstack.com | sudo sh -s -- -A 192.168.1.1'

# list minion keys
$ salt-key -L

# accept minion keys
$ salt-key -y -A

# ping minions
$ salt '*' test.ping
```

## backup volumes

```
$ docker run --rm --volumes-from salt_master_1 -v `pwd`:/backup debian tar cvzf /backup/salt-$(date +%F).tgz /etc/salt/ /srv/salt/
$ ls -l
-rw-r--r-- 1 root root 14243 Jul  8 21:37 salt-2015-07-08.tgz
```

Polipo — a caching web proxy
============================

[Polipo][1] is a small and fast caching web proxy (a web cache, an HTTP proxy, a
proxy server). While Polipo was designed to be used by one person or a small
group of people, there is nothing that prevents it from being used by a larger
group.

## docker-compose.yml

```yaml
polipo:
  image: vimagick/polipo
  command:
    authCredentials=username:password
    socksParentProxy=1.2.3.4:9050
  ports:
    - "8123:8123"
  restart: always
```

## server

```bash
$ docker-compose up -d
```

## client

```bash
$ curl -x http://username:password@server:8123 https://www.youtube.com/
```

[1]: https://www.irif.univ-paris-diderot.fr/~jch/software/polipo/
pptpd
=====

![](https://badge.imagelayers.io/vimagick/pptpd:latest.svg)

The Point-to-Point Tunneling Protocol is a method for implementing virtual private networks.

`PPTP` uses a control channel over TCP and a GRE tunnel operating to encapsulate PPP packets.

## Directory Tree

```
~/fig/pptpd/
├── docker-compose.yml
├── pptpd.conf
├── pptpd-options
└── chap-secrets
```

file: docker-compose.yml

```yaml
pptpd:
  image: vimagick/pptpd
  volumes:
    - ./pptpd.conf:/etc/pptpd.conf
    - ./pptpd-options:/etc/ppp/pptpd-options
    - ./chap-secrets:/etc/ppp/chap-secrets
  privileged: true
  restart: always
```

file: pptpd.conf

```
option /etc/ppp/pptpd-options
pidfile /var/run/pptpd.pid
localip 192.168.127.1
remoteip 192.168.127.100-199
```

file: pptpd-options

```
name pptpd
refuse-pap
refuse-chap
refuse-mschap
require-mschap-v2
require-mppe-128
proxyarp
nodefaultroute
lock
nobsdcomp
novj
novjccomp
nologfd
ms-dns 8.8.8.8
ms-dns 8.8.4.4
```

file: chap-secrets

```
# Secrets for authentication using CHAP
# client    server  secret          IP addresses

username    *       password        *
```

> Please use strong password in `chap-secrets` file to protect your server.

## Server Setup

```bash
$ modprobe nf_conntrack_pptp nf_nat_pptp
$ cd ~/fig/pptpd/
$ docker-compose up -d
$ docker-compose logs -f
```

You need to config firewall:

- To let PPTP tunnel maintenance traffic, `allow port 1723/tcp`.
- To let PPTP tunneled data to pass through router, `allow proto gre`.
- Set `DEFAULT_FORWARD_POLICY=ACCEPT`
- Set `net.ipv4.ip_forward=1` (sysctl)

## Client Setup

Connect PPTP server using `username:password` with `mschap-v2/mppe-128` encyption.

## References

- <https://wiki.archlinux.org/index.php/PPTP_server>
- <https://wiki.archlinux.org/index.php/PPTP_Client>
revive
======

The Free Open Source Ad Server formerly known as OpenX Source.
ocserv
======

![](https://badge.imagelayers.io/vimagick/ocserv:latest.svg)

[OpenConnect server][1] (ocserv) is an SSL VPN server. Its purpose is to be a
secure, small, fast and configurable VPN server.

## docker-compose.yml

```yaml
ocserv:
  image: vimagick/ocserv
  ports:
    - "4443:443/tcp"
    - "4443:443/udp"
  environment:
    - VPN_DOMAIN=vpn.easypi.info
    - VPN_NETWORK=10.20.30.0
    - VPN_NETMASK=255.255.255.0
    - LAN_NETWORK=192.168.0.0
    - LAN_NETMASK=255.255.0.0
    - VPN_USERNAME=username
    - VPN_PASSWORD=password
  cap_add:
    - NET_ADMIN
  restart: always
```

> - :warning: Please choose a strong password to protect VPN service.
> - These environment variables are used to generate config files/keys.
> - VPN accounts can be managed via [ocpasswd][2] command.
> - VPN status can be viewed via `occtl` command
> - You can edit the config file [/etc/ocserv/ocserv.conf][3], then restart service.

## up and running

```bash
$ docker-compose up -d
$ docker-compose exec ocserv bash
>>> cd /etc/ocserv/
>>> echo 'no-route = 1.2.3.4/32' >> ocserv.conf
>>> ocpasswd -c ocpasswd username
    Enter password: ******
    Re-enter password: ******
>>> exit
$ docker-compose restart
$ docker cp ocserv_ocserv_1:/etc/ocserv/certs/client.p12 .
$ docker cp ocserv_ocserv_1:/etc/ocserv/certs/server-cert.pem .
$ docker-compose logs -f
```

To remove the password protection of `client.p12`:

```bash
mv client.p12 client.p12.orig
openssl pkcs12 -in client.p12.orig -nodes -out tmp.pem
openssl pkcs12 -export -in tmp.pem -out client.p12 -passout pass:
rm tmp.pem
```

> :warning: Apple's Keychain Access will refuse to open it with no passphrase.

## mobile client

There are two auth types:

- :-1: passwd: type everytime
- :+1: certificate: import once

```
AnyConnect ->
  Connection ->
    Add New VPN Connection... ->
      Advanced Preferences... ->
        Certificate ->
          Import ->
            File System: client.p12
```

> :question: Android client show warning dialog: `Certificate is not yet valid.` ([WHY?][4])

## desktop client

[download](https://www.cellsystech.com/software/anyconnect/)

`client.p12` and `server-cert.pem` can be imported into keychain.


[1]: http://www.infradead.org/ocserv/
[2]: http://www.gnutls.org/manual/html_node/certtool-Invocation.html
[3]: http://www.infradead.org/ocserv/manual.html
[4]: http://www.cisco.com/c/en/us/td/docs/security/vpn_client/anyconnect/anyconnect31/release/notes/anyconnect31rn.html
vnc2flv
=======

[Vnc2flv][1] is a cross-platform screen recording tool for UNIX, Windows or
Mac. It captures a VNC desktop session (either your own screen or a remote
computer) and saves as a Flash Video (FLV) file.

This docker image works very well with [selenium/standalone-firefox-debug][2].

## docker-compose.yml

```yaml
vnc2flv:
  image: vimagick/vnc2flv
  command: -P /pwdfile -o record.flv localhost 5900
  volumes:
    - ./pwdfile:/pwdfile
    - ./data:/data
  net: host
  stop_signal: SIGINT
```

## Up and Running

```bash
# Create passwd file
$ echo secret > pwdfile

# Start recording
$ docker-compose up -d

# Stop recording
$ docker-compose stop

# View logs
$ docker-compose logs
Attaching to vnc2flv_vnc2flv_1
vnc2flv_1  | start recording
vnc2flv_1  | stop recording
```

[1]: http://www.unixuser.org/~euske/python/vnc2flv/index.html
[2]: https://hub.docker.com/r/selenium/standalone-firefox-debug/
netdata
=======

[netdata][1] is a highly optimized Linux daemon providing real-time performance
monitoring for Linux systems, Applications, SNMP devices, over the web!

[1]: https://github.com/firehol/netdata
tutum/builder
=============

A docker image that builds, tests and pushes docker images from code repositories.

## docker-compose.yml

```
builder:
  image: tutum/builder
  volumes:
    - ~/.docker:/.docker:ro
  environment:
    - GIT_REPO=https://github.com/vimagick/dockerfiles.git
    - DOCKERFILE_PATH=/test
    - IMAGE_NAME=vimagick/test:latest
  privileged: true
```

- If `IMAGE_NAME` not specified, it will be built and tested, but not pushed.
- [Hooks][1] in `$DOCKERFILE_PATH/hooks/` will be executed before and after each step.

## build docker image

```
$ cd ~/fig/builder/
$ docker login
$ docker-compose run --rm -e DOCKERFILE_PATH=/tinc -e IMAGE_NAME=vimagick/tinc builder
$ docker search vimagick/tinc
```

[1]: https://github.com/tutumcloud/builder#hooks
clamav
======

![](https://badge.imagelayers.io/vimagick/clamav:latest.svg)

[ClamAV®][1] is an open source antivirus engine for detecting trojans, viruses,
malware & other malicious threats.

## Up and Running

```
# create a function
function av() {
   local DIR=${1:-$PWD}
   docker run --rm -v "$DIR:/data" --name clamav vimagick/clamav -i -r
}

# scan a directory
av ~/Downloads

# ----------- SCAN SUMMARY -----------
# Known viruses: 4297635
# Engine version: 0.99
# Scanned directories: 1
# Scanned files: 3
# Infected files: 0
# Data scanned: 0.02 MB
# Data read: 0.01 MB (ratio 2.00:1)
# Time: 11.623 sec (0 m 11 s)
```

[1]: http://www.clamav.net/
youtube-worker
==============

`youtube-worker` = `youtube-dl` + `redis`

## docker-compose.yml

```
worker:
  image: vimagick/youtube-worker
  links:
    - redis
  volumes:
    - data:/data
  environment:
    - DATABASE=1
    - PASSWORD=secret-passwd
    - FORMAT=worst
    - OUTTMPL=%(id)s.%(ext)s
  restart: always

redis:
  image: redis
  command: redis-server --requirepass 'secret-passwd'
  ports:
    - "6379:6379"
  restart: always
```

> [format-selection](https://github.com/rg3/youtube-dl#format-selection).

## server

```
$ cd ~/fig/youtube/

$ docker-compose up -d

$ docker-compose logs
Attaching to youtube_worker_1
worker_1 | 2015-07-12T17:50:02 [INFO] connect redis
worker_1 | 2015-07-12T17:50:06 [INFO] process: os6U77Hhm_s
worker_1 | [youtube] os6U77Hhm_s: Downloading webpage
worker_1 | [youtube] os6U77Hhm_s: Downloading video info webpage
worker_1 | [youtube] os6U77Hhm_s: Extracting video information
worker_1 | [youtube] os6U77Hhm_s: Downloading DASH manifest
worker_1 | [youtube] os6U77Hhm_s: Downloading DASH manifest
worker_1 | [info] Writing video description metadata as JSON to: os6U77Hhm_s.info.json
worker_1 | [download] Destination: os6U77Hhm_s.mp4
[download] 100% of 11.03MiB in 00:0014MiB/s ETA 00:001nown ETA
worker_1 | 2015-07-12T17:50:07 [INFO] success: True
```

## client

```
$ redis-cli -h server -n 1 -a 'secret-passwd'
server[1]> lpush pending os6U77Hhm_s
server[1]> keys *
1) "pending"
server[1]> keys *
1) "running"
server[1]> keys *
1) "finished"
server[1]> quit

$ rsync -ahP user@server:fig/youtube/data
receiving file list ...
2 files to consider
drwxr-xr-x          74 2015/07/13 01:50:07 data
-rw-r--r--       19722 2015/06/15 17:19:16 data/os6U77Hhm_s.info.json
-rw-r--r--    11569834 2015/06/15 17:19:16 data/os6U77Hhm_s.mp4

sent 16 bytes  received 116 bytes  29.33 bytes/sec
total size is 11.78M  speedup is 87650.26
```
youtube-dl
==========

[`youtube-dl`][1] is a small command-line program to download videos from
<https://www.youtube.com/> and a few more sites.

## Tutorial

```bash
# create an alias
$ alias yt='docker run --rm -u $(id -u):$(id -g) -v $PWD:/data vimagick/youtube-dl'

# list all formats
$ yt -F nVjsGKrE6E8

# download it
$ yt nVjsGKrE6E8

# play it
$ vlc *-nVjsGKrE6E8.mp4
```

[1]: https://rg3.github.io/youtube-dl/
collectd
========

`collectd` is a daemon which collects system performance statistics periodically
and provides mechanisms to store the values in a variety of ways, for example
in RRD files.

## directory tree

```
~/fig/collectd/
├── data/
│   ├── collectd.conf
│   └── conf.d/
│       └── network.conf
└── docker-compose.yml
```

## config files

collectd.conf

```apache
Hostname "localhost"

FQDNLookup false
Interval 10
Timeout 2
ReadThreads 5
WriteThreads 5

LoadPlugin cpu
LoadPlugin interface
LoadPlugin load
LoadPlugin memory

Include "/etc/collectd/conf.d/*.conf"
```

network.conf

```apache
LoadPlugin network

<Plugin "network">
  Server "influxdb" "25826"
</Plugin>
```

weather.conf

```apache
LoadPlugin curl_json

<Plugin curl_json>
  <URL "https://query.yahooapis.com/v1/public/yql?q=select%20item.condition%20from%20weather.forecast%20where%20woeid%3D2151330%20and%20u%3D'c'&format=json">
    Instance "Beijing"
    <Key "query/results/channel/item/condition/temp">
      Type "gauge"
    </Key>
  </URL>
  <URL "https://query.yahooapis.com/v1/public/yql?q=select%20item.condition%20from%20weather.forecast%20where%20woeid%3D2151849%20and%20u%3D'c'&format=json">
    Instance "Shanghai"
    <Key "query/results/channel/item/condition/temp">
      Type "gauge"
    </Key>
  </URL>
  <URL "https://query.yahooapis.com/v1/public/yql?q=select%20item.condition%20from%20weather.forecast%20where%20woeid%3D23511745%20and%20u%3D'c'&format=json">
    Instance "Silicon_Valley"
    <Key "query/results/channel/item/condition/temp">
      Type "gauge"
    </Key>
  </URL>
</Plugin>
```

## docker-compose.yml

```
collectd:
  image: vimagick/collectd
  volumes:
    - ./data:/etc/collectd
  pid: host
  net: host
  restart: always
```
hugo
====

![](https://badge.imagelayers.io/vimagick/hugo:latest.svg)

[hugo][1] makes the web fun again.

## Directory Tree

```
~/fig/hugo/
|-- docker-compose.yml
`-- www/
    `-- themes/
        `-- hyde/
```

## docker-compose.yml

```
hugo:
  image: vimagick/hugo
  command: >
    server --bind=0.0.0.0
           --port=80
           --baseUrl=http://blog.easypi.info/
           --theme=hyde
           --watch
  ports:
    - "80:80"
  volumes:
    - ./www:/www
  restart: always
```

> **WARNING:** `hugo server` is for testing purpose.
> The default value of `--baseUrl` option is `http://<bind>:<port>/`.
> I set it to the same value as that in `config.toml` here.
> It maybe not a good practice.

## Up and Running

```
$ alias hugo='docker run --rm -v `pwd`:/www vimagick/hugo'

$ cd ~/fig/hugo/www
$ curl -L https://github.com/spf13/hyde/archive/master.tar.gz | tar xz --strip 1 -C ./themes/hyde/

$ hugo new site .
$ vim config.toml

$ hugo new post/hello-world.md
$ vim content/post/hello-world.md

$ docker-compose up -d

$ hugo
```

> The last `hugo` command will use `baseurl` in `config.toml`.

Open <http://blog.easypi.info/> in your browser!

[1]: http://gohugo.io/
hugo-arm
========

![](https://badge.imagelayers.io/vimagick/hugo-arm:latest.svg)

[hugo][1] makes the web fun again.

## Directory Tree

```
~/fig/hugo/
|-- docker-compose.yml
`-- www/
    `-- themes/
        `-- hyde/
```

## docker-compose.yml

```
hugo:
  image: vimagick/hugo-arm
  command: >
    server --bind=0.0.0.0
           --port=80
           --baseUrl=http://blog.easypi.info/
           --theme=hyde
           --watch
  ports:
    - "80:80"
  volumes:
    - ./www:/www
  restart: always
```

> **WARNING:** `hugo server` is for testing purpose.
> The default value of `--baseUrl` option is `http://<bind>:<port>/`.
> I set it to the same value as that in `config.toml` here.
> It maybe not a good practice.

## Up and Running

```
$ alias hugo='docker run --rm -v `pwd`:/www vimagick/hugo-arm'
$ cd ~/fig/hugo/www
$ curl -L https://github.com/spf13/hyde/archive/master.tar.gz | tar xz --strip 1 -C ./themes/hyde/

$ hugo new site .
$ vim config.toml

$ hugo new post/hello-world.md
$ vim content/post/hello-world.md

$ docker-compose up -d

$ hugo
```

> The last `hugo` command will use `baseurl` in `config.toml`.

Open <http://blog.easypi.info/> in your browser!

[1]: http://gohugo.io/
vault
=====

![](https://badge.imagelayers.io/vimagick/vault:latest.svg)

[`Vault`][1] is a tool for securely accessing secrets. A secret is anything
that you want to tightly control access to, such as API keys, passwords,
certificates, and more. Vault provides a unified interface to any secret, while
providing tight access control and recording a detailed audit log.

## docker-compose.yml

```
vault:
  image: vimagick/vault
  ports:
    - "8200:8200"
  volumes:
    - vault/vault.crt:/etc/vault/vault.crt
    - vault/vault.key:/etc/vault/vault.key
  cap_add:
    - IPC_LOCK
  restart: always
```

> Please distribute `vault.crt` to clients.

## server

```
$ cd ~/fig/vault
$ mkdir vault
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout vault/vault.key -out vault/vault.crt
$ docker-compose up -d
$ docker cp vault_vault_1:/usr/bin/vault /usr/local/bin/
$ docker exec -it vault_vault_1 sh
>>> cd /etc/vault
>>> vault init -tls-skip-verify -key-shares=5 -key-threshold=3 | tee vault.secret
>>> exit
$ docker run --rm --volumes-from vault_vault_1 -v `pwd`:/backup alpine tar cvzf /backup/vault.tgz /etc/vault /var/lib/vault
```

> Split `vault.secret`, keep them a secret.

## client

```
$ export VAULT_ADDR='https://server:8200'
$ cp ~/fig/vault/vault/vault.crt /etc/ssl/certs/vault.pem
$ update-ca-certificates
$ vault status
$ vault unseal && vault unseal && vault unseal
$ vault auth
$ vault write secret/name key=value
$ vault read secret/name
$ vault seal
```

[1]: https://www.vaultproject.io/
minidlna
========

![](https://badge.imagelayers.io/vimagick/minidlna:latest.svg)

ReadyMedia (formerly known as [MiniDLNA][1]) is a simple media server software, with
the aim of being fully compliant with DLNA/UPnP-AV clients. It is developed by
a NETGEAR employee for the ReadyNAS product line.

[1]: http://minidlna.sourceforge.net/
cachet
======

[Cachet][1] is a beautiful and powerful open source status page system.

[1]: https://cachethq.io/
json-server
===========

![](https://badge.imagelayers.io/vimagick/json-server:latest.svg)

Get a full fake REST API with zero coding in less than 30 seconds (seriously) with [json-server][1].

## docker-compose.yml

```yaml
json-server:
  image: vimagick/json-server
  command: -w db.json
  ports:
    - "3000:3000"
  volumes:
    - ./db.json:/app/db.json
  restart: always
```

## db.json

```json
{
  "posts": [
    { "id": 1, "title": "json-server", "author": "typicode" }
  ],
  "comments": [
    { "id": 1, "body": "some comment", "postId": 1 }
  ],
  "profile": { "name": "typicode" }
}
```

## up and running

```bash
docker-compose up -d

pip install httpie

http GET :3000/posts
http POST :3000/posts id:=2 title=hello author=world
http PUT :3000/posts/2 title=Hello author=World
http PATCH :3000/posts/2 title=HELLO
http DELETE :3000/posts/2
http GET :3000/db
```

[1]: https://github.com/typicode/json-server
OpenRefine
==========

![](https://badge.imagelayers.io/vimagick/openrefine:latest.svg)

[OpenRefine][1] (formerly Google Refine) is a powerful tool for working with messy
data: cleaning it; transforming it from one format into another; and extending
it with web services and external data.

Please read the [wiki][2] to learn more.

### docker-compose.yml

```yaml
openrefine:
  image: vimagick/openrefine
  ports:
    - "3333:3333"
  volumes:
    - ./data:/data
  restart: always
```

[1]: http://openrefine.org/index.html
[2]: https://github.com/OpenRefine/OpenRefine/wiki
reconcile-csv
=============

[Reconcile-csv][1] is a reconciliation service for [OpenRefine][2] running from a
CSV file. It uses fuzzy matching to match entries in one dataset to entries in
another dataset, helping to introduce unique IDs into the system - so they can
be used to join your data painlessly.

## docker-compose.yml

```yaml
reconcile-csv:
  image: vimagick/openrefine-reconcile-csv
  ports:
    - "8000:8000"
  volumes:
    - ./data:/data
  environment:
    - JAVA_OPTS=-Xmx2g
    - CSV_FILE=input.csv
    - SEARCH_COLUMN=name
    - ID_COLUMN=id
  restart: always
```

## input.csv

```csv
id,name
1,kevin
2,tom
3,sarah
4,mike
5,lucy
```

## up and running

```bash
$ docker-compose up -d
$ curl http://localhost:8000/reconcile?query=kev
$ curl http://localhost:8000/reconcile?query={%22query%22:%22kev%22,%22limit%22:1}
$ curl http://localhost:8000/view/1
```

[1]: http://okfnlabs.org/reconcile-csv/
[2]: https://github.com/OpenRefine/OpenRefine/wiki
jupyter
=======

Minimal Jupyter Notebook Stack

## docker-compose.yml

```yaml
notebook:
  image: jupyter/minimal-notebook
  ports:
    - "8888:8888"
  volumes:
    - ./work:/home/jovyan/work
  restart: unless-stopped
```

## up and running

```
cd ~/fig/jupyter/
mkdir work
chown 1000:1000 work
docker-compose up -d
```
webhook
=======

![](https://badge.imagelayers.io/vimagick/webhook:latest.svg)

[webhook][1] is a lightweight configurable tool written in Go, that allows you
to easily create HTTP endpoints (hooks) on your server, which you can use to
execute configured commands.

## Build Binary

To build a docker image from scratch, we need to build static linked binary.

```bash
$ go get -d github.com/adnanh/webhook
$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -installsuffix cgo -ldflags '-s -extld ld -extldflags -static' -a -x -o webhook github.com/adnanh/webhook
$ file webhook
```

## Directory Tree

```
~/fig/webhook/
├── docker-compose.yml
└── scripts/
    ├── hooks.json
    └── test.sh* (executable)
```

docker-compose.yml

```
webhook:
  image: vimagick/webhook
  command: -hooks hooks.json -verbose
  ports:
    - "9000:9000"
  volumes:
    - "./scripts:/scripts"
  restart: always
```

hooks.json

```
[
  {
    "id": "test",
    "execute-command": "/scripts/test.sh",
    "command-working-directory": "/scripts"
  }
]
```

test.sh

```
#!/bin/bash
echo 'hello world'
```

## Up and Running

```
$ cd ~/fig/webhook/

$ chmod +x scripts/test.sh

$ docker-compose up -d
Creating webhook_webhook_1...

$ curl http://localhost:9000/hooks/test

$ docker-compose logs
Attaching to webhook_webhook_1
webhook_1 | [webhook] 2015/11/05 04:26:52 version 2.3.5 starting
webhook_1 | [webhook] 2015/11/05 04:26:52 setting up os signal watcher
webhook_1 | [webhook] 2015/11/05 04:26:52 attempting to load hooks from hooks.json
webhook_1 | [webhook] 2015/11/05 04:26:52 loaded 1 hook(s) from file
webhook_1 | [webhook] 2015/11/05 04:26:52       > test
webhook_1 | [webhook] 2015/11/05 04:26:52 starting insecure (http) webhook on :9000
webhook_1 | [webhook] 2015/11/05 04:26:52 os signal watcher ready
webhook_1 | [webhook] 2015/11/05 04:27:11 Started GET /hooks/test
webhook_1 | [webhook] 2015/11/05 04:27:11 Completed 200 OK in 390.207µs
webhook_1 | [webhook] 2015/11/05 04:27:11 test got matched (1 time(s))
webhook_1 | [webhook] 2015/11/05 04:27:11 test hook triggered successfully
webhook_1 | [webhook] 2015/11/05 04:27:11 executing /scripts/test.sh (/scripts/test.sh) with arguments [/scripts/test.sh] using /scripts as cwd
webhook_1 | [webhook] 2015/11/05 04:27:11 command output: hello world
webhook_1 |
webhook_1 | [webhook] 2015/11/05 04:27:11 finished handling test
```

[1]: https://github.com/adnanh/webhook
shadowsocks
===========

> :warning: This project has been moved to <https://github.com/EasyPi/docker-shadowsocks-libev>.

name                       | size
-------------------------- | ---------------------------------------------------------------------------
[shadowsocks][1]           | ![](https://badge.imagelayers.io/vimagick/shadowsocks:latest.svg)
[shadowsocks-libev][2]     | ![](https://badge.imagelayers.io/vimagick/shadowsocks-libev:latest.svg)
[shadowsocks-arm][3]       | ![](https://badge.imagelayers.io/vimagick/shadowsocks-arm:latest.svg)
[shadowsocks-libev-arm][4] | ![](https://badge.imagelayers.io/vimagick/shadowsocks-libev-arm:latest.svg)

[shadowsocks][5] is a secure socks5 proxy, designed to protect your Internet traffic.

> If you want to keep a secret, you must also hide it from yourself.

## docker-compose.yml

For Linux Server

```yaml
server:
  image: vimagick/shadowsocks-libev
  ports:
    - "8388:8388"
  environment:
    - METHOD=chacha20
    - PASSWORD=secret
  restart: always

client:
  image: vimagick/shadowsocks-libev
  command: ss-local -s foobar.site -p 8388 -b 0.0.0.0 -l 1080 -k secret -m chacha20
  ports:
    - "1080:1080"
  restart: always
```

For Raspberry Pi

```yaml
client:
  image: vimagick/shadowsocks-libev-arm
  ports:
    - "1080:1080"
  environment:
    - SERVER_ADDR=foobar.site
    - METHOD=chacha20
    - PASSWORD=secret
  restart: always
```

## server

I'm running shadowsocks server on Debian (jessie).

```
$ docker-compose up -d server
```

## client

I'm running shadowsocks client on Raspberry Pi 2.

```
$ docker-compose up -d client
```

[read more][2]

[1]: https://hub.docker.com/r/vimagick/shadowsocks/
[2]: https://hub.docker.com/r/vimagick/shadowsocks-libev/
[3]: https://hub.docker.com/r/vimagick/shadowsocks-arm/
[4]: https://hub.docker.com/r/vimagick/shadowsocks-libev-arm/
[5]: http://shadowsocks.org
[6]: https://github.com/shadowsocks/shadowsocks/wiki/Configuration-via-Config-File
shadowsocks-libev
=================

> ⚠️ This docker image becomes obsolete, and has moved to <https://github.com/EasyPi/docker-shadowsocks-libev>.

![](https://badge.imagelayers.io/vimagick/shadowsocks-libev:latest.svg)

![](http://dockeri.co/image/vimagick/shadowsocks-libev)

[shadowsocks-libev][1] is a lightweight secured socks5 proxy for embedded devices and low end boxes.

It is a port of shadowsocks created by [@clowwindy][2] maintained by [@madeye][3] and [@linusyang][4].

Compared to [vimagick/shadowsocks][5], [vimagick/shadowsocks-libev][6] uses less CPU/Memory/Disk.

## docker-compose.yml

```yaml
shadowsocks:
  image: vimagick/shadowsocks-libev
  ports:
    - "8388:8388"
  environment:
    - PASSWORD=9MLSpPmNt
  restart: always
```

> Please choose a strong password to protect your server.

## CPU/Memory Usage

```bash
$ docker stats shadowsocks_shadowsocks_1

# vimagick/shadowsocks
CONTAINER                   CPU %               MEM USAGE/LIMIT     MEM %               NET I/O
shadowsocks_shadowsocks_1   0.05%               16.56 MiB/741 MiB   2.23%               117.9 KiB/648 B

# vimagick/shadowsocks-libev
CONTAINER                   CPU %               MEM USAGE/LIMIT     MEM %               NET I/O
shadowsocks_shadowsocks_1   0.00%               476 KiB/741 MiB     0.06%               2.334 MiB/2.341 MiB
```

## Disk Usage

```bash
$ docker images

REPOSITORY                   TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
vimagick/shadowsocks-libev   latest              a36f480e696a        8 minutes ago       15.1 MB
vimagick/shadowsocks         latest              35650c84852a        57 minutes ago      51.25 MB
```

## Raspberry Pi

To setup a shadowsocks client on Raspberry Pi, please use [easypi/shadowsocks-libev-arm][7].

[1]: https://github.com/shadowsocks/shadowsocks-libev
[2]: https://github.com/clowwindy
[3]: https://github.com/madeye
[4]: https://github.com/linusyang
[5]: https://hub.docker.com/r/vimagick/shadowsocks/
[6]: https://hub.docker.com/r/vimagick/shadowsocks-libev/
[7]: https://hub.docker.com/r/easypi/shadowsocks-libev-arm/
node-red
========

![](https://badge.imagelayers.io/vimagick/node-red:latest.svg)

[Node-RED][1] is a tool for wiring together hardware devices, APIs and online
services in new and interesting ways.

![](screenshot.png)

## directory tree

```
~/fig/node-red/
├── docker-compose.yml
└── node-red/
    ├── flows_cred.json
    ├── flows.json
    ├── lib/
    │   └── flows
    └── settings.js
```

> The `node-red` directory will be created after first running.

## docker-compose.yml

```yaml
node-red:
  image: vimagick/node-red
  ports:
    - "1880:1880"
  volumes:
    - ./data:/root/.node-red
  restart: always
```

## settings.js

```javascript
module.exports = {

    //...

    flowFile: 'flows.json',

    //...

    adminAuth: {
        type: "credentials",
        users: [{
            username: "admin",
            password: "$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN.",
            permissions: "*"
        }],
        default: {
            permissions: "read"
        }
    },

    //...

}
```

> Password hash can be generated by running `node-red-admin hash-pw`.

## up and running

```
$ cd ~/fig/node-red/
$ docker-compose up -d
$ docker-compose exec node-red node-red-admin hash-pw
$ vi data/settings.js
$ docker-compose exec node-red bash
>>> cd /root/.node-red
>>> apk add -U build-base
>>> npm install node-red-node-irc
>>> npm install node-red-node-daemon
>>> exit
$ docker-compose restart
```

> Install nodes from [node-red-nodes](https://github.com/node-red/node-red-nodes).

[1]: http://nodered.org/
Node-RED PATCH
==============

python2 (on alpine linux) cannot read usb keyboard input device
splash
======

[Splash][1] is a javascript rendering service with an HTTP API. It's a
lightweight browser with an HTTP API, implemented in Python using Twisted and
QT.

It's fast, lightweight and state-less which makes it easy to distribute.

## docker-compose.yml

```yaml
splash:
  image: scrapinghub/splash
  command: --maxrss 4096 --max-timeout 300
  ports:
    - "8050:8050"
    - "8051:8051"
    - "5023:5023"
  volumes:
    - ./data/filters:/etc/splash/filters
    - ./data/js-profiles:/etc/splash/js-profiles
    - ./data/lua_modules:/etc/splash/lua_modules
    - ./data/proxy-profiles:/etc/splash/proxy-profiles
  mem_limit: 4608M
  restart: always
```

## server

```
$ cd ~/fig/splash

$ tree
.
├── docker-compose.yml
└── data
    ├── filters
    │   ├── easylist.txt
    │   └── default.txt
    ├── js-profiles
    └── proxy-profiles

$ cat data/filters/default.txt
||fonts.googleapis.com^
||ajax.googleapis.com^

$ docker-compose up -d
```

> If `default.txt` file is present in `--filters-path` folder it is used by default
> when filters argument is not specified. Pass `filters=none` if you don’t want
> default filters to be applied.

## client

```lua
-- youtube-logo.lua

function main(splash)
    splash:go('https://www.youtube.com/')
    splash:wait(0.5)
    local logo = splash:select('.logo')
    return logo:png()
end
```

```lua
-- crop.lua

function main(splash)
    local url = splash.args.url
    local css1 = splash.args.css1
    local css2 = splash.args.css2
    assert(splash:go(url))
    splash:set_viewport_full()
    local box1 = splash:select(css1):bounds()
    local box2 = splash:select(css2):bounds()
    return splash:png{
        region={
            math.min(box1.left, box2.left),
            math.min(box1.top, box2.top),
            math.max(box1.right, box2.right),
            math.max(box1.bottom, box2.bottom),
        }
    }
end
```

```bash
# whole page
$ http http://server:8050/render.png url==https://www.youtube.com/ > youtube.png

# only logo
$ http http://server:8050/execute lua_source=@youtube-logo.lua > youtube-logo.png

# only form
$ http http://server:8050/execute lua_source=@crop.lua \
                                  url=https://www.facebook.com/ \
                                  css1='#u_0_1' \
                                  css2='#u_0_b' > facebook-form.png
```

[1]: http://splash.readthedocs.org/en/latest/
redis
=====

![](https://redis.io/images/redis-white.png)

[Redis][1] is an open source key-value store that functions as a data structure server.

## docker-compose.yml

```yaml
redis:
  image: redis:alpine
  ports:
    - "127.0.0.1:6379:6379"
  volumes:
    - ./data:/data
  restart: always
```

[1]: https://redis.io/
redis-arm
=========

## docker-compose.yml

```yaml
redis:
  image: easypi/redis-arm
  ports:
    - "6379:6379"
  restart: always
```
squid
=====

![](https://badge.imagelayers.io/vimagick/squid:latest.svg)

[Squid][1] is a caching proxy for the Web supporting HTTP, HTTPS, FTP, and
more. It reduces bandwidth and improves response times by caching and reusing
frequently-requested web pages. Squid has extensive access controls and makes a
great server accelerator. It runs on most available operating systems,
including Windows and is licensed under the GNU GPL.

## docker-compose.yml

```yaml
squid:
  image: vimagick/squid
  ports:
    - "3128:3128"
    - "3130:3130"
  ulimits:
    nofile:
      soft: 65535
      hard: 65535
  restart: always
```

> You can mount `squid.conf` file.

## Up and Running

```
# server
$ docker-compose up -d

# client
$ curl -x https://localhost:3128 https://www.google.com/
```

[1]: http://www.squid-cache.org/
plex
====

The free [Plex Media Server][1] simplifies your life by organizing all of your
personal media, making it beautiful and streaming it to all of your devices.

## docker-compose.yml

```yaml
plex:
  image: vimagick/plex
  volumes:
    - "./data:/data:ro"
    - "./data/plex:/var/lib/plexmediaserver/Library/Application Support"
  environment:
    - "HOME=/data"
  working_dir: /data
# user: 1000:1000
  net: host
  restart: always
```

> Please uncomment `user: UID:GID` if you're not the `root` user.

## up and running

```bash
$ cd ~/fig/plex/
$ mkdir -p data/{plex,Movies,Music,Photos}
$ echo "$(id -u):$(id -g)"
$ vim docker-compose.yml
$ docker-compose up -d
$ firefox http://localhost:32400/web
```

[1]: https://plex.tv/
facedetect
==========

[facedetect][1] is a simple face detector for batch processing. It answers the
basic question: “Is there a face in this image?” and gives back either an exit
code or the coordinates of each detected face in the standard output.

Quick Start
-----------

```
$ alias facedetect='2>/dev/null docker run --rm -v `pwd`:/work -w /work vimagick/facedetect'
$ facedetect image.png
```

[1]: http://www.thregr.org/~wavexx/software/facedetect/
discuz
======

[Discuz!][1] is an internet forum software written in PHP and developed by a chinese company.

:-1: Discuz! is not a open source software.

- Initial release: March 2002
- Stable release: X3.2 (September 23, 2013; 2 years ago)
- License: proprietary (costs about ¥3000)

:+1: Try open source softwares:

- [discourse](https://hub.docker.com/r/discourse/discourse/)
- [nodebb](https://hub.docker.com/r/vimagick/phpbb/)
- [phpbb](https://hub.docker.com/r/vimagick/phpbb/)

## docker-compose.yml

```
discuz:
  image: vimagick/discuz
  ports:
    - "8000:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=discuz
    - MYSQL_USER=discuz
    - MYSQL_PASSWORD=discuz
  restart: always
```

## /etc/nginx/sites-enabled/discuz

```
server {
    listen 80;
    server_name discuz.easypi.info;
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

> :warning: You should pass HTTP headers to fix wrong URL.

[1]: http://www.discuz.net
ot-recorder
===========

The [OwnTracks Recorder][1] is a lightweight program for storing and accessing
location data published via MQTT (or HTTP) by the OwnTracks apps.

## docker-compose.yml

```yaml
ot-recorder:
  image: vimagick/ot-recorder
  ports:
    - "8083:8083"
  volumes:
    - ./data:/var/spool/owntracks/recorder/store
  environment:
    - OTR_HOST=iot.eclipse.org
    - OTR_PORT=8883
    - OTR_USER=username
    - OTR_PASS=password
    - OTR_CAFILE=/etc/ssl/certs/DST_Root_CA_X3.pem
    - OTR_TOPICS=owntracks/#
    - OTR_BROWSERAPIKEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    - OTR_OPTIONS=--qos 0
  restart: always
```

You can pass any options to process via `OTR_OPTIONS` environment variable.

Click [this][2] to generate a google map api key.

[1]: https://github.com/owntracks/recorder
[2]: https://developers.google.com/maps/documentation/javascript/?authuser=1
nifi
====

[Apache nifi][1] is an easy to use, powerful, and reliable system to process
and distribute data.

![](https://nifi.apache.org/assets/images/flow-th.png)

[1]: https://nifi.apache.org

pure-ftpd
=========

![](https://www.pureftpd.org/images/pure-ftpd.png)

[Pure-FTPd][1] is a free (BSD), secure, production-quality and standard-conformant
FTP server. It doesn't provide useless bells and whistles, but focuses on
efficiency and ease of use. It provides simple answers to common needs, plus
unique useful features for personal users as well as hosting providers. 


## ~/fig/pureftpd/docker-compose.yml

```yaml
pureftpd:
  image: vimagick/pure-ftpd
  ports:
    - "21:21"
  volumes:
    - ./data/ftpuser:/home/ftpuser
    - ./data/pure-ftpd:/etc/pure-ftpd
  privileged: true
  restart: always
```

> We only need to expose port `21` to accept client ftp connection.
> Pure-FTPd will open random port to accept client ftp-data connection.
> At this time, host machine is a router for DNAT.

## server

```bash
$ cd ~/fig/pureftpd/
$ docker-compose up -d
$ docker-compose exec pureftpd bash
>>> pure-pw useradd kev -u ftpuser -d /home/ftpuser/kev -t 1024 -T 1024 -y 1 -m
>>> pure-pw list
>>> pure-pw show kev
>>> pure-pw passwd kev -m
>>> pure-pw userdel kev -m
>>> pure-ftpwho -n
>>> exit
$ tree -F
.
├── docker-compose.yml
└── data/
    ├── ftpuser/
    │   └── kev/
    │       └── file.txt
    └── pure-ftpd/
        ├── pureftpd.passwd
        └── pureftpd.pdb
```

## client

```bash
$ ftp remote-server
Name: kev
Password: ******
ftp> !touch file.txt
ftp> !ls
ftp> put file.txt
ftp> !rm file.txt
ftp> get file.txt
ftp> del file.txt
ftp> ls
ftp> bye
```

[1]: https://www.pureftpd.org/project/pure-ftpd
pdnsd
=====

[![](https://badge.imagelayers.io/vimagick/pdnsd:latest.svg)](https://imagelayers.io/?images=vimagick/pdnsd:latest)

`pdnsd` is a proxy DNS server with permanent caching (the cache contents are
written to hard disk on exit) that is designed to cope with unreachable or down
DNS servers (for example in dial-in networking).

Since version 1.1.0, pdnsd supports negative caching.

## docker-compose.yml

```
pdnsd:
  image: vimagick/pdnsd
# volumes:
#   - pdnsd.conf:/etc/pdnsd.conf
  ports:
    - "53:53/tcp"
    - "53:53/udp"
  restart: always
```

> Uncomment `volumes` to use customized config file.

## server

```
$ cd ~/fig/pdnsd/
$ docker-compose up -d
$ docker exec pdnsd_pdnsd_1 pdnsd-ctl status
```

## client

```
$ dig @1.2.3.4 www.youtube.com
```
ProxyHub
========

## About

`ProxyHub` is powered by:

- `shadowsocks`: A secure socks5 proxy, designed to protect your Internet traffic.
- `haproxy`: A free, very fast and reliable solution offering high availability,
  load balancing, and proxying for TCP and HTTP-based applications.
- `dnscrypt`: A protocol for securing communications between a client and a DNS resolver.
- `pdnsd`: A DNS server designed for local caching of DNS information.
- `polipo`: A lightweight forwarding and caching web proxy server.
- `nginx`: An open source reverse proxy server for HTTP, HTTPS, SMTP, POP3,
  and IMAP protocols, as well as a load balancer, caching and SSL offload.

## Fig

    proxyhub:
      image: vimagick/proxyhub
      ports:
        - "1080:1080"
        - "8123:8123"
      restart: always

## Run

    fig up -d

## Test

    # socks5 proxy
    curl -x socks5h://localhost:1080 https://www.youtube.com

    # http(s) proxy
    curl -x http://localhost:8123 https://www.youtube.com

## Todo

- add proxy.pac via nginx
- add transparent socks5 proxy via redsocks

selenium
========

[Selenium][1] is an umbrella project for a range of tools and libraries that enable
and support the automation of web browsers.

Watch [this][2] video to get started.

## Server

docker-compose.yml

```yaml
firefox:
  image: selenium/standalone-firefox-debug
  ports:
    - "4444:4444"
    - "5900:5900"
  environment:
    - JAVA_OPTS=-Xmx512m
  restart: always
```

docker-compose-grid.yml

```yaml
hub:
  image: selenium/hub
  container_name: hub
  ports:
    - "4444:4444"
  environment:
    - GRID_TIMEOUT=60
    - GRID_BROWSER_TIMEOUT=30
  retart: always

chrome:
  image: selenium/node-chrome
  container_name: chrome
  ports:
    - "5555"
  links:
    - hub
  retart: always

firefox:
  image: selenium/node-firefox
  container_name: firefox
  ports:
    - "5555"
  links:
    - hub
  retart: always
```

> Access grid console at <http://127.0.0.1:4444/grid/console>

docker-compose-node.yml

```yaml
firefox:
  image: selenium/node-firefox
  ports:
    - "5555:5555"
    - "5900:5900"
  environment:
    - JAVA_OPTS=-Xmx512m
    - NODE_MAX_INSTANCES=2
    - NODE_MAX_SESSION=2
    - SE_OPTS=-host 5.6.7.8 -port 5555
    - HUB_PORT_4444_TCP_ADDR=1.2.3.4
    - HUB_PORT_4444_TCP_PORT=4444
  restart: always
```

```bash
$ docker-compose up -d
```

> Another way to start selenium server:

```bash
$ npm install -g selenium-standalone
$ selenium-standalone install
$ selenium-standalone start
```

## Client

baidu-search.py

```python
from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

driver = webdriver.Remote(
    command_executor='http://127.0.0.1:4444/wd/hub',
    desired_capabilities=DesiredCapabilities.FIREFOX
)

driver.get('http://www.baidu.com/')
driver.find_element_by_id('kw').send_keys('webdriver')
driver.find_element_by_id('su').click()
driver.save_screenshot('baidu.png')
driver.close()
```

search-baidu.js

```javascript
var webdriver = require('selenium-webdriver'),
    By = require('selenium-webdriver').By,
    until = require('selenium-webdriver').until,
    fs = require('fs');

webdriver.WebDriver.prototype.saveScreenshot = function(filename) {
    return driver.takeScreenshot().then(function(data) {
        fs.writeFile(filename, data.replace(/^data:image\/png;base64,/,''), 'base64', function(err) {
            if(err) throw err;
        });
    })
};

var driver = new webdriver.Builder()
    .forBrowser('firefox')
    .usingServer('http://127.0.0.1:4444/wd/hub')
    .build();

driver.get('http://www.baidu.com/');
driver.findElement(By.id('kw')).sendKeys('webdriver');
driver.findElement(By.id('su')).click();
driver.wait(until.titleIs('webdriver_百度搜索'), 1000);
driver.saveScreenshot('baidu.png');
driver.quit();
```

```bash
# VNC
$ open vnc://:secret@127.0.0.1:5900

# PYTHON
$ pip3 install selenium
$ python3 baidu-search.py

# NODEJS
$ npm install -g selenium-webdriver
$ node search-baidu.js
```

[1]: http://seleniumhq.org/
[2]: https://www.youtube.com/watch?v=S4OkrnFb-YY
ffserver - FFserver video server
================================

![](https://badge.imagelayers.io/vimagick/ffserver:latest.svg)

[FFserver][1] is a streaming server for both audio and video. It supports
several live feeds, streaming from files and time shifting on live
feeds (you can seek to positions in the past on each live feed,
provided you specify a big enough feed storage in `ffserver.conf`).

## docker-compose.yml

```yaml
ffserver:
  image: vimagick/ffserver
  ports:
    - "554:554"
    - "8090:8090"
  volumes:
    - ./ffserver.conf:/etc/ffserver.conf
    - ./data:/data
  restart: always
```

## ffserver.conf

```
HTTPPort 8090
HTTPBindAddress 0.0.0.0

RTSPPort 554
RTSPBindAddress 0.0.0.0

MaxClients 100
MaxBandwidth 100000
CustomLog -

<Feed feed.ffm>
File /data/feed.ffm
</Feed>

<Stream video.mp4>
Format rtp
Feed feed.ffm
VideoCodec libx264
VideoFrameRate 24
VideoBitRate 100
VideoSize 640x480
AVPresetVideo default
AVPresetVideo baseline
AVOptionVideo flags +global_header
NoAudio
#AudioCodec libmp3lame
#AudioBitRate 32
#AudioChannels 2
#AudioSampleRate 22050
#AVOptionAudio flags +global_header
</Stream>

<Stream status.html>
Format status
</Stream>
```

> :warning: I've not figured out how to stream video+audio yet!

## Server Setup

```bash
$ cd ~/fig/ffserver
$ tree
.
├── data
│   └── video.mp4
├── docker-compose.yml
└── ffserver.conf

$ docker-compose up -d
$ docker-compose exec ffserver ffmpeg -re -i video.mp4 http://localhost:8090/feed.ffm
```

> :hammer: `data/video.mp4` is for testing purposes only.

## Client Setup

```bash
$ ffmpeg -re -i video.mp4 http://easypi.info:8090/feed.ffm
```

## Player Setup

```bash
$ ffplay rtsp://easypi.info/video.mp4
$ vlc http://easypi.info:8090/video.rtsp
$ firefox http://easypi.info:8090/status.html
```

## References

- <http://ffmpeg.org/ffserver.html>
- <http://www.ffmpeg.org/sample.html>
- <https://trac.ffmpeg.org/wiki/ffserver>

[1]: https://www.ffmpeg.org/ffserver.html
`Joomla` is an award-winning content management system (CMS),
which enables you to build Web sites and powerful online applications.

This docker image support mysqli/postgres.

```
# run container
$ docker run -d --restart always --name joomla -p 8000:80 vimagick/joomla

# setup website
$ firefox http://localhost:8000/install
$ firefox http://localhost:8000/installation/index.php
```
tftpd
=====
scrapyd
=======

![](http://dockeri.co/image/vimagick/scrapyd)

[scrapy][1] is an open source and collaborative framework for extracting the
data you need from websites. In a fast, simple, yet extensible way.

[scrapyd][2] is a service for running Scrapy spiders.  It allows you to deploy
your Scrapy projects and control their spiders using a HTTP JSON API.

[scrapyd-client][3] is a client for scrapyd. It provides the scrapyd-deploy
utility which allows you to deploy your project to a Scrapyd server.

[scrapy-splash][4] provides Scrapy+JavaScript integration using Splash.

[scrapyrt][5] allows you to easily add HTTP API to your existing Scrapy project.

[pillow][6] is the Python Imaging Library to support the ImagesPipeline.

This image is based on `debian:jessie`, 6 latest python packages are installed:

- `scrapy`: git+https://github.com/scrapy/scrapy.git
- `scrapyd`: git+https://github.com/scrapy/scrapyd.git
- `scrapyd-client`: git+https://github.com/scrapy/scrapyd-client.git
- `scrapy-splash`: git+https://github.com/scrapinghub/scrapy-splash.git
- `scrapyrt`: git+https://github.com/scrapinghub/scrapyrt.git
- `pillow`: git+https://github.com/python-pillow/Pillow.git

Please use this as base image for your own project.

## docker-compose.yml

```yaml
scrapyd:
  image: vimagick/scrapyd
  ports:
    - "6800:6800"
  volumes:
    - ./data:/var/lib/scrapyd
    - /usr/local/lib/python2.7/dist-packages
  restart: always

scrapy:
  image: vimagick/scrapyd
  command: bash
  volumes:
    - .:/code
  working_dir: /code
  restart: always
```

## Run it as background-daemon for scrapyd

```bash
$ docker-compose up -d scrapyd
$ docker-compose logs -f scrapyd
$ docker cp scrapyd_scrapyd_1:/var/lib/scrapyd/items .
$ tree items
└── myproject
    └── myspider
        └── ad6153ee5b0711e68bc70242ac110005.jl
```

```bash
$ mkvirtualenv webbot
$ pip install scrapy scrapyd-client

$ scrapy startproject myproject
$ cd myproject
$ setvirtualenvproject

$ scrapy genspider myspider mydomain.com
$ scrapy edit myspider
$ scrapy list

$ vi scrapy.cfg
$ scrapyd-client deploy
$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=myspider
$ firefox http://localhost:6800
```

File: scrapy.cfg

```ini
[settings]
default = myproject.settings

[deploy]
url = http://localhost:6800/
project = myproject
```

## Run it as interactive-shell for scrapy

```bash
$ cat > stackoverflow_spider.py << _EOF_
import scrapy

class StackOverflowSpider(scrapy.Spider):
    name = 'stackoverflow'
    start_urls = ['http://stackoverflow.com/questions?sort=votes']

    def parse(self, response):
        for href in response.css('.question-summary h3 a::attr(href)'):
            full_url = response.urljoin(href.extract())
            yield scrapy.Request(full_url, callback=self.parse_question)

    def parse_question(self, response):
        yield {
            'title': response.css('h1 a::text').extract()[0],
            'votes': response.css('.question .vote-count-post::text').extract()[0],
            'body': response.css('.question .post-text').extract()[0],
            'tags': response.css('.question .post-tag::text').extract(),
            'link': response.url,
        }
_EOF_

$ docker-compose run --rm scrapy
>>> scrapy runspider stackoverflow_spider.py -o top-stackoverflow-questions.json
>>> cat top-stackoverflow-questions.json
>>> exit
```

[1]: https://github.com/scrapy/scrapy
[2]: https://github.com/scrapy/scrapyd
[3]: https://github.com/scrapy/scrapyd-client
[4]: https://github.com/scrapinghub/scrapy-splash
[5]: https://github.com/scrapinghub/scrapyrt
[6]: https://github.com/python-pillow/Pillow
scrapyd-onbuild
===============

Dockerfile for building an image that runs [scrapyd][1].  

Please use this image as base for your own project.

You may edit these files:

- `packages.txt` - additional packages to be installed (via `apt-get install`)
- `dependencies.txt` - dependencies for python requirements (via `apt-get install/purge`)
- `requirements.txt` - additional python packages to be installed (via `pip install`)

[1]: https://github.com/scrapy/scrapyd
openHAB
=======

[openHAB][1] - a vendor and technology agnostic open source automation software
for your home. Build your smart home in no time!

## docker-compose.yml

```yaml
openhab:
  image: vimagick/openhab
  ports:
    - "8080:8080"
    - "8443:8443"
    - "5555:5555"
  restart: always
```

[1]: http://www.openhab.org/
Music Player Daemon
===================

[Music Player Daemon][1] (MPD) is a flexible, powerful, server-side application
for playing music. Through plugins and libraries it can play a variety of sound
files while being controlled by its network protocol.

:+1: [easypi/mpd-arm][2] works on Raspberry Pi very well.

## docker-compose.yml

```yaml
mpd:
  image: vimagick/mpd
  ports:
    - "6600:6600"
    - "8800:8800"
  volumes:
    - ./mpd.conf:/etc/mpd.conf
    - ./music:/var/lib/mpd/music
    - ./playlists:/var/lib/mpd/playlists
  devices:
    - /dev/snd
  restart: always
```

## Server Setup

```bash
$ mkdir -p ~/fig/mpd/{music,playlists}
$ cd ~/fig/mpd/

$ wget https://upload.wikimedia.org/wikipedia/commons/d/d5/Pop_Goes_the_Weasel.ogg -P music

$ curl -s -X POST -H 'Content-Length: 0' http://www.shoutcast.com/Home/Top |
    jq '.[].ID' |
      parallel --eta -k curl -s 'http://yp.shoutcast.com/sbin/tunein-station.m3u?id={}' |
        sed '1!s@#EXTM3U@@' |
          cat -s > playlists/shoutcast.m3u

$ cat > playlists/microphone.m3u << _EOF_
#EXTM3U
#EXTINF:-1,microphone
alsa://plughw:1,0
_EOF_

$ docker-compose up -d
$ docker-compose exec mpd sh
>>> mpc help
>>> mpc update
>>> mpc ls | mpc add
>>> mpc repeat on
>>> mpc random on
>>> mpc
>>> mpc clear
>>> mpc lsplaylists
>>> mpc load shoutcast
>>> mpc play
>>> exit
```

## Client Setup

- Android: https://play.google.com/store/apps/details?id=com.namelessdev.mpdroid
- Desktop: http://rybczak.net/ncmpcpp/

```yaml
Host: x.x.x.x
Port: 6600
Streaming host: x.x.x.x
Streaming port: 8800
```

## Read More

- <https://wiki.archlinux.org/index.php/Music_Player_Daemon>
- <https://wiki.archlinux.org/index.php/Music_Player_Daemon/Tips_and_tricks>
- <https://wiki.archlinux.org/index.php/Streaming_With_Icecast>
- <https://stmllr.net/blog/streaming-audio-with-mpd-and-icecast2-on-raspberry-pi/>
- <https://www.musicpd.org/doc/user/input_plugins.html>

[1]: https://www.musicpd.org/
[2]: https://hub.docker.com/r/easypi/mpd-arm/
Music Player Daemon
===================

## Enable Audio Device (ArchLinuxArm)

```bash
$ echo 'snd-bcm2835' > /etc/modules-load.d/raspberrypi.conf
$ echo 'dtparam=audio=on' >> /boot/config.txt
$ reboot
```
hydra
=====

A very fast network logon cracker which support many different services.

- [x] openssl
- [x] idn
- [x] curses
- [x] pcre
- [x] Postgres
- [x] SVN
- [ ] firebird
- [x] MYSQL client
- [ ] AFP
- [ ] NCP
- [ ] SAP/R3
- [x] libssh
- [ ] Oracle
- [ ] GUI req's
- [x] Android specialities
- [x] secure compile option support in gcc


```
$ docker run --rm -it -v `pwd`:/work vimagick/hydra

>>> cat passwd.txt
    123456
    admin
    ...

>>> hydra -l admin -P passwd.txt -o hacked.txt http://www.target.com/login.php
    [DATA] max 1 task per 1 server, overall 64 tasks, 3 login try (l:1/p:1), ~0 tries per task
    [DATA] attacking service http-get on port 2812
    [2812][http-get] host: ss.easypi.info   login: admin   password: admin
    1 of 1 target successfully completed, 1 valid password found

>>> cat hacked.txt
    [2812][http-get] host: ss.easypi.info   login: admin   password: admin
```
httpbin
=======

![](https://badge.imagelayers.io/vimagick/httpbin:latest.svg)

[httpbin][1] is a HTTP Request & Response Service, written in Python + Flask.

## docker-compose.yml

```yaml
httpbin:
  image: vimagick/httpbin
  ports:
    - "27815:80"
  restart: always
```

## up and running

```
$ docker-compose up -d
$ curl http://127.0.0.1:27815/ip
```

[1]: http://httpbin.org
gitlab
======

[GitLab][1] includes Git repository management, code reviews, issue tracking,
wikis, and more, plus GitLab CI, an easy-to-use continuous integration and
deployment tool.

## docker-compose.yml

```yaml
gitlab:
  image: gitlab/gitlab-ce
  hostname: git.example.com
  environment:
    GITLAB_OMNIBUS_CONFIG: |
      external_url 'https://git.example.com'
      gitlab_rails['gitlab_shell_ssh_port'] = 2222
  ports:
    - "443:443"
    - "2222:22"
  volumes:
    - ./gitlab/config:/etc/gitlab
    - ./gitlab/logs:/var/log/gitlab
    - ./gitlab/data:/var/opt/gitlab
  restart: always
```

You can put TLS crt+key into `./gitlab/config/ssl/`:

- `git.example.com.crt`
- `git.example.com.key`

If you bind port 22, you need to change host `sshd` config:

```bash
$ vi /etc/ssh/sshd_config
- Port 22
+ Port 2222
$ systemctl restart ssh
$ ssh -p 2222 localhost
```

## up and running

```bash
$ mkdir -p ~/fig/gitlab/gitlab/config/ssh
$ cd ~/fig/gitlab/gitlab/config/ssh
$ openssl req -newkey rsa:4096 -nodes -sha256 -x509 -days 365 \
              -keyout git.example.com.key \
              -out git.example.com.crt
$ docker-compose up -d
```

Open <https://git.example.com> in your web browser:

- username: `root`
- password: `5iveL!fe`

## backup volumes

```bash
$ docker run --rm \
             --volumes-from gitlab_gitlab_1 \
             -v $PWD:/tmp \
             alpine \
             tar czf /tmp/gitlab.tgz /etc/gitlab /var/opt/gitlab /var/log/gitlab

$ tar tzf gitlab.tgz
```

## read more

- http://docs.gitlab.com/omnibus/docker/
- https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/doc/settings/nginx.md
- https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/doc/settings/configuration.md

[1]: https://gitlab.com/
urlwatch
========

[urlwatch][1] is a tool for monitoring webpages for updates.

```
cron: triggered every 15 minutes
    -> make: generate urls.txt from urls.yml
        -> urlwatch: fetch webpages
            -> hooks.py: extract info
                -> email: send via smtp
                    -> (^_^)
```

## docker-compose.yml

```
urlwatch:
  image: vimagick/urlwatch
  volumes:
    - urlwatch/Makefile:/root/.urlwatch/Makefile
    - urlwatch/urls.yml:/root/.urlwatch/urls.yml
  restart: always
```

## Makefile

```
SHELL = /bin/sh
PATH := /usr/local/bin:$(PATH)

SMTP = smtp.easypi.info:587
FROM = urlwatch@easypi.info
PASS = password
TO = noreply@easypi.info

all: setup urls.txt
	urlwatch -s $(SMTP) -f $(FROM) -t $(TO) -A -T

urls.txt: urls.yml
	python lib/hooks.py

setup:
	python -c 'import keyring; keyring.set_password("$(SMTP)", "$(FROM)", "$(PASS)")'
```

> Please change `STMP`/`FROM`/`PASS`/`TO` to correct value.

## urls.yml

```
python:
  url: https://www.python.org/downloads/
  exp: //div[@class="download-unknown"]/p[@class]/a[1]/text()

ubuntu:
  url: http://www.ubuntu.com/download/server
  exp: //div[contains(@class, "row-hero")]//h2/text()

coreos:
  url: https://coreos.com
  exp: //div[@class="co-p-homepage-release-text"]/text()

urlwatch:
  url: https://github.com/thp/urlwatch/releases
  exp: //ul[@class="release-timeline-tags"]/li[1]//span[@class="tag-name"]/text()
```

## Email alert

```
***************************************************************************
CHANGED: https://coreos.com#coreos
***************************************************************************
--- @   Tue, 07 Jul 2015 17:15:01 +0000
+++ @   Tue, 07 Jul 2015 20:00:01 +0000
@@ -1 +1 @@
-coreos: 723.1.0
+coreos: 735.0.0

***************************************************************************
```

[1]: thp.io/2008/urlwatch/
influxdb
========

> :warning: Please use the official image [influxdb](https://hub.docker.com/_/influxdb/)!

[InfluxDB][1] is an open source distributed time series database with no external
dependencies. It's useful for recording metrics, events, and performing
analytics.

## docker-compose.yml

For official image:

```yaml
influxdb:
  image: influxdb:alpine
  ports:
    - "8083:8083"
    - "8086:8086"
  volumes:
    - ./data:/var/lib/influxdb
  restart: always
```

For unofficial image:

```yaml
influxdb:
  image: vimagick/influxdb
  ports:
    - "8083:8083"
    - "8086:8086"
    - "8088:8088"
    - "25826:25826/udp"
  restart: always
```

## up and running

```bash
$ docker-compose up -d
$ docker-compose exec influxdb influx
>>> show databases
name: databases
name
----
_internal
db_name

>>> use db_name
Using database db_name

>>> show series
key
---
cpu

>>> quit
```

- Open url: <http://localhost:8083>
- Create user: `CREATE USER "username" WITH PASSWORD 'password'`
- Create database: `CREATE DATABASE "db_name"`
- Select database: `db_name`
- Write data: `INSERT cpu,host=serverA,region=us_west value=0.64`
- Query data: `SELECT * FROM cpu`

[1]: https://influxdata.com/
[hbdg][1]
=========

Homebridge plugin for Home Assistant.

Control your accessories from Home Assistant with Siri and HomeKit. Set it up
and poof, all of your supported accessories will be instantly controllable via
Siri.

Here's a list of the devices that are currently exposed:

- Binary Sensor - door, leak, moisture, motion, smoke, and window state
- Climate - current temperature, target temperature, heat/cool mode
- Cover - exposed as a garage door or window covering (see notes)
- Device Tracker - home/not home status appears as an occupancy sensor
- Fan - on/off/speed
- Group - on/off
- Input boolean - on/off
- Lights - on/off/brightness
- Lock - lock/unlock lock
- Media Players - exposed as an on/off switch
- Scenes - exposed as an on/off switch
- Sensors - carbon dioxide (CO2), humidity, light, temperature sensors
- Switches - on/off


[1]: https://github.com/home-assistant/homebridge-homeassistant
ELK
===

- Elasticsearch
- Logstash
- Kibana
elasticsearch
=============

WARNING: IT DOES NOT WORK ON RASPBERRY PI!

```
$ sudo vi /etc/dphys-swapfile
- CONF_SWAPSIZE=100
+ CONF_SWAPSIZE=2048

$ sudo systemctl restart dphys-swapfile.service

$ docker-compose up -d

$ docker-compose logs -f
elasticsearch_1  | Error occurred during initialization of VM
elasticsearch_1  | Could not reserve enough space for 2097152KB object heap
```
OpenVPN over Obfsproxy
======================

`Obfsproxy` is a pluggable transport proxy written in python.  
It provides several obfuscation method. I consider `scramblesuit` the best.  
I will update this image if there's better one.

![obfsproxy](http://www.cs.kau.se/philwint/scramblesuit/images/big_picture.png)

`scramblesuit` can transport any application that supports SOCKS.  
This includes `Tor`, `VPN`, `SSH`, and many other protocols. 

We can transport `OpenVPN` over `Obfsproxy`, so that firewall cannot detect it.  
In the following example, you should run `vimagick/openvpn` container first.  
Don't forget to edit `/etc/openvpn/openvpn.conf` to use `proto tcp`.  

## docker-compose.yml

```
data:
  image: busybox
  volumes:
    - /etc/openvpn

server:
  image: vimagick/openvpn
  ports:
    - "1194:1194/tcp"
  volumes_from:
    - data
  cap_add:
    - NET_ADMIN
  restart: always

obfsproxy:
  image: vimagick/obfsproxy
  ports:
    - "4911:4911"
  links:
    - server:openvpn
  environment:
    - PASSWORD=J23TNHPJPAOQJLTCPLFD4CQYVFY6MEVP
    - DEST_ADDR=openvpn
    - DEST_PORT=1194
    - LISTEN_ADDR=0.0.0.0
    - LISTEN_PORT=4911
  restart: always
```

To link a existing `openvpn` container, please use `external_links` instead of `links`.

```
obfsproxy:
  image: vimagick/obfsproxy
  ports:
    - "4911:4911"
  external_links:
    - openvpn_server_1:openvpn
  environment:
    - PASSWORD=J23TNHPJPAOQJLTCPLFD4CQYVFY6MEVP
    - DEST_ADDR=openvpn
    - DEST_PORT=1194
    - LISTEN_ADDR=0.0.0.0
    - LISTEN_PORT=4911
  restart: always
```

The default run mode is `server`. You can also run container in `client` mode.  
The following example shows us how to make a OpenVPN relay:

```
obfsproxy:
  image: vimagick/obfsproxy
  ports:
    - "1194:1194/tcp"
  environment:
    - PASSWORD=J23TNHPJPAOQJLTCPLFD4CQYVFY6MEVP
    - DEST_ADDR=vpn.easypi.info
    - DEST_PORT=4911
    - RUN_MODE=client
    - LISTEN_ADDR=0.0.0.0
    - LISTEN_PORT=1194
  restart: always
```

The password should be encoded by Base32 with fixed length.  
You can generate one via this command:

```
python -c 'import base64, os; print base64.b32encode(os.urandom(20))'
```

Note: There's no ports exposed in Dockerfile. You need to expose port explicitly.
The Bro Network Security Monitor
================================

[![](https://badge.imagelayers.io/vimagick/bro:latest.svg)](https://imagelayers.io/?images=vimagick/bro:latest 'Get your own badge on imagelayers.io')

`Bro` is a powerful network analysis framework that is much different from the
typical IDS you may know.

- Adaptable
- Efficient
- Flexible
- Forensics
- Commercially Supported
- In-depth Analysis
- Highly Stateful
- Open Interfaces
- Open Source

## docker-compose.yml

```
bro:
  image: vimagick/bro
  command: bro -C -i eth0
  volumes:
    - ./logs:/opt/bro/logs
  net: container:shadowsocks_shadowsocks_1
```

> We are going to monitor `shadowsocks` which is a socks5 server.

## up and running

```
$ cd ~/fig/bro/

$ docker-compose up -d

$ docker exec -it bro_bro_1 bash
>>> cat dns.log | bro-cut query | sort | uniq -c | sort -nr | head -5
    10 www.youtube.com
    3 twitter.com
    2 www.google.com
    1 www.baidu.com
    1 www.facebook.com
>>> exit
```

> Don't be evil!
portia
======

![](https://badge.imagelayers.io/vimagick/portia:latest.svg)

[Portia][1] is a tool that allows you to visually scrape websites without any
programming knowledge required.

## docker-compose.yml

```yaml
portia:
  image: vimagick/portia
  container_name: portia
  ports:
    - "9001:9001"
  volumes:
    - ./data:/app/slyd/data
  restart: always
```

## up and running

```bash
$ cd ~/fig/portia/
$ docker-compose up -d
$ firefox http://localhost:9001/static/index.html
```

[1]: https://github.com/scrapinghub/portia
piwik
=====

[`Piwik`][1] is the leading open alternative to Google Analytics that gives you
full control over your data. Piwik lets you easily collect data from websites,
apps & IoT and visualise this data and extract insights. Privacy is built-in.

## docker-compose.yml

```
piwik:
  image: vimagick/piwik
  ports:
    - "8000:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=piwik
  restart: always
```

[1]: http://piwik.org/

piknik
======

Copy/paste anything over the network.

## docker-compose.yml

```yaml
piknik:
  image: vimagick/piknik
  ports:
    - "8075:8075"
  volumes:
    - ./piknik.toml:/etc/piknik.toml
  restart: always
```

## Server Setup

```bash
$ touch piknik.toml
$ docker-compose run --rm piknik -genkeys > piknik.toml
$ vim piknik.toml
$ docker-compose up -d
```

> `piknik.toml` contains server/client settings.

## Client Setup

```bash
# ~/.bashrc

# pkc : read the content to copy to the clipboard from STDIN
alias pkc='piknik -copy'

# pkp : paste the clipboard content
alias pkp='piknik -paste'

# pkm : move the clipboard content
alias pkm='piknik -move'

# pkz : delete the clipboard content
alias pkz='piknik -copy < /dev/null' 
```

```bash
$ vim ~/.piknik.toml

$ source ~/.bashrc

$ pkc
hello world  
^D

$ pkp
hello world

$ pkm
hello world

$ pkm
The clipboard might be empty 
```
mosquitto
=========

![](https://badge.imagelayers.io/vimagick/mosquitto:latest.svg)

[Mosquitto][1] is an open source (BSD licensed) message broker that implements
the MQ Telemetry Transport protocol versions 3.1 and 3.1.1.

## docker-compose.yml

```yaml
mosquitto:
  image: vimagick/mosquitto
  ports:
    - "1883:1883"
    - "8080:8080"
    - "8883:8883"
  volumes:
    - ./data/mosquitto.conf:/etc/mosquitto/mosquitto.conf
    - ./data/pwfile:/etc/mosquitto/pwfile
    - ./data:/var/lib/mosquitto
  restart: always
```

## mosquitto.conf

```ini
port 1883
log_dest stdout
allow_anonymous false
password_file /etc/mosquitto/pwfile
persistence true
persistence_location /var/lib/mosquitto

###### ENABLE TLS ######
listener 8883
protocol mqtt
capath /etc/ssl/certs
certfile /var/lib/mosquitto/fullchain.pem
keyfile /var/lib/mosquitto/privkey.pem
require_certificate false
```

- `pwfile` is managed by [mosquitto_passwd][3].
- Two methods to support TLS:
  - You can get free TLS certificates from letsencrypt, `capath` is needed.
  - Self-signed TLS keys can be generated by [openssl][2], `cafile` is needed.

> It is important to use different certificate subject parameters for your self-signed CA, server and clients.

## server

```bash
$ mkdir -m 777 data
$ touch data/mosquitto.conf data/pwfile
$ vi data/mosquitto.conf
$ docker-compose up -d
$ docker-compose exec mosquitto sh
>>> cd /etc/mosquitto
>>> mosquitto_passwd -b pwfile username password
>>> cat pwfile
username:$6$IuF7JUzS1k/QoF3y$YpiClom7/==
>>> exit
$ docker-compose restart
$ docker-compose logs -f
Attaching to mosquitto_mosquitto_1
mosquitto_1  | 1478107412: mosquitto version 1.4.8 (build date 2016-05-16 14:17:19+0000) starting
mosquitto_1  | 1478107412: Config loaded from /etc/mosquitto/mosquitto.conf.
mosquitto_1  | 1478107412: Opening ipv4 listen socket on port 8883.
mosquitto_1  | 1478107412: Opening ipv6 listen socket on port 8883.
mosquitto_1  | 1478107437: New connection from 192.168.31.102 on port 8883.
mosquitto_1  | 1478107437: New client connected from 192.168.31.102 as mosqsub/38158-Kevins-Ma (c1, k60).
mosquitto_1  | 1478107585: New client connected from 192.168.31.102 as mosqpub/38324-Kevins-Ma (c1, k60).
mosquitto_1  | 1478107585: Client mosqpub/38324-Kevins-Ma disconnected.
```

## client

```bash
$ mosquitto_sub -d -h 192.168.31.231 -p 8883 --cafile ca.crt --insecure -u username -P password -t hello
Client mosqsub/38158-Kevins-Ma sending CONNECT
Client mosqsub/38158-Kevins-Ma received CONNACK
Client mosqsub/38158-Kevins-Ma sending SUBSCRIBE (Mid: 1, Topic: hello, QoS: 0)
Client mosqsub/38158-Kevins-Ma received SUBACK
Subscribed (mid: 1): 0
Client mosqsub/38158-Kevins-Ma sending PINGREQ
Client mosqsub/38158-Kevins-Ma received PINGRESP
received PUBLISH (d0, q0, r0, m0, 'hello', ... (5 bytes))
world
Client mosqsub/38158-Kevins-Ma sending PINGREQ
Client mosqsub/38158-Kevins-Ma received PINGRESP
```

```bash
$ mosquitto_pub -d -h 192.168.31.231 -p 8883 --cafile ca.crt --insecure -u username -P password -t hello -m world
Client mosqpub/38324-Kevins-Ma sending CONNECT
Client mosqpub/38324-Kevins-Ma received CONNACK
Client mosqpub/38324-Kevins-Ma sending PUBLISH (d0, q0, r0, m1, 'hello', ... (5 bytes))
Client mosqpub/38324-Kevins-Ma sending DISCONNECT
```

[1]: http://mosquitto.org/
[2]: https://mosquitto.org/man/mosquitto-tls-7.html
[3]: https://mosquitto.org/man/mosquitto_passwd-1.html
shoutcast
=========

![](https://upload.wikimedia.org/wikipedia/en/thumb/f/f7/SHOUTcast_logo.svg/200px-SHOUTcast_logo.svg.png)

[SHOUTcast][1] Server (DNAS) - The most popular online streaming server
software on the planet, used by over 50,000 broadcasters.

## How it works

```
+-----+       +-----------+
| mpd | ----> | shoutcast |
+-----+  PUB  +-----------+
                    ^
                    |SUB
           +--------+--------+
           |        |        |
        +-----+  +-----+  +-----+
        | mpd |  | mpd |  | mpd |
        +-----+  +-----+  +-----+
```

## docker-compose.yml

Server:

```yaml
shoutcast:
  image: vimagick/shoutcast
  ports:
    - "8000:8000"
    - "8001:8001"
  volumes:
    - ./sc_serv.conf:/opt/shoutcast/sc_serv.conf
  restart: always
```

Client:

```yaml
mpd:
  image: easypi/mpd-arm
  ports:
    - "6600:6600"
    - "8800:8800"
  volumes:
    - ./mpd.conf:/etc/mpd.conf
    - ./music:/var/lib/mpd/music
    - ./playlists:/var/lib/mpd/playlists
  devices:
    - /dev/snd
  restart: always
```

## sc_serv.conf

```ini
#
# http://wiki.shoutcast.com/wiki/SHOUTcast_DNAS_Server_2
#

adminpassword=hackme1
password=hackme2
requirestreamconfigs=1
streamid_1=1
streampath_1=/stream/1/
logfile=logs/sc_serv.log
w3clog=logs/sc_w3c.log
banfile=control/sc_serv.ban
ripfile=control/sc_serv.rip
```

## nginx.conf

```
http {
    server {
        listen 80;
        server_name shoutcast.easypi.info;
        location / {
            proxy_pass http://127.0.0.1:8000;
        }
    }
}

stream {
    server {
        listen 81;
        proxy_pass 127.0.0.1:8001;
    }
}
```

## mpd.conf

```
music_directory    "/var/lib/mpd/music"
playlist_directory "/var/lib/mpd/playlists"
db_file            "/var/lib/mpd/database"
log_file           "/var/log/mpd/mpd.log"
pid_file           "/var/run/mpd.pid"
state_file         "/var/lib/mpd/state"
sticker_file       "/var/lib/mpd/sticker.sql"

input {
        plugin "curl"
}

audio_output {
        type            "alsa"
        name            "My ALSA Device"
        mixer_type      "software"
}

audio_output {
        type            "httpd"
        name            "My HTTP Stream"
        encoder         "vorbis"
        port            "8800"
        bitrate         "128"
        format          "44100:16:1"
        always_on       "yes"
        tags            "yes"
}

audio_output {
        type            "shout"
        protocol        "shoutcast"
        encoding        "mp3"
        name            "My Shout Stream"
        host            "shoutcast.easypi.info"
        port            "80"
        mount           "/stream/1/"
        password        "hackme2"
        bitrate         "128"
        format          "44100:16:1"
}
```

> `MPD` doesn't support `streampath` setting.
> The `mount` setting doesn't work.

```bash
$ export MPD_HOST=192.168.31.104
$ mpc update
$ mpc lsplaylists
$ mpc load playlist
$ mpc repeat on
$ mpc play
$ vlc http://shoutcast.easypi.info/stream/1/
```

[1]: http://wiki.shoutcast.com/wiki/SHOUTcast
sslsplit
========

[SSLsplit][1] is a tool for man-in-the-middle attacks against SSL/TLS encrypted
network connections.

## docker-compose.yml

```yaml
sslsplit:
  image: vimagick/sslsplit
  command:
    -k keys/ca.key -c keys/ca.crt -P
    -l logs/connections.log -S logs
    tcp 0.0.0.0 8080
    ssl 0.0.0.0 8443
  net: host
  volumes:
    - ./data:/data
  working_dir: /data
  restart: unless-stopped
```

## Server Setup

```bash
$ mkdir -p data/{key,log}
$ openssl req -x509 -newkey rsa:2048 -nodes -keyout data/key/ca.key -out data/key/ca.crt -days 3650 -subj '/CN=EasyPi'
$ docker-compose up -d
```

```bash
# setup
sysctl -w net.ipv4.ip_forward=1
iptables -t nat -N SSLSPLIT
iptables -t nat -A SSLSPLIT -p tcp --dport 80 -j REDIRECT --to-ports 8080
iptables -t nat -A SSLSPLIT -p tcp --dport 443 -j REDIRECT --to-ports 8443

# enable
iptables -t nat -A PREROUTING -j SSLSPLIT

# disable
iptables -t nat -D PREROUTING -j SSLSPLIT
```

## Client Setup

```bash
sudo route change default 192.168.31.231
curl -k https://www.baidu.com/s?wd=hello+world
```

> ProTip: No warning dialog after importing `ca.crt` into system/browser.

## read more

- <https://blog.heckel.xyz/2013/08/04/use-sslsplit-to-transparently-sniff-tls-ssl-connections/>

[1]: <http://www.roe.ch/SSLsplit>
ShadowVPN
=========

![](https://badge.imagelayers.io/vimagick/shadowvpn:latest.svg)

[`ShadowVPN`][1] is a fast, safe VPN based on libsodium.

## directory tree

```
~/fig/shadowvpn/
├── docker-compose.yml
└── server.conf
```

> Please change password in [server.conf][2] to protect your server.

## docker-compose.yml

```
shadowvpn:
  image: vimagick/shadowvpn
  ports:
    - "1123:1123/udp"
  volumes:
    - "./server.conf:/etc/shadowvpn/server.conf"
  privileged: true
  restart: always
```

## server

```
$ docker-compose up -d
```

## client

```
$ shadowvpn -c /etc/shadowvpn/client.conf -v
```

> Change settings in [client.conf][3].

[1]: https://shadowvpn.org/
[2]: https://github.com/vimagick/ShadowVPN/blob/master/samples/server.conf
[3]: https://github.com/vimagick/ShadowVPN/blob/master/samples/client.conf
mongo
=====

[MongoDB][1] is an open-source, document database designed for ease of development and scaling.

## docker-compose.yml

```yaml
mongo:
  image: mongo
  command: --auth
  container_name: mongo
  ports:
    - "27017:27017"
  restart: always
```

## up and running

```bash
$ pwgen -1 8 2
pah4Xa0o
Aedahwa7

$ cd ~/fig/mongo/

$ docker-compose up -d

$ docker exec -it mongo bash
/# mongo
> use admin
> db.createUser({user: 'root', pwd: 'pah4Xa0o', roles: [{role: 'userAdminAnyDatabase', db: 'admin'}]})
> db.auth('root', 'pah4Xa0o')
> db.runCommand({usersInfo: 1})
> exit
/# exit

$ docker cp mongo:/usr/bin/mongo /usr/local/bin/

$ mongo mongodb://root:pah4Xa0o@localhost:27017/admin
> use mydb
> db.createUser({user: 'myuser', pwd: 'Aedahwa7', roles: [{role: 'readWrite', db: 'mydb'}]})
> exit

$ mongo mongodb://myuser:Aedahwa7@localhost:27017/mydb
> show collections
> exit
```

Read [this][2] and [this][3] for more help.

[1]: https://www.mongodb.org/
[2]: https://docs.mongodb.org/manual/tutorial/enable-authentication/
[3]: https://docs.mongodb.org/manual/reference/built-in-roles/
phpMyAdmin
==========

[phpMyAdmin][1] is a free software tool written in PHP, intended to handle the
administration of MySQL over the Web. phpMyAdmin supports a wide range of
operations on MySQL and MariaDB. Frequently used operations (managing
databases, tables, columns, relations, indexes, users, permissions, etc) can be
performed via the user interface, while you still have the ability to directly
execute any SQL statement.

[1]: http://www.phpmyadmin.net/
neo4j
=====

[Neo4j][1] is a highly scalable native graph database that leverages data
relationships as first-class entities, helping enterprises build intelligent
applications to meet today’s evolving data challenges.

## docker-compose

```yaml
neo4j:
  image: neo4j
  ports:
    - "7474:7474"
    - "7687:7687"
  volumes:
    - ./data:/data
  environment:
    - NEO4J_CACHE_MEMORY=4G
  ulimits:
    nofile:
      soft: 65535
      hard: 65535
  restart: always
```

## up and running

```
$ docker-compose up -d

$ docker-compose exec neo4j bin/neo4j-shell
>>> man
>>> export person_id="123"
>>> env
>>> create index on :Person(id);
>>> create (p:Person {id: {person_id}}) return p;
>>> match (p:Person {id: {person_id}}) return p;
>>> exit
```

[1]: http://neo4j.com/
nodebb
======

![](https://badge.imagelayers.io/vimagick/nodebb:latest.svg)

[NodeBB][1] Forum Software is powered by Node.js and built on either a Redis or MongoDB database.

Read the [docs][2].

## docker-compose.yml

```yaml
nodebb:
  image: vimagick/nodebb
  ports:
    - "4567:4567"
  links:
    - redis
  volumes:
    - ./data:/var/lib/nodebb
    - /usr/src/nodebb/build
  restart: always

redis:
  image: redis:alpine
  ports:
    - "127.0.0.1:6379:6379"
  volumes:
    - ./data:/data
  restart: always
```

## up and running

```bash
$ docker-compose up -d
$ vim data/config.json
$ docker-compose restart nodebb
$ firefox http://localhost:4567
```

> Host IP or address of your Redis instance: `redis`

## software upgrade

```bash
$ docker-compose pull nodebb
$ docker-compose exec nodebb sh
>>> ./nodebb build
>>> exit
```

[1]: https://nodebb.org/
[2]: https://docs.nodebb.org/en/latest/
webgoat
=======

![](https://badge.imagelayers.io/vimagick/webgoat:latest.svg)

[WebGoat][1] is a deliberately insecure Web Application.

### docker-compose.yml

```yaml
webgoat:
  image: vimagick/webgoat
  ports:
    - "8080:8080"
  restart: always
```

### up and running

```bash
docker-compose up -d
firefox http://localhost:8080/WebGoat/
```

[1]: https://github.com/WebGoat/WebGoat
dnsmasq
=======

[Dnsmasq][1] is a Domain Name System forwarder and Dynamic Host Configuration
Protocol server for small computer networks, created as free software.

## docker-compose.yml

```yaml
dnsmasq:
  image: vimagick/dnsmasq
  ports:
    - "53:53/tcp"
    - "53:53/udp"
    - "67:67/udp"
  volumes:
    - ./dnsmasq.d:/etc/dnsmasq.d
  cap_add:
    - NET_ADMIN
  restart: always
```

[1]: http://www.thekelleys.org.uk/dnsmasq/doc.html
pxe
===

The Preboot Execution Environment (PXE) is an environment to bootstrap
computers using a network card (i.e Ethernet, Token-Ring) independently of
available data storage devices (like hard disks) or installed operating
systems.

## docker-compose.yml

```yaml
pxe:
  image: vimagick/dnsmasq
  net: host
  volumes:
    - ./dnsmasq.conf:/etc/dnsmasq.d/dnsmasq.conf
    - ./tftpboot:/tftpboot
  restart: always

web:
  image: nginx:alpine
  ports:
    - "80:80"
  volumes:
    - ./nginx.conf:/etc/nginx/nginx.conf
    - ./html:/var/lib/nginx/html
  restsart: always
```

> :warning: The local mirror doesn't work!

## dnsmasq.conf

```
interface=eth0
port=0
no-hosts
no-resolv
server=8.8.8.8
dhcp-range=192.168.1.10,192.168.1.20,1h,proxy
dhcp-option=3,192.168.1.1
dhcp-option=6,192.168.1.1
dhcp-boot=pxelinux.0
enable-tftp
tftp-root=/tftpboot
```

> You can get all `dhcp-option` codes via `dnsmasq --help dhcp`.

## nginx.conf

```
worker_processes  1;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
            autoindex on;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
```

## up and running

```bash
cd ~/fig/pxe/

mkdir tftpboot
wget http://ftp.nl.debian.org/debian/dists/jessie/main/installer-amd64/current/images/netboot/netboot.tar.gz
tar xzf netboot.tar.gz -C tftpboot

mkdir html
wget http://cdimage.debian.org/debian-cd/8.5.0/amd64/iso-cd/debian-8.5.0-amd64-CD-1.iso
mount debian-8.5.0-amd64-CD-1.iso html

docker-compose up -d
docker-compose logs -f
```

> You should stop DHCP service on local network before starting PXE.
> Also take a look at `preseed.cfg` for unattended installation.
# All files in this directory which end in .conf
# will be read by dnsmasq as configuration files
#
# This can be changed by editing /etc/dnsmasq.conf
tesseract
=========

![](https://badge.imagelayers.io/vimagick/tesseract:latest.svg)

[Tesseract][1] is an Open Source OCR engine, available under the Apache 2.0
license. It can be used directly, or (for programmers) using an API. It
supports a wide variety of languages.

Tesseract doesn't have a built-in GUI, but there are several available from the
3rdParty page.

Quick Start
-----------

```
$ alias tesseract='docker run --rm -v `pwd`:/work -w /work vimagick/tesseract'
$ tesseract myscan.png out
$ cat out.txt
```

[1]: https://github.com/tesseract-ocr/tesseract
kcptun
======

[kcptun][1] is an extremely simple & fast udp tunnel based on kcp protocol.

## How It Works

![][2]

## docker-compose.yml

```yaml
server:
  image: vimagick/kcptun
  command:
    --listen :29900
    --target google-public-dns-a.google.com:53
  ports:
    - "29900:29900/udp"
  environment:
    - KCPTUN_KEY=******
  restart: always

client:
  image: vimagick/kcptun
  entrypoint: client_linux_amd64
  command:
    --localaddr :12948
    --remoteaddr easypi.info:29900
  ports:
    - "12948:12948/tcp"
  environment:
    - KCPTUN_KEY=******
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d server
$ docker-compose logs -f server
```

## Client Setup

```bash
$ docker-compose up -d client
$ docker-compose logs -f client
$ dig @127.0.0.1 -p 12948 www.google.com +tcp
```

[1]: https://github.com/xtaci/kcptun
[2]: https://github.com/xtaci/kcptun/raw/master/kcptun.png
taskd
=====

[![](https://badge.imagelayers.io/vimagick/taskd:latest.svg)](https://imagelayers.io/?images=vimagick/taskd:latest)

`Taskwarrior` is Free and Open Source Software that manages your TODO list from
your command line. It is flexible, fast, efficient, and unobtrusive. It does
its job then gets out of your way.

## directory tree

```
~/fig/taskd/
├── docker-compose.yml
├── setup.sh*
└── taskd/
    ├── ca.cert.pem
    ├── ca.key.pem
    ├── client.cert.pem
    ├── client.key.pem
    ├── config
    ├── orgs/
    │   └── datageek/
    │       ├── groups/
    │       └── users/
    │           └── cf31f287-ee9e-43a8-843e-e8bbd5de4294/
    │               └── config
    ├── pki/
    │   ├── generate*
    │   ├── generate.ca*
    │   ├── generate.client*
    │   ├── generate.crl*
    │   ├── generate.server*
    │   ├── README
    │   └── vars
    ├── server.cert.pem
    ├── server.crl.pem
    ├── server.key.pem
    └── users/
        ├── kev.cert.pem
        └── kev.key.pem
```

## server setup

```
#!/bin/bash
#
# setup taskd server
#

mkdir -p ~/fig/taskd/taskd/{orgs,users}
cd ~/fig/taskd/taskd/

wget -O- http://taskwarrior.org/download/taskd-1.1.0.tar.gz |
    tar xvz --strip 1 taskd-1.1.0/pki

cd pki
vim vars
./generate
./generate.client kev
mv kev.* ../users
mv *.pem ..
cd ..

cat > config << _EOF_
confirmation=1
extensions=/usr/libexec/taskd
ip.log=on
log=/dev/stdout
pid.file=/run/taskd.pid
queue.size=10
request.limit=1048576
root=/var/taskd
server=0.0.0.0:53589
trust=strict
verbose=1
client.cert=/var/taskd/client.cert.pem
client.key=/var/taskd/client.key.pem
server.cert=/var/taskd/server.cert.pem
server.key=/var/taskd/server.key.pem
server.crl=/var/taskd/server.crl.pem
ca.cert=/var/taskd/ca.cert.pem
_EOF_
```

## docker-compose.yml

```
taskd:
  image: vimagick/taskd
  ports:
    - "53589:53589"
  volumes:
    - taskd:/var/taskd
  restart: always
```

## server

```
$ cd ~/fig/taskd/
$ ./setup.sh
$ docker-compose up -d
$ docker exec -it taskd_taskd_1 sh
>>> taskd add org datageek
    Created organization 'datageek'
>>> taskd add user datageek kev
    New user key: cf31f287-ee9e-43a8-843e-e8bbd5de4294
    Created user 'kev' for organization 'datageek'
>>> exit
$ tree -F
```

## client

```
$ brew install task --with-gnutls
$ cd ~/.task
$ scp jp:fig/taskd/taskd/{ca.cert,users/kev.*}.pem .
$ task config taskd.certificate -- ~/.task/kev.cert.pem
$ task config taskd.key         -- ~/.task/kev.key.pem
$ task config taskd.ca          -- ~/.task/ca.cert.pem
$ task config taskd.server      -- easypi.info:53589
$ task config taskd.credentials -- datageek/kev/cf31f287-ee9e-43a8-843e-e8bbd5de4294
$ task config taskd.trust       -- ignore hostname
$ task sync init
```
swarm-arm
=========

![](https://badge.imagelayers.io/vimagick/swarm-arm:latest.svg)

- https://docs.docker.com/swarm/install-manual/
- https://docs.docker.com/swarm/discovery/
- https://github.com/docker/swarm-library-image

```bash
# on master
$ docker run --rm vimagick/swarm-arm create
f9723931d522b39b756bb0b24a7cfc04

# on slave 1
$ docker run -d --restart always vimagick/swarm-arm join --addr=192.168.1.204:2375 token://f9723931d522b39b756bb0b24a7cfc04

# on slave 2
$ docker run -d --restart always vimagick/swarm-arm join --addr=192.168.1.204:2375 token://f9723931d522b39b756bb0b24a7cfc04

# on slave 3
$ docker run -d --restart always vimagick/swarm-arm join --addr=192.168.1.204:2375 token://f9723931d522b39b756bb0b24a7cfc04

# on master
$ curl https://discovery.hub.docker.com/v1/clusters/f9723931d522b39b756bb0b24a7cfc04
$ docker run --rm vimagick/swarm-arm list token://f9723931d522b39b756bb0b24a7cfc04
$ docker run -d --restart always -p 2375:2375 vimagick/swarm-arm manage token://f9723931d522b39b756bb0b24a7cfc04
$ export DOCKER_HOST=tcp://127.0.0.1:2375
$ docker info
```

> Any host can be a master as long as it can reach slaves!
tinc
====

[tinc][1] is a Virtual Private Network (VPN) daemon that uses tunnelling and
encryption to create a secure private network between hosts on the Internet.

To use this image, you need to have basic knowledges of tinc. (See this [tutor][2])

## Directory Tree

```
~/fig/tinc/
├── docker-compose.yml
└── tinc/
    └── netname/
        ├── hosts/
        │   ├── client
        │   ├── client-down*
        │   ├── client-up*
        │   └── server
        ├── rsa_key.priv
        ├── tinc.conf
        ├── tinc-down*
        └── tinc-up*
```

## docker-compose.yml

```yaml
tinc:
  image: vimagick/tinc
  ports:
    - "655:655/tcp"
    - "655:655/udp"
  volumes:
    - ./tinc:/etc/tinc
  environment:
    - IP_ADDR=1.2.3.4
  cap_add:
    - NET_ADMIN
  dns: 8.8.8.8
  restart: always
```

## Server Setup

```bash
# run
$ docker-compose up -d

# monitor
$ docker-compose logs

# stats
$ watch docker-compose exec tinc netstat -an
```

## Client Setup

```bash
# start
$ tincd -d -D -n netname --pidfile /tmp/tinc.pid

# stop
$ tincd -k --pidfile /tmp/tinc.pid
```

## Client Setup (openwrt)

```bash
$ opkg install tinc ip
 
$ cat > /etc/config/tinc
config tinc-net netname
    option enabled 1
config tinc-host linkit
    option enabled 1
    option net netname
config tinc-host server
    option enabled 1
    option net netname
 
$ mkdir -p /etc/tinc/netname/hosts
 
$ cat > /etc/tinc/netname/tinc.conf
Name = linkit
Interface = tun0
ConnectTo = server
 
$ cat > /etc/tinc/netname/hosts/linkit
Subnet = 10.0.0.125
 
$ tincd -n netname -K < /dev/null
Generating 2048 bits keys:
......+++ p
.....+++ q
Done.
 
$ cat > /etc/tinc/netname/tinc-up
#!/bin/sh
ip link set $INTERFACE up
ip addr add  10.0.0.125/24 dev $INTERFACE
 
$ cat > /etc/tinc/netname/tinc-down
#!/bin/sh
ip addr del 10.0.0.125/24 dev $INTERFACE
ip link set $INTERFACE down
 
$ cat > /etc/tinc/netname/hosts/server-up
#!/bin/sh
ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-3`
ip route add $REMOTEADDRESS $ORIGINAL_GATEWAY
ip route add 0.0.0.0/1 dev $INTERFACE
ip route add 128.0.0.0/1 dev $INTERFACE
 
$ cat > /etc/tinc/netname/hosts/server-down
#!/bin/sh
ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-3`
ip route del $REMOTEADDRESS $ORIGINAL_GATEWAY
ip route del 0.0.0.0/1 dev $INTERFACE
ip route del 128.0.0.0/1 dev $INTERFACE
 
$ chmod +x /etc/tinc/netname/tinc-*
$ chmod +x /etc/tinc/netname/hosts/server-*
 
$ scp /etc/tinc/netname/hosts/linkit root@remote-server:/etc/tinc/netname/hosts/
$ scp root@remote-server:/etc/tinc/netname/hosts/server /etc/tinc/netname/hosts/
 
$ /etc/init.d/tinc start
$ /etc/init.d/tinc enable
 
$ ifconfig tun0
 
$ firefox http://192.168.1.125/cgi-bin/luci/
 
# Firewall:
# | lan => wan, vpn | ooo | xx |
# | wan =>          | oox | oo |
# | vpn => wan      | ooo | ox |
```

[1]: http://tinc-vpn.org/
[2]: https://www.digitalocean.com/community/tutorials/how-to-install-tinc-and-set-up-a-basic-vpn-on-ubuntu-14-04
tinc-client
===========

```
[root@alarmpi ~]# mkdir -p /etc/tinc/netname/hosts

[root@alarmpi ~]# cat > /etc/tinc/netname/tinc.conf
Name = alarmpi
Interface = tun0
ConnectTo = server

[root@alarmpi ~]# cat > /etc/tinc/netname/hosts/alarmpi
Subnet = 10.0.0.2

[root@alarmpi ~]# tincd -n netname -K
Generating 2048 bits keys:
.................................................+++ p
.................................................+++ q
Done.
Please enter a file to save private RSA key to [/etc/tinc/netname/rsa_key.priv]:
Please enter a file to save public RSA key to [/etc/tinc/netname/hosts/alarmpi]:

[root@alarmpi ~]# cat > /etc/tinc/netname/tinc-up
#!/bin/sh
ip link set $INTERFACE up
ip addr add  10.0.0.2/24 dev $INTERFACE

[root@alarmpi ~]# cat > /etc/tinc/netname/tinc-down
#!/bin/sh
ip addr del 10.0.0.2/24 dev $INTERFACE
ip link set $INTERFACE down

[root@alarmpi ~]# cat > /etc/tinc/netname/hosts/server-up
#!/bin/sh
ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-3`
ip route add $REMOTEADDRESS $ORIGINAL_GATEWAY
ip route add 0.0.0.0/1 dev $INTERFACE
ip route add 128.0.0.0/1 dev $INTERFACE

[root@alarmpi ~]# cat > /etc/tinc/netname/hosts/server-down
#!/bin/sh
ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-3`
ip route del $REMOTEADDRESS $ORIGINAL_GATEWAY
ip route del 0.0.0.0/1 dev $INTERFACE
ip route del 128.0.0.0/1 dev $INTERFACE

[root@alarmpi ~]# chmod +x /etc/tinc/netname/tinc-{up,down}
[root@alarmpi ~]# chmod +x /etc/tinc/netname/hosts/server-{up,down}

[root@alarmpi ~]# scp /etc/tinc/netname/hosts/alarmpi root@remote-server:/etc/tinc/netname/hosts/
[root@alarmpi ~]# scp root@remote-server:/etc/tinc/netname/hosts/server /etc/tinc/netname/hosts/

[root@alarmpi ~]# tree -AF /etc/tinc/
/etc/tinc/
└── netname/
    ├── hosts/
    │   ├── alarmpi
    │   ├── server
    │   ├── server-down
    │   └── server-up
    ├── rsa_key.priv
    ├── tinc-down*
    ├── tinc-up*
    └── tinc.conf

[root@alarmpi ~]# cat > /etc/systemd/network/eth0.network
[Match]
Name=eth0

[Network]
DHCP=yes
UseDNS=false
DNS=8.8.8.8
IPMasquerade=yes

[root@alarmpi ~]# cat > /etc/sysctl.d/99-sysctl.conf
net.ipv4.ip_forward=1
net.ipv6.conf.all.disable_ipv6=1
[root@alarmpi ~]# systemctl restart systemd-sysctl

[root@alarmpi ~]# iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -j MASQUERADE
[root@alarmpi ~]# iptables-save > /etc/iptables/iptables.rules
[root@alarmpi ~]# systemctl enable iptables

[root@alarmpi ~]# systemctl start tincd@netname
[root@alarmpi ~]# systemctl status tincd@netname
[root@alarmpi ~]# systemctl enable tincd@netname
```
rslsync
=======

[Resilio Sync][1] is a fast, simple, and secure file syncing for IT and individuals.

## docker-compose.yml

```yaml
rslsync:
  image: vimagick/rslsync
  ports:
    - "8888:8888"
    - "55555:55555"
  volumes:
    - ./data:/data
    - ./rslsync.json:/etc/rslsync.json
  restart: always
```

## rslsync.json

```json
{
  "device_name": "Resilio Sync Server",
  "listening_port": 55555,
  "storage_path": "/data/system",
  "pid_file": "/var/run/rslsync.pid",
  "use_upnp": false,
  "download_limit": 0,
  "upload_limit": 0,
  "directory_root": "/data/user",
  "directory_root_policy": "all",
  "webui": {
    "listen": "0.0.0.0:8888",
    "login": "admin",
    "password": "admin"
  }
}
```

> Please change the default login/password.

## up and running

```bash
$ docker-compose run --rm rslsync rslsync --dump-sample-config > rslsync.json
$ vi rslsync.json
$ docker-compose up -d
```

[1]: https://getsync.com/
snort
=====

![](https://badge.imagelayers.io/vimagick/snort:latest.svg)

[`Snort`][1] is an open source intrusion prevention system capable of real-time
traffic analysis and packet logging.

## Quick Start

```
$ alias snort='docker run --rm --net host -v `pwd`:/work -w /work vimagick/snort'
$ snort -vde
```

[1]: https://snort.org/
privoxy
=======

![](https://badge.imagelayers.io/vimagick/privoxy:latest.svg)

[Privoxy][1] is a non-caching web proxy with advanced filtering capabilities for
enhancing privacy, modifying web page data and HTTP headers, controlling
access, and removing ads and other obnoxious Internet junk. Privoxy has a
flexible configuration and can be customized to suit individual needs and
tastes. It has application for both stand-alone systems and multi-user
networks.

## directory tree

```
~/fig/privoxy/
├── docker-compose.yml
└── privoxy/
    ├── user.action
    └── user.filter
```

file: docker-compose.yml

```yaml
privoxy:
  image: vimagick/privoxy
  ports:
    - "8118:8118"
  volumes:
    - ./privoxy/user.action:/etc/privoxy/user.action
    - ./privoxy/user.filter:/etc/privoxy/user.filter
  cap_add:
    - NET_ADMIN
  restart: always
```

file: user.action

```
{+filter{rot}}
/

{+block}
127.0.0.1
45.32.57.113
.easypi.info
```

> :warning: Make sure you block ip/domain which point to server itself.

file: user.filter

```
FILTER: rot rotate image
s|</head>|<style>img{transform: rotate(180deg);}</style></head>|gisU
```

## server

```bash
$ cd ~/fig/privoxy/
$ docker-compose up -d
$ docker-compose logs
Attaching to privoxy_privoxy_1
privoxy_1 | 2015-06-28 17:47:32.838 7ff17bdb5048 Info: Privoxy version 3.0.23
privoxy_1 | 2015-06-28 17:47:32.838 7ff17bdb5048 Info: Program name: privoxy
privoxy_1 | 2015-06-28 17:47:32.839 7ff17bdb5048 Info: Loading filter file: /etc/privoxy/default.filter
privoxy_1 | 2015-06-28 17:47:32.845 7ff17bdb5048 Info: Loading filter file: /etc/privoxy/user.filter
privoxy_1 | 2015-06-28 17:47:32.845 7ff17bdb5048 Info: Loading actions file: /etc/privoxy/match-all.action
privoxy_1 | 2015-06-28 17:47:32.845 7ff17bdb5048 Info: Loading actions file: /etc/privoxy/default.action
privoxy_1 | 2015-06-28 17:47:32.852 7ff17bdb5048 Info: Loading actions file: /etc/privoxy/user.action
privoxy_1 | 2015-06-28 17:47:32.852 7ff17bdb5048 Info: Listening on port 8118 on IP address 0.0.0.0
privoxy_1 | 2015-06-28 17:48:27.607 7ff17bff3ab0 Request: www.example.org/
privoxy_1 | 2015-06-28 17:48:53.069 7ff17bff3ab0 Request: www.example.org/
```

## client

```bash
$ http_proxy=127.0.0.1:8118 wget -O- http://www.example.org
$ docker run --rm --net container:privoxy_privoxy_1 alpine wget -O- http://www.example.org
```

In both cases, you will see `<style>img{transform: rotate(180deg);}</style></head>` in output.

[1]: https://www.privoxy.org/
dokuwiki
========

![](https://badge.imagelayers.io/vimagick/dokuwiki:latest.svg)

[DokuWiki][1] is a simple to use and highly versatile Open Source wiki software that doesn't require a database. 

## Run

```
$ docker run -d -p 8000:80 --restart always --name dokuwiki vimagick/dokuwiki
```

## Backup

```
$ docker run --rm --volumes-from dokuwiki -v `pwd`:/backup alpine tar czf /backup/dw-backup.tgz /var/www/html
```

## Restore

```
$ docker run --rm --volumes-from dokuwiki -v `pwd`:/backup alpine tar xzf /backup/dw-backup.tgz
```

## Plugin

You can install [plugins][2] via **Extension Manager**.

- [backup](https://www.dokuwiki.org/plugin:backup)
- [emoji](https://www.dokuwiki.org/plugin:emoji)
- [wrap](https://www.dokuwiki.org/plugin:wrap)

You can also install them manually.

```
$ docker exec -it dokuwiki sh
>>> cd /var/www/html/lib/plugins/
>>> mkdir backup emoji wrap
>>> wget -O- https://github.com/tatewake/dokuwiki-plugin-backup/archive/master.tar.gz | tar xz --strip 1 -C backup
>>> wget -O- https://github.com/ptbrown/dokuwiki-plugin-emoji/archive/master.tar.gz | tar xz --strip 1 -C emoji
>>> wget -O- https://github.com/selfthinker/dokuwiki_plugin_wrap/archive/stable.tar.gz | tar xz --strip 1 -C wrap
>>> chown -R nobody:nobody backup emoji wrap
```

[1]: https://www.dokuwiki.org/dokuwiki
[2]: https://www.dokuwiki.org/plugins
certbot
=======

[Let’s Encrypt][1] is a new Certificate Authority:
It’s free, automated, and open.

## docker-compose.yml

```
certbot:
  image: quay.io/letsencrypt/letsencrypt
  command: certonly --standalone
  ports:
    - "80:80"
    - "443:443"
  volumes:
    - /etc/letsencrypt:/etc/letsencrypt
    - /var/lib/letsencrypt:/var/lib/letsencrypt
```

## up and running

```
# stop nginx (release 80/tcp and 443/tcp)
$ systemctl stop nginx

# generate keys (interactive)
$ docker-compose run --rm --service-ports certbot
>>> email: admin@easypi.info
>>> domains: easypi.info,blog.easypi.info,wiki.easypi.info

# renew keys (headless)
$ crontab -l
0 0 * * * cd ~/fig/certbot && docker-compose run --rm certbot renew >> renew.log

# list keys
$ tree /etc/letsencrypt/live/
/etc/letsencrypt/live/
└── easypi.info
    ├── cert.pem -> ../../archive/easypi.info/cert1.pem
    ├── chain.pem -> ../../archive/easypi.info/chain1.pem
    ├── fullchain.pem -> ../../archive/easypi.info/fullchain1.pem
    └── privkey.pem -> ../../archive/easypi.info/privkey1.pem

# deploy keys
$ mkdir -p /etc/nginx/ssl/
$ cp /etc/letsencrypt/live/easypi.info/fullchain.pem /etc/nginx/ssl/easypi.info.crt
$ cp /etc/letsencrypt/live/easypi.info/privkey.pem /etc/nginx/ssl/easypi.info.key

# reconfig nginx
$ vi /etc/nginx/sites-enabled/default
server {
    listen 80 default;
    server_name _;
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name easypi.info blog.easypi.info;
    ssl_certificate ssl/easypi.info.crt;
    ssl_certificate_key ssl/easypi.info.key;
    location / {
        proxy_pass http://127.0.0.1:8000;
    }
}

# start nginx
$ systemctl start nginx
```

You can also generate keys without docker.

```bash
# install
apt install build-essential dialog libffi-dev libssl-dev python2.7-dev
curl -sSL https://bootstrap.pypa.io/get-pip.py | python2
pip2 install letsencrypt

# generate
letsencrypt certonly --standalone -d easypi.info -d blog.easypi.info -d wiki.easypi.info

# deploy
mkdir -p /etc/nginx/ssl
cp /etc/letsencrypt/live/easypi.info/fullchain.pem /etc/nginx/ssl/easypi.info.crt
cp /etc/letsencrypt/live/easypi.info/privkey.pem /etc/nginx/ssl/easypi.info.key

# renew
letsencrypt renew
```

## references

- <https://letsencrypt.readthedocs.org/en/latest/using.html#running-with-docker>
- <https://docs.docker.com/compose/reference/run/>
- <http://nginx.org/en/docs/http/configuring_https_servers.html>
- <https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-16-04>
- <http://support.ghost.org/setup-ssl-self-hosted-ghost/>

[1]: https://letsencrypt.org/
Dante - A free SOCKS server
===========================

[Dante][1] is a product developed by Inferno Nettverk A/S. It consists of a
SOCKS server and a SOCKS client, implementing RFC 1928 and related standards.
It is a flexible product that can be used to provide convenient and secure
network connectivity. 

## docker-compose.yml

```
dante:
  image: vimagick/dante
  ports:
    - "1080:1080"
  volumes:
    - ./sockd.conf:/etc/sockd.conf
  restart: always
```

## sockd.conf

```
debug: 0
logoutput: stderr
internal: 0.0.0.0 port = 1080
external: eth0
socksmethod: username none
clientmethod: none
user.privileged: root
user.unprivileged: nobody

client pass {
    from: 0.0.0.0/0 port 1-65535 to: 0.0.0.0/0
    log: error
}

socks pass {
    from: 0.0.0.0/0 to: 0.0.0.0/0
    #socksmethod: username
    log: error
}
```

## up and running

```
$ docker-compose up -d

# To enable username authentication, please uncomment `socksmethod: username`.
$ docker exec -it dante_dante_1 bash
>>> useradd username
>>> echo username:password | chpasswd
>>> exit

$ curl -x socks5h://username:password@127.0.0.1:1080 https://www.youtube.com
```

[1]: http://www.inet.no/dante/index.html
phpVirtualBox
=============

This is a fork of [jazzdd86/phpvirtualbox](https://hub.docker.com/r/jazzdd/phpvirtualbox/).

[phpVirtualBox][1] is a web-based front-end to VirtualBox written in PHP.

### docker-compose.yml

```yaml
phpvirtualbox:
  image: vimagick/phpvirtualbox
  ports:
    - "8888:80"
  environment:
    - ID_PORT_18083_TCP=remote-server:18083
    - ID_NAME=Vbox
    - ID_USER=username
    - ID_PW=password
    - CONF_browserRestrictFolders=/data,
  restart: always
```

> - Make sure you can login `remote-server` with `username:password`.
> - ISO images can be placed at `/data` directory on `remote-server`.
> - During the OS installation, you can connect to it with RDP viewer.

### vboxweb.service

```
# /etc/systemd/system/vboxweb.service
[Unit]
Description=VirtualBox Web Service
After=network.target

[Service]
ExecStart=/usr/bin/vboxwebsrv -H 0.0.0.0 -p 18083
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

### server setup

```bash
# install virtualbox
echo "deb http://download.virtualbox.org/virtualbox/debian $(lsb_release -cs) contrib" > /etc/apt/sources.list.d/virtualbox.list
wget -O- http://download.virtualbox.org/virtualbox/debian/oracle_vbox_2016.asc | apt-key add -
apt-get update
apt-get install -y virtualbox-5.1 dkms
systemctl status vboxdrv

# install extpack
wget http://download.virtualbox.org/virtualbox/5.1.0/Oracle_VM_VirtualBox_Extension_Pack-5.1.0-108711.vbox-extpack
VBoxManage extpack install Oracle_VM_VirtualBox_Extension_Pack-5.1.0-108711.vbox-extpack
VBoxManage list extpacks

# install vagrant
wget https://releases.hashicorp.com/vagrant/1.8.5/vagrant_1.8.5_x86_64.deb
dpkg -i vagrant_1.8.5_x86_64.deb
vagrant version

# start vm
vagrant box add debian/jessie64
vagrant init debian/jessie64
vagrant up

# disable vboxweb-service
systemctl stop vboxweb-service
systemctl disable vboxweb-service

# enable vboxweb
systemctl daemon-reload
systemctl start vboxweb
systemctl enable vboxweb
```

[1]: http://sourceforge.net/projects/phpvirtualbox/
phpvirtualbox
=============

This is a fork of [jazzdd86/phpvirtualbox](https://hub.docker.com/r/jazzdd/phpvirtualbox/).

[phpVirtualBox][1] is a modern web interface that allows you to control remote
VirtualBox instances - mirroring the VirtualBox GUI.

## docker-compose.yml

```yaml
phpvirtualbox:
  image: easypi/phpvirtualbox-arm
  ports:
    - "8888:80"
  environment:
    - ID_PORT_18083_TCP=remote-server:18083
    - ID_NAME=Vbox
    - ID_USER=username
    - ID_PW=password
    - CONF_browserRestrictFolders=/data,
  restart: always
```

## screenshot

![][2]

[1]: http://sourceforge.net/projects/phpvirtualbox/
[2]: http://a.fsdn.com/con/app/proj/phpvirtualbox/screenshots/phpvb1.png
traefik
=======

![](https://traefik.io/traefik.logo.svg)

[Træfɪk][1] is a modern HTTP reverse proxy and load balancer made to deploy
microservices with ease. It supports several backends (Docker 🐳, Swarm 🐳🐳,
Mesos/Marathon, Consul, Etcd, Zookeeper, BoltDB, Rest API, file...) to manage
its configuration automatically and dynamically.

## up and running

```bash
$ kubectl cluster-info
$ kubectl create -f k8s.rc.yaml -f k8s.ing.yaml
$ kubectl run website --image=easypi/nginx-arm --port=80 --replicas=4
$ kubectl expose deployment website --type=NodePort
$ curl --resolve website:80:192.168.31.231 http://website/
```

## references

- <https://medium.com/@rothgar/exposing-services-using-ingress-with-on-prem-kubernetes-clusters-f413d87b6d34>

[1]: https://github.com/containous/traefik
graphite
========

[Graphite][1] does three things:

- Kick ass.
- Chew bubblegum.
- Make it easy to store and graph metrics.

## docker-compose.yml

```yaml
graphite:
  image: vimagick/graphite
  ports:
    - "2003:2003"
    - "2004:2004"
    - "7002:7002"
    - "8080:8080"
    - "9001:9001"
  volumes:
    - ./data:/opt/graphite/storage
  restart: always
```

## Up and Running

```bash
$ cd ~/fig/graphite
$ mkdir -p data/log/webapp
$ docker-compose up -d
$ docker-compose exec graphite sh
>>> vi conf/storage-schemas.conf
>>> python webapp/manage.py migrate --run-syncdb --noinput
>>> exit
$ tree -F -L 3
├── data/
│   ├── carbon-cache-a.pid
│   ├── graphite.db
│   ├── log/
│   │   └── webapp/
│   └── whisper/
│       └── carbon/
└── docker-compose.yml
$ curl http://localhost:8080
```

## storage-schemas.conf

```ini
[carbon]
pattern = ^carbon\.
retentions = 60:90d

[leancloud_1day_for_1year]
pattern = ^test\.
retentions = 1d:1y

[default_1min_for_1day]
pattern = .*
retentions = 60s:1d
```

## Resize Whisper

```bash
$ docker-compose exec graphite sh
>>> cd storage/whisper/test
>>> find . -type f -name '*.wsp' -exec whisper-resize.py --nobackup {} 1d:1y \;
```

[1]: http://graphiteapp.org/
nextcloud
=========

[Nextcloud][1] puts your data at your fingertips, under your control.

## docker-compose.yml

```yaml
nextcloud:
  image: indiehosters/nextcloud
  ports:
    - "127.0.0.1:9000:9000"
  volumes:
    - ./data/apps:/var/www/html/apps
    - ./data/config:/var/www/html/config
    - ./data/data:/var/www/html/data
  restart: always

nginx:
  image: nginx:alpine
  volumes:
    - ./nginx.conf:/etc/nginx/conf.d/default.conf
    - ./ssl:/etc/nginx/ssl
  volumes_from:
    - nextcloud
  net: host
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d
$ docker-compose exec --user www-data nextcloud bash
>>> php occ files:scan --all
Starting scan for user 1 out of 1 (admin)
+---------+-------+--------------+
| Folders | Files | Elapsed time |
+---------+-------+--------------+
| 10      | 21    | 00:00:00     |
+---------+-------+--------------+
>>> exit
```

You can use the [occ][2] admin tool.

## Client Setup

- Android: <https://download.nextcloud.com/android/>
- MacOSX: <https://download.nextcloud.com/desktop/releases/Mac/Installer/>
- Windows: <https://download.nextcloud.com/desktop/releases/Windows/>

[1]: https://nextcloud.com/
[2]: https://docs.nextcloud.com/server/9/admin_manual/configuration_server/occ_command.html
errbot
======

![](http://errbot.io/en/latest/_static/errbot.png)

[Errbot][1] is a chatbot, a daemon that connects to your favorite chat service
and brings your tools into the conversation.


## docker-compose.yml

```yaml
errbot:
  image: vimagick/errbot
  volumes:
    - ./data:/home/errbot
    - /usr/lib/python3.5/site-packages
  tty: yes
  restart: always
```

## up and running

```bash
$ mkdir -m 777 data
$ docker-compose run --rm errbot --init
$ vim data/config.py
$ docker-compose up -d
$ docker-compose exec --user root errbot sh
>>> apk add -U py3-lxml
>>> chmod 777 /usr/lib/python3.5/site-packages
>>> chmod 777 /usr/lib/python3.5/site-packages/__pycache__
>>> exit
```

Check the example config.py [here][2].

## chat-ops

```
master [8:50 PM] !tryme
errbot [8:50 PM] It works!

master [8:51 PM] !help
errbot [8:51 PM] All commands ...

master [8:52 PM] !whoami
errbot [8:52 PM]
┏━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ key      ┃ value       ┃
┡━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ person   │ `@master`   │
├──────────┼─────────────┤
│ nick     │ `master`    │
├──────────┼─────────────┤
│ fullname │ `Mr Robot`  │
├──────────┼─────────────┤
│ client   │ `XXXXXXXXX` │
└──────────┴─────────────┘
```

## install plugins

```
master [8:53 PM] !repos update
errbot [8:53 PM] Start updating ... Done.

master [8:53 PM] !repos search nettools
errbot [8:53 PM]
┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Status ┃ Name                    ┃ Description ┃
┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│        │ *errbotio/err-nettools* │ ...         │
└────────┴─────────────────────────┴─────────────┘

master [8:54 PM] !repos install errbotio/err-nettools
errbot [8:54 PM] Installing errbotio/err-nettools ... Done.

master [8:55 PM] !help nettools
errbot [8:55 PM] 
    *Nettools*
    _Various tools to query info about IPs, networks and domain names_
    • *!check rbl* - usage: check_rbl [-h] [-t THREADS] address
    • *!dig* - Call 'dig'
    • *!geoip* - usage: geoip [-h] address
    • *!host* - Call 'host'
    • *!nslookup* - Call 'nslookup'
    • *!whois* - usage: whois [-h] domain

master [8:56 PM] !whois www.youtube.com
errbot [8:56 PM] 
               name : youtube.com
    expiration_date : 2018-02-14 00:00:00-08:00
       last_updated : 2017-01-14 02:23:26-08:00
       name_servers : {'ns2.google.com', 'ns4.google.com', 'ns3.google.com', 'ns1.google.com'}
      creation_date : 2005-02-14 21:13:12-08:00
          registrar : MarkMonitor, Inc.
```

[1]: http://errbot.io
[2]: http://errbot.io/en/latest/_downloads/config-template.py
See: <http://errbot.io/en/latest/user_guide/administration.html#dependencies>
strongswan
==========

![](https://badge.imagelayers.io/vimagick/strongswan:latest.svg)

[strongSwan][1] is an Open Source IPsec-based VPN solution for Linux and other
UNIX based operating systems implementing both the IKEv1 and IKEv2 key exchange
protocols.

> :warning: This docker image only support IKEv2!

### docker-compose.yml

```yaml
version: '2'
services:
  strongswan:
    image: vimagick/strongswan
    ports:
      - 500:500/udp
      - 4500:4500/udp
    volumes:
      - /lib/modules:/lib/modules
      - /etc/localtime:/etc/localtime
    environment:
      - VPN_DOMAIN=vpn.easypi.info
      - VPN_NETWORK=10.20.30.0/24
      - LAN_NETWORK=192.168.0.0/16
      - VPN_P12_PASSWORD=secret
    tmpfs: /run
    privileged: yes
    restart: always
```

### up and running

```bash
docker-compose up -d
docker cp strongswan_strongswan_1:/etc/ipsec.d/client.mobileconfig .
docker cp strongswan_strongswan_1:/etc/ipsec.d/client.cert.p12 .
docker-compose logs -f
```

- Mac/IOS: `client.mobileconfig`
- Android: `client.cert.p12`

[1]: https://strongswan.org/
OpenVPN
=======

[OpenVPN][1] is blocked in China. You need to connect vpn via secure tunnel.

Instead of using [fteproxy][2] as bridge, you can also use [stunnel][3] or [kcptun][4].

- server: vpn.easypi.info
- bridge: bridge.easypi.info
- client: localhost

## docker-compose.yml (server)

```yaml
openvpn:
  image: kylemanna/openvpn
  ports:
    - "1194:1194"
  volumes:
    - ./data:/etc/openvpn
  cap_add:
    - NET_ADMIN
  restart: always

fteproxy:
  image: vimagick/fteproxy
  ports:
    - "4911:4911"
  links:
    - openvpn
  environment:
    - MODE=server
    - SERVER_IP=0.0.0.0
    - SERVER_PORT=4911
    - PROXY_IP=openvpn
    - PROXY_PORT=1194
    - KEY=CB2FBA2BC70490526E749E01BB050F6B555964290DFF58CF24785B4A093F7B18
  restart: always
```

## docker-compose.yml (bridge)

```yaml
fteproxy:
  image: vimagick/fteproxy
  ports:
    - "1194:1194"
  environment:
    - MODE=client
    - SERVER_IP=vpn.easypi.info
    - SERVER_PORT=4911
    - CLIENT_IP=0.0.0.0
    - CLIENT_PORT=1194
    - KEY=CB2FBA2BC70490526E749E01BB050F6B555964290DFF58CF24785B4A093F7B18
  restart: always
```

## Server Setup

```bash
$ ./setup.sh
1) server ...... (Step 1)
2) client ...... (Step 2)
3) revoke
4) quit   ...... (Step 3)

$ docker-compose up -d
```

## Bridge Setup

```bash
$ docker-compose up -d
```

## Client Setup

```bash
$ cat /etc/openvpn/client.conf
...
remote bridge.easypi.info 1194 tcp
route 192.168.1.0 255.255.255.0 net_gateway
...
$ systemctl start openvpn@client
$ systemctl enable openvpn@client
```

## References

- <https://community.openvpn.net/openvpn/wiki>
- <https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-16-04>

[1]: https://openvpn.net/index.php/open-source.html
[2]: https://github.com/vimagick/dockerfiles/tree/master/fteproxy
[3]: https://github.com/vimagick/dockerfiles/tree/master/stunnel
[4]: https://github.com/vimagick/dockerfiles/tree/master/kcptun
openvpn
=======

Setup OpenVPN on Raspberry Pi.

## docker-compose.yml

```yaml
stunnel:
  image: easypi/stunnel-arm
  ports:
    - "1194:1194"
  environment:
    - CLIENT=yes
    - SERVICE=openvpn
    - ACCEPT=0.0.0.0:1194
    - CONNECT=server:4911
  extra_hosts:
    - server:1.2.3.4
  restart: always

openvpn:
  image: easypi/openvpn-arm
  command: --config pi.ovpn
  volumes:
    - ./data:/etc/openvpn
  net: host
  privileged: yes
  restart: always

dnsmasq:
  image: easypi/dnsmasq-arm
  volumes:
    - ./data/dnsmasq.conf:/etc/dnsmasq.d/dnsmasq.conf
  cap_add:
    - NET_ADMIN
  net: host
  restart: always
```

## up and running

```bash
$ echo 'net.ipv4.ip_forward=1' > /etc/sysctl.d/local.conf
$ sysctl -p /etc/sysctl.d/local.conf

$ iptables -t nat -A POSTROUTING -s 192.168.31.0/24 -o tun0 -j MASQUERADE
$ iptables-save -t nat | grep -vi docker > /etc/iptables/iptables.rules
$ systemctl enable iptables

$ docker-compose up -d stunnel # 1st
$ docker-compose up -d openvpn # 2nd
$ docker-compose up -d dnsmasq # 3rd
$ docker-compose logs -f
```

> :warning: The order of execution is very important!
cadvisor
========

![](https://badge.imagelayers.io/vimagick/cadvisor:latest.svg)

[cAdvisor][1] (Container Advisor) provides container users an understanding of
the resource usage and performance characteristics of their running containers.

## docker-compose.yml

```yaml
cadvisor:
  image: vimagick/cadvisor
  ports:
    - "8080:8080"
  volumes:
    - /:/rootfs:ro
    - /var/run:/var/run:rw
    - /sys:/sys:ro
    - /var/lib/docker/:/var/lib/docker:ro
  restart: always
```

[1]: https://github.com/google/cadvisor
rtmp
====

- NGINX-based Media Streaming Server.
- FFMPGE-based Media Streaming Client.

## Directory Tree

```
~/fig/rtmp/
├── data/
│   └── video.mp4
└── docker-compose.yml
```

## docker-compose.yml

```yaml
server:
  image: vimagick/rtmp-server
  ports:
    - "1935:1935"
    - "9999:80"
  volumes:
    - ./data:/data
  restart: always

client:
  image: easypi/rtmp-client-arm
# command:
#   - ffmpeg -i $$RTMP_DEV -video_size 800x600 -vf "hflip,vflip" -f flv $$RTMP_URI
#   - ffmpeg -f alsa -ac 1 -ar 22050 -i hw:1 -i $$RTMP_DEV -c:a aac -c:v flv1 -f flv $$RTMP_URI
  devices:
#   - /dev/snd
    - /dev/video0
  environment:
#   - RTMP_DEV=rtsp://192.168.42.1/live
    - RTMP_URI=rtmp://easypi.info/live/webcam
  restart: always
```

> - You can run customized command. (It should be single item list!)
> - Input can be a stream instead of device. (It works as a relay!)

## Server Setup

```
$ cd ~/fig/rtmp/
$ docker-compose up -d server
$ youtube-dl 'https://www.youtube.com/watch?v=lJZlz-WnXzU' -o data/video.mp4
```

## Client Setup

```
# play remote video (remote -> local)
$ vlc rtmp://easypi.info/vod/video.mp4

# play local video (local -> remote -> local)
$ ffmpeg -re -i video.mp4 -f flv rtmp://easypi.info/live/video
$ vlc rtmp://easypi.info/live/video

# capture desktop (local -> remote)
$ ffmpeg -f avfoundation -pixel_format bgr0 -i 1:0 -f flv rtmp://easypi.info/live/webcam

# record webcam (local -> remote)
$ ffmpeg -f qtkit -i 0 -f flv rtmp://easypi.info/live/webcam

# record pi camera (pi -> remote)
$ ffmpeg -f video4linux2 -r 24 -i /dev/video0 -f flv rtmp://easypi.info/live/webcam

# record pi camera (pi -> remote)
$ /opt/vc/bin/raspivid -o - -t 0 -hf -w 640 -h 360 -fps 25 | ffmpeg -i - -f flv rtmp://easypi.info/live/webcam

# watch webcam (remote -> local)
$ vlc rtmp://easypi.info/live/webcam

# watch webcam (remote -> local)
$ firefox http://easypi.info:9999/
```

Optinally, you can run a docker container as RTMP client on raspberry pi.

```
$ cd ~/fig/rtmp/
$ docker-compose up -d client
```

## OBS Setup

```yaml
Stream Type: Custom Streaming Server
URL: rtmp://192.168.31.254/live/
Streaming key: webcam
```

## Player Setup

- vlc
- ffplay
- [online](https://www.hlsplayer.net/rtmp-player)

## References

- https://github.com/arut/nginx-rtmp-module/wiki/Directives
- https://trac.ffmpeg.org/wiki/StreamingGuide
- https://trac.ffmpeg.org/wiki/Capture/Webcam
- https://trac.ffmpeg.org/wiki/Capture/ALSA
- https://trac.ffmpeg.org/wiki/EncodingForStreamingSites
- http://apk-dl.com/vlc-for-android/org.videolan.vlc/
PUT VIDEO FILES HERE
====================
`MantisBT` is an open source issue tracker that provides
a delicate balance between simplicity and power.

## docker-compose.yml

```
mantisbt:
  image: vimagick/mantisbt:latest
  ports:
    - "8989:80"
  links:
    - mysql
  restart: always

mysql:
  image: mysql:latest
  environment:
    - MYSQL_ROOT_PASSWORD=root
    - MYSQL_DATABASE=bugtracker
    - MYSQL_USER=mantisbt
    - MYSQL_PASSWORD=mantisbt
  restart: always
```

> You can use `mariadb`/`postgres` instead of `mysql`.

## install

```
$ firefox http://localhost:8989/admin/install.php
>>> username: administrator
>>> password: root
```

```
==================================================================================
Installation Options
==================================================================================
Type of Database                                        MySQL/MySQLi
Hostname (for Database Server)                          mysql
Username (for Database)                                 mantisbt
Password (for Database)                                 mantisbt
Database name (for Database)                            bugtracker
Admin Username (to create Database if required)         root
Admin Password (to create Database if required)         root
Print SQL Queries instead of Writing to the Database    [ ]
Attempt Installation                                    [Install/Upgrade Database]
==================================================================================
```

## email

Append following to `/var/www/html/config_inc.php`

```
$g_phpMailer_method = PHPMAILER_METHOD_SMTP;
$g_administrator_email = 'admin@example.org';
$g_webmaster_email = 'webmaster@example.org';
$g_return_path_email = 'mantisbt@example.org';
$g_from_email = 'mantisbt@example.org';
$g_smtp_host = 'smtp.example.org';
$g_smtp_port = 25;
$g_smtp_connection_mode = 'tls';
$g_smtp_username = 'mantisbt';
$g_smtp_password = '********';
```

hans
====

[Hans][1] makes it possible to tunnel IPv4 through ICMP echo packets, so you
could call it a ping tunnel.

## How It Works

```
Client ---> G.F.W ---> Server ---> Internet
 tun0       ping        tun0
```

## docker-compose.yml

```yaml
server:
  image: vimagick/hans
  environment:
    - NETWORK=10.1.2.0
    - PASSWORD=password
  net: host
  privileged: yes
  restart: always

client:
  image: easypi/hans-arm
  environment:
    - SERVER=1.2.3.4
    - PASSWORD=password
  net: host
  privileged: yes
  restart: always
```

## Server Setup

```bash
# Start Server Container
$ docker-compose up -d server

# Enable Masquerade (Method A)
$ iptables -t nat -A POSTROUTING -s 10.1.2.0/24 -o eth0 -j MASQUERADE

# Enable Masquerade (Method B)
$ firewall-cmd --add-masquerade
```

## Client Setup

```bash
# Start Client Container
$ docker-compose up -d client

# Access Server Directly
$ ip route add 1.2.3.4 via 192.168.1.1

# Change Default Route (Method A)
$ ip route change default via 10.1.2.1

# Change Default Route (Method B)
$ ip route add 0.0.0.0/1 dev tun0
$ ip route add 128.0.0.0/1 dev tun0

# Enable IP Forwarding
$ sysctl -w net.ipv4.ip_forward=1
$ iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o tun0 -j MASQUERADE
```

[1]: http://code.gerade.org/hans/
icecast
=======

[Icecast][1] is a streaming media project released as free software maintained
by the Xiph.org Foundation.

## docker-compose.yml

```yaml
icecast:
  image: vimagick/icecast
  ports:
    - "8000:8000"
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d
```

## Client Setup

- Linux: ices
- MacOS: butt

[1]: http://icecast.org/
registry
========

[Registry][1] is the Docker toolset to pack, ship, store, and deliver content.

## docker-compose.yml

```yaml
registry:
  image: registry:2
  ports:
    - "5000:5000"
  volumes:
    - /etc/docker/registry
    - ./data:/var/lib/registry
    - ./certs:/certs
    - ./auth:/auth
  environment:
    - REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt
    - REGISTRY_HTTP_TLS_KEY=/certs/domain.key
    - REGISTRY_AUTH=htpasswd
    - REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm
    - REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd
  restart: always

frontend:
  image: konradkleine/docker-registry-frontend:v2
  ports:
    - "8080:80"
    - "8443:443"
  links:
    - registry
  volume:
    - ./certs/domain.crt:/etc/apache2/domain.crt
    - ./certs/domain.key:/etc/apache2/domain.key
  environment:
    - ENV_DOCKER_REGISTRY_HOST=registry
    - ENV_DOCKER_REGISTRY_PORT=5000
    - ENV_DOCKER_REGISTRY_USE_SSL=1
    - ENV_USE_SSL=yes
  restart: always
```

## Server Setup

```bash
$ mkdir -p ~/fig/registry/{auth,certs}
$ cd ~/fig/registry
$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key -x509 -days 365 -out certs/domain.crt
$ docker-compose up -d
$ docker-compose exec registry sh
>>> htpasswd -Bbn username password >> /auth/htpasswd
>>> cat >> /etc/docker/registry/config.yml
proxy:
  remoteurl: https://registry-1.docker.io
  username: username
  password: password
^D
>>> exit
$ docker-compose restart
```

## Client Setup

```bash
$ scp registry.easypi.info:fig/registry/certs/domain.crt \
      /etc/docker/certs.d/registry.easypi.info:5000/ca.crt

$ systemctl edit docker
# /etc/systemd/system/docker.service.d/override.conf
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// --registry-mirror https://registry.easypi.info:5000
                           
$ systemctl daemon-reload
$ systemctl restart docker

$ docker pull alpine
$ docker tag alpine registry.easypi.info:5000/alpine

$ docker login -u username -p password easypi.info:5000
$ docker push registry.easypi.info:5000/alpine
$ docker rmi registry.easypi.info:5000/alpine
$ docker pull registry.easypi.info:5000/alpine

$ firefox http://registry.easypi.info:8080
```

> Append `--insecure-registry registry.easypi.info:5000` option to disable TLS.

## Read More

- https://github.com/docker/distribution/blob/master/docs/deploying.md
- https://github.com/docker/distribution/blob/master/docs/insecure.md
- https://serversforhackers.com/tcp-load-balancing-with-nginx-ssl-pass-thru
- https://github.com/docker/distribution/blob/master/docs/recipes/mirror.md

[1]: https://github.com/docker/distribution
pptp
====

Containerized PPTP Client

## docker-compose.yml

```yaml
pptp:
  image: vimagick/pptp
  environment:
    - SERVER=1.2.3.4
    - TUNNEL=vps
    - USERNAME=username
    - PASSWORD=password
  net: host
  privileged: yes
  restart: unless-stopped
```

## up and running

```
sudo modprobe nf_conntrack_pptp nf_conntrack_proto_gre

docker-compose up -d
docker-compose logs -f

ip link show
ip addr show
ip route show

curl ifconfig.co
curl ifconfig.ovh
curl ifconfig.me
```

## references

- <http://pptpclient.sourceforge.net/howto-debian.phtml>
- <https://wiki.archlinux.org/index.php/PPTP_Client>
WebKit/PhantomJS
================

> A simple web-service based on PhantomJS for rendering javascript-enabled website.

## server

    docker run -d --restart=always -p 80:80 -p 1024:1024 -p 2812:2812 -p 9001:9001 --name=webkit vimagick/webkit

## client

    http :1024 targetUrl=https://github.com/
    http :1024 targetUrl=https://github.com/ loadImages:=true requestType=png

## admin

    # monit
    firefox http://localhost:2812/
    # supervisor
    firefox http://localhost:9001/
    # haproxy
    firefox http://localhost:1024/haproxy?stats

## issues

- send back html+image at the same time

ices
====

[IceS][1] is a source client for the Icecast streaming server. The purpose of this
client is to provide an audio stream to Icecast, so that one or more listeners
can access the stream. With this layout, the source client can be situated
remotely from the Icecast server.

How It Works
------------

```
                +-> (LAN) -> (icecast) -+
(in) -> (ices) -|                       |-> (out) -> (mpd) -> (file)
                +-> (WAN) -> (icecast) -+
```

docker-compose.yml
------------------

```yaml
ices:
  image: easypi/ices-arm
  volumes:
    - ./data:/etc/ices
  devices:
    - /dev/snd
  links:
    - icecast
  restart: always

icecast:
  image: easypi/icecast-arm
  ports:
    - "8000:8000"
  restart: always
```

ices.xml
--------

```xml
<?xml version="1.0"?>
<ices>
    <background>0</background>
    <loglevel>4</loglevel>
    <consolelog>1</consolelog>
    <pidfile>/var/run/ices.pid</pidfile>
    <stream>
        <metadata>
            <name>Example stream name</name>
            <genre>Example genre</genre>
            <description>A short description of your stream</description>
            <url>http://easypi.info</url>
        </metadata>
        <input>
            <module>alsa</module>
            <param name="rate">44100</param>
            <param name="channels">1</param>
            <param name="device">hw:1,0</param>
            <param name="metadata">1</param>
            <param name="metadatafilename">/etc/ices/ices.txt</param>
        </input>
        <instance>
            <hostname>icecast</hostname>
            <port>8000</port>
            <password>hackme</password>
            <mount>/live.ogg</mount>
            <encode>
                <quality>0</quality>
                <samplerate>44100</samplerate>
                <channels>1</channels>
            </encode>
        </instance>
    </stream>
</ices>
```

> You can setup multiple `instances` (e.g. LAN & WAN).

ices.txt
--------

```ini
artist=Various Artists
title=Untitled Song
```

Running
-------

```bash
$ arecord -l
**** List of CAPTURE Hardware Devices ****
card 1: Device [USB Audio Device], device 0: USB Audio [USB Audio]
  Subdevices: 0/1
  Subdevice #0: subdevice #0
$ alsamixer -c 1
$ docker-compose up -d
$ streamripper http://localhost:8000/live.ogg
$ vi data/ices.txt
$ docker-compose kill -s SIGUSR1
```

[1]: http://icecast.org/ices/
vnstat
======

[vnStat][1] is a console-based network traffic monitor for Linux and BSD that keeps
a log of network traffic for the selected interface(s).

### docker-compose.yml

```yaml
vnstat:
  image: vimagick/vnstat
  volumes:
    - ./data:/var/lib/vnstat
  net: host
  restart: always
```

### up and running

```bash
docker-compose up -d
docker-compose exec vnstat bash
>>> vnstat --help
>>> vnstati --help
>>> exit
```

[1]: http://humdi.net/vnstat/
mysql-proxy
===========

[MySQL Proxy][1] is an application that communicates over the network using the
MySQL client/server protocol and provides communication between one or more
MySQL servers and one or more MySQL clients.

```
# run docker container
$ docker-compose up -d

# view log in shell
$ journalctl -f CONTAINER_NAME=mysql-proxy

# view log in python
$ apt install python3-systemd
$ python3 code.py
```

[1]: https://github.com/mysql/mysql-proxy
red5
====

![](https://badge.imagelayers.io/vimagick/red5-server:latest.svg)

[Red5][1] is an Open Source Flash Server written in Java.

## docker-compose.yml

```yaml
red5:
  image: vimagick/red5
  ports:
    - "1935:1935"  # rtmp/e
    - "5080:5080"  # http
    - "8081:8081"  # websocket
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d
$ docker-compose exec red5 bash
>>> vi /opt/red5/conf/red5.properties
>>> exit
$ docker-compose restart
```

## Client Setup

```bash
$ ffmpeg -i /dev/video0 -video_size 800x600 -f flv rtmp://easypi.info/live/webcam
```

> Please try [easypi/rtmp-client-arm][2] on Raspberry Pi.

## Player Setup

```bash
# kodi
$ echo 'rtmp://easypi.info/live/webcam live=1' > ~/Movies/webcam.strm

# ffplay
$ brew reinstall ffmpeg --with-ffplay --with-rtmpdump
$ ffplay 'rtmp://easypi.info/live/webcam live=1'
```

## references

- <https://github.com/Red5/docker>
- <http://kodi.wiki/view/Internet_video_and_audio_streams>

[1]: https://github.com/Red5/red5-server
[2]: https://hub.docker.com/r/easypi/rtmp-client-arm/
FreeRADIUS
==========

[FreeRADIUS][1] includes a RADIUS server, a BSD licensed client library, a PAM
library, and an Apache module. In most cases, the word FreeRADIUS refers to the
RADIUS server.

## docker-compose.yml

```yaml
freeradius:
  image: vimagick/freeradius
  ports:
    - "1812:1812/udp"
    - "1813:1813/udp"
  links:
    - mysql
  restart: always

mysql:
  image: mysql
  volumes:
    - ./mysql:/docker-entrypoint-initdb.d
  environment:
    - MYSQL_ROOT_PASSWORD=root
  restart: always
```

## Server Setup

```bash
$ docker-compose up -d mysql
$ docker-compose exec mysql mysql -uroot -proot radius
>>> show tables;
+------------------+
| Tables_in_radius |
+------------------+
| nas              |
| radacct          |
| radcheck         |
| radgroupcheck    |
| radgroupreply    |
| radpostauth      |
| radreply         |
| radusergroup     |
+------------------+
8 rows in set (0.00 sec)

>>> SHOW GRANTS FOR radius;
+----------------------------------------------------------------+
| Grants for radius@%                                            |
+----------------------------------------------------------------+
| GRANT USAGE ON *.* TO 'radius'@'%'                             |
| GRANT SELECT ON `radius`.* TO 'radius'@'%'                     |
| GRANT ALL PRIVILEGES ON `radius`.`radacct` TO 'radius'@'%'     |
| GRANT ALL PRIVILEGES ON `radius`.`radpostauth` TO 'radius'@'%' |
+----------------------------------------------------------------+
5 rows in set (0.00 sec)

>>> INSERT INTO radcheck VALUES
    (NULL, 'user', 'MD5-Password', ':=', MD5('pass')),
    (NULL, 'user', 'Expiration', ':=', 'Jul 31 2016 00:00:00');
Query OK, 2 row affected (0.04 sec)
Records: 2  Duplicates: 0  Warnings: 0

>>> SELECT * FROM radcheck;
+----+----------+--------------+----+----------------------------------+
| id | username | attribute    | op | value                            |
+----+----------+--------------+----+----------------------------------+
|  1 | user     | MD5-Password | := | 1a1dc91c907325c69271ddf0c944bc72 |
|  2 | user     | Expiration   | := | Jul 31 2016 00:00:00             |
+----+----------+--------------+----+----------------------------------+
2 rows in set (0.00 sec)

>>> INSERT INTO nas VALUES(NULL, '0.0.0.0/0', 'testing', NULL, NULL, 'testing321', NULL, NULL, NULL);
Query OK, 1 row affected (0.02 sec)

>>> SELECT * FROM nas;
+----+-----------+-----------+------+-------+------------+--------+-----------+-------------+
| id | nasname   | shortname | type | ports | secret     | server | community | description |
+----+-----------+-----------+------+-------+------------+--------+-----------+-------------+
|  1 | 0.0.0.0/0 | testing   | NULL |  NULL | testing321 | NULL   | NULL      | NULL        |
+----+-----------+-----------+------+-------+------------+--------+-----------+-------------+
1 row in set (0.00 sec)

>>> SELECT * FROM radpostauth;
+----+----------+------+---------------+---------------------+
| id | username | pass | reply         | authdate            |
+----+----------+------+---------------+---------------------+
|  1 | user     | pass | Access-Accept | 2016-07-28 06:28:28 |
|  2 | user     | pass | Access-Accept | 2016-07-28 06:30:04 |
|  3 | user     | xxxx | Access-Reject | 2016-07-28 06:30:22 |
+----+----------+------+---------------+---------------------+

>>> EXIT
Bye

$ docker-compose up -d freeradius
$ docker-compose exec freeradius sh
>>> vi /etc/raddb/clients.conf
>>> radtest user pass localhost 0 testing123
>>> cd /etc/raddb/certs
>>> make client.p12
>>> exit
$ docker cp freeradius_freeradius_1:/etc/raddb/certs/ca.pem /tmp
$ docker cp freeradius_freeradius_1:/etc/raddb/certs/client.p12 /tmp
$ docker-compose restart freeradius
```

> The `ca.pem` and `client.p12` (password: whatever) is for `EAP-TLS`.

```
# /etc/raddb/clients.conf

#client testing {
#        ipaddr = 0.0.0.0/0
#        secret = testing321
#}
```

> Manage NAS (Network Access Server) via MySQL.


## OpenWrt Setup

```
Network > Wireless > Wireless Security:
    Encryption: WPA2-EAP
    AuthServer: 192.168.31.138
    AuthSecret: testing321
    AcctServer: 192.168.31.138
    AcctSecret: testing321
```

## Android Setup

```
# Import CA and P12(CRT+KEY)
Settings > Additional settings > Privacy > Install from SD card

# Connect WiFi
Settings > WLAN > TLS:
    CA: xxxxxx
    KEY: xxxxxx
    ID: android
```

## Client Setup

```bash
# ssh root@192.168.31.231
$ pacman -S freeradius freeradius-client
$ radtest user pass 192.168.31.138 0 testing321
$ radtest user xxxx 192.168.31.138 0 testing321
```

[1]: http://freeradius.org/
FreeRadius MySQL
================

## SQL Patch

```
$ wget https://github.com/FreeRADIUS/freeradius-server/raw/release_3_0_11/raddb/mods-config/sql/main/mysql/setup.sql
$ wget https://github.com/FreeRADIUS/freeradius-server/raw/release_3_0_11/raddb/mods-config/sql/main/mysql/schema.sql
```

File: 00-setup.sql

```diff
#
#  Create default administrator for RADIUS
#
CREATE USER [-'radius'@'localhost';-]{+'radius'@'%';+}
SET PASSWORD FOR [-'radius'@'localhost'-]{+'radius'@'%'+} = PASSWORD('radpass');

# The server can read any table in SQL
GRANT SELECT ON radius.* TO [-'radius'@'localhost';-]{+'radius'@'%';+}

# The server can write to the accounting and post-auth logging table.
#
#  i.e.
GRANT ALL on radius.radacct TO [-'radius'@'localhost';-]{+'radius'@'%';+}
GRANT ALL on radius.radpostauth TO [-'radius'@'localhost';-]{+'radius'@'%';+}
```

File: 01-schema.sql

```diff
@@ -1,5 +1,8 @@
+CREATE DATABASE radius;
+USE radius;
```

## MySQL Setup

```ini
server = "mysql"
port = 3306
login = "radius"
password = "radpass"
radius_db = "radius"
```
wekan
=====

![](https://wekan.github.io/wekan-logo.png)

[Wekan][1] is an open-source kanban board which allows a card-based task and
to-do management, similar to tools like WorkFlowy or Trello.

## docker-compose.yml

```yaml
wekan:
  image: wekanteam/wekan
  ports:
    - "8081:80"
  links:
    - mongo
  environment:
    - MONGO_URL=mongodb://mongo/wekan
    - ROOT_URL=https://todo.easypi.pro
  restart: always

mongo:
   image: mongo
   ports:
    - "27017:27017"
   volumes:
     - ./data:/data/db
  restart: always
```

## Up and Running

```bash
$ docker-compose up -d
$ curl http://localhost:8081
```

## Admin Panel » Settings

```yaml
Registration:
    Disable Self-Registration: yes
    Invite People: []

Email:
    SMTP Host: smtp.gmail.com
    SMTP Port: 465
    Username: username
    Password: ********
    TLS support: yes
    From: username@gmail.com
```

[1]: https://wekan.github.io/
iptables
========

![](https://badge.imagelayers.io/vimagick/iptables:latest.svg)

- _iptables_: filter ports (allow: 53/UDP, 80/TCP, 443/TCP)
- _tc_: control traffic via [tbf][1]

## docker-compose.yml

```
shadowsocks:
  image: vimagick/shadowsocks-libev
  environment:
    - DNS_ADDR=8.8.8.8
    - METHOD=chacha20
    - PASSWORD=9MLSpPmNt
  net: container:iptables
  restart: always

iptables:
  image: vimagick/iptables
  ports:
    - "8388:8388"
  environment:
    - TCP_PORTS=80,443
    - UDP_PORTS=53
    - RATE=4mbit
    - BURST=4kb
  cap_add:
    - NET_ADMIN
  restart: always
```

## Up and Running

```
$ docker-compose up -d
Creating shadowsocks_iptables_1...
Creating shadowsocks_shadowsocks_1...

$ docker-compose logs
Every 60s: tc -s qdisc ls dev eth0                          2015-09-27 02:27:57
iptables_1    |
iptables_1    | qdisc tbf 8012: root refcnt 2 rate 4Mbit burst 4Kb lat 50.0ms
iptables_1    |  Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
iptables_1    |  backlog 0b 0p requeues 0
```

[1]: http://linux.die.net/man/8/tc-tbf
grr
===

![](https://raw.githubusercontent.com/google/grr/gh-pages/img/grr_logo_real_sm.png)

[GRR][1] Rapid Response is an incident response framework focused on remote
live forensics.

[1]: https://github.com/google/grr
watchtower
==========

![](https://badge.imagelayers.io/centurylink/watchtower:latest.svg)

[`Watchtower`][1] is an application that will monitor your running Docker containers
and watch for changes to the images that those containers were originally
started from. If watchtower detects that an image has changed, it will
automatically restart the container using the new image.

## docker-compose.yml

```
watchtower:
  image: centurylink/watchtower
  command: >
    mysql
    mongo
    nginx
    redis
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock
  restart: always
```

> It'll monitor all containers if no `command` in YAML.

## Up and Running

```
$ docker-compose up -d

$ docker-compose logs
watchtower_1 | time="2015-09-02T02:31:39Z" level=info msg="Checking containers for updated images"
watchtower_1 | time="2015-09-02T02:31:56Z" level=info msg="Found new redis:latest image (3d86...76)"
watchtower_1 | time="2015-09-02T02:31:56Z" level=info msg="Stopping /redis (8f51...f4) with SIGTERM"
watchtower_1 | time="2015-09-02T02:32:06Z" level=info msg="Starting /redis"
watchtower_1 | time="2015-09-02T02:37:06Z" level=info msg="Checking containers for updated images"
watchtower_1 | time="2015-09-02T02:42:10Z" level=info msg="Checking containers for updated images"
```

[1]: https://hub.docker.com/r/centurylink/watchtower/
aria2
=====

![](https://badge.imagelayers.io/vimagick/aria2:latest.svg)

- `aria2` is a utility for downloading files.
- `yaaw` is yet another aria2 web frontend.

## directory tree

```
~/fig/aria2/
├── docker-compose.yml
├── html/
│   ├── README.md
│   ├── TODO.md
│   ├── css/...
│   ├── img/...
│   ├── index.html
│   ├── js/...
│   └── offline.appcache
├── data -> /home/aria2/
└── keys/
    ├── server.crt
    └── server.key
```

> You may make `data` a symbolic link to `/home/aria2` or somewhere else.  
> To implement disk quota, you can even create a [virtual disk][1].

## docker-compose.yml

```
aria2:
  image: vimagick/aria2
  ports:
    - "6800:6800"
  volumes:
    - "./data:/home/aria2"
    - "./keys:/etc/aria2/keys"
#   - "./aria2.conf:/etc/aria2/aria2.conf"
  environment:
    - TOKEN=e6c3778f-6361-4ed0-b126-f2cf8fca06db
  restart: always

yaaw:
  image: vimagick/nginx
  ports:
    - "8080:80"
  volumes:
    - ./html:/usr/share/nginx/html
  restart: always
```

## aria2.conf

```
dir=/home/aria2
disable-ipv6=true
enable-rpc=true
max-download-limit=0
max-upload-limit=0
rpc-allow-origin-all=true
rpc-certificate=/etc/aria2/keys/server.crt
rpc-listen-all=true
rpc-listen-port=6800
rpc-private-key=/etc/aria2/keys/server.key
rpc-secret=00000000-0000-0000-0000-000000000000
rpc-secure=true
seed-ratio=0
seed-time=0
```

## server

```
$ mkdir -p ~/fig/aria2/{html,keys}/
$ cd ~/fig/aria2/
$ ln -s /home/aria2 data
$ curl -sSL https://github.com/binux/yaaw/archive/master.tar.gz | tar xz --strip 1 -C html
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout keys/server.key -out keys/server.crt
$ vim docker-compose.yml
$ fig up -d
```

## client

```
$ scp server:fig/aria2/keys/server.crt /usr/local/share/ca-certificates/
$ update-ca-certificates --fresh

$ uuidgen
3c5323b8-79f7-49d4-8303-fcfe51488db5

$ http --verify no https://server:6800/jsonrpc \
       id=3c5323b8-79f7-49d4-8303-fcfe51488db5 \
       method=aria2.getGlobalStat \
       params:='["token:e6c3778f-6361-4ed0-b126-f2cf8fca06db"]'

$ curl https://server:6800/jsonrpc --data '
       {
         "id": "3c5323b8-79f7-49d4-8303-fcfe51488db5",
         "method": "aria2.getGlobalStat",
         "params": ["token:e6c3778f-6361-4ed0-b126-f2cf8fca06db"]
       }' | jq .

{
  "id": "3c5323b8-79f7-49d4-8303-fcfe51488db5",
  "jsonrpc": "2.0",
  "result": {
    "downloadSpeed": "0",
    "numActive": "0",
    "numStopped": "0",
    "numStoppedTotal": "0",
    "numWaiting": "0",
    "uploadSpeed": "0"
  }
}

$ firefox http://server:8080/
#
# Settings » JSON-RPC Path:
#   wss://token:e6c3778f-6361-4ed0-b126-f2cf8fca06db@server:6800/jsonrpc
#
# Firefox » Top-Right Corner:
#   Aria2 1.18.10
#   ↓0 KB/s / ↑0 KB/s
#
```

> Please choose `CommonName` properly when generating keys.  
> `httpie` will throw error without `--verify no` option, I don't know why!  
> Open `https://server:6800` in your browser, and accept security certificate.  

[1]: http://souptonuts.sourceforge.net/quota_tutorial.html
mediagoblin
===========

[MediaGoblin][1] is a free software media publishing platform that anyone can
run.  You can think of it as a decentralized alternative to Flickr, YouTube,
SoundCloud, etc.

## docker-compose.yml

```yaml
mediagoblin:
  image: vimagick/mediagoblin
  ports:
    - "8080:80"
  volumes:
    - ./data:/var/lib/mediagoblin
  environment:
    - MG_USER=admin
    - MG_PASS=admin
    - MG_MAIL=admin@easypi.info
  restart: always
```

## up and running

```bash
# start service
$ docker-compose up -d

# open browser
$ firefox http://localhost:8080/
```

[1]: http://mediagoblin.org/
openssh
=======

## docker-compose.yml

```yaml
openssh:
  image: vimagick/openssh
  hostname: alpine
  ports:
    - "2222:22"
  volumes:
    - ./keys:/root/.ssh/keys
  restart: always
```

## up and running

```bash
$ cd ~/fig/openssh/

$ tree keys
├── joe.pub
├── kev.pub
└── tom.pub

$ docker-compose up -d
```

## ssh login

```bash
$ ssh -p 2222 root@server
```
cmus
====

[cmus][1] is a small, fast and powerful console music player for Unix-like operating systems.


## docker-compose.yml

```yaml
cmus:
  image: vimagick/cmus
  ports:
    - "3000:3000"
  volumes:
    - ./data:/etc/cmus
    - ~/music:/root/music
  devices:
    - /dev/snd
  environment:
    - PASSWORD=secret
  tty: yes
  restart: unless-stopped
```

## up and running

```bash
$ docker-compose up -d
$ docker-compose exec cmus cmus-remote --help
$ docker-compose exec cmus cmus-remote --passwd secret
>>> clear
>>> add ~/music
>>> set continue=true
>>> set repeat=true
>>> set shuffle=true
>>> set resume=true
>>> player-play
>>> ^D
$ alsamixer
```

[1]: https://cmus.github.io/
ghost
=====

[Ghost][1] is a free and open source blogging platform written in JavaScript.

## docker-compose.yml

```yaml
ghost:
  image: ghost:alpine
  ports:
    - "127.0.0.1:2368:2368"
  volumes:
    - ./data:/var/lib/ghost/content
  restart: always
```

## Up and Running

```bash
$ docker-compose up -d
$ cd data
$ sed -i 's@http://localhost:2368@https://blog.easypi.info@' config.js
$ grep -rIl 'googleapis' core content | xargs sed -i 's/googleapis/useso/g'
$ docker-compose restart
```

> :warning: `useso.com` doesn't support https!

## Setup SSL

> Read [this][2] to setup SSL.

```
server {
    listen 80 default;
    server_name _;
    location / {
        return 301 https://$host$request_uri;
    }
}

server {
    listen 443 ssl;
    server_name easypi.info blog.easypi.info;
    ssl_certificate ssl/easypi.info.crt;
    ssl_certificate_key ssl/easypi.info.key;
    location / {
        if ($host = 'easypi.info') {
            return 301 $scheme://blog.$host$request_uri;
        }
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $http_host;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_pass http://127.0.0.1:2368;
    }
}
```

## Ghost Setup

[Ghost][3] allows you to inject code into the top and bottom of your theme
files without editing them.

```css
<style>
  pre code, pre tt {
    white-space: pre !important;
  }
</style>
```

[1]: https://ghost.org/
[2]: http://support.ghost.org/setup-ssl-self-hosted-ghost/
[3]: https://blog.easypi.info/ghost/settings/code-injection/
docker-compose-arm
==================

Automated Build for Docker Compose on Raspberry Pi

## Quick Start

```bash
$ docker build -t easypi/docker-compose-arm .

$ docker run --name compose easypi/docker-compose-arm version
docker-compose version 1.12.0, build b31ff33
docker-py version: 2.2.1
CPython version: 3.4.2
OpenSSL version: OpenSSL 1.0.1t  3 May 2016

$ docker cp compose:/usr/local/bin/docker-compose .

$ file docker-compose
docker-compose: ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked, stripped

$ docker rm compose
compose
```
l2tpd
=====

WARNING: This is still a work in progress, a successful connection has not been made.
haproxy
=======

![](https://badge.imagelayers.io/library/haproxy:alpine.svg)

[HAProxy][1] is a free, open source high availability solution, providing load
balancing and proxying for TCP and HTTP-based applications by spreading
requests across multiple servers.

## docker-compose.yml

```yaml
haproxy:
  image: haproxy:alpine
  volumes:
    - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
  net: host
  restart: always
```

## haproxy.cfg

This sample config shows you how to setup a simple tcp load balancer.

```
#                       +- jp (:1081)
#                       |
#                       |- tw (:1082)
# frontend --- backend -+  
# (*:1080)              |- hk (:1083)
#                       |
#                       +- us (:1084)

global
        # daemon # WARNING: DO NOT USE IT!
        maxconn 4000

defaults
        mode tcp
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms

frontend front
        bind *:1080
        default_backend back

backend back
        balance roundrobin
        server jp 127.0.0.1:1081 weight 50
        server tw 127.0.0.1:1082 weight 20
        server hk 127.0.0.1:1083 weight 15
        server us 127.0.0.1:1084 weight 15
```

> :warning: haproxy must not be daemonized!

[1]: http://www.haproxy.org/
samba
=====

![](https://badge.imagelayers.io/vimagick/samba:latest.svg)

[Samba][1] is the standard Windows interoperability suite of programs for
Linux and Unix.

## docker-compose.yml

```yaml
samba:
  image: vimagick/samba
  volumes:
    - ./data/smb.conf:/etc/samba/smb.conf
    - ./share:/share
  net: host
  tty: yes
  restart: always
```

> Uncomment to use a customized config file.

## mnt-usb.mount

An USB flash drive is mounted at `/mnt/usb`.

```
# /etc/systemd/system/mnt-usb.mount
[Unit]
Description=USB Storage Mount

[Mount]
What=/dev/disk/by-uuid/829B-2038
Where=/mnt/usb

[Install]
WantedBy=local-fs.target
```

## smb.conf

```
[global]
netbios name = easypi
workgroup = WORKGROUP
server string = EasyPi Samba Server
server role = standalone
map to guest = bad user
usershare allow guests = yes

[homes]
comment = Home Directories
browseable = no
writable = yes

[share]
comment = Public File Sharing
path = /share
browseable = yes
read only = no
guest ok = yes
admin users = root
```

## server

```
$ cd ~/fig/samba
$ mkdir share
$ touch share/README.txt
$ docker-compose up -d
$ docker exec -it samba_samba_1 sh
>>> testparm
>>> smbpasswd -a root
New SMB password:******
Retype new SMB password:******
>>> exit
```

## client

```
$ smbutil view -NG smb://easypi
Share                                           Type    Comments
-------------------------------
share                                           Disk
IPC$                                            Pipe    IPC Service (Samba Server)
2 shares listed

$ mkdir -p /Volumes/share
$ mount_smbfs //guest@easypi/share /Volumes/share
$ umount /Volumes/share
```

> `root` user can read and write, `guest` user can read only.

[1]: https://www.samba.org/
delegated
=========

![](https://badge.imagelayers.io/vimagick/delegated:latest.svg)

[DeleGate][1] is a multipurpose proxy server which relays various application
protocols on TCP/IP or UDP/IP

## docker-compose.yml

```
delegated:
  image: vimagick/delegated
  command: "-P1080 SERVER=socks"
  ports:
    - "1080:1080"
  restart: always
```

## up and running

```
# server
$ docker-compose up -d

# client
$ curl -x socks5h://localhost:1080 ifconfig.ovh
```

[1]: http://www.delegate.org/delegate/
ngrokd
======

![](https://badge.imagelayers.io/vimagick/ngrokd:latest.svg)

[ngrok][1] is a reverse proxy that creates a secure tunnel from a public endpoint to
a locally running web service. ngrok captures and analyzes all traffic over the
tunnel for later inspection and replay.

## docker-compose.yml

```
data:
  build: .
  entrypoint: /bin/true

service:
  image: debian:jessie
  command: >
    ./ngrokd
    -domain=ngrok.easypi.info
    -httpAddr=:2080
    -httpsAddr=:2443
    -tunnelAddr=:4443
    -tlsCrt=snakeoil.crt
    -tlsKey=snakeoil.key
    -log-level=INFO
  ports:
    - "2080:2080"
    - "2443:2443"
    - "4443:4443"
  volumes:
    - ./ngrok:/ngrok
  working_dir: /ngrok
  restart: always
```

## up and running

```
$ mkdir -p ~/fig/ngrokd/
$ cd ~/fig/ngrokd/
$ wget https://github.com/vimagick/dockerfiles/raw/master/ngrokd/docker-compose.yml
$ wget https://github.com/vimagick/dockerfiles/raw/master/ngrokd/Dockerfile
$ vim Dockerfile

$ docker-compose build data
$ docker-compose up -d data
$ docker cp ngrokd_data_1:/ngrok .
$ docker-compose rm -v data
$ docker rmi ngrokd_data

$ docker-compose up -d service
$ docker-compose logs service
```

## important notes

- [vimagick/ngrokd][2] should not be used directly
- Change `NGROK_BASE_DOMAIN` in [Dockerfile][3]
- Nerver push it to public repo

## raspberry pi

```
/etc/ngrok/
├── conf.d/
│   ├── router.json
│   └── webcam.json
└── ngrok.yml
```

```
{
  "name": "router",
  "proto": "http",
  "addr": "192.168.1.1:80",
  "bind_tls": true,
  "inspect": false,
  "auth": "user:pass"
}
```

```yaml
# /etc/ngrok/ngrok.yml
authtoken: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
console_ui: false
region: ap
web_addr: 0.0.0.0:4040
tunnels:
  ssh:
    proto: tcp
    addr: 22
  web:
    proto: http
    addr: 4040
    bind_tls: true
    inspect: false
    auth: "user:pass"
```

```ini
# /etc/systemd/system/ngrok.service
[Unit]
Description=Secure Tunnels To Localhost
Documentation=https://ngrok.com/docs
After=network.target

[Service]
ExecStart=/usr/bin/ngrok start --config /etc/ngrok/ngrok.yml --log stdout --all
RestartSec=10
Restart=always

[Install]
WantedBy=multi-user.target
```

```ini
# /etc/systemd/system/ngrok@.service
[Unit]
Description=Ngrok Instance Daemon
Requires=ngrok.service
After=ngrok.service

[Service]
ExecStart=/usr/bin/curl -sS -X POST \
                        -H 'Content-Type: application/json' \
                        -d @/etc/ngrok/conf.d/%I.json \
                        http://127.0.0.1:4040/api/tunnels
ExecStop=/usr/bin/curl -sS -X DELETE http://127.0.0.1:4040/api/tunnels/%I
Restart=on-failure
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
```

```bash
$ systemctl daemon-reload
$ systemctl start ngrok
$ systemctl enable ngrok

$ curl http://127.0.0.1:4040/api/tunnels/ssh
{"public_url": "tcp://0.tcp.ap.ngrok.io:19136"}
$ ssh -p 19136 root@0.tcp.ap.ngrok.io

$ systemctl start ngrok@router
$ curl http://127.0.0.1:4040/api/tunnels/router
{"public_url":"https://db45322c.ap.ngrok.io"}
$ w3m https://db45322c.ap.ngrok.io

$ systemctl status ngrok@*
```

## openwrt

```bash
#!/bin/sh /etc/rc.common

START=90
STOP=10

USE_PROCD=1

start_service() {
    procd_open_instance
    procd_set_param command /usr/bin/ngrok start --config /etc/ngrok.yml --all
    procd_set_param respawn 3600 5 0
    procd_close_instance
}
```

[1]: https://github.com/inconshreveable/ngrok
[2]: https://hub.docker.com/r/vimagick/ngrokd/
[3]: https://github.com/vimagick/dockerfiles/raw/master/ngrokd/Dockerfile#L9
postgres
========

![](https://www.postgresql.org/media/img/docs/hdr_logo.png)

## docker-compose.yml

```yaml
postgres:
  image: postgres:alpine
  ports:
    - "5432:5432"
  environment:
    - POSTGRES_USER=root
    - POSTGRES_PASSWORD=root
    - POSTGRES_DB=postgres
  restart: always
```

## up and running

```bash
$ docker-compose up -d

$ docker-compose exec postgres psql postgres
>>> SELECT CURRENT_TIMESTAMP;
              now
-------------------------------
 2016-12-07 01:44:29.872928+00
(1 row)

>>> \q
```
dsniff
======

[dsniff][1] is a collection of tools for network auditing and penetration
testing. dsniff, filesnarf, mailsnarf, msgsnarf, urlsnarf, and webspy passively
monitor a network for interesting data (passwords, e-mail, files, etc.).
arpspoof, dnsspoof, and macof facilitate the interception of network traffic
normally unavailable to an attacker (e.g, due to layer-2 switching). sshmitm
and webmitm implement active monkey-in-the-middle attacks against redirected
SSH and HTTPS sessions by exploiting weak bindings in ad-hoc PKI.

## docker-compose.yml

```yaml
dsniff:
  image: vimagick/dsniff
  net: host
  volumes:
    - ./data:/data
  working_dir: /data
  tty: yes
  restart: unless-stopped
```

## Server Setup

```bash
$ docker-compose up -d
$ docker-compose exec dsniff tmux ls
$ docker-compose exec dsniff tmux a
>>> echo -e '192.168.31.1\twww.baidu.com' >> hosts
>>> dnsspoof -i eth0 -f hosts
>>> arpspoof -i eth0 -t 192.168.31.1 192.168.31.102
>>> arpspoof -i eth0 -t 192.168.31.102 192.168.31.1
```

## Client Setup

```bash
$ ping www.baidu.com
$ curl www.baidu.com
```

[1]: https://www.monkey.org/~dugsong/dsniff/
alpine-arm
==========

:warning: This project has been moved to [here][1]!

![](https://badge.imagelayers.io/vimagick/alpine-arm:latest.svg)

## Make Images

```
$ docker login

$ make RELEASE=edge
$ make test RELEASE=edge
$ make push RELEASE=edge
$ make clean

$ make RELEASE=v3.3
$ make test RELEASE=v3.3
$ make push RELEASE=v3.3
$ make clean

$ make latest RELEASE=v3.3
$ make test RELEASE=latest
$ make push RELEASE=latest
```

[1]: https://github.com/EasyPi/alpine-arm
# Twitch-graph
[![](https://img.shields.io/gitter/room/alfreddobradi/twitch-graph.svg)](https://gitter.im/twitch-graph/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link)
## What does it do?

Parses channel followers from Twitch API, then saves the channel, followers and their relationship (which is currently only :FOLLOW) in Neo4j

## Usage

To run, you'll need Docker, Docker Compose and Python (^3.0 should be fine). (I might create a Docker container for the Python script, but for now it seemed moot)

Creating and running the container using Docker Compose is pretty simple:
```sh
docker-compose up
```
(Neo4j will use the `./neo4j/data`, Redis will use the `./redis/data` directory to store you know... data.)

When you start Neo4j the first time, check the web interface at `http://localhost:7474` and change your password. You'll have to change it in the script in this line:
```python
neo = Graph(user="twitch", password="twitch")
```

Once Neo4j is up and running you can actually run the script:
```sh
CLIENT_ID=<twitch client id> CHANNEL=<twitch channel> python code/twitch.py
```

It will take a bit of time to parse the followers, depending on how many the channel has.  

After it's finished, you can check your DB via the browser interface, run queries and all.

## Neo4j examples

### Count your nodes labeled 'User':
```
MATCH (u:User)
RETURN COUNT(u)
```

### Return your users ordered by the number of saved Channels they follow
```
MATCH (u:User)-[r:FOLLOW]->(c:Channel)
RETURN n, COUNT(r) AS rel_count
ORDER BY rel_count DESC
```

## Plans

* Handling channels and users as one entity (as they share the id anyway)
* Viral discovery (channel -> followers -> their followed channels -> start again)
* Web based "analytics" using the data
* Refactor

## Links

Docker Hub: http://hub.docker.com  
Neo4j: http://www.neo4j.com  
Twitch Developers: http://dev.twitch.tv

### Disclaimer

I'm not a Python guy
### Requirements ###
docker, docker-compose

### How to build docker image ###
```
$ docker build -t "package-name:latest"
```

### How to start the cluster ###
```
$ docker-compose up --force-recreate
```

For more docker images visit http://hub.docker.com

use build_images.sh to build all necessary images 

I Producer(KafkaProducer) used to simulate sending random generated json data which contains 4 fields( id , revenue , revenue_counter , timestamp)

II Spark procesing over kafka producer's data(saving incoming data from producer to redis(via spark) because some of the records may be updated )
	- from spark app is crated a new kafka producer which handles the computated data
	- how to run spark in standalone mode ( not distributed with Hadoop(hdfs)) :
			
			1.First create the datastore container so all the other container can use the datastore container's data volume with createDatastoreContainer.sh docker volumes can be checked with -> docker volume ls , after running the script for datastore type the command and the volume for the datastore should be the first one from top to bottom.

			2.Create spark master container with createSparkMasterContainer.sh script

			3.After second step with spark master then create spark slave container with createSparkSlaveContainer.sh script
			(can create as many workers as we want , link option allows the container automatically connect to the other (master in this case) by being added to the same network.)
				- spark workers can be scaled with the following command docker-compose scale slave = $1 ( whatever number do we need)

			4.Running a spark code using spark-submit
			Another container is created to work as the driver and call the spark cluster. The container only runs while the spark job is running, as soon as it finishes the container is deleted.
			The spark python code should be moved to the shared volume created by the datastore container. Since we did not specify a host volume (when we manually define where in the host machine the container is mapped) docker creates it in the default volume location located on /var/lib/volumes/<container hash>/_data

			5.Run the spark submit container
			docker run --rm -it --link master:master --volumes-from spark-datastore spark-submit:latest spark-submit --master spark://172.17.0.2:7077 /data/sparkContextExample.py
			Or the script spark_submit.sh can be used while passing a spark code .py file as argument

			* check the addres of spark master , last number can be different from 2 -> check on localhost:8080 where spark master runs

	*** sparkContextExample 
	There are two ways of creating spark stream with kafka
	- using KafkaUtils.createStream which takes data from the moment that producer started work.For example if the prodecer started work at 10:00am and the spark app is submited at 02:00pm 'createStream' will take all the data from 10:00 to 02:00pm and will keep working in real time until is stopped.
	- using KafkaUtils.createDirectStream whiche take data from the moment that the spark app is submited.

III Consumer(KafkaConsumer) used to receive data from the producer in spark app
# docker-alpine-elk

To create your own certs:

cd files

openssl req -config /etc/ssl/openssl.cnf -subj '/CN=*/' -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout logstash-forwarder.key -out logstash-forwarder.crt

The logstash configuration files are some samples for beats, if you want to use other configurations you need to store them in:

/opt/logstash/config
/opt/elasticsearch/config
/opt/kibana/config
# Can computers create their own opinions?

ELang is a library of world languages arranged by 

- Language codes. e.g. es_MX
- Parts of speech structured alphabetically

In the future, ELang should be able to parse content and: 

- Fix grammar mistakes
- Fix typos
- Get the context of what's written
- Answer questions like: who, when, where, what, how much, __why__
- Get similarities between different blocks of content
- Be able to generate human-like conclusions from the parsed content such as: _Ben is trying to fix his code and he is requesting Elaine's help_ (in all languages)
- Be able to generate human-like opinions from the parsed content such as: _I don't think you should go to that trip, it's too dangerous considering the current situation in that country_ (in all languages)

## Roadmap

It would be great to have a robust API where the endpoints work something like this:

```
GET //elang.something/{language_code}/verbs?l=a,f,t

GET //elang.something/{language_code}/verbs?conjugate=dance

GET //elang.something/{language_code}/verbs?toPast=code

GET //elang.something/{language_code}/nouns?similarTo=keyboard

GET //elang.something/{language_code}/content?get=verbs

GET //elang.something/{language_code}/content?get=sentences

GET //elang.something/{language_code}/content?who=1&when=1
```

## Resources

### es_MX

- [Partes del diccionario](https://www.unprofesor.com/lengua-espanola/el-diccionario-y-sus-partes-264.html)
- [Graphaware neo4j PHP Client](https://github.com/graphaware/neo4j-php-client)
- [5000 most common spanish words paginated](https://www.memrise.com/course/203799/5000-most-frequent-spanish-words/1/)
- [Wordreference definitions used like `/definicion/{word}`](http://www.wordreference.com/definicion/entender)
- [Abreviaturas y signos usados](http://www.rae.es/sites/default/files/Abreviaturas_y_signos_empleados.pdf)
- [Abreviaturas de las categorías gramaticales](http://www.culturaderioja.org/index.php/notas-para-el-uso-del-diccionario/abreviaturas-de-las-categorias-gramaticales)
## All-in-one Docker image for Deep Learning
Here are Dockerfiles to get you up and running with a fully functional deep learning machine. It contains all the popular deep learning frameworks with CPU and GPU support (CUDA and cuDNN included). The CPU version should work on Linux, Windows and OS X. The GPU version will, however, only work on Linux machines. See [OS support](#what-operating-systems-are-supported) for details

If you are not familiar with Docker, but would still like an all-in-one solution, start here: [What is Docker?](#what-is-docker). If you know what Docker is, but are wondering why we need one for deep learning, [see this](#why-do-i-need-a-docker)

## Update: I've built a quick tool, based on dl-docker, to run your DL project on the cloud with zero setup. You can start running your Tensorflow project on AWS in <30seconds using Floyd. See [www.floydhub.com](https://www.floydhub.com). It's free to try out. 
### Happy to take feature requests/feedback and answer questions - mail me sai@floydhub.com.

## Specs
This is what you get out of the box when you create a container with the provided image/Dockerfile:
* Ubuntu 14.04
* [CUDA 8.0](https://developer.nvidia.com/cuda-toolkit) (GPU version only)
* [cuDNN v5](https://developer.nvidia.com/cudnn) (GPU version only)
* [Tensorflow](https://www.tensorflow.org/)
* [Caffe](http://caffe.berkeleyvision.org/)
* [Theano](http://deeplearning.net/software/theano/)
* [Keras](http://keras.io/)
* [Lasagne](http://lasagne.readthedocs.io/en/latest/)
* [Torch](http://torch.ch/) (includes nn, cutorch, cunn and cuDNN bindings)
* [iPython/Jupyter Notebook](http://jupyter.org/) (including iTorch kernel)
* [Numpy](http://www.numpy.org/), [SciPy](https://www.scipy.org/), [Pandas](http://pandas.pydata.org/), [Scikit Learn](http://scikit-learn.org/), [Matplotlib](http://matplotlib.org/)
* [OpenCV](http://opencv.org/)
* A few common libraries used for deep learning

## Setup
### Prerequisites
1. Install Docker following the installation guide for your platform: [https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)

2. **GPU Version Only**: Install Nvidia drivers on your machine either from [Nvidia](http://www.nvidia.com/Download/index.aspx?lang=en-us) directly or follow the instructions [here](https://github.com/saiprashanths/dl-setup#nvidia-drivers). Note that you _don't_ have to install CUDA or cuDNN. These are included in the Docker container.

3. **GPU Version Only**: Install nvidia-docker: [https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker), following the instructions [here](https://github.com/NVIDIA/nvidia-docker/wiki/Installation). This will install a replacement for the docker CLI. It takes care of setting up the Nvidia host driver environment inside the Docker containers and a few other things.

### Obtaining the Docker image
You have 2 options to obtain the Docker image
#### Option 1: Download the Docker image from Docker Hub
Docker Hub is a cloud based repository of pre-built images. You can download the image directly from here, which should be _much faster_ than building it locally (a few minutes, based on your internet speed). Here is the automated build page for `dl-docker`: [https://hub.docker.com/r/floydhub/dl-docker/](https://hub.docker.com/r/floydhub/dl-docker/). The image is automatically built based on the `Dockerfile` in the Github repo.

**CPU Version**
```bash
docker pull floydhub/dl-docker:cpu
```

**GPU Version**
An automated build for the GPU image is not available currently due to timeout restrictions in Docker's automated build process. I'll look into solving this in the future, but for now you'll have to build the GPU version locally using Option 2 below.

#### Option 2: Build the Docker image locally
Alternatively, you can build the images locally. Also, since the GPU version is not available in Docker Hub at the moment, you'll have to follow this if you want to GPU version. Note that this will take an hour or two depending on your machine since it compiles a few libraries from scratch.

```bash
git clone https://github.com/saiprashanths/dl-docker.git
cd dl-docker
```	

**CPU Version**
```bash
docker build -t floydhub/dl-docker:cpu -f Dockerfile.cpu .
```

**GPU Version**
```bash
docker build -t floydhub/dl-docker:gpu -f Dockerfile.gpu .
```
This will build a Docker image named `dl-docker` and tagged either `cpu` or `gpu` depending on the tag your specify. Also note that the appropriate `Dockerfile.<architecture>` has to be used.

## Running the Docker image as a Container
Once we've built the image, we have all the frameworks we need installed in it. We can now spin up one or more containers using this image, and you should be ready to [go deeper](http://imgur.com/gallery/BvuWRxq)

**CPU Version**
```bash
docker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:cpu bash
```
	
**GPU Version**
```bash
nvidia-docker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:gpu bash
```
Note the use of `nvidia-docker` rather than just `docker`

| Parameter      | Explanation |
|----------------|-------------|
|`-it`             | This creates an interactive terminal you can use to iteract with your container |
|`-p 8888:8888 -p 6006:6006`    | This exposes the ports inside the container so they can be accessed from the host. The format is `-p <host-port>:<container-port>`. The default iPython Notebook runs on port 8888 and Tensorboard on 6006 |
|`-v /sharedfolder:/root/sharedfolder/` | This shares the folder `/sharedfolder` on your host machine to `/root/sharedfolder/` inside your container. Any data written to this folder by the container will be persistent. You can modify this to anything of the format `-v /local/shared/folder:/shared/folder/in/container/`. See [Docker container persistence](#docker-container-persistence)
|`floydhub/dl-docker:cpu`   | This the image that you want to run. The format is `image:tag`. In our case, we use the image `dl-docker` and tag `gpu` or `cpu` to spin up the appropriate image |
|`bash`       | This provides the default command when the container is started. Even if this was not provided, bash is the default command and just starts a Bash session. You can modify this to be whatever you'd like to be executed when your container starts. For example, you can execute `docker run -it -p 8888:8888 -p 6006:6006 floydhub/dl-docker:cpu jupyter notebook`. This will execute the command `jupyter notebook` and starts your Jupyter Notebook for you when the container starts

## Some common scenarios
### Jupyter Notebooks
The container comes pre-installed with iPython and iTorch Notebooks, and you can use these to work with the deep learning frameworks. If you spin up the docker container with `docker-run -p <host-port>:<container-port>` (as shown above in the [instructions](#running-the-docker-image-as-a-container)), you will have access to these ports on your host and can access them at `http://127.0.0.1:<host-port>`. The default iPython notebook uses port 8888 and Tensorboard uses port 6006. Since we expose both these ports when we run the container, we can access them both from the localhost.

However, you still need to start the Notebook inside the container to be able to access it from the host. You can either do this from the container terminal by executing `jupyter notebook` or you can pass this command in directly while spinning up your container using the `docker run -it -p 8888:8888 -p 6006:6006 floydhub/dl-docker:cpu jupyter notebook` CLI. The Jupyter Notebook has both Python (for TensorFlow, Caffe, Theano, Keras, Lasagne) and iTorch (for Torch) kernels.

Note: If you are setting the notebook on Windows, you will need to first determine the IP address of your Docker container. This command on the Docker command-line provides the IP address
```bash
docker-machine ip default
> <IP-address>
```
```default``` is the name of the container provided by default to the container you will spin. 
On obtaining the IP-address, run the docker as per the [instructions](#running-the-docker-image-as-a-container) provided and start the Jupyter notebook as [described above](#jupyter-notebooks). Then accessing ```http://<IP-address>:<host-port>``` on your host's browser should show you the notebook.

### Data Sharing
See [Docker container persistence](#docker-container-persistence). 
Consider this: You have a script that you've written on your host machine. You want to run this in the container and get the output data (say, a trained model) back into your host. The way to do this is using a [Shared Volume](#docker-container-persistence). By passing in the `-v /sharedfolder/:/root/sharedfolder` to the CLI, we are sharing the folder between the host and the container, with persistence. You could copy your script into `/sharedfolder` folder on the host, execute your script from inside the container (located at `/root/sharedfolder`) and write the results data back to the same folder. This data will be accessible even after you kill the container.

## What is Docker?
[Docker](https://www.docker.com/what-docker) itself has a great answer to this question.

Docker is based on the idea that one can package code along with its dependencies into a self-contained unit. In this case, we start with a base Ubuntu 14.04 image, a bare minimum OS. When we build our initial Docker image using `docker build`, we install all the deep learning frameworks and its dependencies on the base, as defined by the `Dockerfile`. This gives us an image which has all the packages we need installed in it. We can now spin up as many instances of this image as we like, using the `docker run` command. Each instance is called a _container_. Each of these containers can be thought of as a fully functional and isolated OS with all the deep learning libraries installed in it. 

## Why do I need a Docker?
Installing all the deep learning frameworks to coexist and function correctly is an exercise in dependency hell. Unfortunately, given the current state of DL development and research, it is almost impossible to rely on just one framework. This Docker is intended to provide a solution for this use case.

If you would rather install all the frameworks yourself manually, take a look at this guide: [Setting up a deep learning machine from scratch](https://github.com/saiprashanths/dl-setup)

### Do I really need an all-in-one container?
No. The provided all-in-one solution is useful if you have dependencies on multiple frameworks (say, load a pre-trained Caffe model, finetune it, convert it to Tensorflow and continue developing there) or if you just want to play around with the various frameworks.

The Docker philosophy is to build a container for each logical task/framework. If we followed this, we should have one container for each of the deep learning frameworks. This minimizes clashes between frameworks and is easier to maintain as things evolve. In fact, if you only intend to use one of the frameworks, or at least use only one framework at a time, follow this approach. You can find Dockerfiles for individual frameworks here:
* [Tensorflow Docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker)
* [Caffe Docker](https://github.com/BVLC/caffe/tree/master/docker)
* [Theano Docker](https://github.com/Kaixhin/dockerfiles/tree/master/cuda-theano)
* [Keras Docker](https://github.com/Kaixhin/dockerfiles/tree/master/cuda-keras/cuda_v7.5)
* [Lasagne Docker](https://github.com/Kaixhin/dockerfiles/tree/master/cuda-lasagne/cuda_v7.5)
* [Torch Docker](https://github.com/Kaixhin/dockerfiles/tree/master/cuda-torch)

## FAQs
### Performance
Running the DL frameworks as Docker containers should have no performance impact during runtime. Spinning up a Docker container itself is very fast and should take only a couple of seconds or less

### Docker container persistence
Keep in mind that the changes made inside Docker container are not persistent. Lets say you spun up a Docker container, added and deleted a few files and then kill the container. The next time you spin up a container using the same image, all your previous changes will be lost and you will be presented with a fresh instance of the image. This is great, because if you mess up your container, you can always kill it and start afresh again. It's bad if you don't know/understand this and forget to save your work before killing the container. There are a couple of ways to work around this:

1. **Commit**: If you make changes to the image itself (say, install a few new libraries), you can commit the changes and settings into a new image. Note that this will create a new image, which will take a few GBs space on your disk. In your next session, you can create a container from this new image. For details on commit, see [Docker's documentaion](https://docs.docker.com/engine/reference/commandline/commit/).

2. **Shared volume**: If you don't make changes to the image itself, but only create data (say, train a new Caffe model), then commiting the image each time is an overkill. In this case, it is easier to persist the data changes to a folder on your host OS using shared volumes. Simple put, the way this works is you share a folder from your host into the container. Any changes made to the contents of this folder from inside the container will persist, even after the container is killed. For more details, see Docker's docs on [Managing data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)
 
### How do I update/install new libraries?
You can do one of:

1. Modify the `Dockerfile` directly to install new or update your existing libraries. You will need to do a `docker build` after you do this. If you just want to update to a newer version of the DL framework(s), you can pass them as CLI parameter using the --build-arg tag ([see](-v /sharedfolder:/root/sharedfolder) for details). The framework versions are defined at the top of the `Dockerfile`. For example, `docker build -t floydhub/dl-docker:cpu -f Dockerfile.cpu --build-arg TENSORFLOW_VERSION=0.9.0rc0 .`

2. You can log in to a container and install the frameworks interactively using the terminal. After you've made sure everything looks good, you can commit the new contains and store it as an image

### What operating systems are supported?
Docker is supported on all the OSes mentioned here: [Install Docker Engine](https://docs.docker.com/engine/installation/) (i.e. different flavors of Linux, Windows and OS X). The CPU version (Dockerfile.cpu) will run on all the above operating systems. However, the GPU version (Dockerfile.gpu) will only run on Linux OS. This is because Docker runs inside a virtual machine on Windows and OS X. Virtual machines don't have direct access to the GPU on the host. Unless PCI passthrough is implemented for these hosts, GPU support isn't available on non-Linux OSes at the moment.
### Please do not use these `Dockerfiles` directly
These files are intended for Docker Hub's automated build process. The automated build for `Dockerfile.gpu` fails due to timeout restrictions (see [https://github.com/saiprashanths/dl-docker/issues/2](https://github.com/saiprashanths/dl-docker/issues/2)). To overcome this, `Dockerfile.gpu` is split into 2 parts: `Dockerfile.gpu1` and `Dockerfile.gpu2`. These files are used to build the final `dl-docker:gpu` that you can pull directly from Docker Hub using `docker pull floydhub/dl-docker:gpu`. 
* /Pipeline template/

Plantilla para /pipelines/ usando ~Luigi~




** Instrucciones rápidas

** Prerequisitos

- =docker=
- =hub=
- =git flow=
- =docker-compose=
- =docker-machine=

** Instalando


1. Copia el template a un repositorio  tuyo (en el ejemplo nanounanue, e.g. mi
   usuario de github) especificando el nombre del proyecto que quieres jitomate

#+BEGIN_SRC sh
curl -fsSL https://raw.githubusercontent.com/nanounanue/pipeline-template/master/installer.sh | bash -- /dev/stdin -p jitomate -u nanounanue
#+END_SRC

1. Crea 5 nodos usando =docker-machine=, por ejemplo usando =virtualbox= como
   =driver=

#+BEGIN_SRC sh 
export MACHINE_DRIVER=virtualbox
for N in $(seq 1 5); do
   docker-machine create node$N
done
#+END_SRC

2. Crea un =swarm=


#+BEGIN_SRC sh 
eval $(docker-machine env node1)

docker swarm init --advertise-addr $(docker-machine ip node1)

TOKEN=$(docker swarm join-token -q manager)
for N in $(seq 2 5); do
  eval $(docker-machine env node$N)
  docker swarm join --token $TOKEN $(docker-machine  ip node1):2377
done
eval $(docker-machine env node1)


docker node ls
#+END_SRC

*NOTA*: Si tienes problemas, ingresa a cada una de las máquinas



3. Crea un =registry= local al =swarm=

#+BEGIN_SRC sh 
 docker service create --name registry --publish 5000:5000 registry:2
#+END_SRC


4. Clona el repositorio en el =nodo1=

4. Registra las imágenes contenidas en =infraestructura=

#+BEGIN_SRC sh 
infraestructura/registrar.sh 
#+END_SRC


4. Instala y ejecuta el pipeline 




#+BEGIN_SRC sh 
make setup 

make run 
#+END_SRC


** Ejecutando pruebas

** /Deployment/


** Construido con...

   - [[some.url][Some URL]]
   -
   -

** ¿Cómo contribuir?


** Autores

** Licencia

** Agradecimientos

 - Plantilla de [[https://gist.github.com/PurpleBooth/109311bb0361f32d87a2][=README=]] 
* Luigi worker
* R

Imagen para ejecutar Rscripts
* Python
#Scrapy y comandos básicos

####Crear proyecto
```
#scrapy startproject [nombre_del_proyecto]
#ejemplo

scrapy startproject mercadolibre
```

####Pruebas de búsquedas
```
#scrapy shell "[url_completo]"
#ejemplo
scrapy shell "http://www.mercadolibre.com.mx/"
```

####Correr una araña
```
#scrapy crawl [nombre_de_la_araña_definido_en_archivo_dentro_del_folder_spiders]
#Nota: tienen que estar dentro de la carpeta del proyecto
#ejemplo
scrapy crawl ml_crawler

```# Eda# Scripts


Los requirements de python estan en proyecto/
El script author_categories.py se corre una sola vez para generar archivos auxiliares .txt.
# CACHE
datos generados en R## Kibana/ElasticSearch/Fluentd container setup
An easy, lightweight Kibana/ElasticSearch/Fluentd setup.  Perfect for a developer wanting to view their logs in Kibana. All 3 containers run on Alpine Linux.

```
docker-compose build
docker-compose up
```
Then fire up your application in a container using the built-in fluentd plugin in docker tools.  For example, if you have a Rails application in image 'myrails', running with a command similar to the following.

```
docker run --rm  -p 3000:3000 --log-driver=fluentd  myrails
```

When you first view Kibana in your browser, you will need to create the index, and thus will be shown the settings page.
Ensure that the fields read as follows and click the 'Create' button.
 - (yes) Index contains time based
 - Index name: logstash-*
 - Time field: @timestamp

## Parsing and Custom Fields
To get the most out of Kibana, you can configure data context for your situation:
 - At the source: If your application can log in JSON format, configuration further down the line gets easier.
 - Fluentd : Refer to [Fluentd docs](http://www.fluentd.org/guides/recipes/parse-syslog) on how to parse your data.
 - Elastic : Refer to [Elastic docs](https://www.elastic.co/guide/en/elasticsearch/guide/current/index-templates.html) for defining templates for your data schema.
 
Image sizes:
```
kibanacompose_fluentd              latest                  b2851afaa395        6 hours ago         55.83 MB
kibanacompose_kibana               latest                  a2899c1e9e16        6 hours ago         112.5 MB
kibanacompose_elastic              latest                  5089269c4bf0        5 weeks ago         152.7 MB
```
# Hadoop cluster - lite
Build a lightweight hadoop cluster with below mentioned hadoop services. These versions match what Hortonworks HDP 2.3.4 distribution has.


## Versions
- Alpine Linux: latest
- Debian: 8.3
- Java: Open JDK 7
- hadoop : 2.7.1
- pig: 0.15.0
- Zookeeper: 3.4.6
- Hbase: 1.1.2
- Kafka: 0.9.0.1
- Kafka manager: latest
- Storm: 0.10.0
- Phoenix: 4.7.0

Note: Hadoop and storm are built on Debian Linux and all others on Alpine Linux

>Dockerhub: https://hub.docker.com/r/anoopnair/

## Prerequsities
- Install [Docker](https://docs.docker.com/engine/installation/)
- Install [Docker compose](https://docs.docker.com/compose/install/)

> The Docker project is tested on Ubuntu host. There is no guarantee that this will work on Windows.


## Building all images
- Pull from docker hub OR
- Build each image individually

>This will individually build the above mentioned images. This will take sometime and is required to be done only once.

- Get zookeeper IP address using 

``docker inspect --format='{{.NetworkSettings.IPAddress}}' zookeeper``
- Get hbase IP address using 

``docker inspect --format='{{.NetworkSettings.IPAddress}}' hbase``
- Add zookeeper and hbase entries in the host /etc/hosts file

## Verification
- ``docker images`` to see all images created with proper tag version
- ``docker logs -f _CONTAINER_NAME_`` to follow logs for a specific container

If there is inconsistency in the image creation process, run the affected build script only. The build script is in each component directory.

## Starting

``./start.sh`` (background process)
OR

``docker-compose up``  (foreground process)

## Stopping

``docker-compose stop``


## Web UI
- [HBASE](http://localhost:60010)
- [Storm](http://localhost:49080)
- [Kafka manager](http://localhost:9000)
- [Hadoop cluster](http://localhost:8088)


## READMEs
Please go through below READMEs to get detailed information about each image

- [Alpine base README](alpine-base/README.md)
- [Debian base README](debian-base/README.md)
- [Hadoop README](hadoop/README.md)
- [Pig README](hadoop/pig/README.md)
- [Zookeeper README](zookeeper/README.md)
- [Hbase README](hbase/README.md)
- [Kafka README](kafka/README.md)
- [Storm README](storm/README.md)

## Reference
- https://hub.docker.com/r/wurstmeister/kafka/
- https://hub.docker.com/r/sheepkiller/kafka-manager/
- https://hub.docker.com/r/wurstmeister/storm/
- https://hub.docker.com/r/sequenceiq/hadoop-ubuntu/
- http://sookocheff.com/post/docker/containerizing-zookeeper-a-guided-tour/
- http://www.slideshare.net/fabiofumarola1/8a-setup-hbase
- https://docs.docker.com/engine/userguide/intro/
- https://docs.docker.com/compose/
- https://phoenix.apache.org/
# Zookeeper docker image
Build a zookeeper docker image on Alpine Linux distro. 
The ports 2181,2888,3888 are exposed.

## Version
3.4.6


## Building the image
- ``docker build -t anoopnair/zookeeper_alpine:3.4.6 .``

## Running zookeeper on a container
- ``docker run --name zookeeper -d -p 2181:2181 -p 2888:2888 -p 3888:3888 anoopnair/zookeeper_alpine:3.4.6``

- Create base image with Debian linux distro 
- Install essential linux packages
- Install Open JDK 7. All hadoop images should extend this one.


Building the image
----
- ``docker build -t anoopnair/hadoop_base_debian .``

Version
---
- Debian linux: 8.3 (jessie)
- Java: Open JDK 7 

# Storm docker image
Build a storm docker image on Debian Linux distro.


## Verison
0.10.0

## Building the image
- ``./build.sh``

## Usage
Start a cluster with zookeeper and docker:

- ``docker-compose up``

Destroy a cluster:

- ``docker-compose stop``

Add more supervisors:

- ``docker-compose scale supervisor=3``


## Storm UI
http://localhost:49080


## Deploy a topology

	
	docker run -v _HOST-WORKSPACE_:/home/storm --rm --entrypoint storm anoopnair/storm-nimbus_debian:0.10.0 jar /home/storm/_ARTIFACT-NAME_.jar _MAIN-CLASS-PACKAGE_ local -c nimbus.host=`docker inspect --format='{{.NetworkSettings.IPAddress}}' storm-nimbus`
# Hbase docker image
Build a hbase docker image on Alpine Linux. Also install Apache Phoenix.

## Version
- Hbase: 1.1.2
- Phoenix: 4.7.0

## Exposed ports
- HBase Master API port: 60000
- HBase Master Web UI: 60010
- Regionserver API port: 60020
- HBase Regionserver web UI: 60030

## Building the image       
- ``docker build -t anoopnair/hbase_alpine:1.1.2 .``

## Usage
Start a cluster with zookeeper and hbase:

- ``docker-compose up -d ``

Destroy a cluster:

- ``docker-compose stop``


## Running hbase commands
- ``docker exec -it hbase bash``
- ``hbase shell``
- Type the hbase commands

## Hbase UI
http://localhost:60010

## Using SQL client to connect to Hbase
Apache Phoenix is the SQL interface to the Hbase world. You can use DBeaver or Squirrel to write SQL. SQL will be translated by the Phoenix server component in the hbase region server into Hbase command. 

### Setup DBeaver/Squirrel
- Phoenix client jar is at the current location
- Open DBeaver/Squirrel
- Add new driver through Driver manager
- Set URL to jdbc:phoenix:zk. Type “org.apache.phoenix.jdbc.PhoenixDriver” into the Class Name textbox and click OK to close this dialog. Add the Phoenix client jar into DBeaver /Squirrel.
- Ensure Zookeeper and Hbase is running locally with Zookeper host as 'zk'
- Test connection
- Create base image with [Alpine Linux distro](http://www.alpinelinux.org/) 
- Install essential linux packages
- Install Open JDK 7. All hadoop images should extend this one.
- Prepare image with directories like /opt


Building the image
----
- ``docker build -t anoopnair/hadoop_base_alpine .``

Version
---
- Alpine linux:  latest
- Java: Open JDK 7 

# Kafka docker image
Build a kafka docker image on Alpine Linux distro.

## Version
- Kafka: 0.9.0.1
- Kafka manager: latest

## Building the image
``./build.sh``

This script will also pull kafka manager docker image from docker hub.

## Usage
Start a cluster with zookeeper and kafka:

- ``docker-compose up -d ``

Add more brokers:

- ``docker-compose scale kafka=3``

Destroy a cluster:

- ``docker-compose stop``


## Notes
The default _docker-compose.yml_ should be seen as a starting point. By default each broker will get a new port number and broker id on restart. Depending on your use case this might not be desirable. If you need to use specific ports and broker ids, modify the docker-compose configuration accordingly

- Find the IP address of the host and put that in kafka.yml against _KAFKA_ADVERTISED_HOST_NAME_


### Automatically create topics
If you want to have kafka-docker automatically create topics in Kafka during creation, a _KAFKA_CREATE_TOPICS_ environment variable can be
added in _docker-compose.yml_.

Here is an example snippet from _docker-compose.yml_:

        environment:
          KAFKA_CREATE_TOPICS: "INBOUND:1:2,OUTBOUND:1:1"

_INBOUND_ topic will have 1 partition and 2 replicas, _OUTBOUND_ topic will have 1 partition and 1 replica.

## Custom environments variables
1. Refer environment variables in [Kafka documentation](http://kafka.apache.org/documentation.html#configuration)
2. Convert the variable to uppercase, replace '.' with '_'
3. Prepend KAFKA_ to the variable
4. Use the variable as an environment variable

- Advertised hostname: _KAFKA_ADVERTISED_HOST_NAME_
- Advertised port: _KAFKA_ADVERTISED_PORT_


## Kafka manager UI
- http://localhost:9090
- Create a cluster with zookeeper connection as 'zk:2181'
- Automatically detect broker and topics
# Hadoop docker image
This is a hadoop docker build project on Debian Linux distro.

## Version
2.7.1


## Building the image
- ``docker build -t anoopnair/hadoop_debian:2.7.1 .``

## Running
- ``docker run --name hadoop -p 49707:49707 -p 50010:50010 -p 50020:50020 -p 50030:50030 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -it anoopnair/hadoop_debian:2.7.1``

## UI
- http://localhost:8088/

# Pig docker image
This is a hadoop pig docker build project on Debian Linux distro.

## Version
- hadoop: 2.7.1
- pig: 0.15.0


## Building the image
- ``docker build -t anoopnair/pig_hadoop_debian:0.15.0 .``

## Running
- ``docker run --name pig -it anoopnair/pig_hadoop_debian:0.15.0 bash``
- and then start running pig scripts

# vaadin-kootlin-example
```
cd docker
docker-compose up --build
```
access to localhost:8080

# Elasticsearch, Logstash and Kibana 5.5.1

This is a small container at only 300Mb compressed, running a full functional ELK 5 stack.

## Important

Make sure your docker hosts has the folowing sysctl setting, this is required for ELK

insert in /etc/sysctl.conf

```
vm.max_map_count = 262144
```

or run

```
sysctl -w vm.max_map_count=262144
```

## Features

* filebeat support
* cisco syslog support
* yum.log support via filebeat
* nginx accesslogs support
* updated upstream grok patterns
* running on Alpine Linux with s6, small, clean and efficient
* Maxmind geo data enabled
* Each process runs as own user
* multi input index is created based on type
* docker HEALTHCHECK for elasticsearch

## Instructions

Start the container

```
docker run -d -p 5601:5601 -p 9200:9200 -p 5044:5044 \
  -v /var/lib/elasticsearch:/var/lib/elasticsearch \
  --name elk \
  cdrocker/elk5:latest
```

Check progress with

```
docker logs -f elk
```

You can now open kibana http://elasticsearchhost:5601

There will probably be no index patterns, you'll have to import them manually. For beats you can use the new import_dashboards script which automate this process. (Install filebeat for this functionality.)

```
/usr/share/filebeat/scripts/import_dashboards -es http://<elasticsearch>:9200
/usr/share/metricbeat/scripts/import_dashboards -es http://<elasticsearch>:9200
/usr/share/packetbeat/scripts/import_dashboards -es http://<elasticsearch>:9200
```

## Todo

* Add java environment options
* autoupdate GEO data
* curator install
* auto cleanup of old indices
* elasticsearch plugins
# Elasticsearch, Logstash and Kibana 6.0.0 (beta)

This is a small container at only 300Mb compressed, running a full functional ELK 6 stack.

## Important

Make sure your docker hosts has the folowing sysctl setting, this is required for ELK

insert in /etc/sysctl.conf

```
vm.max_map_count = 262144
```

or run

```
sysctl -w vm.max_map_count=262144
```

## Features

* filebeat support
* cisco syslog support
* yum.log support via filebeat
* nginx accesslogs support
* updated upstream grok patterns
* running on Alpine Linux with s6, small, clean and efficient
* Maxmind geo data enabled
* Each process runs as own user
* multi input index is created based on type
* docker HEALTHCHECK for elasticsearch

## Instructions

Start the container

```
docker run -d -p 5601:5601 -p 9200:9200 -p 5044:5044 \
  -v /var/lib/elasticsearch:/var/lib/elasticsearch \
  --name elk \
  cdrocker/elk5:latest
```

Check progress with

```
docker logs -f elk
```

You can now open kibana http://elasticsearchhost:5601

There will probably be no index patterns, you'll have to import them manually. For beats you can use the new import_dashboards script which automate this process. (Install filebeat for this functionality.)

```
/usr/share/filebeat/scripts/import_dashboards -es http://<elasticsearch>:9200
/usr/share/metricbeat/scripts/import_dashboards -es http://<elasticsearch>:9200
/usr/share/packetbeat/scripts/import_dashboards -es http://<elasticsearch>:9200
```

## Todo

* Add java environment options
* autoupdate GEO data
* curator install
* auto cleanup of old indices
* elasticsearch plugins
# Elasticsearch, Logstash and Kibana 5.5.1

This is a small container at only 300Mb compressed, running a full functional ELK 5 stack.

## Important

Make sure your docker hosts has the folowing sysctl setting, this is required for ELK

insert in /etc/sysctl.conf

```
vm.max_map_count = 262144
```

or run

```
sysctl -w vm.max_map_count=262144
```

## Features

* filebeat support
* cisco syslog support
* yum.log support via filebeat
* nginx accesslogs support
* updated upstream grok patterns
* running on Alpine Linux with s6, small, clean and efficient
* Maxmind geo data enabled
* Each process runs as own user
* multi input index is created based on type
* docker HEALTHCHECK for elasticsearch

## Instructions

Start the container

```
docker run -d -p 5601:5601 -p 9200:9200 -p 5044:5044 \
  -v /var/lib/elasticsearch:/var/lib/elasticsearch \
  --name elk \
  cdrocker/elk5:latest
```

Check progress with

```
docker logs -f elk
```

You can now open kibana http://elasticsearchhost:5601

There will probably be no index patterns, you'll have to import them manually. For beats you can use the new import_dashboards script which automate this process. (Install filebeat for this functionality.)

```
/usr/share/filebeat/scripts/import_dashboards -es http://<elasticsearch>:9200
/usr/share/metricbeat/scripts/import_dashboards -es http://<elasticsearch>:9200
/usr/share/packetbeat/scripts/import_dashboards -es http://<elasticsearch>:9200
```

## Todo

* Add java environment options
* autoupdate GEO data
* curator install
* auto cleanup of old indices
* elasticsearch plugins
# esrally-docker
[![](https://dockerbuildbadges.quelltext.eu/status.svg?organization=ryanmaclean&repository=esrally)](https://hub.docker.com/r/ryanmaclean/esrally/builds/) [![CircleCI](https://circleci.com/gh/ryanmaclean/esrally-docker.svg?style=svg)](https://circleci.com/gh/ryanmaclean/esrally-docker)

An Elastic ES Rally Docker container image that allows you to run ESRally in a container and point it at a remote Elasticsearch host/cluster in order to run and re-run benchmarks. 

Based on the work found here: https://github.com/honestbee/docker-esrally but much less grandiose in scope, and more automated.

It does a good job of punishing smaller clusters, here's a sample from the Elastic Cloud trial cluster:

![Elastic Cloud Dashboard](https://raw.githubusercontent.com/ryanmaclean/esrally-docker/master/images/esrally1.png?raw=true "Sample from Elastic Cloud Trial")
![Elastic Cloud Dashboard](https://raw.githubusercontent.com/ryanmaclean/esrally-docker/master/images/esrally2.png?raw=true "Sample from Elastic Cloud Trial")

# ES Rally in Docker

This docker container is expected to be used as a remote target benchmark. 

Warning: it will download a 250MB+ file and expand it to 3.30GB. It will ingest this data into the Elasticsearch cluster that you point it at. 

As the disclaimer will warn you: 

```
************************************************************************
************** WARNING: A dark dungeon lies ahead of you  **************
************************************************************************
```
## More Info About ES Rally

This project came about after watching the fantastic ES Rally intro video from Daniel Mitterdorfer: https://www.youtube.com/watch?v=HriBY2zoChw

The ES Rally docs can be found here: http://esrally.readthedocs.io/en/latest/

## Usage

There's are three ENV variables, one `PIPELINE`, which defaults to `--pipeline=benchmark-only`, an `ENDPOINT` var that defaults to `192.168.99.100:9200` for Kitematic use, but you can over-ride it as follows: 

```
docker run -it -e ENDPOINT="54.32.263.2:9200" ryanmaclean/esrally:latest
```

Another is `CREDENTIALS`, and is left blank, but may be needed when testing X-Pack-enabled clusters. That can be done as follows:

```
docker run -it -e CREDENTIALS=--client-options="basic_auth_user:'user',basic_auth_password:'password'" ryanmaclean/esrally:latest 
```

This assumes that `54.32.263.2` is the primary endpoint of your cluster, and that you're using `9200` as the Elasticsearch port. 

## Usage with Elastic Cloud

Elastic Cloud has a remote server connection and security as well as certs enabled, so our string gets a bit long. On top of this, if you're using the trial, the budget allocated for two weeks means that the cluster will be yellow, requiring an final third CLUSTERHEALTH Docker ENV var that translates to an esrally parameter which will allow the test to continue.

For example:

```
docker run -it \
   - e CREDENTIALS=\
      --client-options=\
         use_ssl:true,\
         verify_certs:true,\
         basic_auth_user:'elastic',\
         basic_auth_password:'lkjsadioioasdlkjas822'
   -e ENDPOINT=\
      0983209823883892039265cd8046e4.us-west-1.aws.found.io:9243
   -e CLUSTERHEALTH=\
      --cluster-health=yellow\
   ryanmaclean/esrally:latest
```

## Sample Output

This is from a terminal session running against the official Elastic Docker container for version 5.4 on a Mid 2015 maxed-out Macbook Pro (not pretty).

It took quite a while to run, and you can see that the boot2docker host should have been tweaked beforehand, but this should be a good baseline of a badly tuned cluster. 
  
```
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

INFO:rally.config:Upgrading configuration from version [6] to [8].
INFO:rally.config:Creating a backup of the current config file at [/root/.rally/rally.ini].
INFO:rally.config:Successfully self-upgraded configuration to version [8]
[INFO] Writing logs to /root/.rally/logs/rally_out_20170527T222757Z.log
Cloning into '/root/.rally/benchmarks/tracks/default'...
remote: Counting objects: 1057, done.
remote: Total 1057 (delta 0), reused 0 (delta 0), pack-reused 1057
Receiving objects: 100% (1057/1057), 755.53 KiB | 1.46 MiB/s, done.
Resolving deltas: 100% (799/799), done.
Checking connectivity... done.

************************************************************************
************** WARNING: A dark dungeon lies ahead of you  **************
************************************************************************

Rally does not have control over the configuration of the benchmarked
Elasticsearch cluster.

Be aware that results may be misleading due to problems with the setup.
Rally is also not able to gather lots of metrics at all (like CPU usage
of the benchmarked cluster) or may even produce misleading metrics (like
the index size).

************************************************************************
****** Use this pipeline only if you are aware of the tradeoffs.  ******
*************************** Watch your step! ***************************
************************************************************************

[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [external]

[INFO] Downloading data from [http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames/documents-2.json.bz2] (252 MB) to [/root/.rally/benchmarks/data/geonames/documents-2.json.bz2] ... [OK]
[INFO] Decompressing track data from [/root/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/root/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: 3.30 GB) ... [OK]
[INFO] Preparing file offset table for [/root/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
Running index-append                                                           [100% done]
Running force-merge                                                            [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------
```

|   Lap |                         Metric |              Operation |     Value |   Unit |
|------:|-------------------------------:|-----------------------:|----------:|-------:|
|   All |                  Indexing time |                        |   41.7569 |    min |
|   All |                     Merge time |                        |    39.541 |    min |
|   All |                   Refresh time |                        |    15.867 |    min |
|   All |                     Flush time |                        | 0.0291167 |    min |
|   All |            Merge throttle time |                        |  0.995683 |    min |
|   All |             Total Young Gen GC |                        |   525.372 |      s |
|   All |               Total Old Gen GC |                        |    12.543 |      s |
|   All |         Heap used for segments |                        |   17.9723 |     MB |
|   All |       Heap used for doc values |                        |   0.11013 |     MB |
|   All |            Heap used for terms |                        |   16.7452 |     MB |
|   All |            Heap used for norms |                        | 0.0749512 |     MB |
|   All |           Heap used for points |                        |  0.213588 |     MB |
|   All |    Heap used for stored fields |                        |  0.828438 |     MB |
|   All |                  Segment count |                        |       102 |        |
|   All |                 Min Throughput |           index-append |   4045.99 | docs/s |
|   All |              Median Throughput |           index-append |   4200.45 | docs/s |
|   All |                 Max Throughput |           index-append |   4817.98 | docs/s |
|   All |      50.0th percentile latency |           index-append |   8832.61 |     ms |
|   All |      90.0th percentile latency |           index-append |   13494.6 |     ms |
|   All |      99.0th percentile latency |           index-append |   19720.5 |     ms |
|   All |      99.9th percentile latency |           index-append |     22807 |     ms |
|   All |       100th percentile latency |           index-append |   23539.8 |     ms |
|   All | 50.0th percentile service time |           index-append |   8832.61 |     ms |
|   All | 90.0th percentile service time |           index-append |   13494.6 |     ms |
|   All | 99.0th percentile service time |           index-append |   19720.5 |     ms |
|   All | 99.9th percentile service time |           index-append |     22807 |     ms |
|   All |  100th percentile service time |           index-append |   23539.8 |     ms |
|   All |                     error rate |           index-append |         0 |      % |
|   All |                 Min Throughput |            force-merge |   3.30106 |  ops/s |
|   All |              Median Throughput |            force-merge |   3.30106 |  ops/s |
|   All |                 Max Throughput |            force-merge |   3.30106 |  ops/s |
|   All |       100th percentile latency |            force-merge |    302.75 |     ms |
|   All |  100th percentile service time |            force-merge |    302.75 |     ms |
|   All |                     error rate |            force-merge |         0 |      % |
|   All |                 Min Throughput |            index-stats |   99.9312 |  ops/s |
|   All |              Median Throughput |            index-stats |   100.049 |  ops/s |
|   All |                 Max Throughput |            index-stats |   100.093 |  ops/s |
|   All |      50.0th percentile latency |            index-stats |   4.56831 |     ms |
|   All |      90.0th percentile latency |            index-stats |   5.37362 |     ms |
|   All |      99.0th percentile latency |            index-stats |   10.4568 |     ms |
|   All |      99.9th percentile latency |            index-stats |   16.8071 |     ms |
|   All |       100th percentile latency |            index-stats |   20.6186 |     ms |
|   All | 50.0th percentile service time |            index-stats |    4.2707 |     ms |
|   All | 90.0th percentile service time |            index-stats |   4.89174 |     ms |
|   All | 99.0th percentile service time |            index-stats |   9.63112 |     ms |
|   All | 99.9th percentile service time |            index-stats |   16.1339 |     ms |
|   All |  100th percentile service time |            index-stats |   20.4829 |     ms |
|   All |                     error rate |            index-stats |         0 |      % |
|   All |                 Min Throughput |             node-stats |   99.7653 |  ops/s |
|   All |              Median Throughput |             node-stats |   100.085 |  ops/s |
|   All |                 Max Throughput |             node-stats |   100.535 |  ops/s |
|   All |      50.0th percentile latency |             node-stats |   4.55892 |     ms |
|   All |      90.0th percentile latency |             node-stats |    6.2264 |     ms |
|   All |      99.0th percentile latency |             node-stats |   85.0515 |     ms |
|   All |      99.9th percentile latency |             node-stats |   153.335 |     ms |
|   All |       100th percentile latency |             node-stats |   159.538 |     ms |
|   All | 50.0th percentile service time |             node-stats |   4.22034 |     ms |
|   All | 90.0th percentile service time |             node-stats |   5.08135 |     ms |
|   All | 99.0th percentile service time |             node-stats |   10.5145 |     ms |
|   All | 99.9th percentile service time |             node-stats |   31.6439 |     ms |
|   All |  100th percentile service time |             node-stats |   159.403 |     ms |
|   All |                     error rate |             node-stats |         0 |      % |
|   All |                 Min Throughput |                default |   35.7368 |  ops/s |
|   All |              Median Throughput |                default |   38.1349 |  ops/s |
|   All |                 Max Throughput |                default |   38.7257 |  ops/s |
|   All |      50.0th percentile latency |                default |   6194.75 |     ms |
|   All |      90.0th percentile latency |                default |   8757.89 |     ms |
|   All |      99.0th percentile latency |                default |   9139.22 |     ms |
|   All |      99.9th percentile latency |                default |    9168.4 |     ms |
|   All |       100th percentile latency |                default |   9176.96 |     ms |
|   All | 50.0th percentile service time |                default |   23.1244 |     ms |
|   All | 90.0th percentile service time |                default |   32.4216 |     ms |
|   All | 99.0th percentile service time |                default |   42.7273 |     ms |
|   All | 99.9th percentile service time |                default |   47.2948 |     ms |
|   All |  100th percentile service time |                default |   47.3488 |     ms |
|   All |                     error rate |                default |         0 |      % |
|   All |                 Min Throughput |                   term |   199.627 |  ops/s |
|   All |              Median Throughput |                   term |   199.953 |  ops/s |
|   All |                 Max Throughput |                   term |   200.106 |  ops/s |
|   All |      50.0th percentile latency |                   term |   3.45829 |     ms |
|   All |      90.0th percentile latency |                   term |   9.61196 |     ms |
|   All |      99.0th percentile latency |                   term |   25.0415 |     ms |
|   All |      99.9th percentile latency |                   term |   30.6498 |     ms |
|   All |       100th percentile latency |                   term |   31.8718 |     ms |
|   All | 50.0th percentile service time |                   term |   2.67088 |     ms |
|   All | 90.0th percentile service time |                   term |   3.56639 |     ms |
|   All | 99.0th percentile service time |                   term |   17.2176 |     ms |
|   All | 99.9th percentile service time |                   term |   24.8637 |     ms |
|   All |  100th percentile service time |                   term |    26.781 |     ms |
|   All |                     error rate |                   term |         0 |      % |
|   All |                 Min Throughput |                 phrase |   181.584 |  ops/s |
|   All |              Median Throughput |                 phrase |   193.872 |  ops/s |
|   All |                 Max Throughput |                 phrase |   199.885 |  ops/s |
|   All |      50.0th percentile latency |                 phrase |   165.916 |     ms |
|   All |      90.0th percentile latency |                 phrase |   272.091 |     ms |
|   All |      99.0th percentile latency |                 phrase |   293.071 |     ms |
|   All |      99.9th percentile latency |                 phrase |   297.487 |     ms |
|   All |       100th percentile latency |                 phrase |   299.297 |     ms |
|   All | 50.0th percentile service time |                 phrase |   3.25947 |     ms |
|   All | 90.0th percentile service time |                 phrase |    7.3253 |     ms |
|   All | 99.0th percentile service time |                 phrase |   25.5142 |     ms |
|   All | 99.9th percentile service time |                 phrase |   32.5812 |     ms |
|   All |  100th percentile service time |                 phrase |   36.5469 |     ms |
|   All |                     error rate |                 phrase |         0 |      % |
|   All |                 Min Throughput |   country_agg_uncached |   1.10785 |  ops/s |
|   All |              Median Throughput |   country_agg_uncached |   1.24299 |  ops/s |
|   All |                 Max Throughput |   country_agg_uncached |   1.29339 |  ops/s |
|   All |      50.0th percentile latency |   country_agg_uncached |    605116 |     ms |
|   All |      90.0th percentile latency |   country_agg_uncached |    805617 |     ms |
|   All |      99.0th percentile latency |   country_agg_uncached |    854591 |     ms |
|   All |      99.9th percentile latency |   country_agg_uncached |    859400 |     ms |
|   All |       100th percentile latency |   country_agg_uncached |    859942 |     ms |
|   All | 50.0th percentile service time |   country_agg_uncached |   703.377 |     ms |
|   All | 90.0th percentile service time |   country_agg_uncached |    741.63 |     ms |
|   All | 99.0th percentile service time |   country_agg_uncached |   800.875 |     ms |
|   All | 99.9th percentile service time |   country_agg_uncached |   870.371 |     ms |
|   All |  100th percentile service time |   country_agg_uncached |    894.24 |     ms |
|   All |                     error rate |   country_agg_uncached |         0 |      % |
|   All |                 Min Throughput |     country_agg_cached |   100.026 |  ops/s |
|   All |              Median Throughput |     country_agg_cached |   100.063 |  ops/s |
|   All |                 Max Throughput |     country_agg_cached |   100.141 |  ops/s |
|   All |      50.0th percentile latency |     country_agg_cached |   3.57384 |     ms |
|   All |      90.0th percentile latency |     country_agg_cached |   4.93876 |     ms |
|   All |      99.0th percentile latency |     country_agg_cached |   16.0598 |     ms |
|   All |      99.9th percentile latency |     country_agg_cached |   22.5004 |     ms |
|   All |       100th percentile latency |     country_agg_cached |   26.4915 |     ms |
|   All | 50.0th percentile service time |     country_agg_cached |   3.15232 |     ms |
|   All | 90.0th percentile service time |     country_agg_cached |   4.06166 |     ms |
|   All | 99.0th percentile service time |     country_agg_cached |   15.8608 |     ms |
|   All | 99.9th percentile service time |     country_agg_cached |   22.1849 |     ms |
|   All |  100th percentile service time |     country_agg_cached |   26.2606 |     ms |
|   All |                     error rate |     country_agg_cached |         0 |      % |
|   All |                 Min Throughput |                 scroll |   39.7068 |  ops/s |
|   All |              Median Throughput |                 scroll |   40.5957 |  ops/s |
|   All |                 Max Throughput |                 scroll |   41.0411 |  ops/s |
|   All |      50.0th percentile latency |                 scroll |    259404 |     ms |
|   All |      90.0th percentile latency |                 scroll |    371192 |     ms |
|   All |      99.0th percentile latency |                 scroll |    395720 |     ms |
|   All |       100th percentile latency |                 scroll |    398442 |     ms |
|   All | 50.0th percentile service time |                 scroll |    598.62 |     ms |
|   All | 90.0th percentile service time |                 scroll |   625.847 |     ms |
|   All | 99.0th percentile service time |                 scroll |   651.197 |     ms |
|   All |  100th percentile service time |                 scroll |   658.699 |     ms |
|   All |                     error rate |                 scroll |         0 |      % |
|   All |                 Min Throughput |             expression |   0.67594 |  ops/s |
|   All |              Median Throughput |             expression |  0.678439 |  ops/s |
|   All |                 Max Throughput |             expression |  0.680025 |  ops/s |
|   All |      50.0th percentile latency |             expression |    293177 |     ms |
|   All |      90.0th percentile latency |             expression |    369571 |     ms |
|   All |      99.0th percentile latency |             expression |    386814 |     ms |
|   All |       100th percentile latency |             expression |    388714 |     ms |
|   All | 50.0th percentile service time |             expression |   1459.99 |     ms |
|   All | 90.0th percentile service time |             expression |   1492.65 |     ms |
|   All | 99.0th percentile service time |             expression |   1522.07 |     ms |
|   All |  100th percentile service time |             expression |   1527.01 |     ms |
|   All |                     error rate |             expression |         0 |      % |
|   All |                 Min Throughput |        painless_static |  0.495775 |  ops/s |
|   All |              Median Throughput |        painless_static |  0.514832 |  ops/s |
|   All |                 Max Throughput |        painless_static |  0.519771 |  ops/s |
|   All |      50.0th percentile latency |        painless_static |    433935 |     ms |
|   All |      90.0th percentile latency |        painless_static |    570563 |     ms |
|   All |      99.0th percentile latency |        painless_static |    603637 |     ms |
|   All |       100th percentile latency |        painless_static |    607317 |     ms |
|   All | 50.0th percentile service time |        painless_static |   2003.61 |     ms |
|   All | 90.0th percentile service time |        painless_static |    2512.4 |     ms |
|   All | 99.0th percentile service time |        painless_static |   2795.82 |     ms |
|   All |  100th percentile service time |        painless_static |   2993.33 |     ms |
|   All |                     error rate |        painless_static |         0 |      % |
|   All |                 Min Throughput |       painless_dynamic |  0.463116 |  ops/s |
|   All |              Median Throughput |       painless_dynamic |  0.471679 |  ops/s |
|   All |                 Max Throughput |       painless_dynamic |  0.488911 |  ops/s |
|   All |      50.0th percentile latency |       painless_dynamic |    487720 |     ms |
|   All |      90.0th percentile latency |       painless_dynamic |    629961 |     ms |
|   All |      99.0th percentile latency |       painless_dynamic |    657571 |     ms |
|   All |       100th percentile latency |       painless_dynamic |    660803 |     ms |
|   All | 50.0th percentile service time |       painless_dynamic |   2132.71 |     ms |
|   All | 90.0th percentile service time |       painless_dynamic |   2692.83 |     ms |
|   All | 99.0th percentile service time |       painless_dynamic |   3210.24 |     ms |
|   All |  100th percentile service time |       painless_dynamic |   3589.56 |     ms |
|   All |                     error rate |       painless_dynamic |         0 |      % |
|   All |                 Min Throughput |            large_terms |           |  ops/s |
|   All |              Median Throughput |            large_terms |           |  ops/s |
|   All |                 Max Throughput |            large_terms |           |  ops/s |
|   All |      50.0th percentile latency |            large_terms |    203.65 |     ms |
|   All |      90.0th percentile latency |            large_terms |   368.147 |     ms |
|   All |      99.0th percentile latency |            large_terms |   567.241 |     ms |
|   All |       100th percentile latency |            large_terms |   906.133 |     ms |
|   All | 50.0th percentile service time |            large_terms |   201.502 |     ms |
|   All | 90.0th percentile service time |            large_terms |   362.732 |     ms |
|   All | 99.0th percentile service time |            large_terms |   517.201 |     ms |
|   All |  100th percentile service time |            large_terms |   905.241 |     ms |
|   All |                     error rate |            large_terms |       100 |      % |
|   All |                 Min Throughput |   large_filtered_terms |  0.461976 |  ops/s |
|   All |              Median Throughput |   large_filtered_terms |   0.46281 |  ops/s |
|   All |                 Max Throughput |   large_filtered_terms |  0.464654 |  ops/s |
|   All |      50.0th percentile latency |   large_filtered_terms |    500066 |     ms |
|   All |      90.0th percentile latency |   large_filtered_terms |    628508 |     ms |
|   All |      99.0th percentile latency |   large_filtered_terms |    658435 |     ms |
|   All |       100th percentile latency |   large_filtered_terms |    661668 |     ms |
|   All | 50.0th percentile service time |   large_filtered_terms |   2145.39 |     ms |
|   All | 90.0th percentile service time |   large_filtered_terms |   2216.07 |     ms |
|   All | 99.0th percentile service time |   large_filtered_terms |   2311.95 |     ms |
|   All |  100th percentile service time |   large_filtered_terms |   2634.08 |     ms |
|   All |                     error rate |   large_filtered_terms |         0 |      % |
|   All |                 Min Throughput | large_prohibited_terms |  0.501823 |  ops/s |
|   All |              Median Throughput | large_prohibited_terms |  0.504007 |  ops/s |
|   All |                 Max Throughput | large_prohibited_terms |  0.505685 |  ops/s |
|   All |      50.0th percentile latency | large_prohibited_terms |    447401 |     ms |
|   All |      90.0th percentile latency | large_prohibited_terms |    562210 |     ms |
|   All |      99.0th percentile latency | large_prohibited_terms |    589447 |     ms |
|   All |       100th percentile latency | large_prohibited_terms |    592572 |     ms |
|   All | 50.0th percentile service time | large_prohibited_terms |   2010.96 |     ms |
|   All | 90.0th percentile service time | large_prohibited_terms |   2074.62 |     ms |
|   All | 99.0th percentile service time | large_prohibited_terms |   2185.27 |     ms |
|   All |  100th percentile service time | large_prohibited_terms |   2230.82 |     ms |
|   All |                     error rate | large_prohibited_terms |         0 |      % |


----------------------------------
[INFO] SUCCESS (took 8844 seconds)
----------------------------------



# Contributing

Contributions in the form of pull requests and issues are welcome! For example, version increments, upstream fixes and failing automated builds in Docker Hub all sound like future issues that might arise as the images are built everytime the Python image defined in `FROM` is updated. 
* /Pipeline template/

Plantilla para /pipelines/ usando ~Luigi~




** Instrucciones rápidas

** Prerequisitos

- =docker=
- =ag=
- =hub=
- =git flow=
- =docker-compose=
- =docker-machine=

** Instalando


1. Copia el template a un repositorio  tuyo (en el ejemplo *nanounanue*, e.g. mi
   usuario de github) especificando el nombre del proyecto que quieres (en el ejemplo, *jitomate*)

#+BEGIN_SRC sh
curl -fsSL https://raw.githubusercontent.com/nanounanue/pipeline-template/master/installer.sh | bash -- /dev/stdin -p jitomate -u nanounanue
#+END_SRC

1. Crea 5 nodos usando =docker-machine=, por ejemplo usando =virtualbox= como
   =driver=

#+BEGIN_SRC sh 
export MACHINE_DRIVER=virtualbox
for N in $(seq 1 5); do
   docker-machine create node$N
done
#+END_SRC

2. Crea un =swarm=


#+BEGIN_SRC sh 
eval $(docker-machine env node1)

docker swarm init --advertise-addr $(docker-machine ip node1)

TOKEN=$(docker swarm join-token -q manager)
for N in $(seq 2 5); do
  eval $(docker-machine env node$N)
  docker swarm join --token $TOKEN $(docker-machine  ip node1):2377
done
eval $(docker-machine env node1)


docker node ls
#+END_SRC

*NOTA*: Si tienes problemas, ingresa a cada una de las máquinas



3. Crea un =registry= local al =swarm=

#+BEGIN_SRC sh 
 docker service create --name registry --publish 5000:5000 registry:2
#+END_SRC


4. Clona el repositorio en el =nodo1=

4. Registra las imágenes contenidas en =infraestructura=

#+BEGIN_SRC sh 
infraestructura/registrar.sh 
#+END_SRC


4. Instala y ejecuta el pipeline 




#+BEGIN_SRC sh 
make setup 

make run 
#+END_SRC


** Ejecutando pruebas

** /Deployment/


** Construido con...

   - [[some.url][Some URL]]
   -
   -

** ¿Cómo contribuir?


** Autores

** Licencia

** Agradecimientos

 - Plantilla de [[https://gist.github.com/PurpleBooth/109311bb0361f32d87a2][README]] 
* Luigi worker
* R

Imagen para ejecutar Rscripts
* Python
* /Pipeline template/

Plantilla para /pipelines/ usando ~Luigi~




** Instrucciones rápidas

** Prerequisitos

- =docker=
- =hub=
- =git flow=
- =docker-compose=
- =docker-machine=

** Instalando


1. Copia el template a un repositorio  tuyo (en el ejemplo nanounanue, e.g. mi
   usuario de github) especificando el nombre del proyecto que quieres jitomate

#+BEGIN_SRC sh
curl -fsSL https://raw.githubusercontent.com/nanounanue/pipeline-template/master/installer.sh | bash -- /dev/stdin -p jitomate -u nanounanue
#+END_SRC

1. Crea 5 nodos usando =docker-machine=, por ejemplo usando =virtualbox= como
   =driver=

#+BEGIN_SRC sh 
export MACHINE_DRIVER=virtualbox
for N in $(seq 1 5); do
   docker-machine create node$N
done
#+END_SRC

2. Crea un =swarm=


#+BEGIN_SRC sh 
eval $(docker-machine env node1)

docker swarm init --advertise-addr $(docker-machine ip node1)

TOKEN=$(docker swarm join-token -q manager)
for N in $(seq 2 5); do
  eval $(docker-machine env node$N)
  docker swarm join --token $TOKEN $(docker-machine  ip node1):2377
done
eval $(docker-machine env node1)


docker node ls
#+END_SRC

*NOTA*: Si tienes problemas, ingresa a cada una de las máquinas



3. Crea un =registry= local al =swarm=

#+BEGIN_SRC sh 
 docker service create --name registry --publish 5000:5000 registry:2
#+END_SRC


4. Clona el repositorio en el =nodo1=

4. Registra las imágenes contenidas en =infraestructura=

#+BEGIN_SRC sh 
infraestructura/registrar.sh 
#+END_SRC


4. Instala y ejecuta el pipeline 




#+BEGIN_SRC sh 
make setup 

make run 
#+END_SRC


** Ejecutando pruebas

** /Deployment/


** Construido con...

   - [[some.url][Some URL]]
   -
   -

** ¿Cómo contribuir?


** Autores

** Licencia

** Agradecimientos

 - Plantilla de [[https://gist.github.com/PurpleBooth/109311bb0361f32d87a2][=README=]] 
* Luigi worker
* R

Imagen para ejecutar Rscripts
* Python
Hari Sekhon Docker
==================
[![Build Status](https://travis-ci.org/HariSekhon/Dockerfiles.svg?branch=master)](https://travis-ci.org/HariSekhon/Dockerfiles) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/de6229f2d2ba4945acde9f86f59d2c66)](https://www.codacy.com/app/harisekhon/Dockerfiles) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/Dockerfiles.svg)](https://github.com/harisekhon/Dockerfiles/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/Dockerfiles.svg)](https://github.com/harisekhon/Dockerfiles/network) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/HariSekhon/Dockerfiles#hari-sekhon-docker) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/u/harisekhon/)

Docker Images containing hundreds of my published tools and the supporting technologies to run full functional test suites.

##### Contains 41 repos with over 140 tags, many different versions of [official software](https://github.com/HariSekhon/Dockerfiles#official-technologies):

* Hadoop, Big Data, NoSQL & OS images
* [My GitHub repos](https://github.com/HariSekhon) containing hundreds of tools related to these technologies with all dependencies pre-built

These images are all available pre-built on [My DockerHub: https://hub.docker.com/u/harisekhon/](https://hub.docker.com/u/harisekhon/).

Hari Sekhon

Big Data Contractor, United Kingdom

https://www.linkedin.com/in/harisekhon
###### (you're welcome to connect with me on LinkedIn)

### Ready to run Docker images

```
docker search harisekhon
docker run harisekhon/nagios-plugins
```

To see more than the 25 DockerHub repos limited by ```docker search``` ([docker issue 23055](https://github.com/docker/docker/issues/23055)) I wrote ```dockerhub_search.py``` using the DockerHub API, available in my [PyTools github repo](https://github.com/harisekhon/pytools) and as a pre-built docker image:

```
docker run harisekhon/pytools dockerhub_search.py harisekhon
```

There are lots of tagged versions of official software in my repos to allow development testing across multiple versions, usually more versions than available from the official repos (and new version updates available on request, just [raise a GitHub issue](https://github.com/harisekhon/Dockerfiles/issues)).

DockerHub tags are not shown by ```docker search``` ([docker issue 17238](https://github.com/docker/docker/issues/17238)) so I wrote ```dockerhub_show_tags.py``` available in my [PyTools github repo](https://github.com/harisekhon/pytools) and as a pre-built docker image - eg. to see an organized list of all tags for the official CentOS & Ubuntu repos dynamically using the DockerHub API:

```
docker run harisekhon/pytools dockerhub_show_tags.py centos ubuntu
```

(you might need to add --timeout 60 as ubuntu has a lot of tags now I've noticed to stop it timing out while fetching all the pages)

For service technologies like Hadoop, HBase, ZooKeeper etc for which you'll also want port mappings, each directory in the [GitHub project](https://github.com/harisekhon/dockerfiles) contains both a standard ` docker-compose ` configuration as well as a ` make run ` shortcut (which doesn't require ` docker-compose ` to be installed) - either way you don't have to remember all the command line switches and port number specifics:

```
cd zookeeper
docker-compose up
```

or for technologies with interactive shells like Spark, ZooKeeper, HBase, Drill, Cassandra where you want to be dropped in to an interactive shell, use the ` make run ` shortcut instead:

```
cd zookeeper
make run
```

which is much easier to type and remember than the equivalent bigger commands like:

```
docker run -ti -p 2181:2181 harisekhon/zookeeper
```

which gets much worse for more complex services like Hadoop / HBase:

```
docker run -ti -p 2181:2181 -p 8080:8080 -p 8085:8085 -p 9090:9090 -p 9095:9095 -p 16000:16000 -p 16010:16010 -p 16201:16201 -p 16301:16301 harisekhon/hbase
```

```
docker run -ti -p 8020:8020 -p 8032:8032 -p 8088:8088 -p 9000:9000 -p 10020:10020 -p 19888:19888 -p 50010:50010 -p 50020:50020 -p 50070:50070 -p 50075:50075 -p 50090:50090 harisekhon/hadoop
```

### Full Inventory:

##### Official Technologies:

More specific information can be found in the readme page under each respective directory in the [Dockerfiles git repo](https://github.com/HariSekhon/Dockerfiles).

- [Alluxio](http://www.alluxio.org/)
- [Apache Drill](https://drill.apache.org/) (opens Drill shell)
- [Cassandra](http://cassandra.apache.org/) (opens CQL shell, bundled with [nagios-plugins](https://github.com/harisekhon/nagios-plugins))
- [Consul](https://www.consul.io/)
- [H2O](https://www.h2o.ai/) by 0xdata
- [Hadoop](http://hadoop.apache.org/) (HDFS + Yarn)
- [HBase](https://hbase.apache.org/) (opens HBase shell)
- [Jython](http://www.jython.org/)
- [Kafka](https://kafka.apache.org/)
- [Mesos](http://mesos.apache.org/)
- [Nifi](https://nifi.apache.org/)
- [RabbitMQ](https://www.rabbitmq.com/) Cluster (supports all same env vars as RabbitMQ official base, plus ones for enabling plugins)
- [Riak KV](http://basho.com/products/riak-kv/)
- [Riak KV](http://basho.com/products/riak-kv/) (bundled with [nagios-plugins](https://github.com/harisekhon/nagios-plugins))
- [Serf](https://www.serf.io/)
- [Solr](http://lucene.apache.org/solr/)
- [SolrCloud](http://lucene.apache.org/solr/)
- [Spark](https://spark.apache.org/) (opens Spark shell)
- [Superset](http://airbnb.io/projects/superset/) by Airbnb
- [Tachyon](http://www.alluxio.org/) (Alluxio < 1.0)
- [ZooKeeper](https://zookeeper.apache.org/) (opens ZK shell)

Repos suffixed with ```-dev``` are the official technologies + development & debugging tools + my github repos with all dependencies pre-built.

##### My GitHub Repos (with all libs + deps pre-built):

- [Advanced Nagios Plugins Collection](https://github.com/harisekhon/nagios-plugins) - 350+ nagios plugins for every Hadoop distribution and every major NoSQL technology - Hadoop, Redis, Elasticsearch, Solr, HBase, Cassandra & DataStax OpsCenter, MongoDB, MySQL, Kafka, Riak, Memcached, Couchbase, Mesos, Spark, Neo4j, Datameer, H2O, WanDisco, Yarn, HDFS, Impala, Apache Drill, ZooKeeper, Cloudera, Hortonworks, MapR, IBM BigInsights, Infrastructure - Linux, DNS, Whois, SSL Certs etc
  - Tags:
    - nagios-plugins:latest (alpine)
    - nagios-plugins:centos
    - nagios-plugins:debian
    - nagios-plugins:ubuntu
- [Python Tools](https://github.com/harisekhon/pytools) - ~ 50 tools for Hadoop, Spark, Pig, Ambari Blueprints, AWS CloudFormation, Linux, Data Converters & Validators (Avro/Parquet/JSON/CSV/XML/YAML), Elasticsearch, Solr, IPython - CLI tools
- [Perl Tools](https://github.com/harisekhon/tools) - 25+ tools for Hadoop, Hive, Solr, Linux, SQL, Ambari, Datameer, Web and various Linux CLI Tools
- [Spotify Tools](https://github.com/harisekhon/spotify-tools) - Backup & Play Automation: Spotify Lookup - converts Spotify URIs to 'Artist - Track' form by querying the Spotify Metadata API. Spotify Cmd - command line control of Spotify on Mac via AppleScript for automation, auto timed track flick through etc.

- CentOS + all Github repos pre-built
- Debian + all Github repos pre-built
- Ubuntu + all Github repos pre-built
- Alpine + all Github repos pre-built

##### Base Images:

Dev images:

- CentOS latest with Java JDK, Perl, Python, Jython, Ruby, Scala, Groovy, GCC, Maven, SBT, Gradle, Make, Expect, EPEL etc.
- Debian latest with Java JDK, Perl, Python, Jython, Ruby, Scala, Groovy, GCC, Maven, SBT, Gradle, Make, Expect etc.
- Ubuntu latest with Java JDK, Perl, Python, Jython, Ruby, Scala, Groovy, GCC, Maven, SBT, Gradle, Make, Expect etc.
- Alpine latest with Java JDK, Perl, Python, Jython, Ruby, Scala, Groovy, GCC, Maven, SBT, Gradle, Make, Expect etc.

###### Base Images of Java / Scala:

All builds use OpenJDK with ```jre``` and ```jdk``` numbered tags. See this article below for why it might be illegal to bundle Oracle Java (and why no Linux distributions do this either):

https://www.javacodegeeks.com/2016/03/running-java-docker-youre-breaking-law.html

- CentOS latest combinations of Java 7/8 and Scala 2.10/2.11
- Debian latest with Java 7
- Ubuntu 14.04 with Java 7
- Ubuntu latest with Java 8, 9

### Build from Source

```
git clone https://github/harisekhon/Dockerfiles

cd Dockerfiles
```

To build all Docker images, just run the ```make``` command at the top level:

```
make
```

To build a specific Docker image, enter it's directory and run make:

```
cd nagios-plugins

make
```

### Support

Please raise tickets for issues and improvements at https://github.com/harisekhon/dockerfiles/issues
[Superset](http://airbnb.io/projects/superset/) by Airbnb
=========================================================

Superset open source analytics UI by Airbnb.

Starts the web UI on port `8088`.

Username and password are both set to `admin` by default.

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
CentOS Development Build
========================

CentOS Linux latest version + Development Tools including:

* Java JDK
* Perl
* Python
* Jython
* Ruby
* Scala
* Groovy
* GCC
* Maven
* SBT
* Gradle
* Make
* Expect
* EPEL repository
* Git
* various Linux development and systems administration tools, eg. ViM enhanced, system and network tools, curl, wget, MySQL dev libraries etc.

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
[Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins)
==================================
[![Build Status](https://travis-ci.org/HariSekhon/nagios-plugins.svg?branch=master)](https://travis-ci.org/HariSekhon/nagios-plugins) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/e6fcf7cb4dcc4905ab0a4cb91567fdda)](https://www.codacy.com/app/harisekhon/nagios-plugins) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/network) [![Dependency Status](https://gemnasium.com/badges/github.com/HariSekhon/nagios-plugins.svg)](https://gemnasium.com/github.com/HariSekhon/nagios-plugins) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/nagios-plugins#advanced-nagios-plugins-collection) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins/) [![](https://images.microbadger.com/badges/image/harisekhon/nagios-plugins.svg)](http://microbadger.com/#/images/harisekhon/nagios-plugins)

Docker image containing the [Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins) - the largest, most advanced collection of production-grade Nagios monitoring code (over 350 programs).

<!-- the DockerHub page for harisekhon/nagios-plugins will show the README of the latest build so make all of them generic enough -->
This docker image contains all dependencies pre-built on Alpine, CentOS, Debian and Ubuntu latest docker base images and are tagged as `:latest`, `:centos`, `:debian` and `:ubuntu` respectively. The source for each OS build is available in adjacent directories in the [Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).

Specialised plugins for Hadoop, Big Data & NoSQL technologies, written by a former Clouderan ([Cloudera](http://www.cloudera.com) was the first Hadoop Big Data vendor) and current [Hortonworks](http://www.hortonworks.com) consultant.

Hadoop and extensive API integration with all major Hadoop vendors ([Hortonworks](http://www.hortonworks.com), [Cloudera](http://www.cloudera.com), [MapR](http://www.mapr.com), [IBM BigInsights](http://www-03.ibm.com/software/products/en/ibm-biginsights-for-apache-hadoop)), as well as most major open source NoSQL technologies, Pub-Sub / Message Buses, CI, Web and Linux based infrastructure.

Most enterprise monitoring systems come with basic generic checks, while this project extends their monitoring capabilities significantly further in to advanced infrastructure, application layer, APIs etc.

For more documentation on the Nagios Plugins contained in this image, see the [GitHub project page](https://github.com/HariSekhon/nagios-plugins#usage---help)

List all plugins:

```
docker run harisekhon/nagios-plugins
```

Run any given plugin by suffixing it to the docker run command:

```
docker run harisekhon/nagios-plugins <program> <args>
```

eg.

```
docker run harisekhon/nagios-plugins check_ssl_cert.pl --help
```

Supports a a wide variety of [compatible Enterprise Monitoring servers](https://github.com/harisekhon/nagios-plugins#enterprise-monitoring-systems).

The project is a treasure trove of essentials for every single "DevOp" / sysadmin / engineer, with extensive goodies for people running:

* Web Infrastructure
* [Hadoop](http://hadoop.apache.org/)
* [Kafka](http://kafka.apache.org/)
* [RabbitMQ](http://www.rabbitmq.com/)
* [Mesos](http://mesos.apache.org/)
* [Consul](https://www.consul.io/)

NoSQL technologies:

* [Cassandra](http://cassandra.apache.org/)
* [HBase](https://hbase.apache.org/)
* [MongoDB](https://www.mongodb.com/)
* [Memcached](https://memcached.org/)
* [Couchbase](http://www.couchbase.com/)
* [Redis](http://redis.io/)
* [Riak](http://basho.com/products/)
* [Solr / SolrCloud](http://lucene.apache.org/solr/)
* [Elasticsearch](https://www.elastic.co/products/elasticsearch)

Linux & Infrastructure technologies:

* [Jenkins](https://jenkins.io/)
* [Travis CI](https://travis-ci.org/)
* SSL Certificate expiry
* advanced DNS record checks
* Whois domain expiry check
* [MySQL](https://www.mysql.com/)

and many more ...

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
[Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins)
==================================
[![Build Status](https://travis-ci.org/HariSekhon/nagios-plugins.svg?branch=master)](https://travis-ci.org/HariSekhon/nagios-plugins) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/e6fcf7cb4dcc4905ab0a4cb91567fdda)](https://www.codacy.com/app/harisekhon/nagios-plugins) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/network) [![Dependency Status](https://gemnasium.com/badges/github.com/HariSekhon/nagios-plugins.svg)](https://gemnasium.com/github.com/HariSekhon/nagios-plugins) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/nagios-plugins#advanced-nagios-plugins-collection) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins/) [![](https://images.microbadger.com/badges/image/harisekhon/nagios-plugins.svg)](http://microbadger.com/#/images/harisekhon/nagios-plugins)

Docker image containing the [Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins) - the largest, most advanced collection of production-grade Nagios monitoring code (over 350 programs).

<!-- the DockerHub page for harisekhon/nagios-plugins will show the README of the latest build so make all of them generic enough -->
This docker image contains all dependencies pre-built on Alpine, CentOS, Debian and Ubuntu latest docker base images and are tagged as `:latest`, `:centos`, `:debian` and `:ubuntu` respectively. The source for each OS build is available in adjacent directories in the [Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).

Specialised plugins for Hadoop, Big Data & NoSQL technologies, written by a former Clouderan ([Cloudera](http://www.cloudera.com) was the first Hadoop Big Data vendor) and current [Hortonworks](http://www.hortonworks.com) consultant.

Hadoop and extensive API integration with all major Hadoop vendors ([Hortonworks](http://www.hortonworks.com), [Cloudera](http://www.cloudera.com), [MapR](http://www.mapr.com), [IBM BigInsights](http://www-03.ibm.com/software/products/en/ibm-biginsights-for-apache-hadoop)), as well as most major open source NoSQL technologies, Pub-Sub / Message Buses, CI, Web and Linux based infrastructure.

Most enterprise monitoring systems come with basic generic checks, while this project extends their monitoring capabilities significantly further in to advanced infrastructure, application layer, APIs etc.

For more documentation on the Nagios Plugins contained in this image, see the [GitHub project page](https://github.com/HariSekhon/nagios-plugins#usage---help)

List all plugins:

```
docker run harisekhon/nagios-plugins
```

Run any given plugin by suffixing it to the docker run command:

```
docker run harisekhon/nagios-plugins <program> <args>
```

eg.

```
docker run harisekhon/nagios-plugins check_ssl_cert.pl --help
```

Supports a a wide variety of [compatible Enterprise Monitoring servers](https://github.com/harisekhon/nagios-plugins#enterprise-monitoring-systems).

The project is a treasure trove of essentials for every single "DevOp" / sysadmin / engineer, with extensive goodies for people running:

* Web Infrastructure
* [Hadoop](http://hadoop.apache.org/)
* [Kafka](http://kafka.apache.org/)
* [RabbitMQ](http://www.rabbitmq.com/)
* [Mesos](http://mesos.apache.org/)
* [Consul](https://www.consul.io/)

NoSQL technologies:

* [Cassandra](http://cassandra.apache.org/)
* [HBase](https://hbase.apache.org/)
* [MongoDB](https://www.mongodb.com/)
* [Memcached](https://memcached.org/)
* [Couchbase](http://www.couchbase.com/)
* [Redis](http://redis.io/)
* [Riak](http://basho.com/products/)
* [Solr / SolrCloud](http://lucene.apache.org/solr/)
* [Elasticsearch](https://www.elastic.co/products/elasticsearch)

Linux & Infrastructure technologies:

* [Jenkins](https://jenkins.io/)
* [Travis CI](https://travis-ci.org/)
* SSL Certificate expiry
* advanced DNS record checks
* Whois domain expiry check
* [MySQL](https://www.mysql.com/)

and many more ...

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Ubuntu Development Build
========================

Ubuntu Linux latest version + Development Tools including:

* Java JDK
* Perl
* Python
* Jython
* Ruby
* Scala
* Groovy
* GCC
* Maven
* SBT
* Gradle
* Make
* Expect
* Git
* various Linux development and systems administration tools, eg. ViM enhanced, system and network tools, curl, wget, MySQL dev libraries etc.

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
[Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins)
==================================
[![Build Status](https://travis-ci.org/HariSekhon/nagios-plugins.svg?branch=master)](https://travis-ci.org/HariSekhon/nagios-plugins) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/e6fcf7cb4dcc4905ab0a4cb91567fdda)](https://www.codacy.com/app/harisekhon/nagios-plugins) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/network) [![Dependency Status](https://gemnasium.com/badges/github.com/HariSekhon/nagios-plugins.svg)](https://gemnasium.com/github.com/HariSekhon/nagios-plugins) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/nagios-plugins#advanced-nagios-plugins-collection) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins/) [![](https://images.microbadger.com/badges/image/harisekhon/nagios-plugins.svg)](http://microbadger.com/#/images/harisekhon/nagios-plugins)

Docker image containing the [Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins) - the largest, most advanced collection of production-grade Nagios monitoring code (over 350 programs).

<!-- the DockerHub page for harisekhon/nagios-plugins will show the README of the latest build so make all of them generic enough -->
This docker image contains all dependencies pre-built on Alpine, CentOS, Debian and Ubuntu latest docker base images and are tagged as `:latest`, `:centos`, `:debian` and `:ubuntu` respectively. The source for each OS build is available in adjacent directories in the [Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).

Specialised plugins for Hadoop, Big Data & NoSQL technologies, written by a former Clouderan ([Cloudera](http://www.cloudera.com) was the first Hadoop Big Data vendor) and current [Hortonworks](http://www.hortonworks.com) consultant.

Hadoop and extensive API integration with all major Hadoop vendors ([Hortonworks](http://www.hortonworks.com), [Cloudera](http://www.cloudera.com), [MapR](http://www.mapr.com), [IBM BigInsights](http://www-03.ibm.com/software/products/en/ibm-biginsights-for-apache-hadoop)), as well as most major open source NoSQL technologies, Pub-Sub / Message Buses, CI, Web and Linux based infrastructure.

Most enterprise monitoring systems come with basic generic checks, while this project extends their monitoring capabilities significantly further in to advanced infrastructure, application layer, APIs etc.

For more documentation on the Nagios Plugins contained in this image, see the [GitHub project page](https://github.com/HariSekhon/nagios-plugins#usage---help)

List all plugins:

```
docker run harisekhon/nagios-plugins
```

Run any given plugin by suffixing it to the docker run command:

```
docker run harisekhon/nagios-plugins <program> <args>
```

eg.

```
docker run harisekhon/nagios-plugins check_ssl_cert.pl --help
```

Supports a a wide variety of [compatible Enterprise Monitoring servers](https://github.com/harisekhon/nagios-plugins#enterprise-monitoring-systems).

The project is a treasure trove of essentials for every single "DevOp" / sysadmin / engineer, with extensive goodies for people running:

* Web Infrastructure
* [Hadoop](http://hadoop.apache.org/)
* [Kafka](http://kafka.apache.org/)
* [RabbitMQ](http://www.rabbitmq.com/)
* [Mesos](http://mesos.apache.org/)
* [Consul](https://www.consul.io/)

NoSQL technologies:

* [Cassandra](http://cassandra.apache.org/)
* [HBase](https://hbase.apache.org/)
* [MongoDB](https://www.mongodb.com/)
* [Memcached](https://memcached.org/)
* [Couchbase](http://www.couchbase.com/)
* [Redis](http://redis.io/)
* [Riak](http://basho.com/products/)
* [Solr / SolrCloud](http://lucene.apache.org/solr/)
* [Elasticsearch](https://www.elastic.co/products/elasticsearch)

Linux & Infrastructure technologies:

* [Jenkins](https://jenkins.io/)
* [Travis CI](https://travis-ci.org/)
* SSL Certificate expiry
* advanced DNS record checks
* Whois domain expiry check
* [MySQL](https://www.mysql.com/)

and many more ...

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Alluxio
=======

http://www.alluxio.org/

In-memory filesystem, formerly Tachyon (older versions are found under harisekhon/tachyon and the tachyon directory adjacent in this repo)

Starts one master and one worker and then tails the logs, maps the master and worker ports 19999 and 30000.

```
docker-compose up
```

or without `docker-compose`

```
make run
```

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Apache Drill
============

https://drill.apache.org/

Schema-free SQL Query Engine for Hadoop, NoSQL and Cloud Storage

Starts a single Apache Drillbit and maps port 8047.

If started interactively will drop in to a SQL shell.

```
docker-compose up
```

or without `docker-compose`

```
make run
```

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
[Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins)
==================================
[![Build Status](https://travis-ci.org/HariSekhon/nagios-plugins.svg?branch=master)](https://travis-ci.org/HariSekhon/nagios-plugins) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/e6fcf7cb4dcc4905ab0a4cb91567fdda)](https://www.codacy.com/app/harisekhon/nagios-plugins) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/nagios-plugins.svg)](https://github.com/harisekhon/nagios-plugins/network) [![Dependency Status](https://gemnasium.com/badges/github.com/HariSekhon/nagios-plugins.svg)](https://gemnasium.com/github.com/HariSekhon/nagios-plugins) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/nagios-plugins#advanced-nagios-plugins-collection) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins/) [![](https://images.microbadger.com/badges/image/harisekhon/nagios-plugins.svg)](http://microbadger.com/#/images/harisekhon/nagios-plugins)

Docker image containing the [Advanced Nagios Plugins Collection](https://github.com/HariSekhon/nagios-plugins) - the largest, most advanced collection of production-grade Nagios monitoring code (over 350 programs).

<!-- the DockerHub page for harisekhon/nagios-plugins will show the README of the latest build so make all of them generic enough -->
This docker image contains all dependencies pre-built on Alpine, CentOS, Debian and Ubuntu latest docker base images and are tagged as `:latest`, `:centos`, `:debian` and `:ubuntu` respectively. The source for each OS build is available in adjacent directories in the [Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).

Specialised plugins for Hadoop, Big Data & NoSQL technologies, written by a former Clouderan ([Cloudera](http://www.cloudera.com) was the first Hadoop Big Data vendor) and current [Hortonworks](http://www.hortonworks.com) consultant.

Hadoop and extensive API integration with all major Hadoop vendors ([Hortonworks](http://www.hortonworks.com), [Cloudera](http://www.cloudera.com), [MapR](http://www.mapr.com), [IBM BigInsights](http://www-03.ibm.com/software/products/en/ibm-biginsights-for-apache-hadoop)), as well as most major open source NoSQL technologies, Pub-Sub / Message Buses, CI, Web and Linux based infrastructure.

Most enterprise monitoring systems come with basic generic checks, while this project extends their monitoring capabilities significantly further in to advanced infrastructure, application layer, APIs etc.

For more documentation on the Nagios Plugins contained in this image, see the [GitHub project page](https://github.com/HariSekhon/nagios-plugins#usage---help)

List all plugins:

```
docker run harisekhon/nagios-plugins
```

Run any given plugin by suffixing it to the docker run command:

```
docker run harisekhon/nagios-plugins <program> <args>
```

eg.

```
docker run harisekhon/nagios-plugins check_ssl_cert.pl --help
```

Supports a a wide variety of [compatible Enterprise Monitoring servers](https://github.com/harisekhon/nagios-plugins#enterprise-monitoring-systems).

The project is a treasure trove of essentials for every single "DevOp" / sysadmin / engineer, with extensive goodies for people running:

* Web Infrastructure
* [Hadoop](http://hadoop.apache.org/)
* [Kafka](http://kafka.apache.org/)
* [RabbitMQ](http://www.rabbitmq.com/)
* [Mesos](http://mesos.apache.org/)
* [Consul](https://www.consul.io/)

NoSQL technologies:

* [Cassandra](http://cassandra.apache.org/)
* [HBase](https://hbase.apache.org/)
* [MongoDB](https://www.mongodb.com/)
* [Memcached](https://memcached.org/)
* [Couchbase](http://www.couchbase.com/)
* [Redis](http://redis.io/)
* [Riak](http://basho.com/products/)
* [Solr / SolrCloud](http://lucene.apache.org/solr/)
* [Elasticsearch](https://www.elastic.co/products/elasticsearch)

Linux & Infrastructure technologies:

* [Jenkins](https://jenkins.io/)
* [Travis CI](https://travis-ci.org/)
* SSL Certificate expiry
* advanced DNS record checks
* Whois domain expiry check
* [MySQL](https://www.mysql.com/)

and many more ...

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Jython
======

http://www.jython.org/

Jython environment built on minimal OpenJDK Alpine build.

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
[Kafka Scala API Tester - Advanced Nagios Plugin / CLI Tool with Kerberos support](https://github.com/HariSekhon/nagios-plugin-kafka)
==================================
[![Build Status](https://travis-ci.org/HariSekhon/nagios-plugin-kafka.svg?branch=master)](https://travis-ci.org/HariSekhon/nagios-plugin-kafka) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/de500bf4f90d401ba5c98ed903c8a612)](https://www.codacy.com/app/harisekhon/nagios-plugin-kafka) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/nagios-plugin-kafka) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins/) [![](https://images.microbadger.com/badges/image/harisekhon/nagios-plugin-kafka.svg)](http://microbadger.com/#/images/harisekhon/nagios-plugin-kafka)

Docker image containing the [Kafka Scala API Tester - Advanced Nagios Plugin / CLI Tool with Kerberos support](https://github.com/HariSekhon/nagios-plugin-kafka)

This docker image contains all dependencies pre-built on Alpine Linux.

See upstream project for more details on its usage [here](https://github.com/HariSekhon/nagios-plugin-kafka)

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Alpine Development Build
========================

Alpine Linux latest version + Development Tools including:

* Java JDK
* Perl
* Python
* Jython
* Ruby
* Scala
* Groovy
* GCC
* Maven
* SBT
* Gradle
* Make
* Expect
* Git
* various Linux development and systems administration tools, eg. ViM, system and network tools, curl, wget, MariaDB dev libraries etc.

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Debian Development Build
========================

Debian Linux latest version + Development Tools including:

* Java JDK
* Perl
* Python
* Jython
* Ruby
* Scala
* Groovy
* GCC
* Maven
* SBT
* Gradle
* Make
* Expect
* Git
* various Linux development and systems administration tools, eg. ViM enhanced, system and network tools, curl, wget, MariaDB dev libraries etc.

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
Apache Ranger
=============

http://ranger.apache.org/

Hadoop security adminitration console for ACL management across components and auditing.

Starts a standalone Ranger admin console and maps port 6080.

```
docker-compose up
```

or without `docker-compose`

```
make run
```

See [master GitHub repo](https://github.com/HariSekhon/Dockerfiles/) top level for a tonne of related technologies.
[Hari Sekhon PyTools](https://github.com/harisekhon/pytools)
===================
[![Build Status](https://travis-ci.org/HariSekhon/pytools.svg?branch=master)](https://travis-ci.org/HariSekhon/pytools) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/f7af72140c3b408b9659207ced17544f)](https://www.codacy.com/app/harisekhon/pytools) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/pytools.svg)](https://github.com/harisekhon/pytools/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/pytools.svg)](https://github.com/harisekhon/pytools/network) [![Dependency Status](https://gemnasium.com/badges/github.com/HariSekhon/pytools.svg)](https://gemnasium.com/github.com/HariSekhon/pytools) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/pytools#hari-sekhon-pytools) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/pytools/) [![](https://images.microbadger.com/badges/image/harisekhon/pytools.svg)](http://microbadger.com/#/images/harisekhon/pytools)

### Hadoop, Spark / PySpark, HBase, Pig, Ambari, IPython and Linux Tools ###

A few of the Hadoop, Spark & Linux tools I've written over the years.

See a list of included programs with descriptions on the [GitHub project page](https://github.com/harisekhon/pytools#pytools).

Running the docker container without arguments will show just the list of programs:

```
docker run harisekhon/pytools
```

Run any given program:

```
docker run harisekhon/pytools <program> <args>
```

All programs have `--help` to list the available options.

For many more tools see the [Tools](https://github.com/harisekhon/tools) and [Advanced Nagios Plugins Collection](https://github.com/harisekhon/nagios-plugins) repos which contains many Hadoop, NoSQL, Web and infrastructure tools and Nagios plugins which have docker builds in the adjacent directories.

Hari Sekhon

Big Data Contractor, United Kingdom

https://www.linkedin.com/in/harisekhon

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
[Hari Sekhon Tools](https://github.com/harisekhon/tools)
=================
[![Build Status](https://travis-ci.org/HariSekhon/tools.svg?branch=master)](https://travis-ci.org/HariSekhon/tools) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/1769cc854b5246968ee2bae1818f771a)](https://www.codacy.com/app/harisekhon/tools) [![GitHub stars](https://img.shields.io/github/stars/harisekhon/tools.svg)](https://github.com/harisekhon/tools/stargazers) [![GitHub forks](https://img.shields.io/github/forks/harisekhon/tools.svg)](https://github.com/harisekhon/tools/network) [![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20OS%20X-blue.svg)](https://github.com/harisekhon/tools#hari-sekhon-tools) [![DockerHub](https://img.shields.io/badge/docker-available-blue.svg)](https://hub.docker.com/r/harisekhon/tools/) [![](https://images.microbadger.com/badges/image/harisekhon/tools.svg)](http://microbadger.com/#/images/harisekhon/tools)

### Hadoop, Hive, Solr, NoSQL, Web, Linux Tools ###

A few of the Hadoop, NoSQL, Web & Linux tools I've written over the years. All programs have --help to list the available options.

See a list of included programs with descriptions on the [GitHub project page](https://github.com/harisekhon/tools#a-sample-of-cool-programs-in-this-toolbox).

Running the docker container without arguments will show just the list of programs:

```
docker run harisekhon/tools
```

Run any given program:

```
docker run harisekhon/tools <program> <args>
```

All programs have `--help` to list the available options.

For many more tools see [PyTools](https://github.com/harisekhon/pytools) and the [Advanced Nagios Plugins Collection](https://github.com/harisekhon/nagios-plugins) which contains many more Hadoop, NoSQL and Linux/Web tools.

Hari Sekhon

Big Data Contractor, United Kingdom

https://www.linkedin.com/in/harisekhon

Related Docker images can be found for many Open Source, Big Data and NoSQL technologies on [my DockerHub profile](https://hub.docker.com/r/harisekhon). The source for them all can be found in the [master Dockerfiles GitHub repo](https://github.com/HariSekhon/Dockerfiles/).
pathfinder_graph [![Phovea][phovea-image]][phovea-url] [![NPM version][npm-image]][npm-url] [![Build Status][travis-image]][travis-url] [![Dependency Status][daviddm-image]][daviddm-url]
=====================

Pathfinder python server plugin for communicating with [Neo4j](https://neo4j.com/).

Installation
------------

```
git clone https://github.com/phovea/pathfinder_graph.git
cd pathfinder_graph
npm install
```

Testing
-------

```
npm test
```

Building
--------

```
npm run build
```



***

<a href="https://caleydo.org"><img src="http://caleydo.org/assets/images/logos/caleydo.svg" align="left" width="200px" hspace="10" vspace="6"></a>
This repository is part of **[Phovea](http://phovea.caleydo.org/)**, a platform for developing web-based visualization applications. For tutorials, API docs, and more information about the build and deployment process, see the [documentation page](http://phovea.caleydo.org).


[phovea-image]: https://img.shields.io/badge/Phovea-Client%20Plugin-F47D20.svg
[phovea-url]: https://phovea.caleydo.org
[npm-image]: https://badge.fury.io/js/pathfinder_graph.svg
[npm-url]: https://npmjs.org/package/pathfinder_graph
[travis-image]: https://travis-ci.org/phovea/pathfinder_graph.svg?branch=master
[travis-url]: https://travis-ci.org/phovea/pathfinder_graph
[daviddm-image]: https://david-dm.org/phovea/pathfinder_graph/status.svg
[daviddm-url]: https://david-dm.org/phovea/pathfinder_graph
### Requirements ###
docker, docker-compose

### How to build docker image ###
```
$ docker build -t "package-name:latest"
```

### How to start the cluster ###
```
$ docker-compose up --force-recreate
```

For more docker images visit http://hub.docker.com

use build_images.sh to build all necessary images 

I Producer(KafkaProducer) used to simulate sending random generated json data which contains 4 fields( id , revenue , revenue_counter , timestamp)

II Spark procesing over kafka producer's data(saving incoming data from producer to redis(via spark) because some of the records may be updated )
	- from spark app is crated a new kafka producer which handles the computated data
	- how to run spark in standalone mode ( not distributed with Hadoop(hdfs)) :
			
			1.First create the datastore container so all the other container can use the datastore container's data volume with createDatastoreContainer.sh docker volumes can be checked with -> docker volume ls , after running the script for datastore type the command and the volume for the datastore should be the first one from top to bottom.

			2.Create spark master container with createSparkMasterContainer.sh script

			3.After second step with spark master then create spark slave container with createSparkSlaveContainer.sh script
			(can create as many workers as we want , link option allows the container automatically connect to the other (master in this case) by being added to the same network.)
				- spark workers can be scaled with the following command docker-compose scale slave = $1 ( whatever number do we need)

			4.Running a spark code using spark-submit
			Another container is created to work as the driver and call the spark cluster. The container only runs while the spark job is running, as soon as it finishes the container is deleted.
			The spark python code should be moved to the shared volume created by the datastore container. Since we did not specify a host volume (when we manually define where in the host machine the container is mapped) docker creates it in the default volume location located on /var/lib/volumes/<container hash>/_data

			5.Run the spark submit container
			docker run --rm -it --link master:master --volumes-from spark-datastore spark-submit:latest spark-submit --master spark://172.17.0.2:7077 /data/sparkContextExample.py
			Or the script spark_submit.sh can be used while passing a spark code .py file as argument

			* check the addres of spark master , last number can be different from 2 -> check on localhost:8080 where spark master runs

	*** sparkContextExample 
	There are two ways of creating spark stream with kafka
	- using KafkaUtils.createStream which takes data from the moment that producer started work.For example if the prodecer started work at 10:00am and the spark app is submited at 02:00pm 'createStream' will take all the data from 10:00 to 02:00pm and will keep working in real time until is stopped.
	- using KafkaUtils.createDirectStream whiche take data from the moment that the spark app is submited.

III Consumer(KafkaConsumer) used to receive data from the producer in spark app
# dockerfiles
Dockerfile collection

## License
[MIT](https://github.com/asakaguchi/dockerfiles/blob/master/LICENSE)
# magellan-cli/Dockerfile

This dockerfile is the magellan-cli in a container.

[![MIT](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)]()


## Usage

# alpine-anaconda3 / Dockerfile

This Dockerfile builds a Python environment using [Anaconda 3](https://www.continuum.io/) on Alpine Linux.

It is made on the premise of using with Jupyter Notebook. I also make IRuby usable. For IRuby related, the following package is also included.

* pry
* pry-doc
* awesome_print
* gnuplot
* rubyvis
* nyaplot

## Requirements

* [Docker](https://www.docker.com) 1.12.1+

## Usage

To create a Docker image, use the following command.

```text
docker build -t my-anaconda3 .
```

The following command is an example of starting Jupyter Notebook.

```text
docker run -it --rm -v $(pwd):/opt/notebooks -p 8888:8888 my-anaconda3 jupyter notebook --notebook-dir=/opt/notebooks --ip="0.0.0.0" --port=8888 --no-browser
```

When Jupyter Notebook starts up, the following message will be displayed.

```text
The Jupyter Notebook is running at: http://0.0.0.0:8888/?token=************************************************
```

Opening `http://0.0.0.0:8888/?token=************************************************` in your web browser.

## License

This is licensed under the [MIT](https://github.com/asakaguchi/dockerfiles/blob/master/LICENSE) license.# alpine-anaconda3 / Dockerfile

この Dockerfile は、Alpine Linux 上に [Anaconda3](https://www.continuum.io/) を使って Python 環境を構築します。

Jupyter Notebook での利用を前提とした作りになっています。また、IRuby も使えるようにしています。IRuby 関連では、以下のパッケージを同梱しています。

* pry
* pry-doc
* awesome_print
* gnuplot
* rubyvis
* nyaplot

## Requirements

* [Docker](https://www.docker.com) 1.12.1+

## Usage

Docker イメージを作るには、次のコマンドを使用します。

```text
docker build -t my-anaconda3 .
```

以下のコマンドは、Jupyter Notebook の起動例です。

```text
docker run -it --rm -v $(pwd):/opt/notebooks -p 8888:8888 my-anaconda3 jupyter notebook --notebook-dir=/opt/notebooks --ip="0.0.0.0" --port=8888 --no-browser
```

Jupyter Notebook が起動すると、以下のようなメッセージが表示されます。

```text
The Jupyter Notebook is running at: http://0.0.0.0:8888/?token=************************************************
```

このメッセージで示された `http://0.0.0.0:8888/?token=************************************************` をウェブブラウザーでオープンします。

## License

This is licensed under the [MIT](https://github.com/asakaguchi/dockerfiles/blob/master/LICENSE) license.# alpine-ngx_mruby/Dockerfile
This dockerfile is the ngx_mruby in a container.

## Usage example
build:

```
docker build -t ngx_mruby .
```

run:

```
echo 'hello, world' > index.html
docker run -d -p 8080:80 -v $(pwd):/usr/share/nginx/html ngx_mruby
curl http://$(docker-machine ip default):8080
```
# alpine-node/Dockerfile
This dockerfile is the Node.js in a container.

## Usage example
```
docker build -t alpine-node:6.2.1 .
```
# alpine-textlint / Dockerfile

この Dockerfile は、[textlint](https://github.com/textlint/textlint) の導入を簡単にする Docker イメージを作成するためのファイルです。作成する Docker イメージのベースイメージには、Alpine Linux を使用しています。

textlint 自身は、文章をどのようにチェックするかのルールを持っていません。この Dockerfile では、以下の文章チェックルールをインストールしています。

* [textlint\-rule\-no\-todo](https://github.com/azu/textlint-rule-no-todo)
* [textlint\-rule\-preset\-ja\-technical\-writing](https://github.com/textlint-ja/textlint-rule-preset-ja-technical-writing)
* [textlint\-rule\-ja\-yahoo\-kousei](https://www.npmjs.com/package/textlint-rule-ja-yahoo-kousei)
* [textlint\-rule\-spellchecker](https://github.com/nodaguti/textlint-rule-spellchecker)
* [textlint\-rule\-spacing](https://github.com/textlint-ja/textlint-rule-spacing)

このうち、`no-todo` ルールと `preset-ja-technical-writing` ルールをすぐ使えるようにしています。これは、`/textlint/.textlintrc` 設定ファイルで次のように定義しています。

```
{
  "rules": {
    "no-todo": true,
    "preset-ja-technical-writing": true
  }
}
```

## Requirements

* [Docker](https://www.docker.com) 1.12.1+

## Usage

自身で Docker イメージを作成して使用する場合は、`docker build` コマンドを使います。

```
docker build -t asakaguchi/textlint .
```

もし Docker イメージを作らずに、既製の Docker イメージを使用する場合は、`docker pull` コマンドを使います。

```
docker pull asakaguchi/textlint
```

この textlint Docker イメージを使用して文章をチェックするには、`docker run --rm` コマンドを使います。

以下は、カレントディレクトリの `foo.txt` というファイルに保存された文章をチェックする例です。`foo.txt` ファイルのパスが `/data/foo.txt` となっていることに注意してください。

```
docker run --rm -v $(pwd):/data asakaguchi/textlint /data/foo.txt
```

`-v $(pwd):/data` により Docker イメージ化した textlint から `/data` のパスで、カレントディレクトリのファイルを参照できるようにしています。

HTML ファイル中の文章をチェックしたい場合は、`--plugin html` を指定します。

```
docker run --rm -v $(pwd):/data asakaguchi/textlint --plugin html /data/bar.html
```

スペルチェックのルールを追加して、文章をチェックしたい場合は、`--rule spellchecker` を指定します。

```
docker run --rm -v $(pwd):/data asakaguchi/textlint --rule spellchecker /data/baz.txt
```

デフォルトの設定ファイルをカスタマイズして文章をチェックしたい場合は、以下のように `.textlintrc` ファイルをダウンロードします。

```
docker run --rm --entrypoint sh asakaguchi/textlint -c "cat /textlint/.textlintrc" > .textlintrc
```

ダウンロードしたファイルを編集します。以下は `ja-yahoo-kousei` ルールを追加した例です（"*****" はアプリケーション ID）。

```
{
  "rules": {
    "no-todo": true,
    "preset-ja-technical-writing": true,
    "ja-yahoo-kousei": {
      "appID": "********************************************************"
    }
  }
}
```

`-w /data` を指定して、文章をチェックします。

```
docker run --rm -v $(pwd):/data -w /data asakaguchi/textlint foo.txt
```

ファイルの変更を監視して、変更されたファイルの文章をチェックしたい場合は、[chokidar-cli](https://www.npmjs.com/package/chokidar-cli) を使って `textlint` を実行します。この例では、カレントディレクトリ配下のテキストファイル（`/data/**/*.txt`）とマークダウンファイル（`/data/**/*.md`）を監視対象としています。

```
docker run --rm -v $(pwd):/data --entrypoint chokidar asakaguchi/textlint '/data/**/*.md' '/data/**/*.txt' -c 'textlint {path}'
```

## License
This is licensed under the [MIT](https://github.com/asakaguchi/dockerfiles/blob/master/LICENSE) license.# ubuntu-atom/Dockerfile
This dockerfile is the atom text editor in a container.

## Usage example
build and run:

```
docker build -t ubuntu-atom .
docker run -it -e DISPLAY=192.168.99.1:0 --rm ubuntu-atom bash
```

start atom:

```
cd /path/to/workspace
atom .
```
# alpine-nginx/Dockerfile
This dockerfile is the Nginx in a container.

## Usage example
```
docker build -t alpine-nginx .
echo 'hello, world' > index.html
docker run -d -p 8080:80 -v $(pwd):/usr/share/nginx/html alpine-nginx
curl http://$(docker-machine ip default):8080/
```
#### Kubernetes based Jupyter Hub training enviroment for Scalable Machine Learning using Spark and Vora

A training environment based on Kubernetes that allows to launch applications within a few minutes. In this document we describe the usage in Google Cloud Environment as well as a bare metal Kubernetes cluster, but it is designed to work everywhere (e.g. AWS, Azure).

Currently we support the following :

  - Apache Spark
         
  - Apache Kafka (relevant for IoT scenarios and real-time machine learning applications)
         
  - HDFS
         
  - Machine Learning (TensorFlow, Spark ML, Sci-Kit Learn, R, …)
         
  - SAP Vora (all engines are supported incl. Vora vFlow;please note that we are utilizing a setup that
    was developed by the Vora team)
        
  - Advanced Scala (Scala is an ideal language for distributed data processing and a command
     of the language is beneficial to everyone who is actively working with Spark)
          
          

For the user interface,we are using Jupyter notebooks.The training environment has some nice features. After logging on with
an SAP account (using single-sign-on, we also support github based authentication), every user gets his/her individual Jupyter environment.

They are  three separate content folders:
 
 - Public Folder (read-only) - Containing training content and datasets
         
 - Personal Folder - Users have their own personal (and private) persistent hard drives .
         
 - Shared Folder - This is a shared drive to which everyone has read/write access to.It’s used for collaboration and 
    content sharing
       
       
Getting Started on Google container Engine 
 
  -  Clone the repository 
         
         git clone git@github.wdf.sap.corp:i033085/jupyterHub.git

  -  Set the Compute Zone for gcloud
         
         gcloud config set compute/zone us-west1-a

  -  Create a Container Engine Cluster with n nodes with kubernetes version 1.6.4 : 

         gcloud container clusters create jupyterhub --num-nodes=n --cluster-version=1.6.4

  -  Create a namespace for Kubernetes

         kubectl create namespace jupyterhub 

  -  For persistency we use an NFS based Storage.If you have an NFS server already running you can skip this step
      and proceed to step for provisioning the NFS Volumes
         
         -  Create Disk which acts as the underlying storage for NFS.This script creates a disk 
            on the google cloud,attaches the disk to a provisioner instance
            and mounts the disk in the configured path

               ./make-disk.sh jupyterhub-persistency 100GB

         -  Create the required folder structures to store Public,Shared and Personal Data

               ./prepare-dish.sh jupyterhub-persistency

         -  You can also copy your Data-Sets to this disk as below (Optional): 

                gcloud compute copy-files ~/LOCAL-FILE-1 ~/LOCAL-FILE-2 provisioner-01:/mnt/disks/RawData 
                gcloud compute copy-files ~/LOCAL-FILE-1 ~/LOCAL-FILE-2 provisioner-01:/mnt/disks/SharedData

         -  Un-Mount the disk and delete the provisioner instance :
                           
                ./unmount-disk.sh jupyterhub-persistency
                  
 -  Running the NFS Server
               
         -  Create the NFS Server. Make note of the cluster IP of the NFS Server 
                  kubectl create --namespace=jupyterhub -f nfsDisk/nfsServer.yaml
                 
 - Provision the NFS volumes
        
         -  For installation in the google cloud please use nfsDisk/nfsChart/gke_values.template 
            as reference and update the NFS server IP address from above.
          
         -  If you have followed the instructions above to run the NFS server 
            the simplest way would be to :
          
                  cp nfsDisk/nfsChart/gke_values.template nfsDisk/nfsChart/values.yaml
          
         -   After adjusting the helm-chart values
             run the helm chart for provisioning the NFS volumes as below:
                  
                           helm install nfsDisk/nfsChart --name=nfs --namespace=jupyterhub
                  
- Install Dynamic NFS Provisioner for Individual User Persistency
  
         -   Each user is assigned a persistent volume.We use Dynamic NFS provisioning for that .
             To install this,the simplest way would be to use nfs-client/nfsProvisioner/gke_values.yaml
             as reference and update the NFS server IP from above.
                  
                           cp nfs-client/nfsProvisioner/gke_values.yaml nfs-client/nfsProvisioner/values.yaml
                  
         -   Install the helm-chart as below :
                   
                   helm install nfs-client/nfsProvisioners --name=provisioner --namespace=jupyterhub
   
- Set up a Kubernetes based Spark Cluster
   
         -   ./start-spark.sh gke
              For detailed instructions on setting up a spark cluster refer to: 
              [spark](./spark/README.md)
      
- Launch the Jupyter Hub environment
         
         -   helm init
   
         -  We support Oauth based user authentication for Jupyterhub.Currently there is support
            for SAP single sign on using HCP oauth services as well as Github based authentication 
            or you can skip the authentication.
            
            Simplest way to get started here is using jupyterHub/helm-chart/gke_values.template as reference 
            and update the authentication mechanism.To make the set-up simpler
            we set the authentication as dummy and start the Hub server as below :
           
              helm install jupyterHub/helm-chart --name=jupyterhub --namespace=jupyterhub -f jupyterhub/config.yaml
         
                  
                  
         
                  
         
         
                  
         
         
         
         
     
# kubernetes-HDFS
Repository holding configuration files for running an HDFS cluster in Kubernetes
Helm charts for launching HDFS in a K8s cluster. They should be launched in
the following order.

  1. `hdfs-resolv-conf`: Creates a config map containing resolv.conf used by
     the HDFS daemons. See `hdfs-resolv-conf/README.md` for how to launch.
  2. `hdfs-namenode-k8s`: Launches the hdfs namenode. See
     `hdfs-namenode-k8s/README.md` for how to launch.
  3. `hdfs-datanode-k8s`: Launches the hdfs datanode daemons. See
     `hdfs-datanode-k8s/README.md` for how to launch.
ConfigMap entry storing the resolv.conf file that goes inside HDFS `namenode`
and `datanode` pods.

### Usage

  1. Find the service IP of your `kube-dns` of your k8s cluster.
     Try the following command and find the IP value in the output.
     It will be supplied below as the `clusterDnsIP` parameter.

  ```
  $ kubectl get svc --all-namespaces | grep kube-dns
  ```

  2. Find the domain name of your cluster that is part of
     cluster node host names. e.g. MYCOMPANY.COM in kube-n1.MYCOMPANY.COM.
     Default is "".  This will be supplied below as
     the `hostNetworkDomains` parameter.  You can find these from the `search`
     line in the following `kubectl run` output. `hostNetworkDomains` comes
     after the pod and service domain name such as `cluster.local`.

  ```
  $ kubectl run -i -t --rm busybox --image=busybox --restart=Never  \
      --command -- cat /etc/resolv.conf
  ...
  search default.svc.cluster.local svc.cluster.local cluster.local MYCOMPANY.COM
  ...
  ```

     See `values.yaml`
     for additional parameters to change.

  3. Launch this helm chart, `hdfs-resolv-conf`, while specifying
     the kube-dns name server IP and other parameters. (You can add multiple
     of them below in --set as comma-separated entries)

  ```
  $ helm install -n my-hdfs-resolv-conf \
      --set clusterDnsIP=MY-KUBE-DNS-IP,hostNetworkDomains=MYCOMPANY.COM  \
      hdfs-resolv-conf
  ```
HDFS `namenode` running inside a kubernetes cluster. See the other chart for
`datanodes`.

### Prerequisite

  Requires Kubernetes version 1.5 and beyond, because `namenode` is using
  `StatefulSet`, which is available only in version 1.5 and later.

  Make sure the `hdfs-resolv-conf` chart is launched.

### Usage

  1. Attach a label to one of your k8s cluster host that will run the `namenode`
     daemon. (This is required as `namenode` currently mounts a local disk
     `hostPath` volume. We will switch to persistent volume in the future, so
     we can skip this step.)

  ```
  $ kubectl label nodes YOUR-HOST hdfs-namenode-selector=hdfs-namenode-0
  ```

  2. Launch this helm chart, `hdfs-namenode-k8s`.

  ```
  $ helm install -n my-hdfs-namenode hdfs-namenode-k8s
  ```

  3. Confirm the daemon is launched.

  ```
  $ kubectl get pods | grep hdfs-namenode
  hdfs-namenode-0 1/1 Running   0 7m
  ```

There will be only one `namenode` instance. i.e. High Availability (HA) is not
supported at the moment. The `namenode` instance is supposed to be pinned to
a cluster host using a node label, as shown in the usage above. `Namenode`
mount a local disk directory using k8s `hostPath` volume.

`namenode` is using `hostNetwork` so it can see physical IPs of datanodes
without an overlay network such as weave-net mask them.

###Credits

This chart is using public Hadoop docker images hosted by
  [uhopper](https://hub.docker.com/u/uhopper/).
HDFS `datanodes` running inside a kubernetes cluster. See the other chart for
`namenode`.

### Prerequisite

  Requires Kubernetes version 1.5 and beyond, because `namenode` is using
  `StatefulSet`, which is available only in version 1.5 and later.

  Make sure the `hdfs-resolv-conf` chart is launched. Also ensure `namenode` is
  fully launched using the corresponding chart. `Datanodes` rely
  on DNS to resolve the hostname of the namenode when they start up.

### Usage

  1. Optionally, find the domain name of your k8s cluster that becomes part of
     pod and service host names. Default is `cluster.local`. See `values.yaml`
     for additional parameters to change. You can add them below in `--set`,
     as comma-separated entries.

  2. Launch this helm chart, `hdfs-datanode-k8s`.

  ```
  $ helm install -n my-hdfs-datanode hdfs-datanode-k8s
  ```

  3. Confirm the daemons are launched.

  ```
  $ kubectl get pods | grep hdfs-datanode-
  hdfs-datanode-ajdcz 1/1 Running 0 7m
  hdfs-datanode-f1w24 1/1 Running 0 7m
  ```

`Datanode` daemons run on every cluster node. They also mount k8s `hostPath`
local disk volumes.

`Datanodes` are using `hostNetwork` to register to `namenode` using
physical IPs.

Note they run under the `default` namespace.

###Credits

This chart is using public Hadoop docker images hosted by
  [uhopper](https://hub.docker.com/u/uhopper/).
# Spark

This is a Docker image appropriate for running Spark on Kuberenetes. It produces three main images:
* `spark-master` - Runs a Spark master in Standalone mode and exposes a port for Spark and a port for the WebUI.
* `spark-worker` - Runs a Spark worer in Standalone mode and connects to the Spark master via DNS name `spark-master`.
* `zeppelin` - Runs a Zeppelin web notebook and connects to the Spark master via DNS name `spark-master` and exposes a port for the WebUI.

In addition, there are two additional pushed images:
* `spark-base` - This base image for `spark-master` and `spark-worker` that starts nothing.
* `spark-driver` - This image, just like the `zeppelin` image, allows running things like `pyspark` to connect to `spark-master`, but is lighter weight than the `zeppelin` image.
# kubernetes nfs-client-provisioner
- pv provisioned as ${namespace}-${pvcName}-${pvName}
- pv recycled as archieved-${namespace}-${pvcName}-${pvName}

# deploy
- modify and deploy `deploy/deployment.yaml`
- modify and deploy `deploy/class.yaml`

# test
- `kubectl create -f deploy/test-claim.yaml`
- `kubectl create -f deploy/test-pod.yaml`
- check the folder and file "SUCCESS" created
- `kubectl delete -f deploy/test-pod.yaml`
- `kubectl delete -f deploy/test-claim.yaml`
- check the folder renamed to `archived-???`
# TF_Intro_Notebooks

This is a codelab material for TensorFlow introduction with data from Google BigQuery downloaded as CSV. 
Get started from the '0. Preparation.ipynb' notebook.

Install Anaconda for your OS: https://www.continuum.io/downloads.

Once Anaconda is installed, open a command prompt and test by running the following command: jupyter-notebook.

If successful, this command would open a browser window.
Close the browser, hit Ctrl+C on your command prompt and run the following command: pip install tensorflow.

If all goes well, you can clone this repository and run jupyter-notebook from the folder where you downloaded the repo.

This repository is built on the notebooks available from https://github.com/kazunori279/TensorFlow-Intro/
JupyterHub on Kubernetes for Data 8
=======

[![Build Status](https://travis-ci.org/data-8/jupyterhub-k8s.svg?branch=master)](https://travis-ci.org/data-8/jupyterhub-k8s)

This repo contains the Kubernetes config, container images, and docs for Data
8's deployment of JupyterHub on Kubernetes.

Getting Started
-------

### Google Cloud Engine ###

Log into the gcloud console at [console.cloud.google.com](console.cloud.google.com/)

Create a cluster. Go to `Container Engine` > `Container clusters` > `+ Create Cluster`. Fill out the required information and make sure you know how many instances you will need and what the memory and cpu requirements will be.

Go back to your dashboard in the GCP console. Click `Activate Google Cloud Shell` in the upper right-hand corner. It is an icon that looks like a small terminal.

In the new terminal window, clone the jupyterhub-k8s repository.

```
git clone https://github.com/data-8/jupyterhub-k8s
```

Set the zone.
```
gcloud config set compute/zone <your zone>
```

Get credentials for your cluster
```
gcloud container clusters get-credentials <your cluster>
```

Edit the docker-settings.json file. Set the docker repo name corresponding to your cloud provider. Set the image types. You can leave this blank if you are only using the base image. Set the context prefix to whatever you want.

Here is an example:
```
{
    "clusters": ["dev", "prod"],
    "buildSettings": {
        "dockerRepo": "gcr.io/<your project>",
        "imageTypes": ["datahub", "prob140", "stat28"],
    },
    "gcloud": {
        "project": "<your project>",
        "zone": "<your zone>"
    }
}
```

Run the build script to generate Docker images.
```
./build.bash [ hub | proxy | base | user {user_type} ]
```

`hub` is for the jupyterhub image and `proxy` is for the jupyterhub proxy image. You will find their Dockerfiles in the respective subdirectories. The singleuser server builds utilize a shared base image specified by user/Dockerfile.base. Various other singleuser server images are built from this specified by user/Dockerfile.{user_type}. For example to build the singleuser image for the course Stat 28, you would run `./build.bash base` at least once, then `./build.bash user stat28`.

Each docker image is tagged with the git commit hash corresponding with the last git revision of the build files. When the build completes, the script will output the name of the tagged docker image along with the suggestion to run `populate.bash`. The populate step preseeds the docker images onto cluster nodes.

Edit the `helm-chart/values.yaml` file where it says `# Must be overridden`. Set the image tags to the tags of the docker images you just built using `./build.bash`. Also make sure to set the correct docker images. You may also adjust some of the other settings in the `values.yaml` file if necessary.

Install [helm](https://github.com/kubernetes/helm/blob/master/docs/install.md).

Run helm.
```
helm init

helm --kube-context=<your context prefix><your cluster> install ./helm-chart
```

Later, when you want to change your deployment run:
```
helm list

helm --kube-context=<your context prefix><your cluster> upgrade <release name> ./helm-chart
```

Congragulations! You just deployed your own jupyterhub cluster using kubernetes! :D



File / Folder structure
-------

The `manifest.yaml` file in the project root directory contains the entirety of
the Kubenetes configuration for this deployment.

The subdirectories contain the Dockerfiles and scripts for the images used for
this deployment.

All the images for this deployment are pushed to the [data8 Docker Hub][]
organization and are named `data8/jupyterhub-k8s-<name>` where `<name>` is the
name of the containing folder for that image.

[data8 Docker Hub]: http://hub.docker.com/r/data8/

Development
-------

Current work on this project lives in a [ZenHub][] board for this repo. You
must install the browser extension to see the board.

After installing the extension, navigate to [the issue board](#boards) or press
`b`. You'll see a screen that looks something like this:

![screenshot 2016-11-04 13 24 21](https://cloud.githubusercontent.com/assets/2468904/20021193/084bb660-a292-11e6-9720-10746f475746.png)

- **Icebox** contains future tasks that haven't been prioritized.
- **This week** contains tasks that we plan to finish this week.
- **In Progress** contains tasks that someone is currently working on. All of
  these tasks have at least one person assigned to them.
- When the task is complete, we close the related issue.

**Epics** are groups of tasks that correspond to a complete feature. To see
only issues that belong to a specific Epic, you can click / unclick the
"Filter by this epic" button on the Epic.

[ZenHub]: https://www.zenhub.com/

### Workflow

1. As tasks / issues first get created, they land in the **Icebox** pipeline
   and are categorized into an **Epic** if needed.
2. During our weekly planning meetings we'll move tasks from **Icebox** to
   **This Week**.
3. When team members start actively working on a task, they'll assign
   themselves to the task and move it into the **In Progress** pipeline.
4. When team members finish a task, they'll make a Pull Request for the task.
   When the PR gets merged, they'll close the task to take it off the board.

## Cal Blueprint

<a href="http://www.calblueprint.org/">
![bp](https://cloud.githubusercontent.com/assets/2468904/11998649/8a12f970-aa5d-11e5-8dab-7eef0766c793.png "BP Banner")
</a>

This project was worked on in close collaboration with
**[Cal Blueprint](http://www.calblueprint.org/)**.
Cal Blueprint is a student-run UC Berkeley organization devoted to matching
the skills of its members to our desire to see
social good enacted in our community. Each semester, teams of 4-5 students work
closely with a non-profit to bring technological solutions to the problems they
face every day.
# Tiniest notebook stack #

This is the tiniest possible docker image that can run a Jupyter Notebook. It
is primarily meant for demo purposes where speed of pulling is important.

Not recommended for non-demo uses!

## What it gives you

It is based on [Alpine Linux](https://alpinelinux.org/). It uses pip to install
both the notebook and jupyterhub packages - the latter allows us to use this
image for single-user servers in Kubernetes / Docker spawners.

You can use pip3 to install packages (with `pip3` or `apk`) as root. There
are no non-root users provisioned.

## Basic usage

You can run a notebook with:

```
sudo docker run  -it  --rm  -p 8888:8888 jupyter/tiniest-notebook
```

The default command is `/usr/bin/jupyter` (not `/usr/local/bin/jupyth`). For
use with jupyterhub, `/usr/bin/jupyterhub-singleuser` is also available.
JupyterHub Kubernetes Autoscaler
===================================

### Settings

Settings for the autoscaler should be loaded through environment variables. They are defined as follows:

`MAX_UTILIZATION`, `MIN_UTILIZATION`, `OPTIMAL_UTILIZATION` refer to the minimum, maximum and optimal utilization rate. They are by default set to 0.85, 0.65, 0.75.

`MIN_NODES`, `MAX_NODES` refer to an additional bound of the number of nodes. They are by default set to 3 and 72.

`PREEMPTIBLE_LABELS` are a list of label keys in Kubernetes. Pods with these label keys are dynamic, expected to be created and deleted when the workload changes. Using `':'` as the delimiter, the list should have such a format: `jupyter:student:notebook`. **Despite the name, in the current version these pods will not be preempted in any case.** This list is by default empty.

`OMIT_LABELS`, `OMIT_NAMESPACES` are lists of label keys and namespace names. Pods with the given label keys or in the given namespaces **will not be taken into account at all** by the autoscaler. Using `':'` as the delimiter, the list should have such a format: `jupyter:student:notebook`. They are by default set to `""` and `"kube-system"`.


### Definitions

**Critical Pods** = Pods that are not omitted or assigned a label indicating that they are "preemptible".

**Workload** =  The sum of memory requests of pods that are not omitted.

**Capacity** = Total memeory of the cluster

**Utilization** = On certain nodes, the ratio between the sum of `Workload` and the sum of `Capacity`

### Expected Behavior

When `scale.py` is exected

1. The autoscaler will calculate the **Utilization** of the cluster.
2. If the **Utilization** of the cluster is between a **predefined minimum** and a **predefined maximum**, move the `Unschedulable` flag provided by Kubernetes between nodes, to make them deleted as soon as possible. Otherwise, the autoscaler will add or remove `Unschedulable` flags to approximate a **predefined optimal utilization**; if optimal utilization is not reached, new nodes can be created to meet the goal, to the predefined **maximum number of nodes**.
**2a. Nodes running `critical pods` will never be marked unschedulable**.
3. Make sure there are at least **predefined minimum number** of nodes schedulable by removing flags or add new nodes.
4. Shutdown all empty and unschedulable nodes

### How to run

0. Read `settings.py` to make sure you like the current settings.
1. Run `scale.py`, a one-time scaling should happen, and the script will quit.

### Requirements

Python 3, with `kubernetes` installed;

Google Cloud client `google-api-python-client`;

Necessary privilege or credentials.

### Supported Service Providers

Only Google Cloud Platform is supported for booting and shutting down nodes for now.
# Spark example

Following this example, you will create a functional [Apache
Spark](http://spark.apache.org/) cluster using Kubernetes and
[Docker](http://docker.io).

You will setup a Spark master service and a set of Spark workers using Spark's [standalone mode](http://spark.apache.org/docs/latest/spark-standalone.html).

For the impatient expert, jump straight to the [tl;dr](#tldr)
section.

### Sources

The Docker images are heavily based on https://github.com/mattf/docker-spark.
And are curated in https://github.com/kubernetes/application-images/tree/master/spark

The Spark UI Proxy is taken from https://github.com/aseigneurin/spark-ui-proxy.

The PySpark examples are taken from http://stackoverflow.com/questions/4114167/checking-if-a-number-is-a-prime-number-in-python/27946768#27946768

## Step Zero: Prerequisites

This example assumes

- You have a Kubernetes cluster installed and running.
- That you have installed the ```kubectl``` command line tool installed in your path and configured to talk to your Kubernetes cluster
- That your Kubernetes cluster is running [kube-dns](https://github.com/kubernetes/dns) or an equivalent integration.

Optionally, your Kubernetes cluster should be configured with a Loadbalancer integration (automatically configured via kube-up or GKE)

## Step One: Create namespace

```sh
$ kubectl create -f examples/spark/namespace-spark-cluster.yaml
```

Now list all namespaces:

```sh
$ kubectl get namespaces
NAME          LABELS             STATUS
default       <none>             Active
spark-cluster name=spark-cluster Active
```

To configure kubectl to work with our namespace, we will create a new context using our current context as a base:

```sh
$ CURRENT_CONTEXT=$(kubectl config view -o jsonpath='{.current-context}')
$ USER_NAME=$(kubectl config view -o jsonpath='{.contexts[?(@.name == "'"${CURRENT_CONTEXT}"'")].context.user}')
$ CLUSTER_NAME=$(kubectl config view -o jsonpath='{.contexts[?(@.name == "'"${CURRENT_CONTEXT}"'")].context.cluster}')
$ kubectl config set-context spark --namespace=spark-cluster --cluster=${CLUSTER_NAME} --user=${USER_NAME}
$ kubectl config use-context spark
```

## Step Two: Start your Master service

The Master [service](../../docs/user-guide/services.md) is the master service
for a Spark cluster.

Use the
[`spark-master-controller.yaml`](spark-master-controller.yaml)
file to create a
[replication controller](../../docs/user-guide/replication-controller.md)
running the Spark Master service.

```console
$ kubectl --namespace=jupyterhub create -f spark-master-controller.yaml
replicationcontroller "spark-master-controller" created
```

Then, use the
[`examples/spark/spark-master-service.yaml`](spark-master-service.yaml) file to
create a logical service endpoint that Spark workers can use to access the
Master pod:

```console
$ kubectl --namespace=jupyterhub create -f spark-master-service.yaml
service "spark-master" created
```

### Check to see if Master is running and accessible

```console
$ kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
spark-master-controller-5u0q5   1/1       Running   0          8m
```

Check logs to see the status of the master. (Use the pod retrieved from the previous output.)

```sh
$ kubectl logs spark-master-controller-5u0q5
starting org.apache.spark.deploy.master.Master, logging to /opt/spark-1.5.1-bin-hadoop2.6/sbin/../logs/spark--org.apache.spark.deploy.master.Master-1-spark-master-controller-g0oao.out
Spark Command: /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/spark-1.5.1-bin-hadoop2.6/sbin/../conf/:/opt/spark-1.5.1-bin-hadoop2.6/lib/spark-assembly-1.5.1-hadoop2.6.0.jar:/opt/spark-1.5.1-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark-1.5.1-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar:/opt/spark-1.5.1-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar -Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip spark-master --port 7077 --webui-port 8080
========================================
15/10/27 21:25:05 INFO Master: Registered signal handlers for [TERM, HUP, INT]
15/10/27 21:25:05 INFO SecurityManager: Changing view acls to: root
15/10/27 21:25:05 INFO SecurityManager: Changing modify acls to: root
15/10/27 21:25:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/10/27 21:25:06 INFO Slf4jLogger: Slf4jLogger started
15/10/27 21:25:06 INFO Remoting: Starting remoting
15/10/27 21:25:06 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@spark-master:7077]
15/10/27 21:25:06 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
15/10/27 21:25:07 INFO Master: Starting Spark master at spark://spark-master:7077
15/10/27 21:25:07 INFO Master: Running Spark version 1.5.1
15/10/27 21:25:07 INFO Utils: Successfully started service 'MasterUI' on port 8080.
15/10/27 21:25:07 INFO MasterWebUI: Started MasterWebUI at http://spark-master:8080
15/10/27 21:25:07 INFO Utils: Successfully started service on port 6066.
15/10/27 21:25:07 INFO StandaloneRestServer: Started REST server for submitting applications on port 6066
15/10/27 21:25:07 INFO Master: I have been elected leader! New state: ALIVE
```

Once the master is started, we'll want to check the Spark WebUI. In order to access the Spark WebUI, we will deploy a [specialized proxy](https://github.com/aseigneurin/spark-ui-proxy). This proxy is neccessary to access worker logs from the Spark UI.

Deploy the proxy controller with [`examples/spark/spark-ui-proxy-controller.yaml`](spark-ui-proxy-controller.yaml):

```console
$ kubectl --namespace=jupyterhub create -f spark-ui-proxy-controller.yaml
replicationcontroller "spark-ui-proxy-controller" created
```

We'll also need a corresponding Loadbalanced service for our Spark Proxy [`examples/spark/spark-ui-proxy-service.yaml`](spark-ui-proxy-service.yaml):

```console
$ kubectl create -f examples/spark/spark-ui-proxy-service.yaml
service "spark-ui-proxy" created
```

After creating the service, you should eventually get a loadbalanced endpoint:

```console
$ kubectl get svc spark-ui-proxy -o wide
 NAME             CLUSTER-IP    EXTERNAL-IP                                                              PORT(S)   AGE       SELECTOR
spark-ui-proxy   10.0.51.107   aad59283284d611e6839606c214502b5-833417581.us-east-1.elb.amazonaws.com   80/TCP    9m        component=spark-ui-proxy
```

The Spark UI in the above example output will be available at http://aad59283284d611e6839606c214502b5-833417581.us-east-1.elb.amazonaws.com

If your Kubernetes cluster is not equipped with a Loadbalancer integration, you will need to use the [kubectl proxy](../../docs/user-guide/accessing-the-cluster.md#using-kubectl-proxy) to
connect to the Spark WebUI:

```console
kubectl proxy --port=8001
```

At which point the UI will be available at
[http://localhost:8001/api/v1/proxy/namespaces/spark-cluster/services/spark-master:8080/](http://localhost:8001/api/v1/proxy/namespaces/spark-cluster/services/spark-master:8080/).

## Step Three: Start your Spark workers

The Spark workers do the heavy lifting in a Spark cluster. They
provide execution resources and data cache capabilities for your
program.

The Spark workers need the Master service to be running.

Use the [`spark-worker-controller.yaml`](spark-worker-controller.yaml) file to create a
[replication controller](../../docs/user-guide/replication-controller.md) that manages the worker pods.

```console
$ kubectl --namespace=jupyterhub create -f spark-worker-controller.yaml
replicationcontroller "spark-worker-controller" created
```

### Check to see if the workers are running

If you launched the Spark WebUI, your workers should just appear in the UI when
they're ready. (It may take a little bit to pull the images and launch the
pods.) You can also interrogate the status in the following way:

```console
$ kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
spark-master-controller-5u0q5   1/1       Running   0          25m
spark-worker-controller-e8otp   1/1       Running   0          6m
spark-worker-controller-fiivl   1/1       Running   0          6m
spark-worker-controller-ytc7o   1/1       Running   0          6m

$ kubectl logs spark-master-controller-5u0q5
[...]
15/10/26 18:20:14 INFO Master: Registering worker 10.244.1.13:53567 with 2 cores, 6.3 GB RAM
15/10/26 18:20:14 INFO Master: Registering worker 10.244.2.7:46195 with 2 cores, 6.3 GB RAM
15/10/26 18:20:14 INFO Master: Registering worker 10.244.3.8:39926 with 2 cores, 6.3 GB RAM
```

## Step Four: Start the Zeppelin UI to launch jobs on your Spark cluster

The Zeppelin UI pod can be used to launch jobs into the Spark cluster either via
a web notebook frontend or the traditional Spark command line. See
[Zeppelin](https://zeppelin.incubator.apache.org/) and
[Spark architecture](https://spark.apache.org/docs/latest/cluster-overview.html)
for more details.

Deploy Zeppelin:

```console
$ kubectl --namespace=jupyterhub create -f zeppelin-controller.yaml
replicationcontroller "zeppelin-controller" created
```

And the corresponding service:

```console
$ kubectl --namespace=jupyterhub create -f zeppelin-service.yaml
service "zeppelin" created
```

Zeppelin needs the spark-master service to be running.

### Check to see if Zeppelin is running

```console
$ kubectl get pods -l component=zeppelin
NAME                        READY     STATUS    RESTARTS   AGE
zeppelin-controller-ja09s   1/1       Running   0          53s
```

## Step Five: Do something with the cluster

Now you have two choices, depending on your predilections. You can do something
graphical with the Spark cluster, or you can stay in the CLI.

For both choices, we will be working with this Python snippet:

```python
from math import sqrt; from itertools import count, islice

def isprime(n):
    return n > 1 and all(n%i for i in islice(count(2), int(sqrt(n)-1)))

nums = sc.parallelize(xrange(10000000))
print nums.filter(isprime).count()
```

### Do something fast with pyspark!

Simply copy and paste the python snippet into pyspark from within the zeppelin pod:

```console
$ kubectl --namespace=jupyterhub exec zeppelin-controller-ja09s -it pyspark
Python 2.7.9 (default, Mar  1 2015, 12:57:24)
[GCC 4.9.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.5.1
      /_/

Using Python version 2.7.9 (default, Mar  1 2015 12:57:24)
SparkContext available as sc, HiveContext available as sqlContext.
>>> from math import sqrt; from itertools import count, islice
>>>
>>> def isprime(n):
...     return n > 1 and all(n%i for i in islice(count(2), int(sqrt(n)-1)))
...
>>> nums = sc.parallelize(xrange(10000000))

>>> print nums.filter(isprime).count()
664579
```

Congratulations, you now know how many prime numbers there are within the first 10 million numbers!

### Do something graphical and shiny!

Creating the Zeppelin service should have yielded you a Loadbalancer endpoint:

```console
$ kubectl get svc zeppelin -o wide
 NAME       CLUSTER-IP   EXTERNAL-IP                                                              PORT(S)   AGE       SELECTOR
zeppelin   10.0.154.1   a596f143884da11e6839506c114532b5-121893930.us-east-1.elb.amazonaws.com   80/TCP    3m        component=zeppelin
```

If your Kubernetes cluster does not have a Loadbalancer integration, then we will have to use port forwarding.

Take the Zeppelin pod from before and port-forward the WebUI port:

```console
$ kubectl port-forward zeppelin-controller-ja09s 8080:8080
```

This forwards `localhost` 8080 to container port 8080. You can then find
Zeppelin at [http://localhost:8080/](http://localhost:8080/).

Once you've loaded up the Zeppelin UI, create a "New Notebook". In there we will paste our python snippet, but we need to add a `%pyspark` hint for Zeppelin to understand it:

```
%pyspark
from math import sqrt; from itertools import count, islice

def isprime(n):
    return n > 1 and all(n%i for i in islice(count(2), int(sqrt(n)-1)))

nums = sc.parallelize(xrange(10000000))
print nums.filter(isprime).count()
```

After pasting in our code, press shift+enter or click the play icon to the right of our snippet. The Spark job will run and once again we'll have our result!

## Result

You now have services and replication controllers for the Spark master, Spark
workers and Spark driver.  You can take this example to the next step and start
using the Apache Spark cluster you just created, see
[Spark documentation](https://spark.apache.org/documentation.html) for more
information.

## tl;dr

```console
kubectl --namespace=jupyterhub create -f examples/spark
```

After it's setup:

```console
kubectl get pods # Make sure everything is running
kubectl get svc -o wide # Get the Loadbalancer endpoints for spark-ui-proxy and zeppelin
```

At which point the Master UI and Zeppelin will be available at the URLs under the `EXTERNAL-IP` field.

You can also interact with the Spark cluster using the traditional `spark-shell` /
`spark-subsubmit` / `pyspark` commands by using `kubectl exec` against the
`zeppelin-controller` pod.

If your Kubernetes cluster does not have a Loadbalancer integration, use `kubectl proxy` and `kubectl port-forward` to access the Spark UI and Zeppelin.

For Spark UI:

```console
kubectl proxy --port=8001
```

Then visit [http://localhost:8001/api/v1/proxy/namespaces/spark-cluster/services/spark-ui-proxy/](http://localhost:8001/api/v1/proxy/namespaces/spark-cluster/services/spark-ui-proxy/).

For Zeppelin:

```console
kubectl port-forward zeppelin-controller-abc123 8080:8080 &
```

Then visit [http://localhost:8080/](http://localhost:8080/).

## Known Issues With Spark

* This provides a Spark configuration that is restricted to the cluster network,
  meaning the Spark master is only available as a cluster service. If you need
  to submit jobs using external client other than Zeppelin or `spark-submit` on
  the `zeppelin` pod, you will need to provide a way for your clients to get to
  the
  [`examples/spark/spark-master-service.yaml`](spark-master-service.yaml). See
  [Services](../../docs/user-guide/services.md) for more information.

## Known Issues With Zeppelin

* The Zeppelin pod is large, so it may take a while to pull depending on your
  network. The size of the Zeppelin pod is something we're working on, see issue #17231.

* Zeppelin may take some time (about a minute) on this pipeline the first time
  you run it. It seems to take considerable time to load.

* On GKE, `kubectl port-forward` may not be stable over long periods of time. If
  you see Zeppelin go into `Disconnected` state (there will be a red dot on the
  top right as well), the `port-forward` probably failed and needs to be
  restarted. See #12179.

<!-- BEGIN MUNGE: GENERATED_ANALYTICS -->
[![Analytics](https://kubernetes-site.appspot.com/UA-36037335-10/GitHub/examples/spark/README.md?pixel)]()
<!-- END MUNGE: GENERATED_ANALYTICS -->
# Spark on GlusterFS example

This guide is an extension of the standard [Spark on Kubernetes Guide](../../../examples/spark/) and describes how to run Spark on GlusterFS using the [Kubernetes Volume Plugin for GlusterFS](../../../examples/volumes/glusterfs/)

The setup is the same in that you will setup a Spark Master Service in the same way you do with the standard Spark guide but you will deploy a modified Spark Master and a Modified Spark Worker ReplicationController, as they will be modified to use the GlusterFS volume plugin to mount a GlusterFS volume into the Spark Master and Spark Workers containers. Note that this example can be used as a guide for implementing any of the Kubernetes Volume Plugins with the Spark Example.

[There is also a video available that provides a walkthrough for how to set this solution up](https://youtu.be/xyIaoM0-gM0)

## Step Zero: Prerequisites

This example assumes that you have been able to successfully get the standard Spark Example working in Kubernetes and that you have a GlusterFS cluster that is accessible from your Kubernetes cluster. It is also recommended that you are familiar with the GlusterFS Volume Plugin and how to configure it.

## Step One: Define the endpoints for your GlusterFS Cluster

Modify the `examples/spark/spark-gluster/glusterfs-endpoints.yaml` file to list the IP addresses of some of the servers in your GlusterFS cluster. The GlusterFS Volume Plugin uses these IP addresses to perform a Fuse Mount of the GlusterFS Volume into the Spark Worker Containers that are launched by the ReplicationController in the next section.

Register your endpoints by running the following command:

```console
$ kubectl create -f examples/spark/spark-gluster/glusterfs-endpoints.yaml
```

## Step Two: Modify and Submit your Spark Master ReplicationController

Modify the `examples/spark/spark-gluster/spark-master-controller.yaml` file to reflect the GlusterFS Volume that you wish to use in the PATH parameter of the volumes subsection.

Submit the Spark Master Pod

```console
$ kubectl create -f examples/spark/spark-gluster/spark-master-controller.yaml
```

Verify that the Spark Master Pod deployed successfully.

```console
$ kubectl get pods
```

Submit the Spark Master Service

```console
$ kubectl create -f examples/spark/spark-gluster/spark-master-service.yaml
```

Verify that the Spark Master Service deployed successfully.

```console
$ kubectl get services
```

## Step Three: Start your Spark workers

Modify the `examples/spark/spark-gluster/spark-worker-controller.yaml` file to reflect the GlusterFS Volume that you wish to use in the PATH parameter of the Volumes subsection.

Make sure that the replication factor for the pods is not greater than the amount of Kubernetes nodes available in your Kubernetes cluster.

Submit your Spark Worker ReplicationController by running the following command:

```console
$ kubectl create -f examples/spark/spark-gluster/spark-worker-controller.yaml
```

Verify that the Spark Worker ReplicationController deployed its pods successfully.

```console
$ kubectl get pods
```

Follow the steps from the standard example to verify the Spark Worker pods have registered successfully with the Spark Master.

## Step Four: Submit a Spark Job

All the Spark Workers and the Spark Master in your cluster have a mount to GlusterFS. This means that any of them can be used as the Spark Client to submit a job. For simplicity, lets use the Spark Master as an example.


The Spark Worker and Spark Master containers include a setup_client utility script that takes two parameters, the Service IP of the Spark Master and the port that it is running on. This must be to setup the container as a Spark client prior to submitting any Spark Jobs.

Obtain the Service IP (listed as IP:) and Full Pod Name by running

```console
$ kubectl describe pod spark-master-controller
```

Now we will shell into the Spark Master Container and run a Spark Job. In the example below, we are running the Spark Wordcount example and specifying the input and output directory at the location where GlusterFS is mounted in the Spark Master Container. This will submit the job to the Spark Master who will distribute the work to all the Spark Worker Containers.

All the Spark Worker containers  will be able to access the data as they all have the same GlusterFS volume mounted at /mnt/glusterfs. The reason we are submitting the job from a Spark Worker and not an additional Spark Base container (as in the standard Spark Example) is due to the fact that the Spark instance submitting the job must be able to access the data. Only the Spark Master and Spark Worker containers have GlusterFS mounted.

The Spark Worker and Spark Master containers include a setup_client utility script that takes two parameters, the Service IP of the Spark Master and the port that it is running on. This must be done to setup the container as a Spark client prior to submitting any Spark Jobs.

Shell into the Master Spark Node (spark-master-controller) by running

```console
kubectl exec spark-master-controller-<ID> -i -t -- bash -i

root@spark-master-controller-c1sqd:/# . /setup_client.sh <Service IP> 7077
root@spark-master-controller-c1sqd:/# pyspark

Python 2.7.9 (default, Mar  1 2015, 12:57:24)
[GCC 4.9.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
15/06/26 14:25:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.4.0
      /_/
Using Python version 2.7.9 (default, Mar  1 2015 12:57:24)
SparkContext available as sc, HiveContext available as sqlContext.
>>> file = sc.textFile("/mnt/glusterfs/somefile.txt")
>>> counts = file.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
>>> counts.saveAsTextFile("/mnt/glusterfs/output")
```

While still in the container, you can see the output of your Spark Job in the Distributed File System by running the following:

```console
root@spark-master-controller-c1sqd:/# ls -l /mnt/glusterfs/output
```

<!-- BEGIN MUNGE: GENERATED_ANALYTICS -->
[![Analytics](https://kubernetes-site.appspot.com/UA-36037335-10/GitHub/examples/spark/spark-gluster/README.md?pixel)]()
<!-- END MUNGE: GENERATED_ANALYTICS -->
# hub.docker.com/tiredofit/redis

# Introduction

This will build a [Redis](https://www.redis.org) Database Container.

This Container uses Alpine:3.5 as a base. Also included are
* [s6 overlay](https://github.com/just-containers/s6-overlay) enabled for PID 1 Init capabilities
* [zabbix-agent](https://zabbix.org) based on TRUNK compiled for individual container monitoring.
* Cron installed along with other tools (bash,curl, less, logrotate, nano, vim) for easier management.



[Changelog](CHANGELOG.md)

# Authors

- [Dave Conroy](https://github.com/tiredofit)

# Table of Contents

- [Introduction](#introduction)
    - [Changelog](CHANGELOG.md)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
   - [References](#references)

# Prerequisites

Requires a MysQL Database Container


# Installation

Automated builds of the image are available on 
[Registry](https://hub.docker.com/tiredofit/redis) and is the recommended method of 
installation.


```bash
docker pull hub.docker.com/tiredofit/redis
```

# Quick Start

* The quickest way to get started is using [docker-compose](https://docs.docker.com/compose/). See 
the examples folder for a working [docker-compose.yml](examples/docker-compose.yml) that can be 
modified for development or production use.

* Set various [environment variables](#environment-variables) to understand the capabilities of this 
image.
* Map [persistent storage](#data-volumes) for access to configuration and data files for backup.
* Make [networking ports](#networking) available for public access if necessary



# Configuration

### Data-Volumes

The Following Data Volumes are available.

| Parameter | Description |
|-----------|-------------|
| `/data`    | Application Directory |
      

### Environment Variables

| Parameter | Description |
|-----------|-------------|
| `ZABBIX_HOSTNAME`    | Hostname to report to Zabbix |

### Networking

The following ports are exposed.

| Port      | Description |
|-----------|-------------|
| `6379` | Redis Port |


# Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it (whatever your container name is e.g. redis) bash
```

# References

* https://redis.org/


# hub.docker.com/tiredofit/alpine

# Introduction

Dockerfile to build an [alpine](https://www.alpinelinux.org/) container image.

* Currently tracking 3.4, 3.5, 3.6, and edge
* [s6 overlay](https://github.com/just-containers/s6-overlay) enabled for PID 1 Init capabilities
* [zabbix-agent](https://zabbix.org) based on TRUNK compiled for individual container monitoring.
* Cron installed along with other tools (bash,curl, less, logrotate, nano, vim) for easier management.
* MSMTP enabled to send mail from container to external SMTP server.

# Authors

- [Dave Conroy](dave at tiredofit dot ca) [https://github.com/tiredofit]

# Table of Contents

- [Introduction](#introduction)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
   - [References](#references)

# Prerequisites

No prequisites required

# Installation

Automated builds of the image are available on [Docker Hub](https://hub.docker.com/tiredofit/alpine) and 
is the recommended method of installation.


```bash
docker pull tiredofit/alpine:(imagetag)
```

The following image tags are available:

* `3.4:latest` - Alpine 3.4
* `3.5:latest` - Alpine 3.5
* `3.6:latest` - Alpine 3.6
* `edge:latest` - Alpine edge
* `latest` - Alpine 3.6

# Quick Start

Utilize this image as a base for further builds. By default it does not start the S6 Overlay system, but 
Bash. Please visit the [s6 overlay repository](https://github.com/just-containers/s6-overlay) for 
instructions on how to enable the S6 Init system when using this base or look at some of my other images 
which use this as a base.

# Configuration

### Data-Volumes
The following directories are used for configuration and can be mapped for persistent storage.

| Directory                           | Description                 |
|-------------------------------------|-----------------------------|
| `/etc/zabbix/zabbix_agentd.conf.d/` | Zabbix Agent Configuration Directory |



### Environment Variables

Below is the complete list of available options that can be used to customize your installation.

| Parameter         | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `DEBUG_MODE`      | Enable Debug Mode - Default: `FALSE`                            |
| `ENABLE_CRON`     | Enable Cron - Default: `TRUE`                                   |
| `ENABLE_SMTP`     | Enable SMTP services - Default: `TRUE`						|
| `ENABLE_ZABBIX`   | Enable Zabbix Agent - Default: `TRUE`                           |
| `TIMEZONE`        | Set Timezone - Default: `America/Vancouver`                     |

If you wish to have this send mail, set `ENABLE_SMTP=TRUE` and configure the following environment variables. See the [MSMTP Configuration Options](http://msmtp.sourceforge.net/doc/msmtp.html) for further information on options to configure MSMTP

| Parameter         | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `SMTP_HOST`      | Hostname of SMTP Server - Default: `postfix-relay`                            |
| `SMTP_PORT`      | Port of SMTP Server - Default: `25`                            |
| `SMTP_DOMAIN`     | HELO Domain - Default: `docker`                                   |
| `SMTP_MAILDOMAIN`     | Mail Domain From - Default: `example.org`						|
| `SMTP_AUTHENTICATION`     | SMTP Authentication - Default: `none`                                   |
| `SMTP_USER`     | Enable SMTP services - Default: `user`						|
| `SMTP_PASS`   | Enable Zabbix Agent - Default: `password`                           |
| `SMTP_TLS`        | Use TLS - Default: `off`                     |
| `SMTP_STARTTLS`   | Start TLS from within Dession - Default: `off` |
| `SMTP_TLSCERTCHECK` | Check remote certificate - Default: `off` |

See The [Official Zabbix Agent Documentation](https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd) for information about the following Zabbix values

| Zabbix Parameters | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `ZABBIX_LOGFILE` | Logfile Location - Default: `/var/log/zabbix/zabbix_agentd.log` |
| `ZABBIX_LOGFILESIZE` | Logfile Size - Default: `1` |
| `ZABBIX_DEBUGLEVEL` | Debug Level - Default: `1` |
| `ZABBIX_REMOTECOMMANDS` | Enable Remote Commands (0/1) - Default: `1` |
| `ZABBIX_REMOTECOMMANDS_LOG` | Enable Remote Commands Log (0/1)| - Default: `1` |
| `ZABBIX_SERVER` | Allow connections from Zabbix Server IP - Default: `0.0.0.0/0` |
| `ZABBIX_LISTEN_PORT` | Zabbix Agent Listening Port - Default: `10050` |
| `ZABBIX_LISTEN_IP` | Zabbix Agent Listening IP - Default: `0.0.0.0` |
| `ZABBIX_START_AGENTS` | How many Zabbix Agents to Start - Default: `3 | 
| `ZABBIX_SERVER_ACTIVE` | Server for Active Checks - Default: `zabbix-proxy` |
| `ZABBIX_HOSTNAME` | Container hostname to report to server - Default: `docker` |
| `ZABBIX_REFRESH_ACTIVE_CHECKS` | Seconds to refresh Active Checks - Default: `120` |
| `ZABBIX_BUFFER_SEND` | Buffer Send - Default: `5` |
| `ZABBIX_BUFFER_SIZE` | Buffer Size - Default: `100` |
| `ZABBIX_MAXLINES_SECOND` | Max Lines Per Second - Default: `20` |
| `ZABBIX_ALLOW_ROOT` | Allow running as root - Default: `1` |
| `ZABBIX_USER` | Zabbix user to start as - Default: `zabbix` |


### Networking

The following ports are exposed.

| Port      | Description  |
|-----------|--------------|
| `10050`   | Zabbix Agent |


# Debug Mode

When using this as a base image, create statements in your startup scripts to check for existence of `DEBUG_MODE=TRUE` and set various parameters in your applications to output more detail, enable debugging modes, and so on. In this base image all it does is set the Zabbix Agent if enabled to a high level of verbosity. 


# Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it (whatever your container name is e.g. alpine) bash
```

# References

* https://www.alpinelinux.org# hub.docker.com/tiredofit/self-service-password

# Introduction

Dockerfile to build a [LTB-Self Service Password](https://ltb-project.org/documentation/self-service-password) container image.

It will automatically download the latest release from git.

This Container uses Alpine:3.5 as a base. Additional Components are PHP7 w/ APC, OpCache, LDAP Support


[Changelog](CHANGELOG.md)

# Authors

- [Dave Conroy][https://github.com/tiredofit]

# Table of Contents

- [Introduction](#introduction)
    - [Changelog](CHANGELOG.md)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
   - [References](#references)

# Prerequisites

This image relies on an external LDAP Server, external SMTP Server, and is meant to be run behind a reverse SSL Proxy such as nginx-proxy.


# Installation

Automated builds of the image are available on [Docker Hub](https://hub.docker.com/tiredofit/self-service-password) and is the recommended method of installation.


```bash
docker pull tiredofit/self-service-password
```

# Quick Start

* The quickest way to get started is using [docker-compose](https://docs.docker.com/compose/). See 
the examples folder for a working [docker-compose.yml](examples/docker-compose.yml) that can be 
modified for development or production use.

* Set various [environment variables](#environment-variables) to understand the capabilities of this 
image.
* Map [persistent storage](#data-volumes) for access to configuration and data files for backup.

# Configuration

### Data-Volumes

The following directories are used for configuration and can be mapped for persistent storage.

| Directory | Description |
|-----------|-------------|
| `/www/ssp` | Root Wordpress Directory |
| `/www/logs` | Nginx and php-fpm logfiles |

### Environment Variables

Along with the Environment Variables from the [Base image](https://hub.docker.com/r/tiredofit/alpine), and the [Nginx+PHP-FPM Engine](https://hub.docker.com/r/tiredofit/nginx-php-fpm) below is the complete list of available options that can be used to customize your installation.

| Parameter | Description |
|-----------|-------------|
| `PHP_MEMORY_LIMIT` |Amount of memory php-fpm process should use (Default 128M) |
| `UPLOAD_MAX_SIZE` | Maximum Upload Size: (Default 2G) |
| `APC_SHM_SIZE` | PHP7 APC SHM Cache Size: (Default 128M) |
| `OPCACHE_MEM_SIZE` | PHP7 OPCache Size (Default 128) |
| `TZ` | Timezone - Use Unix Timezone format (Default America/Vancouver) |
| `DEBUG` | Set this to `true` to enable entrypoint debugging. |
| `LDAP_SERVER` | Ldap server. No default. |
| `LDAP_STARTTLS` | Enable TLS on Ldap bind. No default. |
| `LDAP_BINDDN` | Ldap bind dn. No default. |
| `LDAP_BINDPASS` | Ldap bind password. No default. |
| `LDAP_BASE_SEARCH` | Base where we can search for users. No default. |
| `LDAP_LOGIN_ATTRIBUTE` | Ldap property used for user searching. Defaults to `uid` |
| `LDAP_FULLNAME_ATTRIBUTE` | Ldap property to get user fullname. Defaults to `cn` |
| `ADMODE` | Specifies if LDAP server is Active Directory LDAP server. If your LDAP server is AD, set this to `true`. Defaults to `false`. |
| `AD_OPT_FORCE_UNLOCK` | Force account unlock when password is changed.  Default to `false`.|
| `AD_OPT_FORCE_PWD_CHANGE` | Force user change password at next login.  Defaults to `false`. |
| `AD_OPT_CHANGE_EXPIRED_PASSWORD` | Allow user with expired password to change password. Defaults to `false`. |
| `SAMBA_MODE` | Samba mode, if is `true` update sambaNTpassword and sambaPwdLastSet attributes too; if is `false` just update the password. Defaults to `false`. |
| `SHADOW_OPT_UPDATE_SHADOWLASTCHANGE` | If `true` update shadowLastChange.  Defaults to `false`. |
| `PASSWORD_HASH` |  Hash mechanism for password`SSHA` `SHA` `SMD5` `MD5` `CRYPT` `clear` (the default) `auto` (will check the hash of current password)  **This option is not used with ad_mode = true** |
| `PASSWORD_MIN_LENGTH` | Minimal length. Defaults to `0` (unchecked). |
| `PASSWORD_MAX_LENGTH` | Maximal length. Defaults to `0` (unchecked). |
| `PASSWORD_MIN_LOWERCASE` | Minimal lower characters. Defaults to `0` (unchecked).  |
| `PASSWORD_MIN_UPPERCASE` | Minimal upper characters. Defaults to `0` (unchecked).  |
| `PASSWORD_MIN_DIGIT` | Minimal digit characters. Defaults to `0` (unchecked).  |
| `PASSWORD_MIN_SPECIAL` | Minimal special characters. Defaults to `0` (unchecked).  |
| `PASSWORD_NO_REUSE` | Dont reuse the same password as currently. Defaults to `true`. |
| `PASSWORD_SHOW_POLICY` | Show policy constraints message`always` `never` `onerror`. Defaults to `never` |
| `PASSWORD_SHOW_POLICY_POSITION` | Position of password policy constraints message`above` `below` - 
the form. Defaults to `above` |
| `WHO_CAN_CHANGE_PASSWORD` | Who changes the password?  Also applicable for question/answer save `user`: the user itself `manager`: the above binddn. Defaults to `user` |
| `QUESTIONS_ENABLED` | Use questions/answers?  `true` or `false`. Defaults to `true` |
| `LDAP_MAIL_ATTRIBUTE` | LDAP mail attribute. Defaults to `mail` |
| `MAIL_FROM` | Who the email should come from. Defaults to `admin@example.com` |
| `MAIL_FROM_NAME` | Name for `MAIL_FROM`. Defaults to `No Reply`|
| `NOTIFY_ON_CHANGE` | Notify users anytime their password is changed. Defaults to `false` |
| `SMTP_DEBUG` | SMTP debug mode (following https:////github.com/PHPMailer/PHPMailer instructions). Defaults to `0` |
| `SMTP_HOST` | SMTP host. No default. |
| `SMTP_AUTH_ON` | Force smtp auth with `SMTP_USER` and `SMTP_PASS`. Defaults to `false` |
| `SMTP_USER` | SMTP user. No default. |
| `SMTP_PASS` | SMTP password. No default. |
| `SMTP_PORT` | SMTP port. Defaults to `587` |
| `SMTP_SECURE_TYPE` | SMTP secure type to use. `ssl` or `tls`. Defaults to `tls` |
| `LOGO` | Main Logo - Default images/ltb-logo.png |
| `BACKGROUND` | Change background Default images/unsplash-space.jpg|
| `USE_SMS` | Enable sms notify. (Disabled on this image). Defaults to `false` |
| `IS_BEHIND_PROXY` | Enable reset url parameter to accept reverse proxy. Defaults to `false`  |
| `SHOW_HELP` | Display help messages. Defaults to `true`. |
| `LANG` | Language (NOT WORKING YET). Defaults to `en`.  |
| `DEBUG_MODE` | Debug mode. Defaults to `false`. |
| `SECRETEKEY` | Encryption, decryption keyphrase. Defaults to `secret`. |
| `USE_RECAPTCHA` | Use Google reCAPTCHA (http://www.google.com/recaptcha). Defaults to `false` |
| `RECAPTCHA_PUB_KEY` | Go on the site to get public key |
| `RECAPTCHA_PRIV_KEY` | Go on the site to get private key |
| `RECAPTCHA_THEME` | Theme of ReCaptcha. Default: light|
| `RECAPTCHA_TYPE` | Type of ReCaptcha Default: image|
| `RECAPTCHA_SIZE` | Size of ReCaptcha Default: small|
| `DEFAULT_ACTION` | Default action`change` `sendtoken` `sendsms`. Defaults to `change` |


### Networking

The following ports are exposed.

| Port      | Description |
|-----------|-------------|
| `80` | HTTP |

# Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it (whatever your container name is e.g. ssp) bash
```

# References

* https://ltb-project.org/documentation/self-service-password



# How to manually setup Gitlab Runner for Docker

## Create Docker Private Network for Builds
docker network create gitlab-runner

## Run Docker in Docker
docker run -d --name gitlab-dind --privileged --restart always --network gitlab-runner -v /var/lib/docker tiredofit/docker-dind:latest

## Run GitLab Runner
docker run -d --name gitlab-runner --restart always --network gitlab-runner -v 
/config:/etc/gitlab-runner -e DOCKER_HOST=tcp://gitlab-dind:2375 tiredofit/gitlab-runner:latest

- You can also use the docker-compose.yml for the above commands

## Configure Gitlab Runner
docker run -it --rm -v /var/local/docker/gitlab-runner/config:/etc/glab-runner tiredofit/gitlab-runner \
   register \
    --executor docker \
    --docker-image docker:git \
    --docker-volumes /var/run/docker.sock:/var/run/docker.sock

There are also additional environment variables

Below is the complete list of available options that can be used to customize your installation.

| Parameter         | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `DEBUG_MODE`      | Enable Debug Mode - Default: `FALSE`                            |
| `ENABLE_CRON`     | Enable Cron - Default: `TRUE`                                   |
| `ENABLE_SMTP`     | Enable SMTP services - Default: `TRUE`						|
| `ENABLE_ZABBIX`   | Enable Zabbix Agent - Default: `TRUE`                           |
| `TIMEZONE`        | Set Timezone - Default: `America/Vancouver`                     |

If you wish to have this send mail, set `ENABLE_SMTP=TRUE` and configure the following environment variables. See the [MSMTP Configuration Options](http://msmtp.sourceforge.net/doc/msmtp.html) for further information on options to configure MSMTP

| Parameter         | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `SMTP_HOST`      | Hostname of SMTP Server - Default: `postfix-relay`                            |
| `SMTP_PORT`      | Port of SMTP Server - Default: `25`                            |
| `SMTP_DOMAIN`     | HELO Domain - Default: `docker`                                   |
| `SMTP_MAILDOMAIN`     | Mail Domain From - Default: `example.org`						|
| `SMTP_AUTHENTICATION`     | SMTP Authentication - Default: `none`                                   |
| `SMTP_USER`     | Enable SMTP services - Default: `user`						|
| `SMTP_PASS`   | Enable Zabbix Agent - Default: `password`                           |
| `SMTP_TLS`        | Use TLS - Default: `off`                     |
| `SMTP_STARTTLS`   | Start TLS from within Dession - Default: `off` |
| `SMTP_TLSCERTCHECK` | Check remote certificate - Default: `off` |

See The [Official Zabbix Agent Documentation](https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd) for information about the following Zabbix values

| Zabbix Parameters | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `ZABBIX_LOGFILE` | Logfile Location - Default: `/var/log/zabbix/zabbix_agentd.log` |
| `ZABBIX_LOGFILESIZE` | Logfile Size - Default: `1` |
| `ZABBIX_DEBUGLEVEL` | Debug Level - Default: `1` |
| `ZABBIX_REMOTECOMMANDS` | Enable Remote Commands (0/1) - Default: `1` |
| `ZABBIX_REMOTECOMMANDS_LOG` | Enable Remote Commands Log (0/1)| - Default: `1` |
| `ZABBIX_SERVER` | Allow connections from Zabbix Server IP - Default: `0.0.0.0/0` |
| `ZABBIX_LISTEN_PORT` | Zabbix Agent Listening Port - Default: `10050` |
| `ZABBIX_LISTEN_IP` | Zabbix Agent Listening IP - Default: `0.0.0.0` |
| `ZABBIX_START_AGENTS` | How many Zabbix Agents to Start - Default: `3 | 
| `ZABBIX_SERVER_ACTIVE` | Server for Active Checks - Default: `zabbix-proxy` |
| `ZABBIX_HOSTNAME` | Container hostname to report to server - Default: `docker` |
| `ZABBIX_REFRESH_ACTIVE_CHECKS` | Seconds to refresh Active Checks - Default: `120` |
| `ZABBIX_BUFFER_SEND` | Buffer Send - Default: `5` |
| `ZABBIX_BUFFER_SIZE` | Buffer Size - Default: `100` |
| `ZABBIX_MAXLINES_SECOND` | Max Lines Per Second - Default: `20` |
| `ZABBIX_ALLOW_ROOT` | Allow running as root - Default: `1` |
| `ZABBIX_USER` | Zabbix user to start as - Default: `zabbix` |

# CerebralCortex-DockerCompose<img src="https://avatars3.githubusercontent.com/u/12463357?v=3" />
# docker-elasticfence
Docker container running the Elasticfence Stack

- Elasticsearch 2.4.1 
- Kibi 4.6.4 + Siren 2.4.1
- Elasticfence Auth _(root/elasticFence)_
- Kibana-auth-elasticfence
- KiBrand 0.4.6
- Sentinl 4.6
- Sense/Timelion

#### Usage

Install mixed ES container on new host w/ authentication (default: root/elasticFence)
```
docker pull qxip/docker-elasticfence
```
Create stateful data volume
```
docker volume create -o size=20GB --name esdata
```
Run container and map ports
```
docker run -tid --name elk -p 9200:9200 -p 5606:5606 -v esdata:/usr/share/elasticsearch qxip/docker-elasticfence
```
Connect shell to container
```
docker exec -ti elk /bin/bash
```

##### External ES Connector

Run the image using remote Elastic instance
```
$ docker run -i -t -e ELASTICSEARCH_URL=http://192.168.10.20:9200 -p 5601:5606 qxip/docker-kibi
```

##### Docker Compose

see ```docker-compose.yml```

##### Rancher

see ```rancher``` folder
# docker-elassandra-kibi
Docker Elassandra 2.4.2 + Kibi 4.5.4

```
docker run -ti --rm --name elassandra -p 9200:9222 -p 5606:5606 -P qxip/docker-elassandra-kibi:dev
```

### WARNING
This image is potentially unstable, insecure and only suitable for testing!
# nav2_docker_compose
docker compose with micro containers, one for each navitia's service

# how to use
You'll need docker and docker-compose (tested with docker v1.12.1 and docker-compose v1.8.1)

run them all

`docker-compose up`

you can then add some data in the `default` coverage:

The input dir in in `tyr_beat` in `/srv/ed/input/<name_of_the_coverage>`.

The easiest way is to copy the data via docker:

`docker cp data/dumb_ntfs.zip navitiadockercompose_tyr_worker_1:/srv/ed/input/default/`

`navitiadockercompose_tyr_worker_1` is the name of the container, it can be different since it's dependant of the directory name.

(or you can change the docker-compose and make a shared volume).

Then you can query jormungandr:

http://localhost:9191/v1/coverage/default/lines

# additional instances
If you need additional instances, you can use the `docker-instances.jinja2` to generate another docker-compose file (if you want to do some shiny service discovery instead of this quick and dirty jinja template, we'll hapilly accept the contribution :wink: )

you'll need to install [j2cli](https://github.com/kolypto/j2cli)

`pip install "j2cli[yaml]"`

You need to provide the list of instances (the easiest way is to give it as a yaml file, check artemis/artemis_instances_list.yml for an example)

`j2 docker-instances.jinja2 my_instances_list.yml > additional_navitia_instances.yml`

Then you need to start the docker-compose with the additional instances

`docker-compose -f docker-compose.yml -f additional_navitia_instances.yml up`

To add data to a given instance, you'll need to do:

`docker cp data/dumb_ntfs.zip navitiadockercompose_tyr_worker_1:/srv/ed/input/<my_instance>`

# TODO
- move the tyr and kraken images to alpine :wink:
# docker-alpine

[![CircleCI](https://img.shields.io/circleci/project/gliderlabs/docker-alpine/release.svg)](https://circleci.com/gh/gliderlabs/docker-alpine)
[![Docker Stars](https://img.shields.io/docker/stars/gliderlabs/alpine.svg)][hub]
[![Docker Pulls](https://img.shields.io/docker/pulls/gliderlabs/alpine.svg)][hub]
[![Slack](http://glider-slackin.herokuapp.com/badge.svg)][slack]
[![ImageLayers](https://imagelayers.io/badge/gliderlabs/alpine:latest.svg)](https://imagelayers.io/?images=gliderlabs/alpine:latest 'Get your own badge on imagelayers.io')


A super small Docker image based on [Alpine Linux][alpine]. The image is only 5 MB and has access to a package repository that is much more complete than other BusyBox based images.

## Why?

Docker images today are big. Usually much larger than they need to be. There are a lot of ways to make them smaller, but the Docker populace still jumps to the `ubuntu` base image for most projects. The size savings over `ubuntu` and other bases are huge:

```
REPOSITORY          TAG           IMAGE ID          VIRTUAL SIZE
gliderlabs/alpine   latest        157314031a17      5.03 MB
debian              latest        4d6ce913b130      84.98 MB
ubuntu              latest        b39b81afc8ca      188.3 MB
centos              latest        8efe422e6104      210 MB
```

There are images such as `progrium/busybox` which get us very close to a minimal container and package system. But these particular BusyBox builds piggyback on the OpenWRT package index which is often lacking and not tailored towards generic everyday applications. Alpine Linux has a much more complete and up to date [package index][alpine-packages]:

```console
$ docker run progrium/busybox opkg-install nodejs
Unknown package 'nodejs'.
Collected errors:
* opkg_install_cmd: Cannot install package nodejs.

$ docker run gliderlabs/alpine apk add --no-cache nodejs
fetch http://alpine.gliderlabs.com/alpine/v3.3/main/x86_64/APKINDEX.tar.gz
fetch http://alpine.gliderlabs.com/alpine/v3.3/community/x86_64/APKINDEX.tar.gz
(1/4) Installing libgcc (5.3.0-r0)
(2/4) Installing libstdc++ (5.3.0-r0)
(3/4) Installing libuv (1.7.5-r0)
(4/4) Installing nodejs (4.2.3-r0)
Executing busybox-1.24.1-r7.trigger
OK: 29 MiB in 15 packages
```

This makes Alpine Linux a great image base for utilities and even production applications. [Read more about Alpine Linux here][alpine-about] and you can see how their mantra fits in right at home with Docker images.

## Usage

Stop doing this:

```dockerfile
FROM ubuntu-debootstrap:14.04
RUN apt-get update -q \
  && DEBIAN_FRONTEND=noninteractive apt-get install -qy mysql-client \
  && apt-get clean \
  && rm -rf /var/lib/apt
ENTRYPOINT ["mysql"]
```
This took 19 seconds to build and yields a 164 MB image. Eww. Start doing this:

```dockerfile
FROM gliderlabs/alpine:3.4
RUN apk add --no-cache mysql-client
ENTRYPOINT ["mysql"]
```

Only 3 seconds to build and results in a 36 MB image! Hooray!

## Documentation

This image is well documented. [Check out the documentation at Viewdocs][docs] and the `docs` directory in this repository.

## Contacts

We make reasonable efforts to support our work and are always happy to chat. Join us in [our Slack community][slack] or [submit a GitHub issue][issues] if you have a security or other general question about this Docker image. Please email [security](http://lists.alpinelinux.org/alpine-security/summary.html) or [user](http://lists.alpinelinux.org/alpine-user/summary.html) mailing lists if you have concerns specific to Alpine Linux.

## Inspiration

The motivation for this project and modifications to `mkimage.sh` are highly inspired by Eivind Uggedal (uggedal) and Luis Lavena (luislavena). They have made great strides in getting Alpine Linux running as a Docker container. Check out their [mini-container/base][mini-base] image as well.

## Sponsors

[![Fastly](https://github.com/gliderlabs/docker-alpine/raw/master/logo_fastly.png)][fastly]

[Fastly][fastly] provides the CDN for our Alpine Linux package repository. This is allows super speedy package downloads from all over the globe!

## License

The code in this repository, unless otherwise noted, is BSD licensed. See the `LICENSE` file in this repository.

[mini-base]: https://github.com/mini-containers/base
[alpine-packages]: http://pkgs.alpinelinux.org/
[alpine-about]: https://www.alpinelinux.org/about/
[docs]: http://gliderlabs.viewdocs.io/docker-alpine
[slack]: http://glider-slackin.herokuapp.com/
[issues]: https://github.com/gliderlabs/docker-alpine/issues
[alpine]: http://alpinelinux.org/
[fastly]: https://www.fastly.com/
[hub]: https://hub.docker.com/r/gliderlabs/alpine/
<img src="https://ga-beacon.appspot.com/UA-58928488-2/docker-alpine/readme?pixel" />
# Running

```
docker-compose up -d
```
or
```
docker run --name=unbound -d \
--restart=always \
-e UPSTREAM_DNS1=8.8.8.8 \
-e UPSTREAM_DNS2=8.8.4.4 \
-p 53:53 \
-p 53:53/udp \
athlinks/unbound-forwarder
```
# Distributed Highly Available Hadoop Cluster Docker Image Based on Alpine

[![DockerStars](https://img.shields.io/docker/stars/athlinks/hadoop.svg)](https://registry.hub.docker.com/u/athlinks/hadoop/)
[![DockerPulls](https://img.shields.io/docker/pulls/athlinks/hadoop.svg)](https://registry.hub.docker.com/u/athlinks/hadoop/)

## Standalone Mode

### Run
[![asciicast](https://asciinema.org/a/cbtwm3fv85ujsb4bjetix2x8a.png)](https://asciinema.org/a/cbtwm3fv85ujsb4bjetix2x8a)
```
docker run -d \
--name=hadoop-standalone \
-p 8088:8088 \
-p 50070:50070 \
-p 14000:14000 \
athlinks/hadoop:2.7 && \
docker logs -f hadoop-standalone
```

You can view the services here:</p>
HDFS: http://127.0.0.1:50070</p>
YARN: http://127.0.0.1:8088</p>
HTTPFS: http://127.0.0.1:14000

### Execute Test Job
```
docker exec -it hadoop-standalone bash -c "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar teragen 1000 teragen_out_dir"
```

## Clustered Mode

### Run
Start the cluster for the first time:
[![asciicast](https://asciinema.org/a/49052.png)](https://asciinema.org/a/49052)
```
git clone https://github.com/athlinks/docker-hadoop.git
cd docker-hadoop/hadoop-2.7
./initialize.sh
```

### Execute Test Job
```
docker exec -it hadoop27_client_1 bash -c "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar teragen 1000 teragen_out_dir"
```

If you have previously started and stopped the cluster, you can just run "docker-compose up -d" to restart it as the zookeeper format has already been done.
# sar-docker
Docker created specifically for SAR image processing
[![Build Status](https://travis-ci.org/makoto-nagai/docker-pyspark-pytest.svg?branch=master)](https://travis-ci.org/makoto-nagai/docker-pyspark-pytest)

## Overviews
* [makoto-nagai/docker-pyspark: Docker image of Apache Spark with its Python interface, pyspark.](https://github.com/makoto-nagai/docker-pyspark)
    * Base docker image
* pytest
    * 3.1.2
* pytest-cov
    * latest
* pytest-mock
    * 1.6.2
* pytest-pep8
    * latest
* pytest-faker
    * 2.0.0

To run `pytest`, you need to replace `repository_name` with your name of repository (more specifically directory name).

```
docker run -it --volume $(pwd):/tmp/repository_name --workdir /tmp/repository_name makotonagai/pyspark-pytest pytest
```

or by using short option

```
docker run -it -v $(pwd):/tmp/repository_name -w /tmp/repository_name makotonagai/pyspark-pytest pytest
```

For instance, run the examples,

```
docker run -it --volume $(pwd)/example:/tmp/example --workdir /tmp/example makotonagai/pyspark-pytest pytest
```


## Notes
If you change the version of pytest, you need to add git-tag to the commit.
For instance, if the version of pytest is `3.1.2`, you need to execute the following commands.

```
git tag -a pytest-3.1.2
git push origin pytest-3.1.2
```

Then you can push your commits to the remote repository.

# Docker images

A collection of Docker images and scripts.

Easier to share via GitHub than internal git repositories.

to create a self-signed certificate from the root ca run
sh create-kibana4-certificates.sh

files will be located in ./out directory
take files kibana4.crt (this is the public certificate) and kibana4_pk.pem and store them in the 
 security/certs folder they will be copied by the Dockerfile and used for SSL between the browser and the kibana server. 
 This link is made via the kibana.yml
 
     > sh create-kibana4-certificates.sh
     > cp out/kibana.crt out/kibana_pk.pem out/ca.crt .
     > git add and committo create a self-signed certificate from the root ca run
sh create-kibana4-certificates.sh

files will be located in ./out directory
take files kibana.crt (this is the public certificate) and kibana_pk.pem and store them in the 
 security/certs folder they will be copied by the Dockerfile and used for SSL between the browser and the kibana server. 
 This link is made via the kibana.yml
 
     > sh create-kibana5-certificates.sh
     > cp out/kibana.crt out/kibana_pk.pem out/ca.crt .
     > git add and commit# Notes
This container will attempt to modify the host ip tables.  It has to be ran has --privileged.

# SSL/TLS

Before you run this script you must ensure that the rta-security/certs 
directory contains a dcp_rta_ca.crt, dcp_rta_pk.pem and truststore.jks 

This script will create a new dcp self-signed certificate for our datastorage
nodes. Right now these certificates cannot be hostname verified i.e. we 
use the same secure cert for each node in the datastorage cluster as 
we do not have a DNS in JEDI this means we cannot trust the hostname
expect this to be fixed in the next release.

This script will run and will create an output directory called 
dcp_rta_datastorage_out. Copy the truststore and keystore java keystore 
files to the certs directory. The keystore is a password protected 
store of our public and private keys for the datastorage nodes and 
check them into git.

    > sh create_dcp_rta_datastorage_credentials.sh
    > cp dcp_rta_datastorage_out/*.jks dcp_rta_datastorage_out/*.p12 certs/
    > git add and commit

The .p12 file can be used to load into as a browser cert or with curl requests if
you want to contact the now secure elasticsearch database.

Useful info: if firefox does not like your certificate and says 
'Your certificate contains the same serial number as another certificate issued by the certificate authority' 

    > cd ~/.mozilla/firefox/{profile-name}.default
    > rm cert8.db    --OR-- $ mv cert8.db cert8.db.bak
    > Restart Firefox

# Elasticsearch performance

Also for good performance, one must do the following on the host:
$ sudo sysctl -w vm.max_map_count=262144
$ sudo echo 'vm.max_map_count=262144' >> /etc/sysctl.conf

See https://www.elastic.co/guide/en/elasticsearch/reference/2.1/setup-configuration.html, elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.

# Access

Default users are: 
- elastic: A superuser
- kibana: The user Kibana uses to connect and communicate with Elasticsearch.

Password is: changeme.
Before you run this script you must ensure that the rta-security/certs 
directory contains a dcp_rta_ca.crt, dcp_rta_pk.pem and truststore.jks 

This script will create a new dcp self-signed certificate for our datastorage
nodes. Right now these certificates cannot be hostname verified i.e. we 
use the same secure cert for each node in the datastorage cluster as 
we do not have a DNS in JEDI this means we cannot trust the hostname
expect this to be fixed in the next release.

This script will run and will create an output directory called 
dcp_rta_datastorage_out. Copy the truststore and keystore java keystore 
files to the certs directory. The keystore is a password protected 
store of our public and private keys for the datastorage nodes and 
check them into git.

    > sh create_node_certificates.sh.sh
    > cp out/*.jks out/*.p12 .
    > git add and commit

The .p12 file can be used to load into as a browser cert or with curl requests if
you want to contact the now secure elasticsearch database.

Useful info: if firefox does not like your certificate and says 
'Your certificate contains the same serial number as another certificate issued by the certificate authority' 

    > cd ~/.mozilla/firefox/{profile-name}.default
    > rm cert8.db    --OR-- $ mv cert8.db cert8.db.bak
    > Restart Firefox


    

# Build

As usual:
$ docker build  -t logstash5 .

# Usage

## Start Logstash with commandline configuration
If you need to run logstash with configuration provided on the commandline, you can use the logstash image as follows:

$ docker run -it --rm er/logstash5 logstash -e 'input { stdin { } } output { stdout { } }'

## Start Logstash with configuration file
If you need to run logstash with a configuration file, logstash.conf, that's located in your current directory, you can use the logstash image as follows:

$ docker run --rm -it -v $PWD:/testme logstash5 logstash -f /testme

Or you can use the test logstash.conf file located under ./test:
$ docker run --rm -it -v $PWD/test:/testme logstash5 logstash -f /testme

# Build

As usual:
$ docker build  -t logstash5 .

# Usage

## Start Logstash with commandline configuration
If you need to run logstash with configuration provided on the commandline, you can use the logstash image as follows:

$ docker run -it --rm er/logstash5 logstash -e 'input { stdin { } } output { stdout { } }'

## Start Logstash with configuration file
If you need to run logstash with a configuration file, logstash.conf, that's located in your current directory, you can use the logstash image as follows:

$ docker run --rm -it -v $PWD:/testme logstash5 logstash -f /testme

Or you can use the test logstash.conf file located under ./test:
$ docker run --rm -it -v $PWD/test:/testme logstash5 logstash -f /testme

# SSL/TLS

Before you run this script you must ensure that the rta-security/certs 
directory contains a dcp_rta_ca.crt, dcp_rta_pk.pem and truststore.jks 

This script will create a new dcp self-signed certificate for our datastorage
nodes. Right now these certificates cannot be hostname verified i.e. we 
use the same secure cert for each node in the datastorage cluster as 
we do not have a DNS in JEDI this means we cannot trust the hostname
expect this to be fixed in the next release.

This script will run and will create an output directory called 
dcp_rta_datastorage_out. Copy the truststore and keystore java keystore 
files to the certs directory. The keystore is a password protected 
store of our public and private keys for the datastorage nodes and 
check them into git.

    > sh create_dcp_rta_datastorage_credentials.sh
    > cp dcp_rta_datastorage_out/*.jks dcp_rta_datastorage_out/*.p12 certs/
    > git add and commit

The .p12 file can be used to load into as a browser cert or with curl requests if
you want to contact the now secure elasticsearch database.

Useful info: if firefox does not like your certificate and says 
'Your certificate contains the same serial number as another certificate issued by the certificate authority' 

    > cd ~/.mozilla/firefox/{profile-name}.default
    > rm cert8.db    --OR-- $ mv cert8.db cert8.db.bak
    > Restart Firefox

# Elasticsearch performance

Also for good performance, one must do the following on the host:
$ sudo sysctl -w vm.max_map_count=262144
$ sudo echo 'vm.max_map_count=262144' >> /etc/sysctl.conf

See https://www.elastic.co/guide/en/elasticsearch/reference/2.1/setup-configuration.html, elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.

# Access

Default users are: 
- elastic: A superuser
- kibana: The user Kibana uses to connect and communicate with Elasticsearch.

Password is: changeme.
Before you run this script you must ensure that the rta-security/certs 
directory contains a dcp_rta_ca.crt, dcp_rta_pk.pem and truststore.jks 

This script will create a new dcp self-signed certificate for our datastorage
nodes. Right now these certificates cannot be hostname verified i.e. we 
use the same secure cert for each node in the datastorage cluster as 
we do not have a DNS in JEDI this means we cannot trust the hostname
expect this to be fixed in the next release.

This script will run and will create an output directory called 
dcp_rta_datastorage_out. Copy the truststore and keystore java keystore 
files to the certs directory. The keystore is a password protected 
store of our public and private keys for the datastorage nodes and 
check them into git.

    > sh create_node_certificates.sh.sh
    > cp out/*.jks out/*.p12 .
    > git add and commit

The .p12 file can be used to load into as a browser cert or with curl requests if
you want to contact the now secure elasticsearch database.

Useful info: if firefox does not like your certificate and says 
'Your certificate contains the same serial number as another certificate issued by the certificate authority' 

    > cd ~/.mozilla/firefox/{profile-name}.default
    > rm cert8.db    --OR-- $ mv cert8.db cert8.db.bak
    > Restart Firefox


    

# spark-gradle-alpine-docker
https://github.com/perwendel/spark app example buit by gralde running in alpine linux on docker

Steps to launch this in your local computer: 
 1. In Windows, Mac or Linux install git, Java8, docker add them to your PATH
 2. In Windows or Mac install docker-machine
 3. In Windows or Mac run ```docker-machine create --driver virtualbox default```
 4. In Windows or Mac run ```docker-machine env```
 5. Pull this repo and change java code and tests
 6. Run following to launch the docker container:

```
gradlew build
docker-machine start default
docker-compose up
```

You will see something like following result:

```
Creating java_spark_1
Attaching to java_spark_1
spark_1  | hello world! today is 2017-02-15
```

To see how it's running in Windows:
```
FOR /f "tokens=*" %i IN ('docker-machine ip') DO curl %i:4567/hello
```
This results like:
```
curl 192.168.99.100:4567/hello
Hello World! today is 2016-04-09
```
# coreos docker

## init

    ssh core@yourserverip
    cd /home/core
    git clone https://github.com/ivories/docker.git
    chmod -R 777 docker/shell
    /home/core/docker/shell/shell_init
    export PATH="/home/core/docker/shell:$PATH"
    install
    install_web

## ssh-init

    ssh_config www.youname.com 

## set timezone

    sudo timedatectl set-timezone Asia/Shanghai

## set hostname

    sudo hostnamectl set-hostname yourname

## start/restart web service

    web

## install other service

    s bind                          # install bind server
    s samba                         # install samba share
    s git                           # install git server

## config the server

    cd /home/core/data/nginx
    vi nginx.conf # config nginx domain

    cd /home/core/data/php
    vi php.ini # config php.ini

    cd /home/core/data/mysql
    vi my.cnf # config my.cnf

## command list

    s service_name                  #start/restart fleetctl service
    p service_name                  #stop fleetctl service
    ss service_name                 #start/restart systemctl service
    st service_name                 #status systemctl service
    sp service_name                 #stop systemctl service
    fl                              #list all service
    fl service_name                 #list one fleetctl service(except global service)
    web                             #start/restart fleetctl web service
A Docker image with Yui Compressor

docker pull ivories/yuicompressor
echo "alias yuicompressor='docker run --rm -v $(pwd):/data -w /data ivories/yuicompressor java -jar /yuicompressor-2.4.8.jar'" >> ~/.bashrc
import repository 
use web create repository
git push --mirror ssh://git@hostname:33/user/repository.git
# Dockerizing thunder xware


## 使用

### 拉取镜像

```
docker pull ivories/thunder:1.0
```

### 创建一个下载目录. 用于挂载卷

```
mkdir data
```

### 运行

```
docker run -d --name=thunder -v $(pwd)/data:/app/TDDOWNLOAD ivories/thunder:1.0
```


### 查看日志(激活码)/到迅雷增加设备

```
docker logs thunder
```

```
// output:
starting xunlei service...
Connecting to 127.0.0.1:9000 (127.0.0.1:9000)
Connecting to 127.0.0.1:9000 (127.0.0.1:9000)
Connecting to 127.0.0.1:9000 (127.0.0.1:9000)
.........................
getting xunlei service info...
THE ACTIVE CODE IS: xxx
go to http://yuancheng.xunlei.com, bind your device with the active code.
finished.
```

docker network create openclinica-nw
docker run -d --name openclinica-db --net openclinica-nw -v /home/core/data/openclinica/database:/var/lib/postgresql/data ivories/openclinica-db
docker run -d --name openclinica --net openclinica-nw -p 8080:8080 -v /home/core/data/openclinica/openclinica.data:/usr/local/tomcat/openclinica.data ivories/openclinica

http://ip:8080/OpenClinica
user:root
pass:12345678

import demo:
start openclinica-db
go in openclinica-db
  
    dropdb -U postgres openclinica
    psql -U postgres
    CREATE DATABASE openclinica WITH ENCODING='UTF8' OWNER=clinica;
    find / -name *.backup
    pg_restore -Upostgres -d openclinica < /var/lib/postgresql/data/OpenClinica_JUNO_Demo_Database.backup

README
# Reverse SSH tunnel

> Extension of tifayuki/reverse-ssh-tunnel container, all thanks go to him.

## Usage (using `docker run`)

> Go to `Usage using docker compose` to run the necessary containers easier

At the final state you should have this container running 2 times, one on each machine (public server,behind firewall server).

### On public server, run:
```
[public]$ docker run -d \
    -e ROOT_PASS=<your_password> \
    -p <your_sshd_port>:22 \
    -p <forwarding_behind_firewall_port>:1080 \
    ivories/reverse_ssh:1.0
```
Parameters:
```
  <your_password> is the password used for NATed host to connect the public host
  <your_sshd port> is the port for NATed host to connect to
  <forwarding_behind_firewall_port> is the port allows others to direct access behind firewall server
```

### On behind firewall server, aka destination

[firewall server]$ docker run -d \
    -e PUBLIC_HOST_ADDR=<public_host_address> \
    -e PUBLIC_HOST_PORT=<public_host_port> \
    -e ROOT_PASS=<your_password> \
    ivories/reverse_ssh:1.0
```
Parameters:
```
  <public_host_address> is the ip address or domain of your public host
  <public_host_port> is the same as <your sshd port> set on the public host
  <your_passorwd> is the same as <your passorwd> set on the public host
```

### On client, connect to behind firewall server
```
[client]$ ssh -p <forwarding_behind_firewall_port> root@<public_host_address>
```

Example
-------

On public server(`111.112.113.114`):
```
  docker run -d -e ROOT_PASS=mypass -p 53333:22 -p 51080:1080 ivories/reverse_ssh:1.0
```
On firewall server:
```
  docker run -d -e PUBLIC_HOST_ADDR=111.112.113.114 -e PUBLIC_HOST_PORT=53333 -e ROOT_PASS=mypass ivories/reverse_ssh:1.0
```
On your laptop:
```
  ssh -p 51080 user@111.112.113.114
```
#Problem

When remove /var/lib/etcd2/member,we will set a new token for etcd2.
initital-cluster-token etcd-cluster-newValue
if not set the new value,the etcd2 will shutdown have no error.

A service depend etcd,will stop the etcd2.and will infinite loop,and you will not start etcd2 again.
Slove: copy the etcd2.service to /etc/systemd/system/etcd.service and systemctl start etcd,etcd2 is replace 
etcd,so you can open the etcd2, close the wrong service.the problem was sloved.

sudo systemctl stop etcd2
etcd2 --name infra0  --initial-advertise-peer-urls http://10.162.89.49:2380 \
  --listen-peer-urls http://10.162.89.49:2380 \
  --listen-client-urls http://10.162.89.49:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://10.162.89.49:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster infra0=http://10.162.89.49:2380,infra1=http://10.162.58.57:2380,infra2=http://10.162.56.145:2380  \
  --initial-cluster-state new

sudo systemctl stop etcd2
etcd2 --name infra2  --initial-advertise-peer-urls http://10.162.56.145:2380 \
  --listen-peer-urls http://10.162.56.145:2380 \
  --listen-client-urls http://10.162.56.145:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://10.162.56.145:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster infra0=http://10.162.89.49:2380,infra1=http://10.162.58.57:2380,infra2=http://10.162.56.145:2380  \
  --initial-cluster-state new


sudo systemctl stop etcd2
etcd2 --name infra1  --initial-advertise-peer-urls http://10.162.58.57:2380 \
  --listen-peer-urls http://10.162.58.57:2380 \
  --listen-client-urls http://10.162.58.57:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://10.162.58.57:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster infra0=http://10.162.89.49:2380,infra1=http://10.162.58.57:2380,infra2=http://10.162.56.145:2380  \
  --initial-cluster-state new


docker run -it --rm --net=host --privileged=true -v /home/core/data1:/gluster ivories/gluster:1.0 /bin/bash
service glusterfs-server start 
gluster peer probe master.hadoop.com && gluster peer probe slave1.hadoop.com && gluster peer probe slave2.hadoop.com && gluster peer probe slave3.hadoop.com


gluster volume create data master.hadoop.com:/gluster/brick slave1.hadoop.com:/gluster/brick slave2.hadoop.com:/gluster/brick slave3.hadoop.com:/gluster/brick
gluster volume create data replica 4 transport tcp master.hadoop.com:/gluster/brick slave1.hadoop.com:/gluster/brick slave2.hadoop.com:/gluster/brick slave3.hadoop.com:/gluster/brick


gluster volume start data
mount -t glusterfs 127.0.0.1:/data /data
echo "/data 127.0.0.1(rw,sync,no_root_squash)" /etc/exports

gluster pool list

echo "UUID=0dd7f959-8174-44d5-b5a4-a77b394e6ad7" > /var/lib/glusterd/glusterd.info
echo "operating-version=30800" >> /var/lib/glusterd/glusterd.info
# Distributed Highly Available Hadoop Cluster Docker Image Based on Alpine

[![DockerStars](https://img.shields.io/docker/stars/athlinks/hadoop.svg)](https://registry.hub.docker.com/u/athlinks/hadoop/)
[![DockerPulls](https://img.shields.io/docker/pulls/athlinks/hadoop.svg)](https://registry.hub.docker.com/u/athlinks/hadoop/)

## Standalone Mode

### Run
[![asciicast](https://asciinema.org/a/cbtwm3fv85ujsb4bjetix2x8a.png)](https://asciinema.org/a/cbtwm3fv85ujsb4bjetix2x8a)
```
docker run -d \
--name=hadoop-standalone \
-p 8088:8088 \
-p 50070:50070 \
-p 14000:14000 \
athlinks/hadoop:2.7 && \
docker logs -f hadoop-standalone
```

You can view the services here:</p>
HDFS: http://127.0.0.1:50070</p>
YARN: http://127.0.0.1:8088</p>
HTTPFS: http://127.0.0.1:14000

### Execute Test Job
```
docker exec -it hadoop-standalone bash -c "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 1000 teragen_out_dir"
```

## Clustered Mode

### Run
Start the cluster for the first time:
[![asciicast](https://asciinema.org/a/49052.png)](https://asciinema.org/a/49052)
```
git clone https://github.com/athlinks/docker-hadoop.git
cd docker-hadoop/hadoop-2.7
./initialize.sh
```

### Execute Test Job
```
docker exec -it hadoop27_client_1 bash -c "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 1000 teragen_out_dir"
```

If you have previously started and stopped the cluster, you can just run "docker-compose up -d" to restart it as the zookeeper format has already been done.
https://webtorrent.io/desktop/
https://github.com/webtorrent/webtorrent
http://pytool.com/2016/10/08/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4-2016-01-01-Linux%E5%91%BD%E4%BB%A4-aria2/
https://aria2.github.io/manual/en/html/aria2c.html#download-files-with-remote-metalink
aria2c -x2 -k1M "http://host/file.zip"

# PHPSTORM

* Preferences -> search "xdebug debug port" -> change "Debug port:9003"
* search "IDE KEY HOST PORT" -> change IDE key: "IDE key: PHPSTORM; Host: 172.17.8.101 ; Port: 9003;"
* Button Click: Start Listerning for PHP Debug Connections

# COREOS

* sudo vi ~/data/php/php.ini
* change xdebug open
* web

# CHROME

* url: chrome://extensions
* drag ~/docker/php/xdebug/xdebug.crx to install
* xdebug helper change IDE key to PhpStorm
* xdebug icon change to debug/profile/trace

# PHPSTORM

* open your php script set a breakpoint
* Remote file path is not mapped to any file path
* Click to setup up path mappings
* select the /Data/Web/data/www/project to /www/project

# PROBLEM

* all is done, and port is listen, make sure xdebug.remote_host is your ip.
scifgif
=======

[![Circle CI](https://circleci.com/gh/blacktop/scifgif.png?style=shield)](https://circleci.com/gh/blacktop/scifgif) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/scifgif.svg)](https://store.docker.com/community/images/blacktop/scifgif) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/scifgif.svg)](https://store.docker.com/community/images/blacktop/scifgif) [![Docker Image](https://img.shields.io/badge/docker%20image-2GB-blue.svg)](https://store.docker.com/community/images/blacktop/scifgif)

> Humorous image microservice for isolated networks - `xkcd|giphy` + **keyword/phrase** search API

---

### Dependencies

-	[alpine:3.6](https://hub.docker.com/_/alpine/)
-	[blacktop/elasticsearch:5.5](https://hub.docker.com/r/blacktop/elasticsearch/)

### Image Tags

```bash
REPOSITORY           TAG                 SIZE
blacktop/scifgif     latest              2GB
blacktop/scifgif     0.2.0               2GB
blacktop/scifgif     0.1.0               2GB
```

> **NOTE:** the reason the docker image is so large is that it contains ~2000 animated gifs *(1500 reactions, 250 futurama and 250 star wars)*

Getting Started
---------------

```bash
$ docker run --init -d --name scifgif -p 3993:3993 blacktop/scifgif --host localhost
```

Documentation
-------------

-	[API Docs](http://docs.scifgif.apiary.io)
-	[Use with Mattermost](https://github.com/blacktop/scifgif/blob/master/docs/mattermost.md)

![mattermost](https://raw.githubusercontent.com/blacktop/scifgif/master/docs/imgs/mattermost.png)

### TODO

-	[x] Add meta-data DB for keyword text search (elasticsearch)
-	[x] Add docs for creating [Mattermost](https://github.com/mattermost/platform) slash command or integration
-	[ ] Add ability to use expansion packs (use tag to control types of images available)
-	[ ] Add ascii art emojis (table flippers etc)
-	[ ] Add ephemeral slash command help

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/scifgif/issues/new)

### License

MIT Copyright (c) 2017 **blacktop**

![giphy](https://raw.githubusercontent.com/blacktop/scifgif/master/docs/PoweredBy_200_Horizontal_Light-Backgrounds_With_Logo.gif)
This repository holds supplemental Go packages for low-level interactions with the operating system.

To submit changes to this repository, see http://golang.org/doc/contribute.html.
# Building `sys/unix`

The sys/unix package provides access to the raw system call interface of the
underlying operating system. See: https://godoc.org/golang.org/x/sys/unix

Porting Go to a new architecture/OS combination or adding syscalls, types, or
constants to an existing architecture/OS pair requires some manual effort;
however, there are tools that automate much of the process.

## Build Systems

There are currently two ways we generate the necessary files. We are currently
migrating the build system to use containers so the builds are reproducible.
This is being done on an OS-by-OS basis. Please update this documentation as
components of the build system change.

### Old Build System (currently for `GOOS != "Linux" || GOARCH == "sparc64"`)

The old build system generates the Go files based on the C header files
present on your system. This means that files
for a given GOOS/GOARCH pair must be generated on a system with that OS and
architecture. This also means that the generated code can differ from system
to system, based on differences in the header files.

To avoid this, if you are using the old build system, only generate the Go
files on an installation with unmodified header files. It is also important to
keep track of which version of the OS the files were generated from (ex.
Darwin 14 vs Darwin 15). This makes it easier to track the progress of changes
and have each OS upgrade correspond to a single change.

To build the files for your current OS and architecture, make sure GOOS and
GOARCH are set correctly and run `mkall.sh`. This will generate the files for
your specific system. Running `mkall.sh -n` shows the commands that will be run.

Requirements: bash, perl, go

### New Build System (currently for `GOOS == "Linux" && GOARCH != "sparc64"`)

The new build system uses a Docker container to generate the go files directly
from source checkouts of the kernel and various system libraries. This means
that on any platform that supports Docker, all the files using the new build
system can be generated at once, and generated files will not change based on
what the person running the scripts has installed on their computer.

The OS specific files for the new build system are located in the `${GOOS}`
directory, and the build is coordinated by the `${GOOS}/mkall.go` program. When
the kernel or system library updates, modify the Dockerfile at
`${GOOS}/Dockerfile` to checkout the new release of the source.

To build all the files under the new build system, you must be on an amd64/Linux
system and have your GOOS and GOARCH set accordingly. Running `mkall.sh` will
then generate all of the files for all of the GOOS/GOARCH pairs in the new build
system. Running `mkall.sh -n` shows the commands that will be run.

Requirements: bash, perl, go, docker

## Component files

This section describes the various files used in the code generation process.
It also contains instructions on how to modify these files to add a new
architecture/OS or to add additional syscalls, types, or constants. Note that
if you are using the new build system, the scripts cannot be called normally.
They must be called from within the docker container.

### asm files

The hand-written assembly file at `asm_${GOOS}_${GOARCH}.s` implements system
call dispatch. There are three entry points:
```
  func Syscall(trap, a1, a2, a3 uintptr) (r1, r2, err uintptr)
  func Syscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
  func RawSyscall(trap, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```
The first and second are the standard ones; they differ only in how many
arguments can be passed to the kernel. The third is for low-level use by the
ForkExec wrapper. Unlike the first two, it does not call into the scheduler to
let it know that a system call is running.

When porting Go to an new architecture/OS, this file must be implemented for
each GOOS/GOARCH pair.

### mksysnum

Mksysnum is a script located at `${GOOS}/mksysnum.pl` (or `mksysnum_${GOOS}.pl`
for the old system). This script takes in a list of header files containing the
syscall number declarations and parses them to produce the corresponding list of
Go numeric constants. See `zsysnum_${GOOS}_${GOARCH}.go` for the generated
constants.

Adding new syscall numbers is mostly done by running the build on a sufficiently
new installation of the target OS (or updating the source checkouts for the
new build system). However, depending on the OS, you make need to update the
parsing in mksysnum.

### mksyscall.pl

The `syscall.go`, `syscall_${GOOS}.go`, `syscall_${GOOS}_${GOARCH}.go` are
hand-written Go files which implement system calls (for unix, the specific OS,
or the specific OS/Architecture pair respectively) that need special handling
and list `//sys` comments giving prototypes for ones that can be generated.

The mksyscall.pl script takes the `//sys` and `//sysnb` comments and converts
them into syscalls. This requires the name of the prototype in the comment to
match a syscall number in the `zsysnum_${GOOS}_${GOARCH}.go` file. The function
prototype can be exported (capitalized) or not.

Adding a new syscall often just requires adding a new `//sys` function prototype
with the desired arguments and a capitalized name so it is exported. However, if
you want the interface to the syscall to be different, often one will make an
unexported `//sys` prototype, an then write a custom wrapper in
`syscall_${GOOS}.go`.

### types files

For each OS, there is a hand-written Go file at `${GOOS}/types.go` (or
`types_${GOOS}.go` on the old system). This file includes standard C headers and
creates Go type aliases to the corresponding C types. The file is then fed
through godef to get the Go compatible definitions. Finally, the generated code
is fed though mkpost.go to format the code correctly and remove any hidden or
private identifiers. This cleaned-up code is written to
`ztypes_${GOOS}_${GOARCH}.go`.

The hardest part about preparing this file is figuring out which headers to
include and which symbols need to be `#define`d to get the actual data
structures that pass through to the kernel system calls. Some C libraries
preset alternate versions for binary compatibility and translate them on the
way in and out of system calls, but there is almost always a `#define` that can
get the real ones.
See `types_darwin.go` and `linux/types.go` for examples.

To add a new type, add in the necessary include statement at the top of the
file (if it is not already there) and add in a type alias line. Note that if
your type is significantly different on different architectures, you may need
some `#if/#elif` macros in your include statements.

### mkerrors.sh

This script is used to generate the system's various constants. This doesn't
just include the error numbers and error strings, but also the signal numbers
an a wide variety of miscellaneous constants. The constants come from the list
of include files in the `includes_${uname}` variable. A regex then picks out
the desired `#define` statements, and generates the corresponding Go constants.
The error numbers and strings are generated from `#include <errno.h>`, and the
signal numbers and strings are generated from `#include <signal.h>`. All of
these constants are written to `zerrors_${GOOS}_${GOARCH}.go` via a C program,
`_errors.c`, which prints out all the constants.

To add a constant, add the header that includes it to the appropriate variable.
Then, edit the regex (if necessary) to match the desired constant. Avoid making
the regex too broad to avoid matching unintended constants.


## Generated files

### `zerror_${GOOS}_${GOARCH}.go`

A file containing all of the system's generated error numbers, error strings,
signal numbers, and constants. Generated by `mkerrors.sh` (see above).

### `zsyscall_${GOOS}_${GOARCH}.go`

A file containing all the generated syscalls for a specific GOOS and GOARCH.
Generated by `mksyscall.pl` (see above).

### `zsysnum_${GOOS}_${GOARCH}.go`

A list of numeric constants for all the syscall number of the specific GOOS
and GOARCH. Generated by mksysnum (see above).

### `ztypes_${GOOS}_${GOARCH}.go`

A file containing Go types for passing into (or returning from) syscalls.
Generated by godefs and the types file (see above).
This repository holds supplementary Go networking libraries.

To submit changes to this repository, see http://golang.org/doc/contribute.html.
The *.dat files in this directory are copied from The WebKit Open Source
Project, specifically $WEBKITROOT/LayoutTests/html5lib/resources.
WebKit is licensed under a BSD style license.
http://webkit.org/coding/bsd-license.html says:

Copyright (C) 2009 Apple Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

These test cases come from
http://www.w3.org/International/tests/repository/html5/the-input-byte-stream/results-basics

Distributed under both the W3C Test Suite License
(http://www.w3.org/Consortium/Legal/2008/04-testsuite-license)
and the W3C 3-clause BSD License
(http://www.w3.org/Consortium/Legal/2008/03-bsd-license).
To contribute to a W3C Test Suite, see the policies and contribution
forms (http://www.w3.org/2004/10/27-testcases).
This is a work-in-progress HTTP/2 implementation for Go.

It will eventually live in the Go standard library and won't require
any changes to your code to use.  It will just be automatic.

Status:

* The server support is pretty good. A few things are missing
  but are being worked on.
* The client work has just started but shares a lot of code
  is coming along much quicker.

Docs are at https://godoc.org/golang.org/x/net/http2

Demo test server at https://http2.golang.org/

Help & bug reports welcome!

Contributing: https://golang.org/doc/contribute.html
Bugs:         https://golang.org/issue/new?title=x/net/http2:+
# h2i

**h2i** is an interactive HTTP/2 ("h2") console debugger. Miss the good ol'
days of telnetting to your HTTP/1.n servers? We're bringing you
back.

Features:
- send raw HTTP/2 frames
 - PING
 - SETTINGS
 - HEADERS
 - etc
- type in HTTP/1.n and have it auto-HPACK/frame-ify it for HTTP/2
- pretty print all received HTTP/2 frames from the peer (including HPACK decoding)
- tab completion of commands, options

Not yet features, but soon:
- unnecessary CONTINUATION frames on short boundaries, to test peer implementations 
- request bodies (DATA frames)
- send invalid frames for testing server implementations (supported by underlying Framer)

Later:
- act like a server

## Installation

```
$ go get golang.org/x/net/http2/h2i
$ h2i <host>
```

## Demo

```
$ h2i
Usage: h2i <hostname>
  
  -insecure
        Whether to skip TLS cert validation
  -nextproto string
        Comma-separated list of NPN/ALPN protocol names to negotiate. (default "h2,h2-14")

$ h2i google.com
Connecting to google.com:443 ...
Connected to 74.125.224.41:443
Negotiated protocol "h2-14"
[FrameHeader SETTINGS len=18]
  [MAX_CONCURRENT_STREAMS = 100]
  [INITIAL_WINDOW_SIZE = 1048576]
  [MAX_FRAME_SIZE = 16384]
[FrameHeader WINDOW_UPDATE len=4]
  Window-Increment = 983041
  
h2i> PING h2iSayHI
[FrameHeader PING flags=ACK len=8]
  Data = "h2iSayHI"
h2i> headers
(as HTTP/1.1)> GET / HTTP/1.1
(as HTTP/1.1)> Host: ip.appspot.com
(as HTTP/1.1)> User-Agent: h2i/brad-n-blake
(as HTTP/1.1)>  
Opening Stream-ID 1:
 :authority = ip.appspot.com
 :method = GET
 :path = /
 :scheme = https
 user-agent = h2i/brad-n-blake
[FrameHeader HEADERS flags=END_HEADERS stream=1 len=77]
  :status = "200"
  alternate-protocol = "443:quic,p=1"
  content-length = "15"
  content-type = "text/html"
  date = "Fri, 01 May 2015 23:06:56 GMT"
  server = "Google Frontend"
[FrameHeader DATA flags=END_STREAM stream=1 len=15]
  "173.164.155.78\n"
[FrameHeader PING len=8]
  Data = "\x00\x00\x00\x00\x00\x00\x00\x00"
h2i> ping  
[FrameHeader PING flags=ACK len=8]  
  Data = "h2i_ping"  
h2i> ping  
[FrameHeader PING flags=ACK len=8]
  Data = "h2i_ping"
h2i> ping
[FrameHeader GOAWAY len=22]
  Last-Stream-ID = 1; Error-Code = PROTOCOL_ERROR (1)

ReadFrame: EOF
```

## Status

Quick few hour hack. So much yet to do. Feel free to file issues for
bugs or wishlist items, but [@bmizerany](https://github.com/bmizerany/)
and I aren't yet accepting pull requests until things settle down.


Client:
 -- Firefox nightly with about:config network.http.spdy.enabled.http2draft set true
 -- Chrome: go to chrome://flags/#enable-spdy4, save and restart (button at bottom)

Make CA:
$ openssl genrsa -out rootCA.key 2048
$ openssl req -x509 -new -nodes -key rootCA.key -days 1024 -out rootCA.pem
... install that to Firefox

Make cert:
$ openssl genrsa -out server.key 2048
$ openssl req -new -key server.key -out server.csr
$ openssl x509 -req -in server.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server.crt -days 500


This is a fork of the encoding/xml package at ca1d6c4, the last commit before
https://go.googlesource.com/go/+/c0d6d33 "encoding/xml: restore Go 1.4 name
space behavior" made late in the lead-up to the Go 1.5 release.

The list of encoding/xml changes is at
https://go.googlesource.com/go/+log/master/src/encoding/xml

This fork is temporary, and I (nigeltao) expect to revert it after Go 1.6 is
released.

See http://golang.org/issue/11841
# Elastic

Elastic is an [Elasticsearch](http://www.elasticsearch.org/) client for the
[Go](http://www.golang.org/) programming language.

[![Build Status](https://travis-ci.org/olivere/elastic.svg?branch=release-branch.v5)](https://travis-ci.org/olivere/elastic)
[![Godoc](http://img.shields.io/badge/godoc-reference-blue.svg?style=flat)](http://godoc.org/gopkg.in/olivere/elastic.v5)
[![license](http://img.shields.io/badge/license-MIT-red.svg?style=flat)](https://raw.githubusercontent.com/olivere/elastic/master/LICENSE)

See the [wiki](https://github.com/olivere/elastic/wiki) for additional information about Elastic.


## Releases

**The release branches (e.g. [`release-branch.v5`](https://github.com/olivere/elastic/tree/release-branch.v5))
are actively being worked on and can break at any time.
If you want to use stable versions of Elastic, please use the packages released via [gopkg.in](https://gopkg.in).**

Here's the version matrix:

Elasticsearch version | Elastic version -| Package URL
----------------------|------------------|------------
5.x                   | 5.0              | [`gopkg.in/olivere/elastic.v5`](https://gopkg.in/olivere/elastic.v5) ([source](https://github.com/olivere/elastic/tree/release-branch.v5) [doc](http://godoc.org/gopkg.in/olivere/elastic.v5))
2.x                   | 3.0              | [`gopkg.in/olivere/elastic.v3`](https://gopkg.in/olivere/elastic.v3) ([source](https://github.com/olivere/elastic/tree/release-branch.v3) [doc](http://godoc.org/gopkg.in/olivere/elastic.v3))
1.x                   | 2.0              | [`gopkg.in/olivere/elastic.v2`](https://gopkg.in/olivere/elastic.v2) ([source](https://github.com/olivere/elastic/tree/release-branch.v2) [doc](http://godoc.org/gopkg.in/olivere/elastic.v2))
0.9-1.3               | 1.0              | [`gopkg.in/olivere/elastic.v1`](https://gopkg.in/olivere/elastic.v1) ([source](https://github.com/olivere/elastic/tree/release-branch.v1) [doc](http://godoc.org/gopkg.in/olivere/elastic.v1))

**Example:**

You have installed Elasticsearch 5.0.0 and want to use Elastic.
As listed above, you should use Elastic 5.0.
So you first install the stable release of Elastic 5.0 from gopkg.in.

```sh
$ go get gopkg.in/olivere/elastic.v5
```

You then import it with this import path:

```go
import elastic "gopkg.in/olivere/elastic.v5"
```

### Elastic 5.0

Elastic 5.0 targets Elasticsearch 5.0.0 and later. Elasticsearch 5.0.0 was
[released on 26th October 2016](https://www.elastic.co/blog/elasticsearch-5-0-0-released).

Notice that there are will be a lot of [breaking changes in Elasticsearch 5.0](https://www.elastic.co/guide/en/elasticsearch/reference/5.0/breaking-changes-5.0.html)
and we used this as an opportunity to [clean up and refactor Elastic](https://github.com/olivere/elastic/blob/release-branch.v5/CHANGELOG-5.0.md)
as we did in the transition from Elastic 2.0 (for Elasticsearch 1.x) to Elastic 3.0 (for Elasticsearch 2.x).

Furthermore, the jump in version numbers will give us a chance to be in sync with the Elastic Stack.

### Elastic 3.0

Elastic 3.0 targets Elasticsearch 2.x and is published via [`gopkg.in/olivere/elastic.v3`](https://gopkg.in/olivere/elastic.v3).

Elastic 3.0 will only get critical bug fixes. You should update to a recent version.

### Elastic 2.0

Elastic 2.0 targets Elasticsearch 1.x and is published via [`gopkg.in/olivere/elastic.v2`](https://gopkg.in/olivere/elastic.v2).

Elastic 2.0 will only get critical bug fixes. You should update to a recent version.

### Elastic 1.0

Elastic 1.0 is deprecated. You should really update Elasticsearch and Elastic
to a recent version.

However, if you cannot update for some reason, don't worry. Version 1.0 is
still available. All you need to do is go-get it and change your import path
as described above.


## Status

We use Elastic in production since 2012. Elastic is stable but the API changes
now and then. We strive for API compatibility.
However, Elasticsearch sometimes introduces [breaking changes](https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes.html)
and we sometimes have to adapt.

Having said that, there have been no big API changes that required you
to rewrite your application big time. More often than not it's renaming APIs
and adding/removing features so that Elastic is in sync with Elasticsearch.

Elastic has been used in production with the following Elasticsearch versions:
0.90, 1.0-1.7, and 2.0-2.4.1. Furthermore, we use [Travis CI](https://travis-ci.org/)
to test Elastic with the most recent versions of Elasticsearch and Go.
See the [.travis.yml](https://github.com/olivere/elastic/blob/master/.travis.yml)
file for the exact matrix and [Travis](https://travis-ci.org/olivere/elastic)
for the results.

Elasticsearch has quite a few features. Most of them are implemented
by Elastic. I add features and APIs as required. It's straightforward
to implement missing pieces. I'm accepting pull requests :-)

Having said that, I hope you find the project useful.


## Getting Started

The first thing you do is to create a [Client](https://github.com/olivere/elastic/blob/master/client.go).
The client connects to Elasticsearch on `http://127.0.0.1:9200` by default.

You typically create one client for your app. Here's a complete example of
creating a client, creating an index, adding a document, executing a search etc.

```go
// Create a context
ctx := context.Background()

// Create a client
client, err := elastic.NewClient()
if err != nil {
    // Handle error
    panic(err)
}

// Create an index
_, err = client.CreateIndex("twitter").Do(ctx)
if err != nil {
    // Handle error
    panic(err)
}

// Add a document to the index
tweet := Tweet{User: "olivere", Message: "Take Five"}
_, err = client.Index().
    Index("twitter").
    Type("tweet").
    Id("1").
    BodyJson(tweet).
    Refresh("true").
    Do(ctx)
if err != nil {
    // Handle error
    panic(err)
}

// Search with a term query
termQuery := elastic.NewTermQuery("user", "olivere")
searchResult, err := client.Search().
    Index("twitter").   // search in index "twitter"
    Query(termQuery).   // specify the query
    Sort("user", true). // sort by "user" field, ascending
    From(0).Size(10).   // take documents 0-9
    Pretty(true).       // pretty print request and response JSON
    Do(ctx)             // execute
if err != nil {
    // Handle error
    panic(err)
}

// searchResult is of type SearchResult and returns hits, suggestions,
// and all kinds of other information from Elasticsearch.
fmt.Printf("Query took %d milliseconds\n", searchResult.TookInMillis)

// Each is a convenience function that iterates over hits in a search result.
// It makes sure you don't need to check for nil values in the response.
// However, it ignores errors in serialization. If you want full control
// over iterating the hits, see below.
var ttyp Tweet
for _, item := range searchResult.Each(reflect.TypeOf(ttyp)) {
    if t, ok := item.(Tweet); ok {
        fmt.Printf("Tweet by %s: %s\n", t.User, t.Message)
    }
}
// TotalHits is another convenience function that works even when something goes wrong.
fmt.Printf("Found a total of %d tweets\n", searchResult.TotalHits())

// Here's how you iterate through results with full control over each step.
if searchResult.Hits.TotalHits > 0 {
    fmt.Printf("Found a total of %d tweets\n", searchResult.Hits.TotalHits)

    // Iterate through results
    for _, hit := range searchResult.Hits.Hits {
        // hit.Index contains the name of the index

        // Deserialize hit.Source into a Tweet (could also be just a map[string]interface{}).
        var t Tweet
        err := json.Unmarshal(*hit.Source, &t)
        if err != nil {
            // Deserialization failed
        }

        // Work with tweet
        fmt.Printf("Tweet by %s: %s\n", t.User, t.Message)
    }
} else {
    // No hits
    fmt.Print("Found no tweets\n")
}

// Delete the index again
_, err = client.DeleteIndex("twitter").Do(ctx)
if err != nil {
    // Handle error
    panic(err)
}
```

Here's a [link to a complete working example](https://gist.github.com/olivere/114347ff9d9cfdca7bdc0ecea8b82263).

See the [wiki](https://github.com/olivere/elastic/wiki) for more details.


## API Status

### Document APIs

- [x] Index API
- [x] Get API
- [x] Delete API
- [x] Delete By Query API
- [x] Update API
- [x] Update By Query API
- [x] Multi Get API
- [x] Bulk API
- [x] Reindex API
- [x] Term Vectors
- [x] Multi termvectors API

### Search APIs

- [x] Search
- [x] Search Template
- [ ] Multi Search Template
- [ ] Search Shards API
- [x] Suggesters
  - [x] Term Suggester
  - [x] Phrase Suggester
  - [x] Completion Suggester
  - [x] Context Suggester
- [x] Multi Search API
- [x] Count API
- [ ] Search Exists API
- [ ] Validate API
- [x] Explain API
- [x] Profile API
- [x] Field Stats API

### Aggregations

- Metrics Aggregations
  - [x] Avg
  - [x] Cardinality
  - [x] Extended Stats
  - [x] Geo Bounds
  - [ ] Geo Centroid
  - [x] Max
  - [x] Min
  - [x] Percentiles
  - [x] Percentile Ranks
  - [ ] Scripted Metric
  - [x] Stats
  - [x] Sum
  - [x] Top Hits
  - [x] Value Count
- Bucket Aggregations
  - [x] Children
  - [x] Date Histogram
  - [x] Date Range
  - [x] Filter
  - [x] Filters
  - [x] Geo Distance
  - [ ] GeoHash Grid
  - [x] Global
  - [x] Histogram
  - [x] IP Range
  - [x] Missing
  - [x] Nested
  - [x] Range
  - [x] Reverse Nested
  - [x] Sampler
  - [x] Significant Terms
  - [x] Terms
- Pipeline Aggregations
  - [x] Avg Bucket
  - [x] Derivative
  - [x] Max Bucket
  - [x] Min Bucket
  - [x] Sum Bucket
  - [x] Stats Bucket
  - [ ] Extended Stats Bucket
  - [x] Percentiles Bucket
  - [x] Moving Average
  - [x] Cumulative Sum
  - [x] Bucket Script
  - [x] Bucket Selector
  - [x] Serial Differencing
- [x] Matrix Aggregations
  - [x] Matrix Stats
- [x] Aggregation Metadata

### Indices APIs

- [x] Create Index
- [x] Delete Index
- [x] Get Index
- [x] Indices Exists
- [x] Open / Close Index
- [x] Shrink Index
- [x] Rollover Index
- [x] Put Mapping
- [x] Get Mapping
- [x] Get Field Mapping
- [x] Types Exists
- [x] Index Aliases
- [x] Update Indices Settings
- [x] Get Settings
- [x] Analyze
- [x] Index Templates
- [ ] Shadow Replica Indices
- [x] Indices Stats
- [ ] Indices Segments
- [ ] Indices Recovery
- [ ] Indices Shard Stores
- [ ] Clear Cache
- [x] Flush
- [x] Refresh
- [x] Force Merge
- [ ] Upgrade

### cat APIs

The cat APIs are not implemented as of now. We think they are better suited for operating with Elasticsearch on the command line.

- [ ] cat aliases
- [ ] cat allocation
- [ ] cat count
- [ ] cat fielddata
- [ ] cat health
- [ ] cat indices
- [ ] cat master
- [ ] cat nodeattrs
- [ ] cat nodes
- [ ] cat pending tasks
- [ ] cat plugins
- [ ] cat recovery
- [ ] cat repositories
- [ ] cat thread pool
- [ ] cat shards
- [ ] cat segments
- [ ] cat snapshots

### Cluster APIs

- [x] Cluster Health
- [x] Cluster State
- [x] Cluster Stats
- [ ] Pending Cluster Tasks
- [ ] Cluster Reroute
- [ ] Cluster Update Settings
- [x] Nodes Stats
- [x] Nodes Info
- [x] Task Management API
- [ ] Nodes hot_threads
- [ ] Cluster Allocation Explain API

### Query DSL

- [x] Match All Query
- [x] Inner hits
- Full text queries
  - [x] Match Query
  - [x] Match Phrase Query
  - [x] Match Phrase Prefix Query
  - [x] Multi Match Query
  - [x] Common Terms Query
  - [x] Query String Query
  - [x] Simple Query String Query
- Term level queries
  - [x] Term Query
  - [x] Terms Query
  - [x] Range Query
  - [x] Exists Query
  - [x] Prefix Query
  - [x] Wildcard Query
  - [x] Regexp Query
  - [x] Fuzzy Query
  - [x] Type Query
  - [x] Ids Query
- Compound queries
  - [x] Constant Score Query
  - [x] Bool Query
  - [x] Dis Max Query
  - [x] Function Score Query
  - [x] Boosting Query
  - [x] Indices Query
- Joining queries
  - [x] Nested Query
  - [x] Has Child Query
  - [x] Has Parent Query
  - [x] Parent Id Query
- Geo queries
  - [ ] GeoShape Query
  - [x] Geo Bounding Box Query
  - [x] Geo Distance Query
  - [ ] Geo Distance Range Query
  - [x] Geo Polygon Query
  - [ ] Geohash Cell Query
- Specialized queries
  - [x] More Like This Query
  - [x] Template Query
  - [x] Script Query
  - [x] Percolate Query
- Span queries
  - [ ] Span Term Query
  - [ ] Span Multi Term Query
  - [ ] Span First Query
  - [ ] Span Near Query
  - [ ] Span Or Query
  - [ ] Span Not Query
  - [ ] Span Containing Query
  - [ ] Span Within Query
  - [ ] Span Field Masking Query
- [ ] Minimum Should Match
- [ ] Multi Term Query Rewrite

### Modules

- Snapshot and Restore
  - [x] Repositories
  - [ ] Snapshot
  - [ ] Restore
  - [ ] Snapshot status
  - [ ] Monitoring snapshot/restore status
  - [ ] Stopping currently running snapshot and restore

### Sorting

- [x] Sort by score
- [x] Sort by field
- [x] Sort by geo distance
- [x] Sort by script
- [x] Sort by doc

### Scrolling

Scrolling is supported via a  `ScrollService`. It supports an iterator-like interface.
The `ClearScroll` API is implemented as well.

A pattern for [efficiently scrolling in parallel](https://github.com/olivere/elastic/wiki/ScrollParallel)
is described in the [Wiki](https://github.com/olivere/elastic/wiki).

## How to contribute

Read [the contribution guidelines](https://github.com/olivere/elastic/blob/master/CONTRIBUTING.md).

## Credits

Thanks a lot for the great folks working hard on
[Elasticsearch](https://www.elastic.co/products/elasticsearch)
and
[Go](https://golang.org/).

Elastic uses portions of the
[uritemplates](https://github.com/jtacoma/uritemplates) library
by Joshua Tacoma,
[backoff](https://github.com/cenkalti/backoff) by Cenk Altı and
[leaktest](https://github.com/fortytw2/leaktest) by Ian Chiles.

## LICENSE

MIT-LICENSE. See [LICENSE](http://olivere.mit-license.org/)
or the LICENSE file provided in the repository for details.
# Cluster Test

This directory contains a program you can use to test a cluster.

Here's how:

First, install a cluster of Elasticsearch nodes. You can install them on
different computers, or start several nodes on a single machine.

Build cluster-test by `go build cluster-test.go` (or build with `make`).

Run `./cluster-test -h` to get a list of flags:

```sh
$ ./cluster-test -h
Usage of ./cluster-test:
  -errorlog="": error log file
  -healthcheck=true: enable or disable healthchecks
  -healthchecker=1m0s: healthcheck interval
  -index="twitter": name of ES index to use
  -infolog="": info log file
  -n=5: number of goroutines that run searches
  -nodes="": comma-separated list of ES URLs (e.g. 'http://192.168.2.10:9200,http://192.168.2.11:9200')
  -retries=0: number of retries
  -sniff=true: enable or disable sniffer
  -sniffer=15m0s: sniffer interval
  -tracelog="": trace log file
```

Example:

```sh
$ ./cluster-test -nodes=http://127.0.0.1:9200,http://127.0.0.1:9201,http://127.0.0.1:9202 -n=5 -index=twitter -retries=5 -sniff=true -sniffer=10s -healthcheck=true -healthchecker=5s -errorlog=error.log
```

The above example will create an index and start some search jobs on the
cluster defined by http://127.0.0.1:9200, http://127.0.0.1:9201,
and http://127.0.0.1:9202.

* It will create an index called `twitter` on the cluster (`-index=twitter`)
* It will run 5 search jobs in parallel (`-n=5`).
* It will retry failed requests 5 times (`-retries=5`).
* It will sniff the cluster periodically (`-sniff=true`).
* It will sniff the cluster every 10 seconds (`-sniffer=10s`).
* It will perform health checks periodically (`-healthcheck=true`).
* It will perform health checks on the nodes every 5 seconds (`-healthchecker=5s`).
* It will write an error log file (`-errorlog=error.log`).

If you want to test Elastic with nodes going up and down, you can use a
chaos monkey script like this and run it on the nodes of your cluster:

```sh
#!/bin/bash
while true
do
	echo "Starting ES node"
	elasticsearch -d -Xmx4g -Xms1g -Des.config=elasticsearch.yml -p es.pid
	sleep `jot -r 1 10 300` # wait for 10-300s
	echo "Stopping ES node"
	kill -TERM `cat es.pid`
	sleep `jot -r 1 10 60`  # wait for 10-60s
done
```
# goquery - a little like that j-thing, only in Go [![build status](https://secure.travis-ci.org/PuerkitoBio/goquery.png)](http://travis-ci.org/PuerkitoBio/goquery) [![GoDoc](https://godoc.org/github.com/PuerkitoBio/goquery?status.png)](http://godoc.org/github.com/PuerkitoBio/goquery)

goquery brings a syntax and a set of features similar to [jQuery][] to the [Go language][go]. It is based on Go's [net/html package][html] and the CSS Selector library [cascadia][]. Since the net/html parser returns nodes, and not a full-featured DOM tree, jQuery's stateful manipulation functions (like height(), css(), detach()) have been left off.

Also, because the net/html parser requires UTF-8 encoding, so does goquery: it is the caller's responsibility to ensure that the source document provides UTF-8 encoded HTML. See the [wiki][] for various options to do this.

Syntax-wise, it is as close as possible to jQuery, with the same function names when possible, and that warm and fuzzy chainable interface. jQuery being the ultra-popular library that it is, I felt that writing a similar HTML-manipulating library was better to follow its API than to start anew (in the same spirit as Go's `fmt` package), even though some of its methods are less than intuitive (looking at you, [index()][index]...).

## Installation

Please note that because of the net/html dependency, goquery requires Go1.1+.

    $ go get github.com/PuerkitoBio/goquery

(optional) To run unit tests:

    $ cd $GOPATH/src/github.com/PuerkitoBio/goquery
    $ go test

(optional) To run benchmarks (warning: it runs for a few minutes):

    $ cd $GOPATH/src/github.com/PuerkitoBio/goquery
    $ go test -bench=".*"

## Changelog

**Note that goquery's API is now stable, and will not break.**

*    **2017-02-12 (v1.1.0)** : Add `SetHtml` and `SetText` (thanks to @glebtv).
*    **2016-12-29 (v1.0.2)** : Optimize allocations for `Selection.Text` (thanks to @radovskyb).
*    **2016-08-28 (v1.0.1)** : Optimize performance for large documents.
*    **2016-07-27 (v1.0.0)** : Tag version 1.0.0.
*    **2016-06-15** : Invalid selector strings internally compile to a `Matcher` implementation that never matches any node (instead of a panic). So for example, `doc.Find("~")` returns an empty `*Selection` object.
*    **2016-02-02** : Add `NodeName` utility function similar to the DOM's `nodeName` property. It returns the tag name of the first element in a selection, and other relevant values of non-element nodes (see godoc for details). Add `OuterHtml` utility function similar to the DOM's `outerHTML` property (named `OuterHtml` in small caps for consistency with the existing `Html` method on the `Selection`).
*    **2015-04-20** : Add `AttrOr` helper method to return the attribute's value or a default value if absent. Thanks to [piotrkowalczuk][piotr].
*    **2015-02-04** : Add more manipulation functions - Prepend* - thanks again to [Andrew Stone][thatguystone].
*    **2014-11-28** : Add more manipulation functions - ReplaceWith*, Wrap* and Unwrap - thanks again to [Andrew Stone][thatguystone].
*    **2014-11-07** : Add manipulation functions (thanks to [Andrew Stone][thatguystone]) and `*Matcher` functions, that receive compiled cascadia selectors instead of selector strings, thus avoiding potential panics thrown by goquery via `cascadia.MustCompile` calls. This results in better performance (selectors can be compiled once and reused) and more idiomatic error handling (you can handle cascadia's compilation errors, instead of recovering from panics, which had been bugging me for a long time). Note that the actual type expected is a `Matcher` interface, that `cascadia.Selector` implements. Other matcher implementations could be used.
*    **2014-11-06** : Change import paths of net/html to golang.org/x/net/html (see https://groups.google.com/forum/#!topic/golang-nuts/eD8dh3T9yyA). Make sure to update your code to use the new import path too when you call goquery with `html.Node`s.
*    **v0.3.2** : Add `NewDocumentFromReader()` (thanks jweir) which allows creating a goquery document from an io.Reader.
*    **v0.3.1** : Add `NewDocumentFromResponse()` (thanks assassingj) which allows creating a goquery document from an http response.
*    **v0.3.0** : Add `EachWithBreak()` which allows to break out of an `Each()` loop by returning false. This function was added instead of changing the existing `Each()` to avoid breaking compatibility.
*    **v0.2.1** : Make go-getable, now that [go.net/html is Go1.0-compatible][gonet] (thanks to @matrixik for pointing this out).
*    **v0.2.0** : Add support for negative indices in Slice(). **BREAKING CHANGE** `Document.Root` is removed, `Document` is now a `Selection` itself (a selection of one, the root element, just like `Document.Root` was before). Add jQuery's Closest() method.
*    **v0.1.1** : Add benchmarks to use as baseline for refactorings, refactor Next...() and Prev...() methods to use the new html package's linked list features (Next/PrevSibling, FirstChild). Good performance boost (40+% in some cases).
*    **v0.1.0** : Initial release.

## API

goquery exposes two structs, `Document` and `Selection`, and the `Matcher` interface. Unlike jQuery, which is loaded as part of a DOM document, and thus acts on its containing document, goquery doesn't know which HTML document to act upon. So it needs to be told, and that's what the `Document` type is for. It holds the root document node as the initial Selection value to manipulate.

jQuery often has many variants for the same function (no argument, a selector string argument, a jQuery object argument, a DOM element argument, ...). Instead of exposing the same features in goquery as a single method with variadic empty interface arguments, statically-typed signatures are used following this naming convention:

*    When the jQuery equivalent can be called with no argument, it has the same name as jQuery for the no argument signature (e.g.: `Prev()`), and the version with a selector string argument is called `XxxFiltered()` (e.g.: `PrevFiltered()`)
*    When the jQuery equivalent **requires** one argument, the same name as jQuery is used for the selector string version (e.g.: `Is()`)
*    The signatures accepting a jQuery object as argument are defined in goquery as `XxxSelection()` and take a `*Selection` object as argument (e.g.: `FilterSelection()`)
*    The signatures accepting a DOM element as argument in jQuery are defined in goquery as `XxxNodes()` and take a variadic argument of type `*html.Node` (e.g.: `FilterNodes()`)
*    The signatures accepting a function as argument in jQuery are defined in goquery as `XxxFunction()` and take a function as argument (e.g.: `FilterFunction()`)
*    The goquery methods that can be called with a selector string have a corresponding version that take a `Matcher` interface and are defined as `XxxMatcher()` (e.g.: `IsMatcher()`)

Utility functions that are not in jQuery but are useful in Go are implemented as functions (that take a `*Selection` as parameter), to avoid a potential naming clash on the `*Selection`'s methods (reserved for jQuery-equivalent behaviour).

The complete [godoc reference documentation can be found here][doc].

Please note that Cascadia's selectors do not necessarily match all supported selectors of jQuery (Sizzle). See the [cascadia project][cascadia] for details. Invalid selector strings compile to a `Matcher` that fails to match any node. Behaviour of the various functions that take a selector string as argument follows from that fact, e.g. (where `~` is an invalid selector string):

* `Find("~")` returns an empty selection because the selector string doesn't match anything.
* `Add("~")` returns a new selection that holds the same nodes as the original selection, because it didn't add any node (selector string didn't match anything).
* `ParentsFiltered("~")` returns an empty selection because the selector string doesn't match anything.
* `ParentsUntil("~")` returns all parents of the selection because the selector string didn't match any element to stop before the top element.

## Examples

See some tips and tricks in the [wiki][].

Adapted from example_test.go:

```Go
package main

import (
  "fmt"
  "log"

  "github.com/PuerkitoBio/goquery"
)

func ExampleScrape() {
  doc, err := goquery.NewDocument("http://metalsucks.net")
  if err != nil {
    log.Fatal(err)
  }

  // Find the review items
  doc.Find(".sidebar-reviews article .content-block").Each(func(i int, s *goquery.Selection) {
    // For each item found, get the band and title
    band := s.Find("a").Text()
    title := s.Find("i").Text()
    fmt.Printf("Review %d: %s - %s\n", i, band, title)
  })
}

func main() {
  ExampleScrape()
}
```

## License

The [BSD 3-Clause license][bsd], the same as the [Go language][golic]. Cascadia's license is [here][caslic].

[jquery]: http://jquery.com/
[go]: http://golang.org/
[cascadia]: https://github.com/andybalholm/cascadia
[bsd]: http://opensource.org/licenses/BSD-3-Clause
[golic]: http://golang.org/LICENSE
[caslic]: https://github.com/andybalholm/cascadia/blob/master/LICENSE
[doc]: http://godoc.org/github.com/PuerkitoBio/goquery
[index]: http://api.jquery.com/index/
[gonet]: https://github.com/golang/net/
[html]: http://godoc.org/golang.org/x/net/html
[wiki]: https://github.com/PuerkitoBio/goquery/wiki/Tips-and-tricks
[thatguystone]: https://github.com/thatguystone
[piotr]: https://github.com/piotrkowalczuk
# errors [![Travis-CI](https://travis-ci.org/pkg/errors.svg)](https://travis-ci.org/pkg/errors) [![AppVeyor](https://ci.appveyor.com/api/projects/status/b98mptawhudj53ep/branch/master?svg=true)](https://ci.appveyor.com/project/davecheney/errors/branch/master) [![GoDoc](https://godoc.org/github.com/pkg/errors?status.svg)](http://godoc.org/github.com/pkg/errors) [![Report card](https://goreportcard.com/badge/github.com/pkg/errors)](https://goreportcard.com/report/github.com/pkg/errors)

Package errors provides simple error handling primitives.

`go get github.com/pkg/errors`

The traditional error handling idiom in Go is roughly akin to
```go
if err != nil {
        return err
}
```
which applied recursively up the call stack results in error reports without context or debugging information. The errors package allows programmers to add context to the failure path in their code in a way that does not destroy the original value of the error.

## Adding context to an error

The errors.Wrap function returns a new error that adds context to the original error. For example
```go
_, err := ioutil.ReadAll(r)
if err != nil {
        return errors.Wrap(err, "read failed")
}
```
## Retrieving the cause of an error

Using `errors.Wrap` constructs a stack of errors, adding context to the preceding error. Depending on the nature of the error it may be necessary to reverse the operation of errors.Wrap to retrieve the original error for inspection. Any error value which implements this interface can be inspected by `errors.Cause`.
```go
type causer interface {
        Cause() error
}
```
`errors.Cause` will recursively retrieve the topmost error which does not implement `causer`, which is assumed to be the original cause. For example:
```go
switch err := errors.Cause(err).(type) {
case *MyError:
        // handle specifically
default:
        // unknown error
}
```

[Read the package documentation for more information](https://godoc.org/github.com/pkg/errors).

## Contributing

We welcome pull requests, bug fixes and issue reports. With that said, the bar for adding new symbols to this package is intentionally set high.

Before proposing a change, please discuss your change by raising an issue.

## Licence

BSD-2-Clause
# Logrus <img src="http://i.imgur.com/hTeVwmJ.png" width="40" height="40" alt=":walrus:" class="emoji" title=":walrus:"/>&nbsp;[![Build Status](https://travis-ci.org/sirupsen/logrus.svg?branch=master)](https://travis-ci.org/sirupsen/logrus)&nbsp;[![GoDoc](https://godoc.org/github.com/sirupsen/logrus?status.svg)](https://godoc.org/github.com/sirupsen/logrus)

Logrus is a structured logger for Go (golang), completely API compatible with
the standard library logger. [Godoc][godoc].

**Seeing weird case-sensitive problems?** It's in the past been possible to
import Logrus as both upper- and lower-case. Due to the Go package environment,
this caused issues in the community and we needed a standard. Some environments
experienced problems with the upper-case variant, so the lower-case was decided.
Everything using `logrus` will need to use the lower-case:
`github.com/sirupsen/logrus`. Any package that isn't, should be changed.

To fix Glide, see [these
comments](https://github.com/sirupsen/logrus/issues/553#issuecomment-306591437).
For an in-depth explanation of the casing issue, see [this
comment](https://github.com/sirupsen/logrus/issues/570#issuecomment-313933276).

**Are you interested in assisting in maintaining Logrus?** Currently I have a
lot of obligations, and I am unable to provide Logrus with the maintainership it
needs. If you'd like to help, please reach out to me at `simon at author's
username dot com`.

Nicely color-coded in development (when a TTY is attached, otherwise just
plain text):

![Colored](http://i.imgur.com/PY7qMwd.png)

With `log.SetFormatter(&log.JSONFormatter{})`, for easy parsing by logstash
or Splunk:

```json
{"animal":"walrus","level":"info","msg":"A group of walrus emerges from the
ocean","size":10,"time":"2014-03-10 19:57:38.562264131 -0400 EDT"}

{"level":"warning","msg":"The group's number increased tremendously!",
"number":122,"omg":true,"time":"2014-03-10 19:57:38.562471297 -0400 EDT"}

{"animal":"walrus","level":"info","msg":"A giant walrus appears!",
"size":10,"time":"2014-03-10 19:57:38.562500591 -0400 EDT"}

{"animal":"walrus","level":"info","msg":"Tremendously sized cow enters the ocean.",
"size":9,"time":"2014-03-10 19:57:38.562527896 -0400 EDT"}

{"level":"fatal","msg":"The ice breaks!","number":100,"omg":true,
"time":"2014-03-10 19:57:38.562543128 -0400 EDT"}
```

With the default `log.SetFormatter(&log.TextFormatter{})` when a TTY is not
attached, the output is compatible with the
[logfmt](http://godoc.org/github.com/kr/logfmt) format:

```text
time="2015-03-26T01:27:38-04:00" level=debug msg="Started observing beach" animal=walrus number=8
time="2015-03-26T01:27:38-04:00" level=info msg="A group of walrus emerges from the ocean" animal=walrus size=10
time="2015-03-26T01:27:38-04:00" level=warning msg="The group's number increased tremendously!" number=122 omg=true
time="2015-03-26T01:27:38-04:00" level=debug msg="Temperature changes" temperature=-4
time="2015-03-26T01:27:38-04:00" level=panic msg="It's over 9000!" animal=orca size=9009
time="2015-03-26T01:27:38-04:00" level=fatal msg="The ice breaks!" err=&{0x2082280c0 map[animal:orca size:9009] 2015-03-26 01:27:38.441574009 -0400 EDT panic It's over 9000!} number=100 omg=true
exit status 1
```

#### Case-sensitivity

The organization's name was changed to lower-case--and this will not be changed
back. If you are getting import conflicts due to case sensitivity, please use
the lower-case import: `github.com/sirupsen/logrus`.

#### Example

The simplest way to use Logrus is simply the package-level exported logger:

```go
package main

import (
  log "github.com/sirupsen/logrus"
)

func main() {
  log.WithFields(log.Fields{
    "animal": "walrus",
  }).Info("A walrus appears")
}
```

Note that it's completely api-compatible with the stdlib logger, so you can
replace your `log` imports everywhere with `log "github.com/sirupsen/logrus"`
and you'll now have the flexibility of Logrus. You can customize it all you
want:

```go
package main

import (
  "os"
  log "github.com/sirupsen/logrus"
)

func init() {
  // Log as JSON instead of the default ASCII formatter.
  log.SetFormatter(&log.JSONFormatter{})

  // Output to stdout instead of the default stderr
  // Can be any io.Writer, see below for File example
  log.SetOutput(os.Stdout)

  // Only log the warning severity or above.
  log.SetLevel(log.WarnLevel)
}

func main() {
  log.WithFields(log.Fields{
    "animal": "walrus",
    "size":   10,
  }).Info("A group of walrus emerges from the ocean")

  log.WithFields(log.Fields{
    "omg":    true,
    "number": 122,
  }).Warn("The group's number increased tremendously!")

  log.WithFields(log.Fields{
    "omg":    true,
    "number": 100,
  }).Fatal("The ice breaks!")

  // A common pattern is to re-use fields between logging statements by re-using
  // the logrus.Entry returned from WithFields()
  contextLogger := log.WithFields(log.Fields{
    "common": "this is a common field",
    "other": "I also should be logged always",
  })

  contextLogger.Info("I'll be logged with common and other field")
  contextLogger.Info("Me too")
}
```

For more advanced usage such as logging to multiple locations from the same
application, you can also create an instance of the `logrus` Logger:

```go
package main

import (
  "os"
  "github.com/sirupsen/logrus"
)

// Create a new instance of the logger. You can have any number of instances.
var log = logrus.New()

func main() {
  // The API for setting attributes is a little different than the package level
  // exported logger. See Godoc.
  log.Out = os.Stdout

  // You could set this to any `io.Writer` such as a file
  // file, err := os.OpenFile("logrus.log", os.O_CREATE|os.O_WRONLY, 0666)
  // if err == nil {
  //  log.Out = file
  // } else {
  //  log.Info("Failed to log to file, using default stderr")
  // }

  log.WithFields(logrus.Fields{
    "animal": "walrus",
    "size":   10,
  }).Info("A group of walrus emerges from the ocean")
}
```

#### Fields

Logrus encourages careful, structured logging through logging fields instead of
long, unparseable error messages. For example, instead of: `log.Fatalf("Failed
to send event %s to topic %s with key %d")`, you should log the much more
discoverable:

```go
log.WithFields(log.Fields{
  "event": event,
  "topic": topic,
  "key": key,
}).Fatal("Failed to send event")
```

We've found this API forces you to think about logging in a way that produces
much more useful logging messages. We've been in countless situations where just
a single added field to a log statement that was already there would've saved us
hours. The `WithFields` call is optional.

In general, with Logrus using any of the `printf`-family functions should be
seen as a hint you should add a field, however, you can still use the
`printf`-family functions with Logrus.

#### Default Fields

Often it's helpful to have fields _always_ attached to log statements in an
application or parts of one. For example, you may want to always log the
`request_id` and `user_ip` in the context of a request. Instead of writing
`log.WithFields(log.Fields{"request_id": request_id, "user_ip": user_ip})` on
every line, you can create a `logrus.Entry` to pass around instead:

```go
requestLogger := log.WithFields(log.Fields{"request_id": request_id, "user_ip": user_ip})
requestLogger.Info("something happened on that request") # will log request_id and user_ip
requestLogger.Warn("something not great happened")
```

#### Hooks

You can add hooks for logging levels. For example to send errors to an exception
tracking service on `Error`, `Fatal` and `Panic`, info to StatsD or log to
multiple places simultaneously, e.g. syslog.

Logrus comes with [built-in hooks](hooks/). Add those, or your custom hook, in
`init`:

```go
import (
  log "github.com/sirupsen/logrus"
  "gopkg.in/gemnasium/logrus-airbrake-hook.v2" // the package is named "aibrake"
  logrus_syslog "github.com/sirupsen/logrus/hooks/syslog"
  "log/syslog"
)

func init() {

  // Use the Airbrake hook to report errors that have Error severity or above to
  // an exception tracker. You can create custom hooks, see the Hooks section.
  log.AddHook(airbrake.NewHook(123, "xyz", "production"))

  hook, err := logrus_syslog.NewSyslogHook("udp", "localhost:514", syslog.LOG_INFO, "")
  if err != nil {
    log.Error("Unable to connect to local syslog daemon")
  } else {
    log.AddHook(hook)
  }
}
```
Note: Syslog hook also support connecting to local syslog (Ex. "/dev/log" or "/var/run/syslog" or "/var/run/log"). For the detail, please check the [syslog hook README](hooks/syslog/README.md).

| Hook  | Description |
| ----- | ----------- |
| [Airbrake "legacy"](https://github.com/gemnasium/logrus-airbrake-legacy-hook) | Send errors to an exception tracking service compatible with the Airbrake API V2. Uses [`airbrake-go`](https://github.com/tobi/airbrake-go) behind the scenes. |
| [Airbrake](https://github.com/gemnasium/logrus-airbrake-hook) | Send errors to the Airbrake API V3. Uses the official [`gobrake`](https://github.com/airbrake/gobrake) behind the scenes. |
| [Amazon Kinesis](https://github.com/evalphobia/logrus_kinesis) | Hook for logging to [Amazon Kinesis](https://aws.amazon.com/kinesis/) |
| [Amqp-Hook](https://github.com/vladoatanasov/logrus_amqp) | Hook for logging to Amqp broker (Like RabbitMQ) |
| [Bugsnag](https://github.com/Shopify/logrus-bugsnag/blob/master/bugsnag.go) | Send errors to the Bugsnag exception tracking service. |
| [DeferPanic](https://github.com/deferpanic/dp-logrus) | Hook for logging to DeferPanic |
| [Discordrus](https://github.com/kz/discordrus) | Hook for logging to [Discord](https://discordapp.com/) |
| [ElasticSearch](https://github.com/sohlich/elogrus) | Hook for logging to ElasticSearch|
| [Firehose](https://github.com/beaubrewer/logrus_firehose) | Hook for logging to [Amazon Firehose](https://aws.amazon.com/kinesis/firehose/)
| [Fluentd](https://github.com/evalphobia/logrus_fluent) | Hook for logging to fluentd |
| [Go-Slack](https://github.com/multiplay/go-slack) | Hook for logging to [Slack](https://slack.com) |
| [Graylog](https://github.com/gemnasium/logrus-graylog-hook) | Hook for logging to [Graylog](http://graylog2.org/) |
| [Hiprus](https://github.com/nubo/hiprus) | Send errors to a channel in hipchat. |
| [Honeybadger](https://github.com/agonzalezro/logrus_honeybadger) | Hook for sending exceptions to Honeybadger |
| [InfluxDB](https://github.com/Abramovic/logrus_influxdb) | Hook for logging to influxdb |
| [Influxus](http://github.com/vlad-doru/influxus) | Hook for concurrently logging to [InfluxDB](http://influxdata.com/) |
| [Journalhook](https://github.com/wercker/journalhook) | Hook for logging to `systemd-journald` |
| [KafkaLogrus](https://github.com/goibibo/KafkaLogrus) | Hook for logging to kafka |
| [LFShook](https://github.com/rifflock/lfshook) | Hook for logging to the local filesystem |
| [Logentries](https://github.com/jcftang/logentriesrus) | Hook for logging to [Logentries](https://logentries.com/) |
| [Logentrus](https://github.com/puddingfactory/logentrus) | Hook for logging to [Logentries](https://logentries.com/) |
| [Logmatic.io](https://github.com/logmatic/logmatic-go) | Hook for logging to [Logmatic.io](http://logmatic.io/) |
| [Logrusly](https://github.com/sebest/logrusly) | Send logs to [Loggly](https://www.loggly.com/) |
| [Logstash](https://github.com/bshuster-repo/logrus-logstash-hook) | Hook for logging to [Logstash](https://www.elastic.co/products/logstash) |
| [Mail](https://github.com/zbindenren/logrus_mail) | Hook for sending exceptions via mail |
| [Mattermost](https://github.com/shuLhan/mattermost-integration/tree/master/hooks/logrus) | Hook for logging to [Mattermost](https://mattermost.com/) |
| [Mongodb](https://github.com/weekface/mgorus) | Hook for logging to mongodb |
| [NATS-Hook](https://github.com/rybit/nats_logrus_hook) | Hook for logging to [NATS](https://nats.io) |
| [Octokit](https://github.com/dorajistyle/logrus-octokit-hook) | Hook for logging to github via octokit |
| [Papertrail](https://github.com/polds/logrus-papertrail-hook) | Send errors to the [Papertrail](https://papertrailapp.com) hosted logging service via UDP. |
| [PostgreSQL](https://github.com/gemnasium/logrus-postgresql-hook) | Send logs to [PostgreSQL](http://postgresql.org) |
| [Pushover](https://github.com/toorop/logrus_pushover) | Send error via [Pushover](https://pushover.net) |
| [Raygun](https://github.com/squirkle/logrus-raygun-hook) | Hook for logging to [Raygun.io](http://raygun.io/) |
| [Redis-Hook](https://github.com/rogierlommers/logrus-redis-hook) | Hook for logging to a ELK stack (through Redis) |
| [Rollrus](https://github.com/heroku/rollrus) | Hook for sending errors to rollbar |
| [Scribe](https://github.com/sagar8192/logrus-scribe-hook) | Hook for logging to [Scribe](https://github.com/facebookarchive/scribe)|
| [Sentry](https://github.com/evalphobia/logrus_sentry) | Send errors to the Sentry error logging and aggregation service. |
| [Slackrus](https://github.com/johntdyer/slackrus) | Hook for Slack chat. |
| [Stackdriver](https://github.com/knq/sdhook) | Hook for logging to [Google Stackdriver](https://cloud.google.com/logging/) |
| [Sumorus](https://github.com/doublefree/sumorus) | Hook for logging to [SumoLogic](https://www.sumologic.com/)|
| [Syslog](https://github.com/sirupsen/logrus/blob/master/hooks/syslog/syslog.go) | Send errors to remote syslog server. Uses standard library `log/syslog` behind the scenes. |
| [Syslog TLS](https://github.com/shinji62/logrus-syslog-ng) | Send errors to remote syslog server with TLS support. |
| [TraceView](https://github.com/evalphobia/logrus_appneta) | Hook for logging to [AppNeta TraceView](https://www.appneta.com/products/traceview/) |
| [Typetalk](https://github.com/dragon3/logrus-typetalk-hook) | Hook for logging to [Typetalk](https://www.typetalk.in/) |
| [logz.io](https://github.com/ripcurld00d/logrus-logzio-hook) | Hook for logging to [logz.io](https://logz.io), a Log as a Service using Logstash |
| [SQS-Hook](https://github.com/tsarpaul/logrus_sqs) | Hook for logging to [Amazon Simple Queue Service (SQS)](https://aws.amazon.com/sqs/) |

#### Level logging

Logrus has six logging levels: Debug, Info, Warning, Error, Fatal and Panic.

```go
log.Debug("Useful debugging information.")
log.Info("Something noteworthy happened!")
log.Warn("You should probably take a look at this.")
log.Error("Something failed but I'm not quitting.")
// Calls os.Exit(1) after logging
log.Fatal("Bye.")
// Calls panic() after logging
log.Panic("I'm bailing.")
```

You can set the logging level on a `Logger`, then it will only log entries with
that severity or anything above it:

```go
// Will log anything that is info or above (warn, error, fatal, panic). Default.
log.SetLevel(log.InfoLevel)
```

It may be useful to set `log.Level = logrus.DebugLevel` in a debug or verbose
environment if your application has that.

#### Entries

Besides the fields added with `WithField` or `WithFields` some fields are
automatically added to all logging events:

1. `time`. The timestamp when the entry was created.
2. `msg`. The logging message passed to `{Info,Warn,Error,Fatal,Panic}` after
   the `AddFields` call. E.g. `Failed to send event.`
3. `level`. The logging level. E.g. `info`.

#### Environments

Logrus has no notion of environment.

If you wish for hooks and formatters to only be used in specific environments,
you should handle that yourself. For example, if your application has a global
variable `Environment`, which is a string representation of the environment you
could do:

```go
import (
  log "github.com/sirupsen/logrus"
)

init() {
  // do something here to set environment depending on an environment variable
  // or command-line flag
  if Environment == "production" {
    log.SetFormatter(&log.JSONFormatter{})
  } else {
    // The TextFormatter is default, you don't actually have to do this.
    log.SetFormatter(&log.TextFormatter{})
  }
}
```

This configuration is how `logrus` was intended to be used, but JSON in
production is mostly only useful if you do log aggregation with tools like
Splunk or Logstash.

#### Formatters

The built-in logging formatters are:

* `logrus.TextFormatter`. Logs the event in colors if stdout is a tty, otherwise
  without colors.
  * *Note:* to force colored output when there is no TTY, set the `ForceColors`
    field to `true`.  To force no colored output even if there is a TTY  set the
    `DisableColors` field to `true`. For Windows, see
    [github.com/mattn/go-colorable](https://github.com/mattn/go-colorable).
  * All options are listed in the [generated docs](https://godoc.org/github.com/sirupsen/logrus#TextFormatter).
* `logrus.JSONFormatter`. Logs fields as JSON.
  * All options are listed in the [generated docs](https://godoc.org/github.com/sirupsen/logrus#JSONFormatter).

Third party logging formatters:

* [`logstash`](https://github.com/bshuster-repo/logrus-logstash-hook). Logs fields as [Logstash](http://logstash.net) Events.
* [`prefixed`](https://github.com/x-cray/logrus-prefixed-formatter). Displays log entry source along with alternative layout.
* [`zalgo`](https://github.com/aybabtme/logzalgo). Invoking the P͉̫o̳̼̊w̖͈̰͎e̬͔̭͂r͚̼̹̲ ̫͓͉̳͈ō̠͕͖̚f̝͍̠ ͕̲̞͖͑Z̖̫̤̫ͪa͉̬͈̗l͖͎g̳̥o̰̥̅!̣͔̲̻͊̄ ̙̘̦̹̦.

You can define your formatter by implementing the `Formatter` interface,
requiring a `Format` method. `Format` takes an `*Entry`. `entry.Data` is a
`Fields` type (`map[string]interface{}`) with all your fields as well as the
default ones (see Entries section above):

```go
type MyJSONFormatter struct {
}

log.SetFormatter(new(MyJSONFormatter))

func (f *MyJSONFormatter) Format(entry *Entry) ([]byte, error) {
  // Note this doesn't include Time, Level and Message which are available on
  // the Entry. Consult `godoc` on information about those fields or read the
  // source of the official loggers.
  serialized, err := json.Marshal(entry.Data)
    if err != nil {
      return nil, fmt.Errorf("Failed to marshal fields to JSON, %v", err)
    }
  return append(serialized, '\n'), nil
}
```

#### Logger as an `io.Writer`

Logrus can be transformed into an `io.Writer`. That writer is the end of an `io.Pipe` and it is your responsibility to close it.

```go
w := logger.Writer()
defer w.Close()

srv := http.Server{
    // create a stdlib log.Logger that writes to
    // logrus.Logger.
    ErrorLog: log.New(w, "", 0),
}
```

Each line written to that writer will be printed the usual way, using formatters
and hooks. The level for those entries is `info`.

This means that we can override the standard library logger easily:

```go
logger := logrus.New()
logger.Formatter = &logrus.JSONFormatter{}

// Use logrus for standard log output
// Note that `log` here references stdlib's log
// Not logrus imported under the name `log`.
log.SetOutput(logger.Writer())
```

#### Rotation

Log rotation is not provided with Logrus. Log rotation should be done by an
external program (like `logrotate(8)`) that can compress and delete old log
entries. It should not be a feature of the application-level logger.

#### Tools

| Tool | Description |
| ---- | ----------- |
|[Logrus Mate](https://github.com/gogap/logrus_mate)|Logrus mate is a tool for Logrus to manage loggers, you can initial logger's level, hook and formatter by config file, the logger will generated with different config at different environment.|
|[Logrus Viper Helper](https://github.com/heirko/go-contrib/tree/master/logrusHelper)|An Helper around Logrus to wrap with spf13/Viper to load configuration with fangs! And to simplify Logrus configuration use some behavior of [Logrus Mate](https://github.com/gogap/logrus_mate). [sample](https://github.com/heirko/iris-contrib/blob/master/middleware/logrus-logger/example) |

#### Testing

Logrus has a built in facility for asserting the presence of log messages. This is implemented through the `test` hook and provides:

* decorators for existing logger (`test.NewLocal` and `test.NewGlobal`) which basically just add the `test` hook
* a test logger (`test.NewNullLogger`) that just records log messages (and does not output any):

```go
import(
  "github.com/sirupsen/logrus"
  "github.com/sirupsen/logrus/hooks/test"
  "github.com/stretchr/testify/assert"
  "testing"
)

func TestSomething(t*testing.T){
  logger, hook := test.NewNullLogger()
  logger.Error("Helloerror")

  assert.Equal(t, 1, len(hook.Entries))
  assert.Equal(t, logrus.ErrorLevel, hook.LastEntry().Level)
  assert.Equal(t, "Helloerror", hook.LastEntry().Message)

  hook.Reset()
  assert.Nil(t, hook.LastEntry())
}
```

#### Fatal handlers

Logrus can register one or more functions that will be called when any `fatal`
level message is logged. The registered handlers will be executed before
logrus performs a `os.Exit(1)`. This behavior may be helpful if callers need
to gracefully shutdown. Unlike a `panic("Something went wrong...")` call which can be intercepted with a deferred `recover` a call to `os.Exit(1)` can not be intercepted.

```
...
handler := func() {
  // gracefully shutdown something...
}
logrus.RegisterExitHandler(handler)
...
```

#### Thread safety

By default Logger is protected by mutex for concurrent writes, this mutex is invoked when calling hooks and writing logs.
If you are sure such locking is not needed, you can call logger.SetNoLock() to disable the locking.

Situation when locking is not needed includes:

* You have no hooks registered, or hooks calling is already thread-safe.

* Writing to logger.Out is already thread-safe, for example:

  1) logger.Out is protected by locks.

  2) logger.Out is a os.File handler opened with `O_APPEND` flag, and every write is smaller than 4k. (This allow multi-thread/multi-process writing)

     (Refer to http://www.notthewizard.com/2014/06/17/are-files-appends-really-atomic/)
# Syslog Hooks for Logrus <img src="http://i.imgur.com/hTeVwmJ.png" width="40" height="40" alt=":walrus:" class="emoji" title=":walrus:"/>

## Usage

```go
import (
  "log/syslog"
  "github.com/sirupsen/logrus"
  logrus_syslog "github.com/sirupsen/logrus/hooks/syslog"
)

func main() {
  log       := logrus.New()
  hook, err := logrus_syslog.NewSyslogHook("udp", "localhost:514", syslog.LOG_INFO, "")

  if err == nil {
    log.Hooks.Add(hook)
  }
}
```

If you want to connect to local syslog (Ex. "/dev/log" or "/var/run/syslog" or "/var/run/log"). Just assign empty string to the first two parameters of `NewSyslogHook`. It should look like the following.

```go
import (
  "log/syslog"
  "github.com/sirupsen/logrus"
  logrus_syslog "github.com/sirupsen/logrus/hooks/syslog"
)

func main() {
  log       := logrus.New()
  hook, err := logrus_syslog.NewSyslogHook("", "", syslog.LOG_INFO, "")

  if err == nil {
    log.Hooks.Add(hook)
  }
}
```
cli
===

[![Build Status](https://travis-ci.org/urfave/cli.svg?branch=master)](https://travis-ci.org/urfave/cli)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/rtgk5xufi932pb2v?svg=true)](https://ci.appveyor.com/project/urfave/cli)
[![GoDoc](https://godoc.org/github.com/urfave/cli?status.svg)](https://godoc.org/github.com/urfave/cli)
[![codebeat](https://codebeat.co/badges/0a8f30aa-f975-404b-b878-5fab3ae1cc5f)](https://codebeat.co/projects/github-com-urfave-cli)
[![Go Report Card](https://goreportcard.com/badge/urfave/cli)](https://goreportcard.com/report/urfave/cli)
[![top level coverage](https://gocover.io/_badge/github.com/urfave/cli?0 "top level coverage")](http://gocover.io/github.com/urfave/cli) /
[![altsrc coverage](https://gocover.io/_badge/github.com/urfave/cli/altsrc?0 "altsrc coverage")](http://gocover.io/github.com/urfave/cli/altsrc)

**Notice:** This is the library formerly known as
`github.com/codegangsta/cli` -- Github will automatically redirect requests
to this repository, but we recommend updating your references for clarity.

cli is a simple, fast, and fun package for building command line apps in Go. The
goal is to enable developers to write fast and distributable command line
applications in an expressive way.

<!-- toc -->

- [Overview](#overview)
- [Installation](#installation)
  * [Supported platforms](#supported-platforms)
  * [Using the `v2` branch](#using-the-v2-branch)
  * [Pinning to the `v1` releases](#pinning-to-the-v1-releases)
- [Getting Started](#getting-started)
- [Examples](#examples)
  * [Arguments](#arguments)
  * [Flags](#flags)
    + [Placeholder Values](#placeholder-values)
    + [Alternate Names](#alternate-names)
    + [Ordering](#ordering)
    + [Values from the Environment](#values-from-the-environment)
    + [Values from alternate input sources (YAML, TOML, and others)](#values-from-alternate-input-sources-yaml-toml-and-others)
  * [Subcommands](#subcommands)
  * [Subcommands categories](#subcommands-categories)
  * [Exit code](#exit-code)
  * [Bash Completion](#bash-completion)
    + [Enabling](#enabling)
    + [Distribution](#distribution)
    + [Customization](#customization)
  * [Generated Help Text](#generated-help-text)
    + [Customization](#customization-1)
  * [Version Flag](#version-flag)
    + [Customization](#customization-2)
    + [Full API Example](#full-api-example)
- [Contribution Guidelines](#contribution-guidelines)

<!-- tocstop -->

## Overview

Command line apps are usually so tiny that there is absolutely no reason why
your code should *not* be self-documenting. Things like generating help text and
parsing command flags/options should not hinder productivity when writing a
command line app.

**This is where cli comes into play.** cli makes command line programming fun,
organized, and expressive!

## Installation

Make sure you have a working Go environment.  Go version 1.2+ is supported.  [See
the install instructions for Go](http://golang.org/doc/install.html).

To install cli, simply run:
```
$ go get github.com/urfave/cli
```

Make sure your `PATH` includes the `$GOPATH/bin` directory so your commands can
be easily used:
```
export PATH=$PATH:$GOPATH/bin
```

### Supported platforms

cli is tested against multiple versions of Go on Linux, and against the latest
released version of Go on OS X and Windows.  For full details, see
[`./.travis.yml`](./.travis.yml) and [`./appveyor.yml`](./appveyor.yml).

### Using the `v2` branch

**Warning**: The `v2` branch is currently unreleased and considered unstable.

There is currently a long-lived branch named `v2` that is intended to land as
the new `master` branch once development there has settled down.  The current
`master` branch (mirrored as `v1`) is being manually merged into `v2` on
an irregular human-based schedule, but generally if one wants to "upgrade" to
`v2` *now* and accept the volatility (read: "awesomeness") that comes along with
that, please use whatever version pinning of your preference, such as via
`gopkg.in`:

```
$ go get gopkg.in/urfave/cli.v2
```

``` go
...
import (
  "gopkg.in/urfave/cli.v2" // imports as package "cli"
)
...
```

### Pinning to the `v1` releases

Similarly to the section above describing use of the `v2` branch, if one wants
to avoid any unexpected compatibility pains once `v2` becomes `master`, then
pinning to `v1` is an acceptable option, e.g.:

```
$ go get gopkg.in/urfave/cli.v1
```

``` go
...
import (
  "gopkg.in/urfave/cli.v1" // imports as package "cli"
)
...
```

This will pull the latest tagged `v1` release (e.g. `v1.18.1` at the time of writing).

## Getting Started

One of the philosophies behind cli is that an API should be playful and full of
discovery. So a cli app can be as little as one line of code in `main()`.

<!-- {
  "args": ["&#45;&#45;help"],
  "output": "A new cli application"
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  cli.NewApp().Run(os.Args)
}
```

This app will run and show help text, but is not very useful. Let's give an
action to execute and some help documentation:

<!-- {
  "output": "boom! I say!"
} -->
``` go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()
  app.Name = "boom"
  app.Usage = "make an explosive entrance"
  app.Action = func(c *cli.Context) error {
    fmt.Println("boom! I say!")
    return nil
  }

  app.Run(os.Args)
}
```

Running this already gives you a ton of functionality, plus support for things
like subcommands and flags, which are covered below.

## Examples

Being a programmer can be a lonely job. Thankfully by the power of automation
that is not the case! Let's create a greeter app to fend off our demons of
loneliness!

Start by creating a directory named `greet`, and within it, add a file,
`greet.go` with the following code in it:

<!-- {
  "output": "Hello friend!"
} -->
``` go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()
  app.Name = "greet"
  app.Usage = "fight the loneliness!"
  app.Action = func(c *cli.Context) error {
    fmt.Println("Hello friend!")
    return nil
  }

  app.Run(os.Args)
}
```

Install our command to the `$GOPATH/bin` directory:

```
$ go install
```

Finally run our new command:

```
$ greet
Hello friend!
```

cli also generates neat help text:

```
$ greet help
NAME:
    greet - fight the loneliness!

USAGE:
    greet [global options] command [command options] [arguments...]

VERSION:
    0.0.0

COMMANDS:
    help, h  Shows a list of commands or help for one command

GLOBAL OPTIONS
    --version Shows version information
```

### Arguments

You can lookup arguments by calling the `Args` function on `cli.Context`, e.g.:

<!-- {
  "output": "Hello \""
} -->
``` go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Action = func(c *cli.Context) error {
    fmt.Printf("Hello %q", c.Args().Get(0))
    return nil
  }

  app.Run(os.Args)
}
```

### Flags

Setting and querying flags is simple.

<!-- {
  "output": "Hello Nefertiti"
} -->
``` go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Flags = []cli.Flag {
    cli.StringFlag{
      Name: "lang",
      Value: "english",
      Usage: "language for the greeting",
    },
  }

  app.Action = func(c *cli.Context) error {
    name := "Nefertiti"
    if c.NArg() > 0 {
      name = c.Args().Get(0)
    }
    if c.String("lang") == "spanish" {
      fmt.Println("Hola", name)
    } else {
      fmt.Println("Hello", name)
    }
    return nil
  }

  app.Run(os.Args)
}
```

You can also set a destination variable for a flag, to which the content will be
scanned.

<!-- {
  "output": "Hello someone"
} -->
``` go
package main

import (
  "os"
  "fmt"

  "github.com/urfave/cli"
)

func main() {
  var language string

  app := cli.NewApp()

  app.Flags = []cli.Flag {
    cli.StringFlag{
      Name:        "lang",
      Value:       "english",
      Usage:       "language for the greeting",
      Destination: &language,
    },
  }

  app.Action = func(c *cli.Context) error {
    name := "someone"
    if c.NArg() > 0 {
      name = c.Args()[0]
    }
    if language == "spanish" {
      fmt.Println("Hola", name)
    } else {
      fmt.Println("Hello", name)
    }
    return nil
  }

  app.Run(os.Args)
}
```

See full list of flags at http://godoc.org/github.com/urfave/cli

#### Placeholder Values

Sometimes it's useful to specify a flag's value within the usage string itself.
Such placeholders are indicated with back quotes.

For example this:

<!-- {
  "args": ["&#45;&#45;help"],
  "output": "&#45;&#45;config FILE, &#45;c FILE"
} -->
```go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Flags = []cli.Flag{
    cli.StringFlag{
      Name:  "config, c",
      Usage: "Load configuration from `FILE`",
    },
  }

  app.Run(os.Args)
}
```

Will result in help output like:

```
--config FILE, -c FILE   Load configuration from FILE
```

Note that only the first placeholder is used. Subsequent back-quoted words will
be left as-is.

#### Alternate Names

You can set alternate (or short) names for flags by providing a comma-delimited
list for the `Name`. e.g.

<!-- {
  "args": ["&#45;&#45;help"],
  "output": "&#45;&#45;lang value, &#45;l value.*language for the greeting.*default: \"english\""
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Flags = []cli.Flag {
    cli.StringFlag{
      Name: "lang, l",
      Value: "english",
      Usage: "language for the greeting",
    },
  }

  app.Run(os.Args)
}
```

That flag can then be set with `--lang spanish` or `-l spanish`. Note that
giving two different forms of the same flag in the same command invocation is an
error.

#### Ordering

Flags for the application and commands are shown in the order they are defined.
However, it's possible to sort them from outside this library by using `FlagsByName`
or `CommandsByName` with `sort`.

For example this:

<!-- {
  "args": ["&#45;&#45;help"],
  "output": "add a task to the list\n.*complete a task on the list\n.*\n\n.*\n.*Load configuration from FILE\n.*Language for the greeting.*"
} -->
``` go
package main

import (
  "os"
  "sort"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Flags = []cli.Flag {
    cli.StringFlag{
      Name: "lang, l",
      Value: "english",
      Usage: "Language for the greeting",
    },
    cli.StringFlag{
      Name: "config, c",
      Usage: "Load configuration from `FILE`",
    },
  }

  app.Commands = []cli.Command{
    {
      Name:    "complete",
      Aliases: []string{"c"},
      Usage:   "complete a task on the list",
      Action:  func(c *cli.Context) error {
        return nil
      },
    },
    {
      Name:    "add",
      Aliases: []string{"a"},
      Usage:   "add a task to the list",
      Action:  func(c *cli.Context) error {
        return nil
      },
    },
  }

  sort.Sort(cli.FlagsByName(app.Flags))
  sort.Sort(cli.CommandsByName(app.Commands))

  app.Run(os.Args)
}
```

Will result in help output like:

```
--config FILE, -c FILE  Load configuration from FILE
--lang value, -l value  Language for the greeting (default: "english")
```

#### Values from the Environment

You can also have the default value set from the environment via `EnvVar`.  e.g.

<!-- {
  "args": ["&#45;&#45;help"],
  "output": "language for the greeting.*APP_LANG"
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Flags = []cli.Flag {
    cli.StringFlag{
      Name: "lang, l",
      Value: "english",
      Usage: "language for the greeting",
      EnvVar: "APP_LANG",
    },
  }

  app.Run(os.Args)
}
```

The `EnvVar` may also be given as a comma-delimited "cascade", where the first
environment variable that resolves is used as the default.

<!-- {
  "args": ["&#45;&#45;help"],
  "output": "language for the greeting.*LEGACY_COMPAT_LANG.*APP_LANG.*LANG"
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Flags = []cli.Flag {
    cli.StringFlag{
      Name: "lang, l",
      Value: "english",
      Usage: "language for the greeting",
      EnvVar: "LEGACY_COMPAT_LANG,APP_LANG,LANG",
    },
  }

  app.Run(os.Args)
}
```

#### Values from alternate input sources (YAML, TOML, and others)

There is a separate package altsrc that adds support for getting flag values
from other file input sources.

Currently supported input source formats:
* YAML
* TOML

In order to get values for a flag from an alternate input source the following
code would be added to wrap an existing cli.Flag like below:

``` go
  altsrc.NewIntFlag(cli.IntFlag{Name: "test"})
```

Initialization must also occur for these flags. Below is an example initializing
getting data from a yaml file below.

``` go
  command.Before = altsrc.InitInputSourceWithContext(command.Flags, NewYamlSourceFromFlagFunc("load"))
```

The code above will use the "load" string as a flag name to get the file name of
a yaml file from the cli.Context.  It will then use that file name to initialize
the yaml input source for any flags that are defined on that command.  As a note
the "load" flag used would also have to be defined on the command flags in order
for this code snipped to work.

Currently only the aboved specified formats are supported but developers can
add support for other input sources by implementing the
altsrc.InputSourceContext for their given sources.

Here is a more complete sample of a command using YAML support:

<!-- {
  "args": ["test-cmd", "&#45;&#45;help"],
  "output": "&#45&#45;test value.*default: 0"
} -->
``` go
package notmain

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
  "github.com/urfave/cli/altsrc"
)

func main() {
  app := cli.NewApp()

  flags := []cli.Flag{
    altsrc.NewIntFlag(cli.IntFlag{Name: "test"}),
    cli.StringFlag{Name: "load"},
  }

  app.Action = func(c *cli.Context) error {
    fmt.Println("yaml ist rad")
    return nil
  }

  app.Before = altsrc.InitInputSourceWithContext(flags, altsrc.NewYamlSourceFromFlagFunc("load"))
  app.Flags = flags

  app.Run(os.Args)
}
```

### Subcommands

Subcommands can be defined for a more git-like command line app.

<!-- {
  "args": ["template", "add"],
  "output": "new task template: .+"
} -->
```go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Commands = []cli.Command{
    {
      Name:    "add",
      Aliases: []string{"a"},
      Usage:   "add a task to the list",
      Action:  func(c *cli.Context) error {
        fmt.Println("added task: ", c.Args().First())
        return nil
      },
    },
    {
      Name:    "complete",
      Aliases: []string{"c"},
      Usage:   "complete a task on the list",
      Action:  func(c *cli.Context) error {
        fmt.Println("completed task: ", c.Args().First())
        return nil
      },
    },
    {
      Name:        "template",
      Aliases:     []string{"t"},
      Usage:       "options for task templates",
      Subcommands: []cli.Command{
        {
          Name:  "add",
          Usage: "add a new template",
          Action: func(c *cli.Context) error {
            fmt.Println("new task template: ", c.Args().First())
            return nil
          },
        },
        {
          Name:  "remove",
          Usage: "remove an existing template",
          Action: func(c *cli.Context) error {
            fmt.Println("removed task template: ", c.Args().First())
            return nil
          },
        },
      },
    },
  }

  app.Run(os.Args)
}
```

### Subcommands categories

For additional organization in apps that have many subcommands, you can
associate a category for each command to group them together in the help
output.

E.g.

```go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()

  app.Commands = []cli.Command{
    {
      Name: "noop",
    },
    {
      Name:     "add",
      Category: "template",
    },
    {
      Name:     "remove",
      Category: "template",
    },
  }

  app.Run(os.Args)
}
```

Will include:

```
COMMANDS:
    noop

  Template actions:
    add
    remove
```

### Exit code

Calling `App.Run` will not automatically call `os.Exit`, which means that by
default the exit code will "fall through" to being `0`.  An explicit exit code
may be set by returning a non-nil error that fulfills `cli.ExitCoder`, *or* a
`cli.MultiError` that includes an error that fulfills `cli.ExitCoder`, e.g.:

``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  app := cli.NewApp()
  app.Flags = []cli.Flag{
    cli.BoolTFlag{
      Name:  "ginger-crouton",
      Usage: "is it in the soup?",
    },
  }
  app.Action = func(ctx *cli.Context) error {
    if !ctx.Bool("ginger-crouton") {
      return cli.NewExitError("it is not in the soup", 86)
    }
    return nil
  }

  app.Run(os.Args)
}
```

### Bash Completion

You can enable completion commands by setting the `EnableBashCompletion`
flag on the `App` object.  By default, this setting will only auto-complete to
show an app's subcommands, but you can write your own completion methods for
the App or its subcommands.

<!-- {
  "args": ["complete", "&#45;&#45;generate&#45;bash&#45;completion"],
  "output": "laundry"
} -->
``` go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

func main() {
  tasks := []string{"cook", "clean", "laundry", "eat", "sleep", "code"}

  app := cli.NewApp()
  app.EnableBashCompletion = true
  app.Commands = []cli.Command{
    {
      Name:  "complete",
      Aliases: []string{"c"},
      Usage: "complete a task on the list",
      Action: func(c *cli.Context) error {
         fmt.Println("completed task: ", c.Args().First())
         return nil
      },
      BashComplete: func(c *cli.Context) {
        // This will complete if no args are passed
        if c.NArg() > 0 {
          return
        }
        for _, t := range tasks {
          fmt.Println(t)
        }
      },
    },
  }

  app.Run(os.Args)
}
```

#### Enabling

Source the `autocomplete/bash_autocomplete` file in your `.bashrc` file while
setting the `PROG` variable to the name of your program:

`PROG=myprogram source /.../cli/autocomplete/bash_autocomplete`

#### Distribution

Copy `autocomplete/bash_autocomplete` into `/etc/bash_completion.d/` and rename
it to the name of the program you wish to add autocomplete support for (or
automatically install it there if you are distributing a package). Don't forget
to source the file to make it active in the current shell.

```
sudo cp src/bash_autocomplete /etc/bash_completion.d/<myprogram>
source /etc/bash_completion.d/<myprogram>
```

Alternatively, you can just document that users should source the generic
`autocomplete/bash_autocomplete` in their bash configuration with `$PROG` set
to the name of their program (as above).

#### Customization

The default bash completion flag (`--generate-bash-completion`) is defined as
`cli.BashCompletionFlag`, and may be redefined if desired, e.g.:

<!-- {
  "args": ["&#45;&#45;compgen"],
  "output": "wat\nhelp\nh"
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  cli.BashCompletionFlag = cli.BoolFlag{
    Name:   "compgen",
    Hidden: true,
  }

  app := cli.NewApp()
  app.EnableBashCompletion = true
  app.Commands = []cli.Command{
    {
      Name: "wat",
    },
  }
  app.Run(os.Args)
}
```

### Generated Help Text

The default help flag (`-h/--help`) is defined as `cli.HelpFlag` and is checked
by the cli internals in order to print generated help text for the app, command,
or subcommand, and break execution.

#### Customization

All of the help text generation may be customized, and at multiple levels.  The
templates are exposed as variables `AppHelpTemplate`, `CommandHelpTemplate`, and
`SubcommandHelpTemplate` which may be reassigned or augmented, and full override
is possible by assigning a compatible func to the `cli.HelpPrinter` variable,
e.g.:

<!-- {
  "output": "Ha HA.  I pwnd the help!!1"
} -->
``` go
package main

import (
  "fmt"
  "io"
  "os"

  "github.com/urfave/cli"
)

func main() {
  // EXAMPLE: Append to an existing template
  cli.AppHelpTemplate = fmt.Sprintf(`%s

WEBSITE: http://awesometown.example.com

SUPPORT: support@awesometown.example.com

`, cli.AppHelpTemplate)

  // EXAMPLE: Override a template
  cli.AppHelpTemplate = `NAME:
   {{.Name}} - {{.Usage}}
USAGE:
   {{.HelpName}} {{if .VisibleFlags}}[global options]{{end}}{{if .Commands}} command [command options]{{end}} {{if .ArgsUsage}}{{.ArgsUsage}}{{else}}[arguments...]{{end}}
   {{if len .Authors}}
AUTHOR:
   {{range .Authors}}{{ . }}{{end}}
   {{end}}{{if .Commands}}
COMMANDS:
{{range .Commands}}{{if not .HideHelp}}   {{join .Names ", "}}{{ "\t"}}{{.Usage}}{{ "\n" }}{{end}}{{end}}{{end}}{{if .VisibleFlags}}
GLOBAL OPTIONS:
   {{range .VisibleFlags}}{{.}}
   {{end}}{{end}}{{if .Copyright }}
COPYRIGHT:
   {{.Copyright}}
   {{end}}{{if .Version}}
VERSION:
   {{.Version}}
   {{end}}
`

  // EXAMPLE: Replace the `HelpPrinter` func
  cli.HelpPrinter = func(w io.Writer, templ string, data interface{}) {
    fmt.Println("Ha HA.  I pwnd the help!!1")
  }

  cli.NewApp().Run(os.Args)
}
```

The default flag may be customized to something other than `-h/--help` by
setting `cli.HelpFlag`, e.g.:

<!-- {
  "args": ["&#45;&#45halp"],
  "output": "haaaaalp.*HALP"
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  cli.HelpFlag = cli.BoolFlag{
    Name: "halp, haaaaalp",
    Usage: "HALP",
    EnvVar: "SHOW_HALP,HALPPLZ",
  }

  cli.NewApp().Run(os.Args)
}
```

### Version Flag

The default version flag (`-v/--version`) is defined as `cli.VersionFlag`, which
is checked by the cli internals in order to print the `App.Version` via
`cli.VersionPrinter` and break execution.

#### Customization

The default flag may be customized to something other than `-v/--version` by
setting `cli.VersionFlag`, e.g.:

<!-- {
  "args": ["&#45;&#45print-version"],
  "output": "partay version 19\\.99\\.0"
} -->
``` go
package main

import (
  "os"

  "github.com/urfave/cli"
)

func main() {
  cli.VersionFlag = cli.BoolFlag{
    Name: "print-version, V",
    Usage: "print only the version",
  }

  app := cli.NewApp()
  app.Name = "partay"
  app.Version = "19.99.0"
  app.Run(os.Args)
}
```

Alternatively, the version printer at `cli.VersionPrinter` may be overridden, e.g.:

<!-- {
  "args": ["&#45;&#45version"],
  "output": "version=19\\.99\\.0 revision=fafafaf"
} -->
``` go
package main

import (
  "fmt"
  "os"

  "github.com/urfave/cli"
)

var (
  Revision = "fafafaf"
)

func main() {
  cli.VersionPrinter = func(c *cli.Context) {
    fmt.Printf("version=%s revision=%s\n", c.App.Version, Revision)
  }

  app := cli.NewApp()
  app.Name = "partay"
  app.Version = "19.99.0"
  app.Run(os.Args)
}
```

#### Full API Example

**Notice**: This is a contrived (functioning) example meant strictly for API
demonstration purposes.  Use of one's imagination is encouraged.

<!-- {
  "output": "made it!\nPhew!"
} -->
``` go
package main

import (
  "errors"
  "flag"
  "fmt"
  "io"
  "io/ioutil"
  "os"
  "time"

  "github.com/urfave/cli"
)

func init() {
  cli.AppHelpTemplate += "\nCUSTOMIZED: you bet ur muffins\n"
  cli.CommandHelpTemplate += "\nYMMV\n"
  cli.SubcommandHelpTemplate += "\nor something\n"

  cli.HelpFlag = cli.BoolFlag{Name: "halp"}
  cli.BashCompletionFlag = cli.BoolFlag{Name: "compgen", Hidden: true}
  cli.VersionFlag = cli.BoolFlag{Name: "print-version, V"}

  cli.HelpPrinter = func(w io.Writer, templ string, data interface{}) {
    fmt.Fprintf(w, "best of luck to you\n")
  }
  cli.VersionPrinter = func(c *cli.Context) {
    fmt.Fprintf(c.App.Writer, "version=%s\n", c.App.Version)
  }
  cli.OsExiter = func(c int) {
    fmt.Fprintf(cli.ErrWriter, "refusing to exit %d\n", c)
  }
  cli.ErrWriter = ioutil.Discard
  cli.FlagStringer = func(fl cli.Flag) string {
    return fmt.Sprintf("\t\t%s", fl.GetName())
  }
}

type hexWriter struct{}

func (w *hexWriter) Write(p []byte) (int, error) {
  for _, b := range p {
    fmt.Printf("%x", b)
  }
  fmt.Printf("\n")

  return len(p), nil
}

type genericType struct{
  s string
}

func (g *genericType) Set(value string) error {
  g.s = value
  return nil
}

func (g *genericType) String() string {
  return g.s
}

func main() {
  app := cli.NewApp()
  app.Name = "kənˈtrīv"
  app.Version = "19.99.0"
  app.Compiled = time.Now()
  app.Authors = []cli.Author{
    cli.Author{
      Name:  "Example Human",
      Email: "human@example.com",
    },
  }
  app.Copyright = "(c) 1999 Serious Enterprise"
  app.HelpName = "contrive"
  app.Usage = "demonstrate available API"
  app.UsageText = "contrive - demonstrating the available API"
  app.ArgsUsage = "[args and such]"
  app.Commands = []cli.Command{
    cli.Command{
      Name:        "doo",
      Aliases:     []string{"do"},
      Category:    "motion",
      Usage:       "do the doo",
      UsageText:   "doo - does the dooing",
      Description: "no really, there is a lot of dooing to be done",
      ArgsUsage:   "[arrgh]",
      Flags: []cli.Flag{
        cli.BoolFlag{Name: "forever, forevvarr"},
      },
      Subcommands: cli.Commands{
        cli.Command{
          Name:   "wop",
          Action: wopAction,
        },
      },
      SkipFlagParsing: false,
      HideHelp:        false,
      Hidden:          false,
      HelpName:        "doo!",
      BashComplete: func(c *cli.Context) {
        fmt.Fprintf(c.App.Writer, "--better\n")
      },
      Before: func(c *cli.Context) error {
        fmt.Fprintf(c.App.Writer, "brace for impact\n")
        return nil
      },
      After: func(c *cli.Context) error {
        fmt.Fprintf(c.App.Writer, "did we lose anyone?\n")
        return nil
      },
      Action: func(c *cli.Context) error {
        c.Command.FullName()
        c.Command.HasName("wop")
        c.Command.Names()
        c.Command.VisibleFlags()
        fmt.Fprintf(c.App.Writer, "dodododododoodododddooooododododooo\n")
        if c.Bool("forever") {
          c.Command.Run(c)
        }
        return nil
      },
      OnUsageError: func(c *cli.Context, err error, isSubcommand bool) error {
        fmt.Fprintf(c.App.Writer, "for shame\n")
        return err
      },
    },
  }
  app.Flags = []cli.Flag{
    cli.BoolFlag{Name: "fancy"},
    cli.BoolTFlag{Name: "fancier"},
    cli.DurationFlag{Name: "howlong, H", Value: time.Second * 3},
    cli.Float64Flag{Name: "howmuch"},
    cli.GenericFlag{Name: "wat", Value: &genericType{}},
    cli.Int64Flag{Name: "longdistance"},
    cli.Int64SliceFlag{Name: "intervals"},
    cli.IntFlag{Name: "distance"},
    cli.IntSliceFlag{Name: "times"},
    cli.StringFlag{Name: "dance-move, d"},
    cli.StringSliceFlag{Name: "names, N"},
    cli.UintFlag{Name: "age"},
    cli.Uint64Flag{Name: "bigage"},
  }
  app.EnableBashCompletion = true
  app.HideHelp = false
  app.HideVersion = false
  app.BashComplete = func(c *cli.Context) {
    fmt.Fprintf(c.App.Writer, "lipstick\nkiss\nme\nlipstick\nringo\n")
  }
  app.Before = func(c *cli.Context) error {
    fmt.Fprintf(c.App.Writer, "HEEEERE GOES\n")
    return nil
  }
  app.After = func(c *cli.Context) error {
    fmt.Fprintf(c.App.Writer, "Phew!\n")
    return nil
  }
  app.CommandNotFound = func(c *cli.Context, command string) {
    fmt.Fprintf(c.App.Writer, "Thar be no %q here.\n", command)
  }
  app.OnUsageError = func(c *cli.Context, err error, isSubcommand bool) error {
    if isSubcommand {
      return err
    }

    fmt.Fprintf(c.App.Writer, "WRONG: %#v\n", err)
    return nil
  }
  app.Action = func(c *cli.Context) error {
    cli.DefaultAppComplete(c)
    cli.HandleExitCoder(errors.New("not an exit coder, though"))
    cli.ShowAppHelp(c)
    cli.ShowCommandCompletions(c, "nope")
    cli.ShowCommandHelp(c, "also-nope")
    cli.ShowCompletions(c)
    cli.ShowSubcommandHelp(c)
    cli.ShowVersion(c)

    categories := c.App.Categories()
    categories.AddCommand("sounds", cli.Command{
      Name: "bloop",
    })

    for _, category := range c.App.Categories() {
      fmt.Fprintf(c.App.Writer, "%s\n", category.Name)
      fmt.Fprintf(c.App.Writer, "%#v\n", category.Commands)
      fmt.Fprintf(c.App.Writer, "%#v\n", category.VisibleCommands())
    }

    fmt.Printf("%#v\n", c.App.Command("doo"))
    if c.Bool("infinite") {
      c.App.Run([]string{"app", "doo", "wop"})
    }

    if c.Bool("forevar") {
      c.App.RunAsSubcommand(c)
    }
    c.App.Setup()
    fmt.Printf("%#v\n", c.App.VisibleCategories())
    fmt.Printf("%#v\n", c.App.VisibleCommands())
    fmt.Printf("%#v\n", c.App.VisibleFlags())

    fmt.Printf("%#v\n", c.Args().First())
    if len(c.Args()) > 0 {
      fmt.Printf("%#v\n", c.Args()[1])
    }
    fmt.Printf("%#v\n", c.Args().Present())
    fmt.Printf("%#v\n", c.Args().Tail())

    set := flag.NewFlagSet("contrive", 0)
    nc := cli.NewContext(c.App, set, c)

    fmt.Printf("%#v\n", nc.Args())
    fmt.Printf("%#v\n", nc.Bool("nope"))
    fmt.Printf("%#v\n", nc.BoolT("nerp"))
    fmt.Printf("%#v\n", nc.Duration("howlong"))
    fmt.Printf("%#v\n", nc.Float64("hay"))
    fmt.Printf("%#v\n", nc.Generic("bloop"))
    fmt.Printf("%#v\n", nc.Int64("bonk"))
    fmt.Printf("%#v\n", nc.Int64Slice("burnks"))
    fmt.Printf("%#v\n", nc.Int("bips"))
    fmt.Printf("%#v\n", nc.IntSlice("blups"))
    fmt.Printf("%#v\n", nc.String("snurt"))
    fmt.Printf("%#v\n", nc.StringSlice("snurkles"))
    fmt.Printf("%#v\n", nc.Uint("flub"))
    fmt.Printf("%#v\n", nc.Uint64("florb"))
    fmt.Printf("%#v\n", nc.GlobalBool("global-nope"))
    fmt.Printf("%#v\n", nc.GlobalBoolT("global-nerp"))
    fmt.Printf("%#v\n", nc.GlobalDuration("global-howlong"))
    fmt.Printf("%#v\n", nc.GlobalFloat64("global-hay"))
    fmt.Printf("%#v\n", nc.GlobalGeneric("global-bloop"))
    fmt.Printf("%#v\n", nc.GlobalInt("global-bips"))
    fmt.Printf("%#v\n", nc.GlobalIntSlice("global-blups"))
    fmt.Printf("%#v\n", nc.GlobalString("global-snurt"))
    fmt.Printf("%#v\n", nc.GlobalStringSlice("global-snurkles"))

    fmt.Printf("%#v\n", nc.FlagNames())
    fmt.Printf("%#v\n", nc.GlobalFlagNames())
    fmt.Printf("%#v\n", nc.GlobalIsSet("wat"))
    fmt.Printf("%#v\n", nc.GlobalSet("wat", "nope"))
    fmt.Printf("%#v\n", nc.NArg())
    fmt.Printf("%#v\n", nc.NumFlags())
    fmt.Printf("%#v\n", nc.Parent())

    nc.Set("wat", "also-nope")

    ec := cli.NewExitError("ohwell", 86)
    fmt.Fprintf(c.App.Writer, "%d", ec.ExitCode())
    fmt.Printf("made it!\n")
    return ec
  }

  if os.Getenv("HEXY") != "" {
    app.Writer = &hexWriter{}
    app.ErrWriter = &hexWriter{}
  }

  app.Metadata = map[string]interface{}{
    "layers":     "many",
    "explicable": false,
    "whatever-values": 19.99,
  }

  app.Run(os.Args)
}

func wopAction(c *cli.Context) error {
  fmt.Fprintf(c.App.Writer, ":wave: over here, eh\n")
  return nil
}
```

## Contribution Guidelines

Feel free to put up a pull request to fix a bug or maybe add a feature. I will
give it a code review and make sure that it does not break backwards
compatibility. If I or any other collaborators agree that it is in line with
the vision of the project, we will work with you to get the code into
a mergeable state and merge it into the master branch.

If you have contributed something significant to the project, we will most
likely add you as a collaborator. As a collaborator you are given the ability
to merge others pull requests. It is very important that new code does not
break existing code, so be careful about what code you do choose to merge.

If you feel like you have contributed to the project but have not yet been
added as a collaborator, we probably forgot to add you, please open an issue.
gorilla/mux
===
[![GoDoc](https://godoc.org/github.com/gorilla/mux?status.svg)](https://godoc.org/github.com/gorilla/mux)
[![Build Status](https://travis-ci.org/gorilla/mux.svg?branch=master)](https://travis-ci.org/gorilla/mux)
[![Sourcegraph](https://sourcegraph.com/github.com/gorilla/mux/-/badge.svg)](https://sourcegraph.com/github.com/gorilla/mux?badge)

![Gorilla Logo](http://www.gorillatoolkit.org/static/images/gorilla-icon-64.png)

http://www.gorillatoolkit.org/pkg/mux

Package `gorilla/mux` implements a request router and dispatcher for matching incoming requests to
their respective handler.

The name mux stands for "HTTP request multiplexer". Like the standard `http.ServeMux`, `mux.Router` matches incoming requests against a list of registered routes and calls a handler for the route that matches the URL or other conditions. The main features are:

* It implements the `http.Handler` interface so it is compatible with the standard `http.ServeMux`.
* Requests can be matched based on URL host, path, path prefix, schemes, header and query values, HTTP methods or using custom matchers.
* URL hosts and paths can have variables with an optional regular expression.
* Registered URLs can be built, or "reversed", which helps maintaining references to resources.
* Routes can be used as subrouters: nested routes are only tested if the parent route matches. This is useful to define groups of routes that share common conditions like a host, a path prefix or other repeated attributes. As a bonus, this optimizes request matching.

---

* [Install](#install)
* [Examples](#examples)
* [Matching Routes](#matching-routes)
* [Listing Routes](#listing-routes)
* [Static Files](#static-files)
* [Registered URLs](#registered-urls)
* [Full Example](#full-example)

---

## Install

With a [correctly configured](https://golang.org/doc/install#testing) Go toolchain:

```sh
go get -u github.com/gorilla/mux
```

## Examples

Let's start registering a couple of URL paths and handlers:

```go
func main() {
	r := mux.NewRouter()
	r.HandleFunc("/", HomeHandler)
	r.HandleFunc("/products", ProductsHandler)
	r.HandleFunc("/articles", ArticlesHandler)
	http.Handle("/", r)
}
```

Here we register three routes mapping URL paths to handlers. This is equivalent to how `http.HandleFunc()` works: if an incoming request URL matches one of the paths, the corresponding handler is called passing (`http.ResponseWriter`, `*http.Request`) as parameters.

Paths can have variables. They are defined using the format `{name}` or `{name:pattern}`. If a regular expression pattern is not defined, the matched variable will be anything until the next slash. For example:

```go
r := mux.NewRouter()
r.HandleFunc("/products/{key}", ProductHandler)
r.HandleFunc("/articles/{category}/", ArticlesCategoryHandler)
r.HandleFunc("/articles/{category}/{id:[0-9]+}", ArticleHandler)
```

The names are used to create a map of route variables which can be retrieved calling `mux.Vars()`:

```go
func ArticlesCategoryHandler(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	w.WriteHeader(http.StatusOK)
	fmt.Fprintf(w, "Category: %v\n", vars["category"])
}
```

And this is all you need to know about the basic usage. More advanced options are explained below.

### Matching Routes

Routes can also be restricted to a domain or subdomain. Just define a host pattern to be matched. They can also have variables:

```go
r := mux.NewRouter()
// Only matches if domain is "www.example.com".
r.Host("www.example.com")
// Matches a dynamic subdomain.
r.Host("{subdomain:[a-z]+}.domain.com")
```

There are several other matchers that can be added. To match path prefixes:

```go
r.PathPrefix("/products/")
```

...or HTTP methods:

```go
r.Methods("GET", "POST")
```

...or URL schemes:

```go
r.Schemes("https")
```

...or header values:

```go
r.Headers("X-Requested-With", "XMLHttpRequest")
```

...or query values:

```go
r.Queries("key", "value")
```

...or to use a custom matcher function:

```go
r.MatcherFunc(func(r *http.Request, rm *RouteMatch) bool {
	return r.ProtoMajor == 0
})
```

...and finally, it is possible to combine several matchers in a single route:

```go
r.HandleFunc("/products", ProductsHandler).
  Host("www.example.com").
  Methods("GET").
  Schemes("http")
```

Setting the same matching conditions again and again can be boring, so we have a way to group several routes that share the same requirements. We call it "subrouting".

For example, let's say we have several URLs that should only match when the host is `www.example.com`. Create a route for that host and get a "subrouter" from it:

```go
r := mux.NewRouter()
s := r.Host("www.example.com").Subrouter()
```

Then register routes in the subrouter:

```go
s.HandleFunc("/products/", ProductsHandler)
s.HandleFunc("/products/{key}", ProductHandler)
s.HandleFunc("/articles/{category}/{id:[0-9]+}", ArticleHandler)
```

The three URL paths we registered above will only be tested if the domain is `www.example.com`, because the subrouter is tested first. This is not only convenient, but also optimizes request matching. You can create subrouters combining any attribute matchers accepted by a route.

Subrouters can be used to create domain or path "namespaces": you define subrouters in a central place and then parts of the app can register its paths relatively to a given subrouter.

There's one more thing about subroutes. When a subrouter has a path prefix, the inner routes use it as base for their paths:

```go
r := mux.NewRouter()
s := r.PathPrefix("/products").Subrouter()
// "/products/"
s.HandleFunc("/", ProductsHandler)
// "/products/{key}/"
s.HandleFunc("/{key}/", ProductHandler)
// "/products/{key}/details"
s.HandleFunc("/{key}/details", ProductDetailsHandler)
```

### Listing Routes

Routes on a mux can be listed using the Router.Walk method—useful for generating documentation:

```go
package main

import (
    "fmt"
    "net/http"
    "strings"

    "github.com/gorilla/mux"
)

func handler(w http.ResponseWriter, r *http.Request) {
    return
}

func main() {
    r := mux.NewRouter()
    r.HandleFunc("/", handler)
    r.Methods("POST").HandleFunc("/products", handler)
    r.Methods("GET").HandleFunc("/articles", handler)
    r.Methods("GET", "PUT").HandleFunc("/articles/{id}", handler)
    r.Walk(func(route *mux.Route, router *mux.Router, ancestors []*mux.Route) error {
        t, err := route.GetPathTemplate()
        if err != nil {
            return err
        }
        // p will contain regular expression is compatible with regular expression in Perl, Python, and other languages.
        // for instance the regular expression for path '/articles/{id}' will be '^/articles/(?P<v0>[^/]+)$'
        p, err := route.GetPathRegexp()
        if err != nil {
            return err
        }
        m, err := route.GetMethods()
        if err != nil {
            return err
        }
        fmt.Println(strings.Join(m, ","), t, p)
        return nil
    })
    http.Handle("/", r)
}
```

### Static Files

Note that the path provided to `PathPrefix()` represents a "wildcard": calling
`PathPrefix("/static/").Handler(...)` means that the handler will be passed any
request that matches "/static/*". This makes it easy to serve static files with mux:

```go
func main() {
	var dir string

	flag.StringVar(&dir, "dir", ".", "the directory to serve files from. Defaults to the current dir")
	flag.Parse()
	r := mux.NewRouter()

	// This will serve files under http://localhost:8000/static/<filename>
	r.PathPrefix("/static/").Handler(http.StripPrefix("/static/", http.FileServer(http.Dir(dir))))

	srv := &http.Server{
		Handler:      r,
		Addr:         "127.0.0.1:8000",
		// Good practice: enforce timeouts for servers you create!
		WriteTimeout: 15 * time.Second,
		ReadTimeout:  15 * time.Second,
	}

	log.Fatal(srv.ListenAndServe())
}
```

### Registered URLs

Now let's see how to build registered URLs.

Routes can be named. All routes that define a name can have their URLs built, or "reversed". We define a name calling `Name()` on a route. For example:

```go
r := mux.NewRouter()
r.HandleFunc("/articles/{category}/{id:[0-9]+}", ArticleHandler).
  Name("article")
```

To build a URL, get the route and call the `URL()` method, passing a sequence of key/value pairs for the route variables. For the previous route, we would do:

```go
url, err := r.Get("article").URL("category", "technology", "id", "42")
```

...and the result will be a `url.URL` with the following path:

```
"/articles/technology/42"
```

This also works for host variables:

```go
r := mux.NewRouter()
r.Host("{subdomain}.domain.com").
  Path("/articles/{category}/{id:[0-9]+}").
  HandlerFunc(ArticleHandler).
  Name("article")

// url.String() will be "http://news.domain.com/articles/technology/42"
url, err := r.Get("article").URL("subdomain", "news",
                                 "category", "technology",
                                 "id", "42")
```

All variables defined in the route are required, and their values must conform to the corresponding patterns. These requirements guarantee that a generated URL will always match a registered route -- the only exception is for explicitly defined "build-only" routes which never match.

Regex support also exists for matching Headers within a route. For example, we could do:

```go
r.HeadersRegexp("Content-Type", "application/(text|json)")
```

...and the route will match both requests with a Content-Type of `application/json` as well as `application/text`

There's also a way to build only the URL host or path for a route: use the methods `URLHost()` or `URLPath()` instead. For the previous route, we would do:

```go
// "http://news.domain.com/"
host, err := r.Get("article").URLHost("subdomain", "news")

// "/articles/technology/42"
path, err := r.Get("article").URLPath("category", "technology", "id", "42")
```

And if you use subrouters, host and path defined separately can be built as well:

```go
r := mux.NewRouter()
s := r.Host("{subdomain}.domain.com").Subrouter()
s.Path("/articles/{category}/{id:[0-9]+}").
  HandlerFunc(ArticleHandler).
  Name("article")

// "http://news.domain.com/articles/technology/42"
url, err := r.Get("article").URL("subdomain", "news",
                                 "category", "technology",
                                 "id", "42")
```

## Full Example

Here's a complete, runnable example of a small `mux` based server:

```go
package main

import (
	"net/http"
	"log"
	"github.com/gorilla/mux"
)

func YourHandler(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte("Gorilla!\n"))
}

func main() {
	r := mux.NewRouter()
	// Routes consist of a path and a handler function.
	r.HandleFunc("/", YourHandler)

	// Bind to a port and pass our router in
	log.Fatal(http.ListenAndServe(":8000", r))
}
```

## License

BSD licensed. See the LICENSE file for details.
context
=======
[![Build Status](https://travis-ci.org/gorilla/context.png?branch=master)](https://travis-ci.org/gorilla/context)

gorilla/context is a general purpose registry for global request variables.

Read the full documentation here: http://www.gorillatoolkit.org/pkg/context
gorilla/handlers
================
[![GoDoc](https://godoc.org/github.com/gorilla/handlers?status.svg)](https://godoc.org/github.com/gorilla/handlers) [![Build Status](https://travis-ci.org/gorilla/handlers.svg?branch=master)](https://travis-ci.org/gorilla/handlers)
[![Sourcegraph](https://sourcegraph.com/github.com/gorilla/handlers/-/badge.svg)](https://sourcegraph.com/github.com/gorilla/handlers?badge)


Package handlers is a collection of handlers (aka "HTTP middleware") for use
with Go's `net/http` package (or any framework supporting `http.Handler`), including:

* [**LoggingHandler**](https://godoc.org/github.com/gorilla/handlers#LoggingHandler) for logging HTTP requests in the Apache [Common Log
  Format](http://httpd.apache.org/docs/2.2/logs.html#common).
* [**CombinedLoggingHandler**](https://godoc.org/github.com/gorilla/handlers#CombinedLoggingHandler) for logging HTTP requests in the Apache [Combined Log
  Format](http://httpd.apache.org/docs/2.2/logs.html#combined) commonly used by
  both Apache and nginx.
* [**CompressHandler**](https://godoc.org/github.com/gorilla/handlers#CompressHandler) for gzipping responses.
* [**ContentTypeHandler**](https://godoc.org/github.com/gorilla/handlers#ContentTypeHandler) for validating requests against a list of accepted
  content types.
* [**MethodHandler**](https://godoc.org/github.com/gorilla/handlers#MethodHandler) for matching HTTP methods against handlers in a
  `map[string]http.Handler`
* [**ProxyHeaders**](https://godoc.org/github.com/gorilla/handlers#ProxyHeaders) for populating `r.RemoteAddr` and `r.URL.Scheme` based on the
  `X-Forwarded-For`, `X-Real-IP`, `X-Forwarded-Proto` and RFC7239 `Forwarded`
  headers when running a Go server behind a HTTP reverse proxy.
* [**CanonicalHost**](https://godoc.org/github.com/gorilla/handlers#CanonicalHost) for re-directing to the preferred host when handling multiple 
  domains (i.e. multiple CNAME aliases).
* [**RecoveryHandler**](https://godoc.org/github.com/gorilla/handlers#RecoveryHandler) for recovering from unexpected panics.

Other handlers are documented [on the Gorilla
website](http://www.gorillatoolkit.org/pkg/handlers).

## Example

A simple example using `handlers.LoggingHandler` and `handlers.CompressHandler`:

```go
import (
    "net/http"
    "github.com/gorilla/handlers"
)

func main() {
    r := http.NewServeMux()

    // Only log requests to our admin dashboard to stdout
    r.Handle("/admin", handlers.LoggingHandler(os.Stdout, http.HandlerFunc(ShowAdminDashboard)))
    r.HandleFunc("/", ShowIndex)

    // Wrap our server with our gzip handler to gzip compress all responses.
    http.ListenAndServe(":8000", handlers.CompressHandler(r))
}
```

## License

BSD licensed. See the included LICENSE file for details.

# cascadia

[![](https://travis-ci.org/andybalholm/cascadia.svg)](https://travis-ci.org/andybalholm/cascadia)

The Cascadia package implements CSS selectors for use with the parse trees produced by the html package.

To test CSS selectors without writing Go code, check out [cascadia](https://github.com/suntong/cascadia) the command line tool, a thin wrapper around this package.
# [go-xkcd](https://github.com/nishanths/go-xkcd)

xkcd API client for Golang.

[![wercker status](https://app.wercker.com/status/6c1de0bfd64a428d6ece5a2337268160/s "wercker status")](https://app.wercker.com/project/bykey/6c1de0bfd64a428d6ece5a2337268160) [![Coverage Status](https://coveralls.io/repos/github/nishanths/go-xkcd/badge.svg?branch=master)](https://coveralls.io/github/nishanths/go-xkcd?branch=master)
[![GoDoc](https://godoc.org/github.com/nishanths/go-xkcd?status.svg)](https://godoc.org/github.com/nishanths/go-xkcd)

[<img alt="https://xkcd.com/1481/" title="https://xkcd.com/1481/" src="http://imgs.xkcd.com/comics/api.png" width="250">](https://xkcd.com/1481/)

Details on the xkcd API can be found [here](https://xkcd.com/json.html).

## Install

```
$ go get github.com/nishanths/go-xkcd
```

Import the package using `github.com/nishanths/go-xkcd` and refer to it as `xkcd`. 

Each major version has a separate branch. If you need a specific version, please clone the branch instead.

## Example

The following program prints details about [xkcd.com/599](http://xkcd.com/599):

```go
package main

import (
    "fmt"
    "log"

    "github.com/nishanths/go-xkcd"
)

func main() {
    client := xkcd.NewClient()
    comic, err := client.Get(599)

    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("%s: %s", comic.Title, comic.ImageURL) // Apocalypse: http://imgs.xkcd.com/comics/apocalypse.png
}

```

## Test

To run tests:

```
$ go test
```

## Documentation

View Go Doc [online](https://godoc.org/github.com/nishanths/go-xkcd).

To view go docs locally, after installing the package run:

```
$ godoc -http=:6060
```

Then visit [`http://localhost:6060/pkg/github.com/nishanths/go-xkcd/`](http://localhost:6060/pkg/github.com/nishanths/go-xkcd/) in your browser.

#### Methods

The following methods are available on the client. All the methods return `(Comic, error)`.

* `Latest()`
* `Get(number int)`
* `Random()`
* `RandomInRange(begin, end, latest int)`

The fields available on `Comic` are:

```
type Comic struct {
	Alt         string
	PublishDate time.Time
	ImageURL    string
	URL         string
	News        string
	Number      int
	SafeTitle   string
	Title       string
	Transcript  string
}
```

## Contributing

Pull requests and issues are welcome!

To create a new pull request:

1. Fork the repository
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Add some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request on GitHub

## License

The [MIT License](http://nishanths.mit-license.org). Copyright © [Nishanth Shanmugham](https://github.com/nishanths).
![kafka-logo](https://raw.githubusercontent.com/blacktop/docker-kafka-alpine/master/docs/kafka-logo.png)

docker-kafka-alpine
===================

[![CircleCI](https://circleci.com/gh/blacktop/docker-kafka-alpine.png?style=shield)](https://circleci.com/gh/blacktop/docker-kafka-alpine) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/kafka.svg)](https://hub.docker.com/r/blacktop/kafka/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/kafka.svg)](https://hub.docker.com/r/blacktop/kafka/) [![Docker Image](https://img.shields.io/badge/docker%20image-226MB-blue.svg)](https://hub.docker.com/r/blacktop/kafka/)

Alpine Linux based [Kafka](http://kafka.apache.org/downloads.html) Docker Image

### Dependencies

-	[alpine:3.6](https://hub.docker.com/_/alpine/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/kafka      latest              226MB
blacktop/kafka      0.11                226MB
blacktop/kafka      0.10                223MB
blacktop/kafka      0.9                 238.6MB
blacktop/kafka      0.8                 227.5MB
```

### Getting Started

> **NOTE:** I am assuming use of Docker for Mac with these examples. (`KAFKA_ADVERTISED_HOST_NAME=localhost`\)

```
docker run -d \
           --name kafka \
           -p 9092:9092 \
           -e KAFKA_ADVERTISED_HOST_NAME=localhost \
           -e KAFKA_CREATE_TOPICS="test-topic:1:1" \
           blacktop/kafka
```

This will create a single-node kafka broker (*listening on localhost:9092*), a local zookeeper instance and create the topic `test-topic` with 1 `replication-factor` and 1 `partition`.

You can now test your new single-node kafka broker using [Shopify/sarama's](https://github.com/Shopify/sarama) **kafka-console-producer** and **kafka-console-consumer**

#### Required

 - [Golang](https://golang.org/doc/install)

```bash
$ go get github.com/Shopify/sarama/tools/kafka-console-consumer
$ go get github.com/Shopify/sarama/tools/kafka-console-producer
```

Now start a *consumer* in the background and then send some data to **kafka** via a *producer*

```bash
$ kafka-console-consumer --bootstrap-server=localhost:9092 --topic=test-topic &
$ echo "shrinky-dinks" | kafka-console-producer --broker-list=localhost:9092 --topic=test-topic
```

### Documentation

-	[Usage](https://github.com/blacktop/docker-kafka-alpine/blob/master/docs/usage.md)
-	[Tips and Tricks](https://github.com/blacktop/docker-kafka-alpine/blob/master/docs/tips.md)

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-kafka-alpine/issues/new)

### Credits

Heavily (if not entirely) influenced by https://github.com/wurstmeister/kafka-docker

### Todo

-	[x] Add ability to run a single node kafka broker when you don't supply a zookeeper link.

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-kafka-alpine/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-kafka-alpine/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-kafka-alpine/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2016-2017 **blacktop**
docker-es-demo-data
===================

[![CircleCI](https://circleci.com/gh/blacktop/docker-es-demo-data.png?style=shield)](https://circleci.com/gh/blacktop/docker-es-demo-data) [![License](https://img.shields.io/badge/licence-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/es-data.svg)](https://hub.docker.com/r/blacktop/es-data/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/es-data.svg)](https://hub.docker.com/r/blacktop/es-data/) [![Docker Image](https://img.shields.io/badge/docker%20image-24.2MB-blue.svg)](https://hub.docker.com/r/blacktop/es-data/)

> Nginx Demo Data for Elasticsearch

---

### Dependencies

-	[alpine:3.6](https://hub.docker.com/_/alpine/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/es-data   latest              24.2MB
blacktop/es-data   5.5                 24.2MB
blacktop/es-data   5.0                 21.1MB
```

### Getting Started

Add Nginx Demo Data to Your Elasticsearch cluster

```bash
$ docker run -d --name elastic -p 9200:9200 blacktop/elasticsearch:geoip
$ docker run -d --name kibana --link elastic:elasticsearch -p 5601:5601 blacktop/kibana
$ docker run --rm --link elastic:elasticsearch blacktop/es-data
```

> **NOTE:** To use with an **x-pack** image use the env vars to set the creds like so

```bash
$ docker run -d --name elasticsearch -p 9200:9200 blacktop/elasticsearch:x-pack
$ docker run -d --name kibana --link elasticsearch -p 5601:5601 blacktop/kibana:x-pack
$ docker run --rm --link elasticsearch -e ES_USERNAME=elastic -e ES_PASSWORD=changeme blacktop/es-data
```

![es-data](https://raw.githubusercontent.com/blacktop/docker-es-demo-data/master/add-data-dashboard.png)

To use with a non-docker Elasticsearch node

```bash
$ docker run --rm -e ES_URL=http://localhost:9200 blacktop/es-data
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-es-demo-data/issues/new)

### Credits

-	https://github.com/elastic/examples/tree/master/ElasticStack_NGINX-json

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-es-demo-data/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-es-demo-data/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-es-demo-data/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

Apache License (Version 2.0) Copyright (c) elastic.co
## wonderfall/lychee

![](http://lychee.electerious.com/assets/images/showcase.jpg)

#### What is this?
Lychee is a free photo-management tool, which runs on your server or web-space. Installing is a matter of seconds. Upload, manage and share photos like from a native application. Lychee comes with everything you need and all your photos are stored securely.

#### Features
- Based on Alpine Linux.
- nginx + PHP7.
- Active Imagemagick + pecl ext.

#### Build-time variables
- **LYCHEE_VERSION** : version of Lychee.
- **IMAGICK_EXT_VERSION** : version of imagick pecl extension.

#### Environment variables
- **UID** : lychee user id *(default : 991)*.
- **GID** : lychee group id *(default : 991)*.

#### Volumes
- **/lychee/uploads** : uploads.
- **/lychee/data** : data files.

#### Ports
- **8888** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)
## wonderfall/libresonic

![](https://img.targaryen.house/wMkrBCxj/Fv8JINdd.png)

#### What is Libresonic?
Libresonic is an open-source web-based media streamer and jukebox. Supports MP3, OGG, AAC and other streamable audio and video formats. Indeed, this build comes with **transcoding abilities**. More info here : https://libresonic.github.io/

#### Build-time variables
- **VERSION** : version of libresonic

#### Environment variables
- **GID** : subsonic group id.
- **UID** : subsonic user id.
- **HTTPS_PORT** : set if you access Libresonic through HTTPS *(default : 0 = disabled)*

#### Volumes
- **/musics** : your music files
- **/data** : subsonic data

#### Port
- 4040

#### Reverse proxy
https://github.com/Wonderfall/dockerfiles/tree/master/reverse

https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration

Libresonic does not support SSL/TLS by itself. If you want to use Libresonic through https, this is what I'm using :

```
  location / {
    proxy_pass http://libresonic:4040;
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Real-IP         $remote_addr;
    proxy_set_header X-Forwarded-For   $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto https;
    proxy_set_header Host              $http_host;
    proxy_max_temp_file_size           0;
    proxy_redirect                     http:// https://;
  }
```
## wonderfall/mastodon

![Mastodon](https://github.com/tootsuite/mastodon/blob/master/app/javascript/images/mastodon-getting-started.png?raw=true)

A GNU Social-compatible microblogging server : https://github.com/tootsuite/mastodon

#### Why this image?
This image is not the official one. The main difference you can notice is that all processes (web, streaming, sidekiq) are running in a single container, thanks to s6 (a supervision suite). Therefore it's easier to deploy, but not recommended for scaling on more than one machine.

#### Features
- Based on Alpine Linux 3.6.
- As lightweight as possible. 
- All-in-one container (s6).
- Assets are precompiled.
- Database migrations can be run at startup.
- No root processes.

#### Docker Hub tags
- **master** : latest code for adventurers (builds daily on Docker Hub)
- **stable** : latest stable version (builds weekly on Docker Hub)
- **targaryen** : Targaryen-themed stable version (builds weekly on Docker Hub)

#### Build-time variables
- **VERSION** : version of Mastodon, can be `v1.4.1` or `master`. *(default : master)*
- **REPOSITORY** : location of the code *(default : tootsuite/mastodon)*

#### Environment variables you should change
- **UID** : mastodon user id *(default : 991)*
- **GID** : mastodon group id *(default : 991)*
- **RUN_DB_MIGRATIONS** : run `rake db:migrate` at startup *(default : true)*
- **SIDEKIQ_WORKERS** :  number of Sidekiq workers *(default : 5)*
- Other environment variables : https://github.com/tootsuite/mastodon/blob/master/.env.production.sample

#### Volumes
- **/mastodon/public/system** : shit like media attachments, avatars, etc.
- **/mastodon/public/assets** : Mastodon assets
- **/mastodon/public/packs** : Mastodon assets
- **/mastodon/log** : Mastodon logfiles (mount if you prefer to)

#### Ports
- **3000** : Mastodon web
- **4000** : Mastodon streaming

#### docker-compose sample

```
mastodon:
  image: wonderfall/mastodon:stable
  restart: always
  container_name: mastodon
  env_file: /home/docker/mastodon/.env.production
  environment:
    - WEB_CONCURRENCY=16
    - MAX_THREADS=20
    - SIDEKIQ_WORKERS=25
    - RUN_DB_MIGRATIONS=true
  links:
    - mastodon-pgb
    - mastodon-redis
  volumes:
    - /home/docker/mastodon/public/system:/mastodon/public/system
    - /home/docker/mastodon/public/assets:/mastodon/public/assets
    - /home/docker/mastodon/public/packs:/mastodon/public/packs
```

## wonderfall/rtorrent-flood

![](https://camo.githubusercontent.com/d8f5cb502f06e0ea1cc171550c2bed035293c1a9/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6a6f686e667572726f772e636f6d2f73686172652f666c6f6f642d73637265656e73686f742d612d303630362e706e67)

#### Main features
- Based on Alpine Linux.
- rTorrent and libtorrent are compiled from source.
- Provides by default a solid configuration.
- [Filebot](http://www.filebot.net/) is included, and creates symlinks in `/data/Media`.
- [Flood](https://github.com/jfurrow/flood), a modern web UI for rTorrent with a Node.js backend and React frontend.

#### Build-time variables
- **RTORRENT_VER** : rtorrent version
- **LIBTORRENT_VER** : libtorrent version
- **MEDIAINFO_VER** : libmediainfo version
- **FILEBOT_VER** : filebot version
- **BUILD_CORES** : number of cores used during build

#### Environment variables
- **UID** : user id (default : 991)
- **GID** : group id (defaut : 991)
- **FLOOD_SECRET** : flood secret key (defaut : mysupersecretkey) (CHANGE IT)
- **CONTEXT_PATH** : context path (base_URI) (default : /)
- **RTORRENT_SCGI** : SCGI port (default : 0 for use local socket)
- **PKG_CONFIG_PATH** : `/usr/local/lib/pkgconfig` (don't touch)

### Note
Run this container with tty mode enabled. In your `docker-compose.yml`, add `tty: true`. If you don't do this, [rtorrent will use 100% of CPU](https://github.com/Wonderfall/dockerfiles/issues/156).

#### Ports
- **49184** (bind it).
- **3000** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)

#### Tags
- **latest** : latest versions of rTorrent/libtorrent.
- Use **$RTORRENT_VER-$LIBTORRENT_VER** to get specific versions of rTorrent/libtorrent.

#### Volumes
- **/data** : your downloaded torrents, session files, symlinks...
- **/flood-db** : Flood databases.
## wonderfall/mediawiki

Host your own Wiki!

#### Features
- Based on Alpine Linux (wonderfall/nginx-php image)
- Bundled with nginx and PHP7.1.

#### Build-time variables
- **MEDIAWIKI_VER** : Mediawiki version
- **SUB_VERSION** : Mediawiki subversion

#### Environment variables
- **UID** : privatebin user id
- **GID** : privatebin group id
- **MEMORY_LIMIT** : php memorny limit *(default : 128M)*
- **UPLOAD_MAX_SIZE** : maximum upload size *(default : 10M)*

#### Volumes
- /mediawiki/images
- /extensions
- /skins
- /config
- /mediawiki/custom

#### Ports
- **8888** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)

#### docker-compose.yml sample

```
mywiki:
  image: wonderfall/mediawiki
  container_name: mywiki
  links:
    - mywiki-db:mywiki-db
    - mywiki-parsoid:mywiki-parsoid
  environment:
    - UPLOAD_MAX_SIZE=20M
    - MEMORY_LIMIT=512M
    - UID=1668
    - GID=1668
  volumes:
    - /mnt/mywiki/images:/mediawiki/images
    - /mnt/mywiki/extensions:/extensions
    - /mnt/mywiki/skins:/skins
    - /mnt/mywiki/config:/config
    - /mnt/mywiki/custom:/mediawiki/custom

mywiki-db:
  image: mariadb:10.1
  container_name: mywiki-db
  volumes:
    - /mnt/mywiki/db:/var/lib/mysql
  environment:
    - MYSQL_ROOT_PASSWORD=supersecret
    - MYSQL_DATABASE=mywiki
    - MYSQL_USER=mywiki
    - MYSQL_PASSWORD=supersecret

mywiki-parsoid:
  image: wonderfall/parsoid
  container_name: mywiki-parsoid
  environment:
     - UID=1669
     - GID=1669
     - ADDRESS=https://wiki.domain.com/
     - DOMAIN=mywiki-parsoid
```
### wonderfall/cowrie

#### What is this?
Cowrie is a medium interaction SSH honeypot designed to log brute force attacks and the shell interaction performed by the attacker. Cowrie is based on Kippo.

#### Build-time variables
- **MPFR_VERSION** : GNU MPFR version.
- **MPC_VERSION** : GNU MPC version.
- **GPG_** : fingerprints of signing keys.
- **SHA_** : fingerprints of tarballs

#### Environment variables
- **UID** *(default : 991)*
- **GID** *(default : 991)*

#### How to configure
You should provide your own configuration file from this base : https://raw.githubusercontent.com/micheloosterhof/cowrie/master/cowrie.cfg.dist
You can mount this single file to your Docker container.

#### Volumes
- **/cowrie/dl** : where downloads are stored.
- **/cowrie/log** : cowrie and tty sessions logs.
- **/cowrie/cowrie.cfg** : cowrie configuration file. **Provide yours!**
- **/custom** : customize cowrie structure with your own files

#### Docker compose (example)
```
cowrie:
  image: wonderfall/cowrie
  links:                           ### MySQL output
    - cowrie-db:cowrie-db          ### MySQL output
  ports:
    - "2222:2222"
  volumes:
    - /mnt/cowrie/dl:/cowrie/dl
    - /mnt/cowrie/log:/cowrie/log
    - /mnt/cowrie/custom:/custom
    - /mnt/cowrie/cowrie.cfg:/cowrie/cowrie.cfg
  environment:
    - GID=1000
    - UID=1000

### MySQL output
# First, you'll have to initialise tables with a .sql file
# mkdir -p /mnt/cowrie/sql
# wget https://raw.githubusercontent.com/micheloosterhof/cowrie/master/doc/sql/mysql.sql -P /mnt/cowrie/sql/cowrie.sql
# It needs also to be configured in the cowrie.cfg file

cowrie-db:
  image: mariadb:10
  volumes:
    - /mnt/cowrie/db:/var/lib/mysql
    - /mnt/cowrie/sql:/docker-entrypoint-initdb.d
  environment:
    - MYSQL_ROOT_PASSWORD=supersecretpassword
    - MYSQL_DATABASE=cowrie
    - MYSQL_USER=cowrie
    - MYSQL_PASSWORD=supersecretpassword
```

## wonderfall/searx

![](https://i.goopics.net/ls.png)

#### What is searx?
Searx is a metasearch engine, inspired by the seeks project.
It provides basic privacy by mixing your queries with searches on other platforms without storing search data. Queries are made using a POST request on every browser (except chrome*). Therefore they show up in neither our logs, nor your url history. In case of Chrome* users there is an exception, Searx uses the search bar to perform GET requests. Searx can be added to your browser's search bar; moreover, it can be set as the default search engine. 

#### Features
- Based on Alpine Linux.
- Latest code from [asciimoo/searx](https://github.com/asciimoo/searx)
- A unique secret key is generated when booting the first time.

#### Build-time variables
- **VERSION** : Searx version

#### Environment variables
- **IMAGE_PROXY** : enables images proxying *(default : False)*
- **BASE_URL** : http://domain.tld *(default : False)*

#### Ports
- **8888** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)
## wonderfall/boring-nginx

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Nginx_logo.svg/115px-Nginx_logo.svg.png)

#### What is this?
This is nginx statically linked against BoringSSL, with embedded Brotli support.

#### Features
- Based on Alpine Linux.
- nginx built against **BoringSSL** with SSE/SHA, and AVX2 SIMD-instructions.
- **TLS 1.3** patch : use of TLS 1.3 DRAFT is enforced (haven't found another way yet).
- Built using hardening gcc flags.
- Dynamic TLS records patch (cloudflare).
- TTP/2 (+NPN) support.
- Brotli compression support (and configured).
- No root master process.
- AIO Threads support.
- No unnessary modules (except fastcgi).
- PCRE-jit enabled.
- Strong configurations included.
- Anonymous webserver signature (headers-more).
- ngxpasswd : generates a htpasswd file.
- ngxproxy : generates a proxy virtual host file.

#### Notes
- It is required to change the `listen` directive to 8000/4430 instead of 80/443.
- Linux 3.17+, and the latest Docker stable are recommended.
- BoringSSL is naming ECDH curves differently, some modifications will be required if you want to use your own SSL/TLS config file. For example, `secp384r1` (OpenSSL, LibreSSL) is `P-384` (BoringSSL). BoringSSL does support multiple curves with its implementation of `SSL_CTX_set1_curves_list()`, an example is provided in the default `/etc/nginx/confssl_params`. `X25519` is actually the safest curve you can use so it should be the first curve in your list.
- BoringSSL can use cipher groups : a group is defined by brackets and ciphers are separated by `|` like this : `[cipher1|cipher2|cipher3]`. Ciphers in a group are considered equivalent on the server-side and let the client decide which cipher is the best. This can be useful when using ChaCha20, because AES remains faster than ChaCha20 on AES-NI devices.

#### Volumes
- **/sites-enabled** : vhosts files (*.conf)
- **/conf.d** : additional configuration files
- **/certs** : SSL/TLS certificates
- **/var/log/nginx** : nginx logs
- **/passwds** : authentication files
- **/www** : put your websites there

#### Build-time variables
- **NGINX_VERSION** : version of nginx
- **GPG_NGINX** : fingerprint of signing key package
- **BUILD_CORES** : number of cores used during compilation

#### Environment variables
- **GID** : nginx group id *(default : 991)*
- **UID** : nginx user id *(default : 991)*

#### How to use it?
https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration

You can use `ngxproxy` to generate a *vhost* through an easy process : `docker exec -ti nginx ngxproxy`. `ngxpasswd` can generate htpasswd files : `docker exec -ti nginx ngxpasswd`. Both utilites are interactive so you won't feel lost.

Some configuration files located in `/etc/nginx/conf` are already provided, you can use them with the `include` directive.

- `ssl_params` : Provides a nice balance between compatibility and security.
- `headers_params` : HSTS (+ preload), XSS protection, etc.
- `proxy_params` : use with `proxy_pass`.
## wonderfall/piwik

![](https://i.goopics.net/m3.png)

#### What is this?
It is a web analytics platform. Piwik respects your privacy and gives you full control over your data.

#### Features
- Based on Alpine Linux.
- Latest Piwik stable.
- nginx stable + PHP7.
- mysql drivers (server not built-in).
- Latest GeoLite City Database from maxmind.com.

#### Build-time variables
- **VERSION** : version of Piwik
- **GPG_matthieu** : fingerprint of signing key

#### Environment variables
- **GID** : piwik group id *(default : 991)*
- **UID** : piwik user id *(default : 991)*

#### Volumes
- **/config** : configuration files

#### Update
Piwik can update itself. It works well. I'm also maintaing this Dockerfile, so if you don't want to do upgrades directly from Piwik, you can recreate the container as well whenever I push an update.

#### Configuration
According to Piwik, everything should be fine running this image. You shoudn't have any difficulties to setup your own instance of Piwik. Your `/config/config.ini.php` overwrites the one (in `/piwik/config`)used by Piwik each time the container is started. Moreover, the old config.ini.php is saved as `/config/config.ini.php.bkp` if you want to revert last changes. This should also guarantee transparency through Piwik's updates.

If you're running Piwik behind a reverse proxy (most likely you do), add this to your `config.ini.php` :

```
[General]
#assume_secure_protocol = 1 #uncomment if you use https
proxy_client_headers[] = HTTP_X_FORWARDED_FOR
proxy_client_headers[] = HTTP_X_REAL_IP
proxy_host_headers[] = HTTP_X_FORWARDED_HOST
```

#### Reverse proxy
Use port **8888**.
https://github.com/Wonderfall/dockerfiles/tree/master/reverse
https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration

#### Docker Compose (example)
```
piwik:
  image: wonderfall/piwik
  links:
    - db_piwik:db_piwik
  volumes:
    - /mnt/docker/piwik/config:/config
  environment:
    - GID=1000
    - UID=1000

db_piwik:
  image: mariadb:10
  volumes:
    - /mnt/docker/piwik/db:/var/lib/mysql
  environment:
    - MYSQL_ROOT_PASSWORD=asupersecretpassword
    - MYSQL_DATABASE=piwik
    - MYSQL_USER=piwik
    - MYSQL_PASSWORD=asupersecretpassword
```
## wonderfall/isso

![](https://i.goopics.net/q1.png)


#### What is this?
Isso is a commenting server similar to Disqus. More info on the [official website](https://posativ.org/isso/).

#### Features
- Based on Alpine Linux 3.3.
- Latest Isso installed with `pip`.

#### Build-time variables
- **ISSO_VER** : version of Isso.

#### Environment variables
- **GID** : isso group id *(default : 991)*
- **UID** : isso user id *(default : 991)*

#### Volumes
- **/config** : location of configuration files.
- **/db** : location of SQLite database.

#### Ports
- **8080** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration).

#### Example of simple configuration
Here is the full documentation : https://posativ.org/isso/docs/

```
# /mnt/docker/isso/config/isso.conf
[general]
dbpath = /db/comments.db
host = https://cats.schrodinger.io/
[server]
listen = http://0.0.0.0:8080/

# docker-compose.yml
isso:
  image: wonderfall/isso
  environment:
    - GID=1000
    - UID=1000
  volumes:
    - /mnt/docker/isso/config:/config
    - /mnt/docker/isso/db:/db
```
## wonderfall/tor

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Tor-logo-2011-flat.svg/612px-Tor-logo-2011-flat.svg.png)

#### Features
- Based on Alpine Linux.
- Tor built from source.
- ARM included, allowing real-time monitoring.

#### Usages
- As a relay ([french article](https://wonderfall.xyz/un-relais-tor-avec-docker/)).
- As a hidden service dir.

### Build-time variables
- **TOR_VERSION** : version of Tor.
- **TOR_USER_ID** : tor user id *(default : 45553)*
- **ARM_VERSION** : version of ARM
- **GPG_** : fingerprints of signing keys

#### Environment variables
- **TERM** = xterm (ARM requirement)
- **UID** = tor user id
- **GID** = tor group id

#### Volumes
- **/tor/config** : tor configuration files.
- **/tor/data** : tor data.

#### Ports
- **9001** (bind it) : ORPort.
- **9030** (bind it) : DirPort.
## wonderfall/rutorrent
Originally forked from [xataz/rutorrent](https://github.com/xataz/dockerfiles/tree/master/rutorrent).

#### What is this?
This container contains both rtorrent (whis is a BitTorrent client) and rutorrent (which is a front-end for rtorrent). Filebolt is also included, the default behavior is set to create clean symlinks, so media players like Emby/Plex can easily detect your TV shows and movies.

![](https://pix.schrodinger.io/KDVxwnJA/nEMCzJEd.jpg)

#### Main features
- Lightweight, since it's based on Alpine Linux.
- Everything is almost compiled from source.
- Secured, don't bother about configuration files.
- Filebot is included, and creates symlinks in `/data/Media`.
- rutorrent : Material theme by phlo set by default.
- rutorrent : nginx + PHP7.

#### Ports

- **49184** (bind it).
- **80** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)

#### Volumes
- **/data** : your files, symlinks, and so on.
- **/var/www/torrent/share/users** : rutorrent settings.## wonderfall/nginx

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Nginx_logo.svg/115px-Nginx_logo.svg.png)

#### What is this?
It is nginx statically linked against LibreSSL, with the following modules embedded : ngx_brotli (Brotli compression support) and headers_more. Secured by default (no root processes, even the master one), so it should be safe to use.

#### Features
- Based on Alpine Linux Edge.
- nginx built against **LibreSSL**.
- nginx : Cloudfare's SPDY patch.
- nginx : Cloudfare's dynamic TLS records patch.
- nginx : securely built using hardening gcc flags.
- nginx : HTTP/2 (+NPN) support.
- nginx : Brotli compression support (and configured).
- nginx : Headers More module.
- nginx : no root master process.
- nginx : AIO Threads support.
- nginx : no unnessary modules (except fastcgi).
- nginx : pcre-jit enabled.
- nginx : optimized configuration.
- ngxpasswd : generates a htpasswd file easily.
- ngxproxy : generates a *proxy vhost* after asking you a few questions.

#### Notes
It is required to chown your certs files with the right uid/pid and change the `listen` directive to 8000/4430 instead of 80/443. Linux 3.17+, and the latest Docker stable are recommended.

#### Volumes
- **/sites-enabled** : vhosts files (*.conf)
- **/conf.d** : additional configuration files
- **/certs** : SSL/TLS certificates
- **/var/log/nginx** : nginx logs
- **/passwds** : authentication files
- **/www** : put your websites there

#### Build-time variables
- **NGINX_VERSION** : version of nginx
- **LIBRESSL_VERSION** : version of LibreSSL
- **GPG_NGINX** : fingerprint of signing key package
- **GPG_LIBRESSL** : fingerprint of signing key package
- **BUILD_CORES** : number of cores you'd like to build with (default : all)

#### Environment variables
- **GID** : nginx group id *(default : 991)*
- **UID** : nginx user id *(default : 991)*

#### How to use it?
https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration

You can use `ngxproxy` to generate a *vhost* through an easy process : `docker exec -ti nginx ngxproxy`. `ngxpasswd` can generate htpasswd files : `docker exec -ti nginx ngxpasswd`. Both utilites are interactive so you won't feel lost.

Some configuration files located in `/etc/nginx/conf` are already provided, you can use them with the `include` directive.

- `ssl_params` : Provides a nice balance between compatibility and security.
- `headers_params` : HSTS (+ preload), XSS protection, etc.
- `proxy_params` : use with `proxy_pass`.
## wonderfall/boinc

![boinc](https://boinc.berkeley.edu/logo/boinc_600.jpg)

BOINC is an open-source software platform for computing using volunteered resources.

Please visit website : http://boinc.berkeley.edu/
## wonderfall/rainloop

![](https://i.goopics.net/nI.png)

#### What is this?
Rainloop is a simple, modern & fast web-based client. More info on the [official website](http://www.rainloop.net/).

#### Features
- Based on Alpine 3.3
- Latest Rainloop **Community Edition**
- Contacts (DB) : sqlite, or mysql (server not built-in)
- nginx + PHP7

#### Build-time variables
- **GPG_rainloop** : fingerprint of signing key

#### Environment variables
- **GID** : rainloop group id *(default : 991)*
- **UID** : rainloop user id *(default : 991)*

#### Volumes
- **/rainloop/data** : data files.

#### Ports
- **8888***

#### Reverse proxy
https://github.com/Wonderfall/dockerfiles/tree/master/reverse
https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration
## wonderfall/lutim

![](https://i.goopics.net/rf.png)

#### What is this?
LUTIM means Let's Upload That Image.
It stores images and allows you to see them, download them or share them on social networks. From version 0.5, the gif images can be displayed as animated gifs in Twitter, but you need a HTTPS server (Twitter requires that. Lutim detects if you have a HTTPS server and displays an static image twitter card if you don't);

Images are indefinitly stored unless you request that they will be deleted at first view or after 24 hours / one week / one month / one year.

#### Build-time variables
- **TINI_VER** : version of `tini`.

#### Environment variables
- **GROUPID** : lutim group id. *(default : 1000)*
- **USERID** : lutim user id. *(default : 1000)*
- **SECRET** : random string used to encrypt cookies. *(default : ZyCnLAhYKBIJrukuKZZJ)*
- **CONTACT** : lutim contact. *(default : contact@domain.tld)*
- **MAX_FILE_SIZE** : maximum file size of an uploaded file in bytes. *(default : 1GB)*
- **WEBROOT** : webroot of lutim. *(default : /)*
- **DOMAIN** : your domain used with lutim *(default : domain.tld)*

*Tip : you can use the following command to generate SECRET.*
`cat /dev/urandom | tr -dc 'a-zA-Z' | fold -w 20 | head -n 1`

#### Volumes
- **/data** : where lutim's database is stored.
- **/lutim/files** : location of uploaded files.

#### Ports
- **8181** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration).![nginx-php](http://apmblog.dynatrace.com/wp-content/uploads/2014/10/PHP-on-Nginx.jpg)

> This image is build and push with [drone.io](https://github.com/drone/drone), a circle-ci like self-hosted.
> If you don't trust, you can build yourself.

## Description
What is [Nginx](http://nginx.org)?

nginx (engine x) is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP proxy server, originally written by Igor Sysoev. For a long time, it has been running on many heavily loaded Russian sites including Yandex, Mail.Ru, VK, and Rambler. According to Netcraft, nginx served or proxied 24.29% busiest sites in December 2015. Here are some of the success stories: Netflix, Wordpress.com, FastMail.FM.

What is [PHP](https://secure.php.net/)?

PHP is a popular general-purpose scripting language that is especially suited to web development.
Fast, flexible and pragmatic, PHP powers everything from your blog to the most popular websites in the world.


## BUILD IMAGE
### Build arguments
* BUILD_CORES : Number of cpu's core for compile (default : empty for use all cores)
* NGINX_VER : Nginx version (default : latest version)
* NGINX_GPG : Nginx gpg fingerprint
* NGINX_CONF : Nginx build arguments (default : see Dockerfile)
* PHP_VER : PHP version (default : latest version)
* PHP_MIRROR: Mirror for download PHP (default : http://fr2.php.net)
* PHP_GPG : PHP gpg fingerprint
* PHP_CONF : PHP build arguments (default : see Dockerfile)
* PHP_EXT_LIST : PHP extensions list, for install there (default : see Dockerfile)
* CUSTOM_BUILD_PKGS : Necessary packages for build PHP extension, there packages are remove after build (default : see Dockerfile)
* CUSTOM_PKGS : Necessary package for PHP extension (default : see Dockerfile)

### simple build
```shell
docker build -t xataz/nginx-php github.com/xataz/dockerfiles.git#master:nginx-php
```

### Build with arguments
```shell
docker build -t xataz/nginx-php \
        --build-arg NGINX_VER=1.10.1 \
        --build-arg PHP_VER=5.6.27 \
        --build-arg PHP_EXT_LIST="gd mysqli gmp" \
        --build-arg CUSTOM_BUILD_PKGS="freetype-dev gmp-dev" \
        --build-arg CUSTOM_PKGS="freetype gmp" \
        github.com/xataz/dockerfiles.git#master:nginx-php
```


## Configuration
### Environments
* UID : Choose uid for launch rtorrent (default : 991)
* GID : Choose gid for launch rtorrent (default : 991)

### Volumes
* /nginx/sites-enabled : Place your vhost here
* /nginx/log : Log emplacement
* /nginx/run : Here is pid and lock file
* /nginx/conf/nginx.conf : General configuration of nginx
* /nginx/conf.d : folder for other configuration (ex : php.conf, headers_param.conf)

if you mount /nginx/conf.d, use this php.conf :
```shell
location ~ \.php$ {
    fastcgi_index index.php;
    fastcgi_pass unix:/php/run/php-fpm.sock;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    include /nginx/conf/fastcgi_params;
}
```

### Ports
* 8080


## Usage
### Simple launch
```shell
docker run -d -p 8080:8080 xataz/nginx-php
```
URI access : http://XX.XX.XX.XX:8080

### Advanced launch
```shell
docker run -d -p 80:8080 -p 443:8443 \
	  -v /docker/nginx/sites-enabled:/nginx/sites-enabled \
      -v /docker/nginx/certs:/nginx/certs \
	  -e UID=1001 \
	  -e GID=1001 \
	xataz/nginx-php
```
URI access : http://XX.XX.XX.XX

## Contributing
Any contributions, are very welcome !
## wonderfall/nextcloud


[![](https://images.microbadger.com/badges/version/wonderfall/nextcloud.svg)](http://microbadger.com/images/wonderfall/nextcloud "Get your own version badge on microbadger.com") [![](https://images.microbadger.com/badges/image/wonderfall/nextcloud.svg)](http://microbadger.com/images/wonderfall/nextcloud "Get your own image badge on microbadger.com")

[![](https://images.microbadger.com/badges/version/wonderfall/nextcloud:daily.svg)](https://microbadger.com/images/wonderfall/nextcloud:daily "Get your own version badge on microbadger.com") [![](https://images.microbadger.com/badges/image/wonderfall/nextcloud:daily.svg)](https://microbadger.com/images/wonderfall/nextcloud:daily "Get your own image badge on microbadger.com")

[![](https://images.microbadger.com/badges/version/wonderfall/nextcloud:11.0.svg)](https://microbadger.com/images/wonderfall/nextcloud:11.0 "Get your own version badge on microbadger.com") [![](https://images.microbadger.com/badges/image/wonderfall/nextcloud:11.0.svg)](https://microbadger.com/images/wonderfall/nextcloud:11.0 "Get your own image badge on microbadger.com")

![](https://s32.postimg.org/69nev7aol/Nextcloud_logo.png)

**This image was made for my own use and I have no intention to make this official. Support won't be regular so if there's an update, or a fix, you can open a pull request. Any contribution is welcome, but please be aware I'm very busy currently. Before opening an issue, please check if there's already one related. Also please use Github instead of Docker Hub, otherwise I won't see your comments. Thanks.**

### Features
- Based on Alpine Linux.
- Bundled with nginx and PHP 7.1 (wonderfall/nginx-php image).
- Automatic installation using environment variables.
- Package integrity (SHA512) and authenticity (PGP) checked during building process.
- Data and apps persistence.
- OPCache (opcocde), APCu (local) installed and configured.
- system cron task running.
- MySQL, PostgreSQL (server not built-in) and sqlite3 support.
- Redis, FTP, SMB, LDAP, IMAP support.
- GNU Libiconv for php iconv extension (avoiding errors with some apps).
- No root processes. Never.
- Environment variables provided (see below).

### Tags
- **latest** : latest stable version. (12.0)
- **12.0** : latest 12.0.x version (stable)
- **11.0** : latest 11.0.x version (old stable)
- **10.0** : latest 10.0.x version (old stable) (unmaintained)
- **9.0** : latest 9.0.x version. (old stable) (unmaintained)
- **daily** : latest code (daily build).

Other tags than `daily` are built weekly. For security reasons, you should occasionally update the container, even if you have the latest version of Nextcloud.

### Build-time variables
- **NEXTCLOUD_VERSION** : version of nextcloud
- **GNU_LIBICONV_VERSION** : version of GNU Libiconv
- **GPG_nextcloud** : signing key fingerprint

### Environment variables
- **UID** : nextcloud user id *(default : 991)*
- **GID** : nextcloud group id *(default : 991)*
- **UPLOAD_MAX_SIZE** : maximum upload size *(default : 10G)*
- **APC_SHM_SIZE** : apc memory size *(default : 128M)*
- **OPCACHE_MEM_SIZE** : opcache memory size in megabytes *(default : 128)*
- **MEMORY_LIMIT** : php memory limit *(default : 512M)*
- **CRON_PERIOD** : time interval between two cron tasks *(default : 15m)*
- **CRON_MEMORY_LIMIT** : memory limit for PHP when executing cronjobs *(default : 1024m)*
- **TZ** : the system/log timezone *(default : Etc/UTC)*
- **ADMIN_USER** : username of the admin account *(default : none, web configuration)*
- **ADMIN_PASSWORD** : password of the admin account *(default : none, web configuration)*
- **DOMAIN** : domain to use during the setup *(default : localhost)*
- **DB_TYPE** : database type (sqlite3, mysql or pgsql) *(default : sqlite3)*
- **DB_NAME** : name of database *(default : none)*
- **DB_USER** : username for database *(default : none)*
- **DB_PASSWORD** : password for database user *(default : none)*
- **DB_HOST** : database host *(default : none)*

Don't forget to use a **strong password** for the admin account!

### Port
- **8888** : HTTP Nextcloud port.

### Volumes
- **/data** : Nextcloud data.
- **/config** : config.php location.
- **/apps2** : Nextcloud downloaded apps.
- **/nextcloud/themes** : Nextcloud themes location.
- **/php/session** : php session files.

### Database
Basically, you can use a database instance running on the host or any other machine. An easier solution is to use an external database container. I suggest you to use MariaDB, which is a reliable database server. You can use the official `mariadb` image available on Docker Hub to create a database container, which must be linked to the Nextcloud container. PostgreSQL can also be used as well.

### Setup
Pull the image and create a container. `/mnt` can be anywhere on your host, this is just an example. Change `MYSQL_ROOT_PASSWORD` and `MYSQL_PASSWORD` values (mariadb). You may also want to change UID and GID for Nextcloud, as well as other variables (see *Environment Variables*).

```
docker pull wonderfall/nextcloud:10.0 && docker pull mariadb:10

docker run -d --name db_nextcloud \
       -v /mnt/nextcloud/db:/var/lib/mysql \
       -e MYSQL_ROOT_PASSWORD=supersecretpassword \
       -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud \
       -e MYSQL_PASSWORD=supersecretpassword \
       mariadb:10
       
docker run -d --name nextcloud \
       --link db_nextcloud:db_nextcloud \
       -v /mnt/nextcloud/data:/data \
       -v /mnt/nextcloud/config:/config \
       -v /mnt/nextcloud/apps:/apps2 \
       -v /mnt/nextcloud/themes:/nextcloud/themes \
       -e UID=1000 -e GID=1000 \
       -e UPLOAD_MAX_SIZE=10G \
       -e APC_SHM_SIZE=128M \
       -e OPCACHE_MEM_SIZE=128 \
       -e CRON_PERIOD=15m \
       -e TZ=Etc/UTC \
       -e ADMIN_USER=mrrobot \
       -e ADMIN_PASSWORD=supercomplicatedpassword \
       -e DOMAIN=cloud.example.com \
       -e DB_TYPE=mysql \
       -e DB_NAME=nextcloud \
       -e DB_USER=nextcloud \
       -e DB_PASSWORD=supersecretpassword \
       -e DB_HOST=db_nextcloud \
       wonderfall/nextcloud:10.0
```

You are **not obliged** to use `ADMIN_USER` and `ADMIN_PASSWORD`. If these variables are not provided, you'll be able to configure your admin acccount from your browser.

**Below you can find a docker-compose file, which is very useful!**

Now you have to use a **reverse proxy** in order to access to your container through Internet, steps and details are available at the end of the README.md. And that's it! Since you already configured Nextcloud through setting environment variables, there's no setup page.

### ARM-based devices
You will have to build yourself using an Alpine-ARM image, like `orax/alpine-armhf:edge`.

### Configure
In the admin panel, you should switch from `AJAX cron` to `cron` (system cron).

### Update
Pull a newer image, then recreate the container as you did before (*Setup* step). None of your data will be lost since you're using external volumes. If Nextcloud performed a full upgrade, your apps could be disabled, enable them again.

### Docker-compose
I advise you to use [docker-compose](https://docs.docker.com/compose/), which is a great tool for managing containers. You can create a `docker-compose.yml` with the following content (which must be adapted to your needs) and then run `docker-compose up -d nextcloud-db`, wait some 15 seconds for the database to come up, then run everything with `docker-compose up -d`, that's it! On subsequent runs,  a single `docker-compose up -d` is sufficient!

#### Docker-compose file
Don't copy/paste without thinking! It is a model so you can see how to do it correctly.

```
nextcloud:
  image: wonderfall/nextcloud
  links:
    - nextcloud-db:nextcloud-db   # If using MySQL
    - solr:solr                   # If using Nextant
    - redis:redis                 # If using Redis
  environment:
    - UID=1000
    - GID=1000
    - UPLOAD_MAX_SIZE=10G
    - APC_SHM_SIZE=128M
    - OPCACHE_MEM_SIZE=128
    - CRON_PERIOD=15m
    - TZ=Europe/Berlin
    - ADMIN_USER=admin            # Don't set to configure through browser
    - ADMIN_PASSWORD=admin        # Don't set to configure through browser
    - DOMAIN=localhost
    - DB_TYPE=mysql
    - DB_NAME=nextcloud
    - DB_USER=nextcloud
    - DB_PASSWORD=supersecretpassword
    - DB_HOST=nextcloud-db
  volumes:
    - /mnt/nextcloud/data:/data
    - /mnt/nextcloud/config:/config
    - /mnt/nextcloud/apps:/apps2
    - /mnt/nextcloud/themes:/nextcloud/themes

# If using MySQL
nextcloud-db:
  image: mariadb:10
  volumes:
    - /mnt/nextcloud/db:/var/lib/mysql
  environment:
    - MYSQL_ROOT_PASSWORD=supersecretpassword
    - MYSQL_DATABASE=nextcloud
    - MYSQL_USER=nextcloud
    - MYSQL_PASSWORD=supersecretpassword
    
# If using Nextant
solr:
  image: solr:6-alpine
  container_name: solr
  volumes:
    - /mnt/docker/solr:/opt/solr/server/solr/mycores
  entrypoint:
    - docker-entrypoint.sh
    - solr-precreate
    - nextant

# If using Redis
redis:
  image: redis:alpine
  container_name: redis
  volumes:
    - /mnt/docker/redis:/data
```

You can update everything with `docker-compose pull` followed by `docker-compose up -d`.

### How to configure Redis
Redis can be used for distributed and file locking cache, alongside with APCu (local cache), thus making Nextcloud even more faster. As PHP redis extension is already included, all you have to is to deploy a redis server (you can do as above with docker-compose) and bind it to nextcloud in your config.php file :

```
'memcache.distributed' => '\OC\Memcache\Redis',
'memcache.locking' => '\OC\Memcache\Redis',
'memcache.local' => '\OC\Memcache\APCu',
'redis' => array(
   'host' => 'redis',
   'port' => 6379,
   ),
```

### How to configure Nextant
You will have to deploy a Solr server, I've shown an example above with docker-compose. Once Nextant app is installed, go to "additional settings" in your admin pannel and use http://solr:8983/solr as "Adress of your Solr Servlet". There you go. You may however experience the same issue as mine : https://github.com/nextcloud/server/pull/3160 (let's hope there'll be at least a backport...).

### Tip : how to use occ command
There is a script for that, so you shouldn't bother to log into the container, set the right permissions, and so on. Use `docker exec -ti nexcloud occ command`.

### Reverse proxy
Of course you can use your own solution to do so! nginx, Haproxy, Caddy, h2o, there's plenty of choices and documentation about it on the Web.

Personally I'm using nginx, so if you're using nginx, there are two possibilites :

- nginx is on the host : get the Nextcloud container IP address with `docker inspect nextcloud | grep IPAddress\" | head -n1 | grep -Eo "[0-9.]+" `. But whenever the container is restarted or recreated, its IP address can change. Or you can bind Nextcloud HTTP port (8888) to the host (so the reverse proxy can access with `http://localhost:8888` or whatever port you set), but in this case you should consider using a firewall since it's also listening to `http://0.0.0.0:8888`.

- nginx is in a container, things are easier : you can link nextcloud container to an nginx container so you can use `proxy_pass http://nextcloud:8888`. If you're interested, I provide a nginx image available on Docker Hub : `wonderfall/boring-nginx`, and it comes with a script called `ngxproxy`, which does all the magic after asking you a few questions. Otherwise, an example of configuration would be :

```
server {
  listen 8000;
  server_name example.com;
  return 301 https://$host$request_uri;
}

server {
  listen 4430 ssl http2;
  server_name example.com;

  ssl_certificate /certs/example.com.crt;
  ssl_certificate_key /certs/example.com.key;

  include /etc/nginx/conf/ssl_params.conf;

  client_max_body_size 10G; # change this value it according to $UPLOAD_MAX_SIZE

  location / {
    proxy_pass http://nextcloud:8888;
    include /etc/nginx/conf/proxy_params;
  }
}
```


Headers are already sent by the container, including HSTS, so there's no need to add them again. **It is strongly recommended to use Nextcloud through an encrypted connection (HTTPS).** [Let's Encrypt](https://letsencrypt.org/) provides free SSL/TLS certificates (trustworthy!).
## wonderfall/parsoid

#### What is this? What features?
- A **simple** Parsoid image.
- Based on Alpine Linux so it's lightweight!
- Bundled with latest node.js available (version check is disabled).

#### Build-time variables
- **VERSION** : version of Ghost.

#### Environment variables
- **GID** : ghost user id *(default : 991)*
- **UID** : ghost group id *(default : 991)*
- **ADDRESS** : your address *(default : http://localhost/w/)*
- **DOMAIN** : name of the container *(default : localhost)*

### docker-compose.yml sample

```
mywiki-parsoid:
  image: wonderfall/parsoid
  container_name: mywiki-parsoid
  environment:
     - UID=1669
     - GID=1669
     - ADDRESS=https://wiki.domain.com/
     - DOMAIN=mywiki-parsoid
```
## wonderfall/pgbouncer

Minimal image for [PgBouncer](https://pgbouncer.github.io/).

### Volumes
- /etc/pgbouncer

### Environment variables
- **GID** : user id *(default : 991)*
- **UID** : group id *(default : 991)*
## wonderfall/ghost

![](https://i.goopics.net/lt.png)

**Breaking changes if you're upgrading from 0.x. Please export your current data, and import them again in a new 1.x blog. You also have to move your images to the new volume if you want to keep them. Disqus is also not supported, please move to Isso, a much better comments system. Sorry for the mess!**

#### What is this? What features?
- A **simple** Ghost CMS build made for production.
- Based on Alpine Linux so it's lightweight!
- Bundled with latest node.js available (version check is disabled).
- Offers Isso integration.

#### Build-time variables
- **VERSION** : version of Ghost.

#### Environment variables
- **GID** : ghost user id *(default : 991)*
- **UID** : ghost group id *(default : 991)*
- **ADDRESS** : your domain (with *http(s)://*) *(default : https://my-ghost-blog.com)*
- **ENABLE_ISSO** : enables Isso support if set to *True* *(default : False)*
- **ISSO_HOST**, **ISSO_AVATAR**, **ISSO_VOTE** : Isso settings (*True* or *False*)

#### Volumes
- **/ghost/content** : contents of your blog

### Ports
- **2368** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)

### How to configure?
Everything you need is in `/ghost/content/ghost.conf` (also mounted on your host...).

### docker-compose.yml sample

```
ghost-myblog:
  image: wonderfall/ghost:1
  container_name: ghost-myblog
  environment:
    - UID=8100
    - GID=8100
    - ADDRESS=https://myblog.com
  volumes:
    - /mnt/docker/myblog:/ghost/content
```
## wonderfall/freshrss

A free, self-hostable aggregator : https://github.com/FreshRSS/FreshRSS

#### Features
- Based on Alpine Linux (wonderfall/nginx-php image)
- Bundled with nginx and PHP7.1.
- Automatic feed update (frequency at 30 minutes by default)

#### Build-time variables
- **FRESHRSS_VER** : version of FreshRSS

#### Environment variables
- **UID** : user id
- **GID** : group id
- **MEMORY_LIMIT** : php memory limit *(default : 128M)*
- **UPLOAD_MAX_SIZE** : maximum upload size *(default : 10M)*
- **CRON_PERIOD** : feed update frequency *(default : 30m)*

#### Volumes
- **/freshrss/data**

#### Ports
- **8888** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)
## wonderfall/cryptpad

The Zero Knowledge Cloud.

#### Features
- Based on Alpine Linux
- Bundled with latest node.js available.

#### Build-time variables
- **VERSION** : version of Cryptpad

#### Environment variables
- **UID** : user id *(default : 991)*
- **GID** : group id *(default : 991)*

#### Volumes
- **/cryptpad/datastore** : Cryptpad data
- **/cryptpad/customize** : Cryptpad custom files

#### Ports
- **3000** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)

#### docker-compose.yml sample

```
cryptpad:
  image: wonderfall/cryptpad
  container_name: cryptpad
  environment:
    - UID=1444
    - GID=1444
  volumes:
    - /mnt/docker/cryptpad/files:/cryptpad/datastore
    - /mnt/docker/cryptpad/customize:/cryptpad/customize
```
### wonderfall/kippo-graph

![kippo-graph](https://github.com/ikoniaris/kippo-graph/blob/master/images/kippo-graph-img.png)

#### What is this?
Kippo-Graph is a full featured script to visualize statistics for a Kippo based SSH honeypot.

#### Environment variables
- **UID** *(default : 991)*
- **GID** *(default : 991)*

#### How to configure
You should provide your own configuration file from this base : https://github.com/ikoniaris/kippo-graph/blob/master/config.php.dist
You can mount this single file to your Docker container.

#### Docker compose (example)
```
kippo-graph:
  image: wonderfall/kippo-graph
  links:
    - cowrie-db:cowrie-db
  volumes:
    - /mnt/kippo-graph/config.php:/kippo-graph/config.php
    - /mnt/cowrie/log:/opt/cowrie/log
  environment:
    - GID=991
    - UID=991
```

## wonderfall/privatebin

Paste securely.

#### Features
- Based on Alpine Linux (wonderfall/nginx-php image)
- Uses [PrivateBin](https://github.com/PrivateBin/PrivateBin), originally a fork of zerobin by elrido.
- Bundled with nginx and PHP7.1.

#### Build-time variables
- **PRIVATEBIN_VER** : version of PrivateBin

#### Environment variables
- **UID** : privatebin user id
- **GID** : privatebin group id
- **MEMORY_LIMIT** : php memory limit *(default : 128M)*
- **UPLOAD_MAX_SIZE** : maximum upload size *(default : 10M)*

#### Volumes
- **/privatebin/data**

#### Ports
- **8888** [(reverse proxy!)](https://github.com/hardware/mailserver/wiki/Reverse-proxy-configuration)
## Supported tags and respective `Dockerfile` links
 * `1.2-M2-jdk8`, `1.2-M2`, `1.2-jdk8`, `1.2` [(jdk8/1.2/Dockerfile)](https://github.com/Zenika/alpine-kotlin/blob/master/jdk8/1.2/Dockerfile)
 * `1.2-M2-jdk7`, `1.2-jdk7` [(jdk7/1.2/Dockerfile)](https://github.com/Zenika/alpine-kotlin/blob/master/jdk7/1.2/Dockerfile)
 * `1.1.4-2-jdk8`, `1.1.4-2`, `1.1.4-jdk8`, `1.1.4`, `1.1-jdk8`, `1.1`, `jdk8`, `latest` [(jdk8/1.1/Dockerfile)](https://github.com/Zenika/alpine-kotlin/blob/master/jdk8/1.1/Dockerfile)
 * `1.1.4-2-jdk7`, `1.1.4-jdk7`, `1.1-jdk7`, `jdk7` [(jdk7/1.1/Dockerfile)](https://github.com/Zenika/alpine-kotlin/blob/master/jdk7/1.1/Dockerfile)
# Mesos Marathon in Docker containers

To run Mesos-Marathon with Nginx proxy in order to hide the Marathon UI:

```
docker-compose -f gb-marathon-withproxy.yml up -d
```



To run Mesos-Marathon with default Marathon UI:

```
docker-compose -f gb-marathon-noproxy.yml up -d
```


Mesos UI: http://your-server:5050

Marathon UI: http://your-server:8080


To launch a Marathon app:

```
curl -X POST http://127.0.0.1:8080/v2/apps -d @app-webserver.json -H "Content-type: application/json"
```
## Transmission Docker image

> This Docker image is based on Alpine image (size: 5mb).

> ROOT account password: passw0rd12

You can run:
```
docker run \
  -d \
  -p 9091:9091 \
  giabar/gb-transmission
```

With volumes:
```
docker run \
  -d \
  -p 9091:9091 \
  -v /your-incomplete-folder:/downloads_incomplete \
  -v /your-complete-folder:/downloads_complete \
  giabar/gb-transmission
```

Now you can open your browser at `http://your-ip:9091`.
# Mesos Marathon in Docker containers

To run Mesos-Marathon with Nginx proxy in order to hide the Marathon UI:

```
docker-compose -f gb-marathon-withproxy.yml up -d
```



To run Mesos-Marathon with default Marathon UI:

```
docker-compose -f gb-marathon-noproxy.yml up -d
```


Mesos UI: http://your-server:5050

Marathon UI: http://your-server:8080


To launch a Marathon app:

```
curl -X POST http://127.0.0.1:8080/v2/apps -d @app-webserver.json -H "Content-type: application/json"
```
## Apache Kafka 0.10.2.0 Docker image

> This Docker image is based on openjdk:8u131-jdk-alpine image.

> Kafka ver: "0.10.2.0" Scala ver: "2.11"

You can run:
```
docker run \
  -d \
  --name=kafka \
  --hostname=kafka \
  -p 9092:9092 \
  -e KAFKA_ADVERTISED_LISTENERS=172.17.0.1 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  giabar/gb-kafka
```
The KAFKA_ADVERTISED_LISTENERS variable contains the Docker host ip.

See the official documentation to get more info about parameters: https://kafka.apache.org/documentation/#configuration

This image is configured with a volume at 
* /kafka
to hold the persisted index data.

Use that paths if you would like to keep the data in a mounted volume:
```
docker run \
  -d \
  --name=kafka \
  --hostname=kafka \
  -p 9092:9092 \
  -e KAFKA_ADVERTISED_LISTENERS=172.17.0.1 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /tmp/kafka:/kafka \
  giabar/gb-kafka
```

This image exposes the 9092 (Kafka) and 2181 (Zookeeper) TCP ports.

This image is on Docker Hub @ https://hub.docker.com/r/giabar/gb-kafka/ 
# Docker Private Registry using Nexus

## Run

```
mkdir /your-path/nexus-data
sudo chown -R 100:101 /your-path/nexus-data
docker run \
  -d \
  --name nexus \
  -v /your-path/nexus-data:/nexus-data \
  -p 8081:8081 \
  -p 5000:5000 \
  giabar/docker-registry-nexus
```

The Nexus default credentials are admin/admin123

Follow the instructions on Nexus website to add the Docker registry to your Nexus instance: https://books.sonatype.com/nexus-book/3.0/reference/docker.html?__hstc=239247836.7e789409bfa516187dbb0cc3fb843cee.1487784788853.1489409847685.1489412772483.7&__hssc=239247836.4.1489412772483&__hsfp=2635578825
## Apache Tomcat 8.5.16 Docker image

> This Docker image is based on official Apache Tomcat 8.5.16 Alpine image.

You can run the default command simply:
```
docker run -d -p 8080:8080 giabar/gb-tomcat
```



VOLUME mount points are created at [/usr/local/tomcat/webapps] and [/usr/local/tomcat/logs]
Run your container with volume parameter if you want to mount the aboves volumes:
```
docker run \
  -d \
  -p 8080:8080 \
  -v /data/webapps:/usr/local/tomcat/webapps \
  -v /data/logs:/usr/local/tomcat/logs \
  giabar/gb-tomcat
```



Default credentials:

username: myadmin
password: yourtomcat


If you want to specify different credentials:

```
docker run \
  -d \
  -p 8080:8080 \
  --env ADMIN_USER=myusername \
  --env ADMIN_PASS=mypassword \
  giabar/gb-tomcat
```
## Apache Spark 2.1.1 Docker image

> This Docker image is based on the OpenJDK 1.8 Alpine image.

Run a Spark container:

```
docker run \
  -it \
  giabar/gb-spark
  bash
```

Now you're inside the Spark container and you can start a spark-shell:

```
spark-shell
```
# docker-alpine

Docker images based on Alpine Linux for x86_64

Boris HUISGEN <bhuisgen@hbis.fr>

## Usage

    $ ./configure
    $ make build
# docker-sonnet
[![](https://images.microbadger.com/badges/image/smizy/sonnet.svg)](https://microbadger.com/images/smizy/sonnet "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/sonnet.svg)](https://microbadger.com/images/smizy/sonnet "Get your own version badge on microbadger.com")
[![build status](https://gitlab.com/smizy/docker-sonnet/badges/master/build.svg)](https://gitlab.com/smizy/docker-sonnet/commits/master)

Python3 [Sonnet](https://github.com/deepmind/sonnet) with Jupyter docker image based on alpine

* numpy, scipy, pandas, scikit-learn, seaborn, tensorflow, sonnet installed via pip. See `pip list --format=columns` for detail.
* CPU only
* Note that this image is experimental.

## Usage
```
# verify
docker run -it --rm smizy/sonnet:1.0-cpu-alpine sh
> python
Python 3.5.2 (default, Dec 22 2016, 10:15:38) 
[GCC 6.2.1 20160822] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import sonnet as snt
>>> import tensorflow as tf
>>> snt.resampler(tf.constant([0.]), tf.constant([0.]))
<tf.Tensor 'resampler/Resampler:0' shape=(1,) dtype=float32>

# run examples
> cd /code/examples
> python rnn_shakespeare_test.py 
2017-04-21 08:28:19.513494: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
 :
 :
..
----------------------------------------------------------------------
Ran 2 tests in 867.042s

OK

# jupyter
docker run -it --rm -v $(pwd):/data -w /data -p 8888:8888 smizy/sonnet:1.0-cpu-alpine

```# docker-hadoop-base

[![](https://images.microbadger.com/badges/image/smizy/hadoop-base:2.7.4-alpine.svg)](http://microbadger.com/images/smizy/hadoop-base:2.7.4-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/hadoop-base:2.7.4-alpine.svg)](http://microbadger.com/images/smizy/hadoop-base:2.7.4-alpine "Get your own image badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-hadoop-base.svg?style=shield&circle-token=155cf7c34ea00da94d6d7848796b96d62d95de48)](https://circleci.com/gh/smizy/docker-hadoop-base)

Hadoop(Common/HDFS/YARN/MapReduce) docker image based on alpine

* Namenode is set to high availability mode with multiple namenode
* Non secure mode
* Alpine built native-hadoop library bundled
  *  Native netgroup mapping function missing
* One process per container as possible 
* No sshd setting. Cannot use utility script like start-dfs.sh and start-yarn.sh.  
* conf template applied by mustache.sh

This setup use FQDN with docker embedded DNS instead of editing /etc/hosts. 
Using FQDN on Hadoop require dns lookup and reverse lookup. 

You need set --name and --net (container_name.network_name as hostname) for dns lookup from other containers 
, and set --hostname(-h) for reverse lookup from container itself.


## Small setup  

```
# load default env as needed
eval $(docker-machine env default)

# network 
docker network create vnet

# make docker-compose.yml 
zookeeper=1 namenode=1 datanode=1 ./make_docker_compose_file.sh hdfs yarn > docker-compose.yml

# config test
docker-compose config

# hadoop startup
docker-compose up -d

# tail logs for a while
docker-compose logs -f

# check ps
docker-compose ps

      Name                     Command               State                                Ports                              
----------------------------------------------------------------------------------------------------------------------------
datanode-1          entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp                                 
historyserver-1     entrypoint.sh historyserver-1    Up      10020/tcp, 0.0.0.0:19888->19888/tcp                             
namenode-1          entrypoint.sh namenode-1         Up      0.0.0.0:32820->50070/tcp, 8020/tcp                              
nodemanager-1       entrypoint.sh nodemanager        Up      8040/tcp, 8041/tcp, 8042/tcp                                    
resourcemanager-1   entrypoint.sh resourcemana ...   Up      8030/tcp, 8031/tcp, 8032/tcp, 8033/tcp, 0.0.0.0:32819->8088/tcp 
zookeeper-1         entrypoint.sh -server 1 1 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# check stats
docker ps --format {{.Names}} | xargs docker stats

# run example data (pi calc)
docker exec -it -u hdfs datanode-1 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar pi 10 100

# view job history in web ui
open http://$(docker-machine ip default):19888

# hadoop shutdown  
docker-compose stop

# cleanup container
docker-compose rm -v

```# docker-postgres
[![](https://images.microbadger.com/badges/image/smizy/postgres.svg)](https://microbadger.com/images/smizy/postgres "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/postgres.svg)](https://microbadger.com/images/smizy/postgres "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-postgres.svg?style=svg&circle-token=829311c92db66b7cd382343c146310eca75ca830)](https://circleci.com/gh/smizy/docker-postgres)

PostgreSQL docker image based on Alpine Linux

* referenced [official postgres build](https://registry.hub.docker.com/_/postgres/)
* use su-exec instead of gosu
* install via apk postgresql package

## Small image size

```
REPOSITORY          TAG            IMAGE ID            CREATED             SIZE
postgres            9.6            0e24dd8079dc        3 weeks ago         264.9 MB
smizy/postgres      9.6-alpine     9bf1f53bc202        27 minutes ago      26.15 MB

```

## Usage

This image works in the same way the official `postgres` docker image work.
README: [https://hub.docker.com/_/postgres/](https://hub.docker.com/_/postgres/)

```
# run container
$ docker run  -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -d smizy/postgres
```## /Pipeline template/

Plantilla para /pipelines/ usando **Luigi** + **Docker**

### Prerequisitos

- `pyenv`
- `python version 3.5.2`
- `ag`
- `hub`
- `git flow`
- `docker`
- `docker-compose`
- `docker-machine`

### Instalando

1. Instala y ejecuta el pipeline mediante el comando

``` sh
./iniciar_pipeline.sh
```

Este archivo ejecutará las siguientes tareas:
  1. Crea maquinas con docker localmente
  2. Crea un swarm utilizando esas maquinas 
  3. Instala las imagenes necesarias
  4. Por último ejecuta el pipeline. 

Para poder visualizar el resultado graficamente con Luigi es necesario iniciar un servidor de Luigi:
``` sh
luigid
```
En [localhost](http://localhost:8082/), podremos observar el grafo dirigido aciclico (DAG) de iris pipeline.

![Image of IrisPipeline](https://github.com/eduardomtz/magicloop/blob/master/images/irispipeline.png)

Los archivos resultantes de la ejecución se encontrarán en /data/

![Image of IrisPipeline resultado json](https://github.com/eduardomtz/magicloop/blob/master/images/output.png)

De acuerdo a las siguientes especificaciones:

#### Tarea 3  (Grupal)

``` org

 Our old friend: *The Magic loop*, Ahora en su presentación de /pipeline/

 1.  Vamos a partir del =iris= /dataset/ y vamos a entrenar varios modelos para predecir la variable del tipo de flor.

 2. Estos modelos *no* pueden entrenar en serie. Cada modelo entrenará  en un =Task=, con parámetros: 
   - Nombre del algoritmo
   - Hiperparámetros

 3. La salida de los =Task= debe de ser un archivo =pickle= llamado =nombre_algoritmo/nombre_algoritmo-lista-hiperparámetros.pl= 
   y un archivo =json= con la siguiente estructura:

 {
   "algoritmo": "nombre_algoritmo",
   "hiperparametros": {
       "hiperparametro_1": valor,
       "hiperparametro_1": valor,
       ...
   "path": "path_al_archivo_pickle"
   }     
  
 }

```


>"El archivo necesario colocar iris.csv en la carpeta data (para este ejemplo ya debe estar ahí)."

Basado en el proyecto [pipeline-template](https://github.com/nanounanue/pipeline-template)
* Luigi worker
* R

Imagen para ejecutar Rscripts
* Python
## dockerfiles

[![Travis CI](https://travis-ci.org/jessfraz/dockerfiles.svg?branch=master)](https://travis-ci.org/jessfraz/dockerfiles)

This is a repo to hold various Dockerfiles for images I create.

I try to make sure each has a command at the top for running it,
if a file you are looking at does not have a command, please
pull request it!

Almost all of these live on dockerhub under [jess](https://hub.docker.com/u/jess/).
Because you cannot use notary with autobuilds on dockerhub I also build these
continuously on a private registry at [r.j3ss.co](https://r.j3ss.co/) for public download. (You're
welcome.)

You may also want to checkout my [dotfiles](https://github.com/jessfraz/dotfiles), specifically the aliases for all these files which are here: [github.com/jessfraz/dotfiles/blob/master/.dockerfunc](https://github.com/jessfraz/dotfiles/blob/master/.dockerfunc).
mps-youtube
--------

Docker container for running (mpsyt)[https://github.com/np1/mps-youtube] inside 
a docker container.

Run with

```
docker run -v /dev/snd:/dev/snd -it --rm --privileged rothgar/mpsyt
```
# LMCTVPNFY

Let Me Containerize That VPN For You


## How to use this?

Drop your OpenVPN configuration file in this directory.

Let's pretend that it's called `hacktheplanet.ovpn`.

Then all you have to do is to run:

```
docker-compose run vpn hacktheplanet.ovpn
```

If you need a password (because your OpenVPN configuration specifies `auth-user-pass`) you will be prompted for it.

If the VPN server pushes routes and so forth, they will be added to your machine, because the Compose file specifies `net: host` so the container runs within the hosts namespace.

If you **don't** need to specify a password, you can use `docker-compose run -d vpn hacktheplanet.ovpn` to start the container in the background.

If you OpenVPN configuration needs extra files (certificates etc) you can drop them in this directory too.


## Why?

Because we're the containerati and we like when things are [neatly arranged in their boxes](https://twitter.com/zooeypeng/status/613053137050439681).

# fontpatcher

How to use:

1. Enter the directory where the font file lives you wish to patch.
2. Run:

        $ docker run --rm -it \
            -v $(pwd):/workdir \
            --workdir /workdir \
            r.j3ss.co/fontpatcher myfontfile.otf

3. You should have a `myfontfile-Powerline.otf` as an artifact.
4. Copy the font file into ``~/.fonts`` (or another X font directory)::

        $ cp MyFontFile-Powerline.otf ~/.fonts

   **Note:** If the font is a pure bitmap font (e.g. a PCF font) it will be
   stored in the BDF format. This is usually not a problem, and you may
   convert the font back to the PCF format using ``bdftopcf`` if you want
   to. All other fonts will be stored in the OTF format regardless of the
   original format.

5. Update your font cache::

        $ sudo fc-cache -vf

   **Note:** If you use vim in rxvt-unicode in the client/daemon mode, you
   may need to close all running terminals as well for the font to be
   updated.
# zxinfo-app
This is the frontend [ZXInfo App](http://sinclair.kolbeck.dk) for the Sinclair Information search engine at http://search.kolbeck.dk

The application is built using the following:
* Node.js
* express
* AngularJS
* Angular Material

and requires the following module to run
* zxinfo-services - backend services for the webapp - [github](https://github.com/thomasheckmann/zxinfo-services)

# Installation
````
> npm install
````

# Running a standalone instance - e.g. for development
To start the server local execute the start.sh script and specify environment (usually production or development):
````
> NODE_ENV=development ./start.sh
````
And point your browser to http://localhost:8000/

# Environment configuration

Check config.json for configuration options. NOTE the container section (should be used if running containers only)

# Local development - nodemon
For development it is recommended to use a tool such as nodemon for automatic restart on changes. As config file is generated on app startup, use the following for starting nodemon - to avoid endless loop:
````
NODE_ENV=development nodemon --ignore public/javascripts/config.js
````

# Bulding Docker for zxinfo-app

## Build Docker image
To build & run the application using docker container:
````
> docker build . -t zxinfo-app
> docker run -d -p 8000:3000 -e "NODE_ENV=development" --name "zxinfo-node-local" zxinfo-app
````
And point your browser to http://localhost:8000/

## Move container to another location

Export/save container
````
> docker save --output zxinfo-app.tar zxinfo-app
````
Copy zxinfo-app.tar to server, and then restore on server

````
> docker load --input zxinfo-app.tar
````

# Docker Compose
The docker-compose contains definitions for the following services required for a full setup:
* zxinfo-es, elasticsearch instance
* zxinfo-services, backend services / API used by the frontend
* zxinfo-app, frontend application

To run it all
````
docker-compose -d
````
The following containers are now running:
* Elasticsearch on port 9200/9300
* Backend Services on port 8300
* Frontend application on port 8200

## Development setup
To run only the backend services zxinfo-services
````
docker-compose run --service-ports zxinfo-services
````
And how you have the following containers running:
* zxinfo-services on port 8300
* zxinfo-es in port 9200 & 9300 (known as zxinfo-es from service container)


# TODO
In not particulary order, stuff that eventually will be implemented.
* Search
	* Filtering on search
	* Facets on search
	* As-you-type suggestions from [Simple Search](http://incubator.kolbeck.dk/sinclair/)
* Add more screenshot
* Links from overview/list page
	* (Hardware should use manufactured instead of publisher - 1000018)
	* (Publiser with strange name fails - 0019066)
* New types of overview/browse
	* Screen, Magazine, Publisher, etc.
* API
	* Barcode search, Identifile (file match)
	* Image search


# Changelog
## 05-2017
* Color change
* Added protectionscheme(s) to detail page
* Added available formats to search result and detail page

## 05-2017
* Added Machine type navigation from detail view
* Added status Yes/Empty for Known errors
* Added subtype overview + type/subtype navigation from detail view

## 05-2017
* Series only shows title of group S
* Rest goes into "Additional Info(features)"
* Navigation by group/feature (e.g. games with Currah Speech)

## 05-2017
* Updated to ZXDB 29.04.2017

## 04-2017
* Added Role(s) info - for example 'Frank N Stein Re-booted - 0026834'

## 04-2017
* Download and filetype_id has changed. see [forum](https://www.worldofspectrum.org/forums/discussion/52951/database-model-zxdb/p24)
* - added as format to addtionals and downloads

## 01-2017
* Small changes related to backend change, which now uses ZXDB to create documents

## 01-2017
* Added alpine build for Docker
* Upgraded nodeJS to v7.4.0
* Upgraded Elasticsearch to v2.4.4

## 01-2017
* Refactored all code according to guidelines from: https://github.com/angular/angular-seed

## 01-2017
* Upgraded to AngularJS 1.6.x
* Added docker-compose support
* Added Side menu
* Added Publisher overview
* Release v1.0.0

## 10-2016
Docker file for building a container...

## 09-2016
* Re-factored searchService.js to remove duplicate code

## 09-2016
* Added slide/carousel with images to detail page

## 09-2016
* Added TABS to detailpage containing
	* Downloads
	* Additionals
	* Magazines
	* Adverts
* Nice flexible mobile friendly solution for tables, see [Respinsive Tables](https://css-tricks.com/responsive-data-tables/)

## 09-2016
* Added links from Detail page
	* Links to compilations
* Refactored - does not require elasticsearch, as API from incubator.kolbeck is used now.
* Backend
	* Rename API to REST standards, see [10 best practices](http://blog.mwaysolutions.com/2014/06/05/10-best-practices-for-better-restful-api/)
	* Search API /games/search
	* New API: Get game by gameid /games/:gameid

## 09-2016
* Added link to games in series on detail page

## 09-2016
* Search by publisher now uses the **/api/zxinfo/publisher** API from http://incubator.kolbeck.dk

## 09-2016
* From overview - publisher is a link, searching for all titles by publisher.

## 09-2016
* Search result HTML refactored to create <zx-search-result> component.
* Minor re-factoring into directives, services and controllers.

## 09-2016
* Added SPOT comments
* Added Series as list of titles
* Show default thumb/image - if first in additionals is not 'Loading screen'

## 09-2016 Initial version
Machine Learning/Data Science Platform (Docker Image)
=====================

Requirements
------------

- docker
- docker-machine (to deploy to google cloud)

How to use (in Google Cloud)
----------------------------

```bash
docker-machine create docker-dsp -d google --google-project={project_id} --google-machine-type n1-highmem-8	--google-disk-size "10" --google-disk-type "pd-standard" --google-preemptible --google-machine-image ubuntu-os-cloud/global/images/family/ubuntu-1404-lts --google-scopes "https://www.googleapis.com/auth/cloud-platform"
```

```bash
eval $(docker-machine env docker-dsp)
```

```bash
docker run -d -p 8888:8080 -e "PROJECT_ID={project_id}" eyadsibai/docker-dsp start.sh jupyter lab --NotebookApp.token=''
```

- get the ip address of the machine

```bash
docker-machine ip docker-dsp
```

- open the site http://{docker-machine ip docker-dsp}:8080
- to stop the machine

```bash
docker-machine stop docker-dsp
docker-machine start docker-dsp
```

- to delete the instance

```bash
docker-machine rm docker-dsp
```

### Note
when you stop the machine, it would cost you nothing except for the disk that you have it attached. For Google cloud (10GB of disk would cost ~0.4$/month)


How to use (Locally)
--------------------

```bash
docker run -d -p 8888:8888 -v <local path>:/home/jovyan/work eyadsibai/docker-dsp start.sh jupyter lab --NotebookApp.token=''
```

TODO
----

- write a better readme (Why? and How?)
- access local files (whether running locally or on google machine)
- install caffe
- install vw and xgboots command line
- install other command lines packages
- configs for matplotlib and others
![rekall-logo](https://github.com/blacktop/docker-rekall/raw/master/docs/logo.png) Dockerfile
=============================================================================================

[![CircleCI](https://circleci.com/gh/blacktop/docker-rekall.png?style=shield)](https://circleci.com/gh/blacktop/docker-rekall) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/rekall.svg)](https://hub.docker.com/r/blacktop/rekall/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/rekall.svg)](https://hub.docker.com/r/blacktop/rekall/) [![Docker Image](https://img.shields.io/badge/docker%20image-131-blue.svg)](https://hub.docker.com/r/blacktop/rekall/)

This repository contains a **Dockerfile** of [Rekall](http://www.rekall-forensic.com/index.html).

### Dependencies

-	[blacktop/yara:3.5](https://registry.hub.docker.com/u/blacktop/yara/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/rekall     latest              131 MB
blacktop/rekall     1.6                 131 MB
blacktop/rekall     w-gui               102 MB
blacktop/rekall     1.5                 102 MB
blacktop/rekall     profiles            1.972 GB
```

> NOTE: To use rekall **offline** use `blacktop/rekall:profiles`

### Installation

1.	Install [Docker](https://docs.docker.com).
2.	Download [trusted build](https://hub.docker.com/r/blacktop/rekall/) from public [Docker Registry](https://hub.docker.com/): `docker pull blacktop/rekall`

### Getting Started

```bash
$ docker run --rm -v `pwd`:/data blacktop/rekall:1.5 -q --cache_dir /tmp -f silentbanker.vmem pslist
```

![pslist-example](https://github.com/blacktop/docker-rekall/raw/master/docs/pslist_example.gif)

### Documentation

#### Rekall GUI

```bash
 $ docker run -d -p 80:8000 -v /path/to/mem:/data blacktop/rekall:w-gui webconsole --worksheet /tmp --host 0.0.0.0 --port 8000
```

![gui-home](https://github.com/blacktop/docker-rekall/raw/master/docs/gui-home.png)

![gui-silentbanker](https://github.com/blacktop/docker-rekall/raw/master/docs/gui-silentbanker.png)

##### To use **blacktop/rekall** like a host binary

Add the following to your bash or zsh profile

```bash
alias rekall='docker run -it --rm -v $(pwd):/data:rw blacktop/rekall $@'
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-rekall/issues/new) and I'll get right on it.

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-rekall/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-rekall/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-rekall/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2014-2017 **blacktop**
# docker-wp2txt

[wp2txt](https://github.com/yohasebe/wp2txt) docker image based on alpine

## Usage
```
docker run -it --rm -v $(pwd):/data smizy/wp2txt wp2txt --help
```httpie
======

> Docker image with httpie and jq installed
___

[![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/httpie.svg)](https://store.docker.com/community/images/blacktop/httpie) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/httpie.svg)](https://store.docker.com/community/images/blacktop/httpie) [![Docker Image](https://img.shields.io/badge/docker%20image-63.9MB-blue.svg)](https://store.docker.com/community/images/blacktop/httpie)
![VOL-logo](https://raw.githubusercontent.com/blacktop/docker-volatility/master/logo.png) Volatility Dockerfile
===============================================================================================================

[![CircleCI](https://circleci.com/gh/blacktop/docker-volatility.png?style=shield)](https://circleci.com/gh/blacktop/docker-volatility) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/volatility.svg)](https://hub.docker.com/r/blacktop/volatility/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/volatility.svg)](https://hub.docker.com/r/blacktop/volatility/) [![Docker Image](https://img.shields.io/badge/docker%20image-131%20MB-blue.svg)](https://hub.docker.com/r/blacktop/volatility/)

This repository contains a **Dockerfile** of [Volatility](https://github.com/volatilityfoundation/volatility).

### Dependencies

-	[blacktop/yara:3.5](https://registry.hub.docker.com/u/blacktop/yara/)

### Image Tags

```bash
REPOSITORY            TAG                 SIZE
blacktop/volatility   latest              131 MB
blacktop/volatility   2.6                 131 MB
blacktop/volatility   plugins             143 MB
blacktop/volatility   2.5                 124 MB
blacktop/volatility   2.4                 118 MB
```

> NOTE: tag **plugins** is `volatility:2.6` with as many community plugins as I could find (want more? Open a [pull request](https://github.com/blacktop/docker-volatility/pull/new/master)\)

### Installation

1.	Install [Docker](https://docs.docker.com).
2.	Download [trusted build](https://hub.docker.com/r/blacktop/volatility/) from public [Docker Registry](https://hub.docker.com/): `docker pull blacktop/volatility`

### Getting Started

```bash
$ docker run --rm -v /path/to/mem:/data:ro blacktop/volatility -f silentbanker.vmem pslist
```

```bash
Volatility Foundation Volatility Framework 2.6
Offset(V)  Name                    PID   PPID   Thds     Hnds   Sess  Wow64 Start                          Exit
---------- -------------------- ------ ------ ------ -------- ------ ------ ------------------------------ ------------------------------
0x810b1660 System                    4      0     59      183 ------      0
0xff2ab020 smss.exe                544      4      3       21 ------      0 2010-08-11 06:06:21 UTC+0000
0xff1ecda0 csrss.exe               608    544     11      365      0      0 2010-08-11 06:06:23 UTC+0000
0xff1ec978 winlogon.exe            632    544     18      511      0      0 2010-08-11 06:06:23 UTC+0000
0xff247020 services.exe            676    632     16      269      0      0 2010-08-11 06:06:24 UTC+0000
0xff255020 lsass.exe               688    632     19      345      0      0 2010-08-11 06:06:24 UTC+0000
0xff218230 vmacthlp.exe            844    676      1       24      0      0 2010-08-11 06:06:24 UTC+0000
0x80ff88d8 svchost.exe             856    676     17      199      0      0 2010-08-11 06:06:24 UTC+0000
0xff217560 svchost.exe             936    676     10      270      0      0 2010-08-11 06:06:24 UTC+0000
0x80fbf910 svchost.exe            1028    676     71     1355      0      0 2010-08-11 06:06:24 UTC+0000
0xff22d558 svchost.exe            1088    676      4       79      0      0 2010-08-11 06:06:25 UTC+0000
0xff203b80 svchost.exe            1148    676     14      208      0      0 2010-08-11 06:06:26 UTC+0000
0xff1d7da0 spoolsv.exe            1432    676     13      135      0      0 2010-08-11 06:06:26 UTC+0000
0xff1b8b28 vmtoolsd.exe           1668    676      5      222      0      0 2010-08-11 06:06:35 UTC+0000
0xff1fdc88 VMUpgradeHelper        1788    676      4      100      0      0 2010-08-11 06:06:38 UTC+0000
0xff143b28 TPAutoConnSvc.e        1968    676      5      100      0      0 2010-08-11 06:06:39 UTC+0000
0xff25a7e0 alg.exe                 216    676      6      105      0      0 2010-08-11 06:06:39 UTC+0000
0xff364310 wscntfy.exe             888   1028      1       27      0      0 2010-08-11 06:06:49 UTC+0000
0xff38b5f8 TPAutoConnect.e        1084   1968      1       61      0      0 2010-08-11 06:06:52 UTC+0000
0xff3865d0 explorer.exe           1724   1708     12      317      0      0 2010-08-11 06:09:29 UTC+0000
0xff3667e8 VMwareTray.exe          432   1724      1       49      0      0 2010-08-11 06:09:31 UTC+0000
0xff374980 VMwareUser.exe          452   1724      7      192      0      0 2010-08-11 06:09:32 UTC+0000
0x80f94588 wuauclt.exe             468   1028      4      135      0      0 2010-08-11 06:09:37 UTC+0000
0x80f1b020 IEXPLORE.EXE           1884   1724      9      351      0      0 2010-08-15 18:54:05 UTC+0000
0xff3856c0 cmd.exe                1136   1668      0 --------      0      0 2010-08-15 19:01:51 UTC+0000   2010-08-15 19:01:51 UTC+0000
```

### Documentation

##### Plugins tag

This image includes and enables by default plugins from:

- [contrib/plugins](https://github.com/volatilityfoundation/volatility/tree/master/contrib/plugins) from the main volatility package.
- [Volatility community repo](https://github.com/volatilityfoundation/community)

###### To use additional plugins

```bash
$ docker run --rm -v /path/to/mem:/data:ro \
                  -v /path/to/plugins:/plugins \
                  blacktop/volatility -f silentbanker.vmem custom_plugin
```
> **NOTE:** This will disable all other non-core plugins (contrib and community)
> **NOTE:** See [Specifying Additional Plugin Directories](https://github.com/volatilityfoundation/volatility/wiki/Volatility%20Usage#specifying-additional-plugin-directories)


##### Use **blacktop/volatility** like a host binary

Add the following to your bash or zsh profile

```bash
alias vol='docker run -it --rm -v $(pwd):/data:ro blacktop/volatility $@'
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-volatility/issues/new) and I'll get right on it.

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-volatility/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-volatility/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-volatility/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2014-2017 **blacktop**
*NOTE:* Supported images are available in the [official image library](https://hub.docker.com/_/neo4j/) on Docker Hub.
Please use those for production use.

# Using the Neo4j Docker image

## Neo4j 2.3

Documentation for the Neo4j 2.3 image can be found [here](https://neo4j.com/developer/docker-23/).

You can start a Neo4j 2.3 container like this:

```
docker run \
    --publish=7474:7474 \
    --volume=$HOME/neo4j/data:/data \
    neo4j:2.3
```

## Neo4j 3.0

Documentation for the Neo4j 3.0 image can be found [here](http://neo4j.com/docs/operations-manual/current/deployment/single-instance/docker/).

You can start a Neo4j 3.0 container like this:

```
docker run \
    --publish=7474:7474 --publish=7687:7687 \
    --volume=$HOME/neo4j/data:/data \
    neo4j:3.0
```

# Getting support and contributing

Please create issues and pull requests in the Github repository.
# Docker container for Neo4J
A Dockerimage repository for Neo4J 3.1.0 with an increased heap size.

Based on the existing docker-neo4j-publish 3.1.0 container by <a href="https://github.com/spacecowboy">Jonas Kalderstam</a>.# Graph Analytics for Neo4j

This docker image adds high-performance graph analytics to a [Neo4j graph database](http://www.neo4j.com). This image deploys a container with [Apache Spark](https://spark.apache.org/) and uses [GraphX](https://spark.apache.org/graphx/) to perform ETL graph analysis on subgraphs exported from Neo4j. The results of the analysis are applied back to the data in the Neo4j database.

## Supported Algorithms

*PageRank*

*Closeness Centrality*

*Betweenness Centrality*

*Triangle Counting*

*Connected Components*

*Strongly Connected Components*

### Neo4j Mazerunner Service

The Neo4j Mazerunner service in this image is a [unmanaged extension](http://neo4j.com/docs/stable/server-unmanaged-extensions.html) that adds a REST API endpoint to Neo4j for submitting graph analysis jobs to Apache Spark GraphX. The results of the analysis are applied back to the nodes in Neo4j as property values, making the results queryable using Cypher.

## Installation/Deployment

Installation requires 3 docker image deployments, each containing a separate linked component.

* *Hadoop HDFS* (sequenceiq/hadoop-docker:2.4.1)
* *Neo4j Graph Database* (kbastani/docker-neo4j:2.2.1)
* *Apache Spark Service* (kbastani/neo4j-graph-analytics:1.1.0)

Pull the following docker images:

    docker pull sequenceiq/hadoop-docker:2.4.1
    docker pull kbastani/docker-neo4j:2.2.1
    docker pull kbastani/neo4j-graph-analytics:1.1.0

After each image has been downloaded to your Docker server, run the following commands in order to create the linked containers.

    # Create HDFS
    docker run -i -t --name hdfs sequenceiq/hadoop-docker:2.4.1 /etc/bootstrap.sh -bash

    # Create Mazerunner Apache Spark Service
    docker run -i -t --name mazerunner --link hdfs:hdfs kbastani/neo4j-graph-analytics:1.1.0

    # Create Neo4j database with links to HDFS and Mazerunner
    # Replace <user> and <neo4j-path>
    # with the location to your existing Neo4j database store directory
    docker run -d -P -v /Users/<user>/<neo4j-path>/data:/opt/data --name graphdb --link mazerunner:mazerunner --link hdfs:hdfs kbastani/docker-neo4j:2.2.1

### Use Existing Neo4j Database

To use an existing Neo4j database, make sure that the database store directory, typically `data/graph.db`, is available on your host OS. Read the [setup guide](https://github.com/kbastani/docker-neo4j#start-neo4j-container) for *kbastani/docker-neo4j* for additional details.

> Note: The kbastani/docker-neo4j:2.2.1 image is running Neo4j 2.2.1. If you point it to an older database store, that database may become unable to be attached to a previous version of Neo4j. Make sure you back up your store files before proceeding.

### Use New Neo4j Database

To create a new Neo4j database, use any path to a valid directory.

### Accessing the Neo4j Browser

The Neo4j browser is exposed on the `graphdb` container on port 7474. If you're using boot2docker on MacOSX, follow the directions [here](https://github.com/kbastani/docker-neo4j#boot2docker) to access the Neo4j browser.

## Usage Directions

Graph analysis jobs are started by accessing the following endpoint:

    http://localhost:7474/service/mazerunner/analysis/{analysis}/{relationship_type}

Replace `{analysis}` in the endpoint with one of the following analysis algorithms:

- pagerank
- closeness_centrality
- betweenness_centrality
- triangle_count
- connected_components
- strongly_connected_components

Replace `{relationship_type}` in the endpoint with the relationship type in your Neo4j database that you would like to perform analysis on. The nodes that are connected by that relationship will form the graph that will be analyzed. For example, the equivalent Cypher query would be the following:

    MATCH (a)-[:FOLLOWS]->(b)
    RETURN id(a) as src, id(b) as dst

The result of the analysis will set the property with `{analysis}` as the key on `(a)` and `(b)`. For example, if you ran the `pagerank` analysis on the `FOLLOWS` relationship type, the following Cypher query will display the results:

    MATCH (a)-[:FOLLOWS]-()
    RETURN DISTINCT id(a) as id, a.pagerank as pagerank
    ORDER BY pagerank DESC

## Available Metrics

To begin graph analysis jobs on a particular metric, HTTP GET request on the following Neo4j server endpoints:

### PageRank

    http://172.17.0.21:7474/service/mazerunner/analysis/pagerank/FOLLOWS

* Gets all nodes connected by the `FOLLOWS` relationship and updates each node with the property key `pagerank`.

* The value of the `pagerank` property is a float data type, ex. `pagerank: 3.14159265359`.

* PageRank is used to find the relative importance of a node within a set of connected nodes.

### Closeness Centrality

    http://172.17.0.21:7474/service/mazerunner/analysis/closeness_centrality/FOLLOWS

* Gets all nodes connected by the `FOLLOWS` relationship and updates each node with the property key `closeness_centrality`.

* The value of the `closeness_centrality` property is a float data type, ex. `pagerank: 0.1337`.

* A key node centrality measure in networks is closeness centrality (Freeman, 1978; Opsahl et al., 2010; Wasserman and Faust, 1994). It is defined as the inverse of farness, which in turn, is the sum of distances to all other nodes.

### Betweenness Centrality

    http://172.17.0.21:7474/service/mazerunner/analysis/betweenness_centrality/FOLLOWS

* Gets all nodes connected by the `FOLLOWS` relationship and updates each node with the property key `betweenness_centrality`.

* The value of the `betweenness_centrality` property is a float data type, ex. `betweenness_centrality: 20.345`.

* Betweenness centrality is an indicator of a node's centrality in a network. It is equal to the number of shortest paths from all vertices to all others that pass through that node. A node with high betweenness centrality has a large influence on the transfer of items through the network, under the assumption that item transfer follows the shortest paths.

### Triangle Counting

    http://172.17.0.21:7474/service/mazerunner/analysis/triangle_count/FOLLOWS

* Gets all nodes connected by the `FOLLOWS` relationship and updates each node with the property key `triangle_count`.

* The value of the `triangle_count` property is an integer data type, ex. `triangle_count: 2`.

* The value of `triangle_count` represents the count of the triangles that a node is connected to.

* A node is part of a triangle when it has two adjacent nodes with a relationship between them. The `triangle_count` property provides a measure of clustering for each node.

### Connected Components

    http://172.17.0.21:7474/service/mazerunner/analysis/connected_components/FOLLOWS

* Gets all nodes connected by the `FOLLOWS` relationship and updates each node with the property key `connected_components`.

* The value of `connected_components` property is an integer data type, ex. `connected_components: 181`.

* The value of `connected_components` represents the *Neo4j internal node ID* that has the lowest integer value for a set of connected nodes.

* Connected components are used to find isolated clusters, that is, a group of nodes that can reach every other node in the group through a *bidirectional* traversal.

### Strongly Connected Components

    http://172.17.0.21:7474/service/mazerunner/analysis/strongly_connected_components/FOLLOWS

* Gets all nodes connected by the `FOLLOWS` relationship and updates each node with the property key `strongly_connected_components`.

* The value of `strongly_connected_components` property is an integer data type, ex. `strongly_connected_components: 26`.

* The value of `strongly_connected_components` represents the *Neo4j internal node ID* that has the lowest integer value for a set of strongly connected nodes.

* Strongly connected components are used to find clusters, that is, a group of nodes that can reach every other node in the group through a *directed* traversal.

Architecture
================

Mazerunner uses a message broker to distribute graph processing jobs to [Apache Spark's GraphX](https://spark.apache.org/graphx/) module. When an agent job is dispatched, a subgraph is exported from Neo4j and written to [Apache Hadoop HDFS](https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html).

After Neo4j exports a subgraph to HDFS, a separate Mazerunner service for Spark is notified to begin processing that data. The Mazerunner service will then start a distributed graph processing algorithm using Scala and Spark's GraphX module. The GraphX algorithm is serialized and dispatched to Apache Spark for processing.

Once the Apache Spark job completes, the results are written back to HDFS as a Key-Value list of property updates to be applied back to Neo4j.

Neo4j is then notified that a property update list is available from Apache Spark on HDFS. Neo4j batch imports the results and applies the updates back to the original graph.

License
================

This library is licensed under the Apache License, Version 2.0.
# alpine-neo4j-spatial
Neo4J with spatial and apoc procedures


# Spark Neo4j

**Spark Neo4j** is the *fastest* way to launch or deploy a graph analytics engine for big data graph processing using the new [Docker Compose](https://docs.docker.com/compose/) framework.

This image combines **Neo4j** and **Apache Spark GraphX** containers onto a single Docker host. This approach makes it easy to take advantage of these two powerful tools without worrying about configuring and installing any other dependencies.

## Getting started

The fundamental goal of this Docker image is to get you up and running as fast as possible with a **graph analytics engine**. It should take no longer than *30 minutes* for you to launch **Spark Neo4j** on Mac OSX or Linux.

### Requirements

Get Docker:  [https://docs.docker.com/installation/](https://docs.docker.com/installation/)

### Installation

To install **Spark Neo4j** on your machine, follow this install guide:

* [Mac OSX](https://github.com/kbastani/spark-neo4j/wiki/Mac-OSX-Install-Guide)
* [Linux](https://github.com/kbastani/spark-neo4j/wiki/Linux-Install-Guide)

## Graph Analytics Engine

This Docker image is an all-in-one graph processing solution combining **graph storage** and **graph processing** in a single platform.

### Graph Storage

A **Neo4j graph database** container provides an out of the box database management system with robust (fully ACID) graph data storage and query capabilities. This container configures Neo4j for high-performance OLTP use cases.

### Graph Processing

An **Apache Spark GraphX** container provides a single system that handles iterative graph computation and ETL from data sourced from Neo4j.

### Closed-loop Data Processing

The results of an analysis by the Apache Spark container are applied back to Neo4j. These results can be explored using Neo4j's powerful query capabilities to lookup graph metrics calculated by Spark.

* PageRank
* Closeness Centrality
* Betweenness Centrality
* Triangle Counting
* Connected Components
* Strongly Connected Components

# License

This library is licensed under the Apache License, Version 2.0.
# QubeStash / Varnish Cache

[![TravisCI Status Widget]][TravisCI Status] [![Coverage Status Widget]][Coverage Status]

[TravisCI Status]: https://travis-ci.org/qubestash/http-varnish-cache
[TravisCI Status Widget]: https://travis-ci.org/qubestash/http-varnish-cache.svg?branch=master
[Coverage Status]: https://coveralls.io/r/qubestash/http-varnish-cache
[Coverage Status Widget]: https://coveralls.io/repos/github/qubestash/http-varnish-cache/badge.svg?branch=master

## Supported tags

> **Debian Version is not supported yet**

* ~~`4.1.3`, `4.1`, `1`, `latest` (run `make build-latest`) ([Dockerfile](https://github.com/qubestash/http-varnish-cache/blob/master/4.1/Dockerfile))~~
* `4.1.3-alpine`, `4.1-alpine`, `1-alpine`, `alpine` (run `make build-alpine`) ([Dockerfile](https://github.com/qubestash/http-varnish-cache/blob/master/4.1/alpine/Dockerfile))

## ENV

### VCL_USE_CONFIG

> Default: no

### VCL_CONFIG

> Default: /etc/varnish/default.vcl

### VCL_CACHE_SIZE

> Default: 64m

### VCL_BACKEND_ADDRESS

> Default: 0.0.0.0

### VCL_BACKEND_PORT

> Default: 80

## Running 

### docker

```bash
# Clear containers if they exist by any chance
docker rm -f varnish-cache || true
# Run Varnish
docker run -e VCL_BACKEND_ADDRESS=192.168.0.1 --name varnish-cache -d qubestash/varnish-cache:alpine
```# ffmpeg-alpine# Imaginary Alpine 

Lightweight [Docker image](https://hub.docker.com/r/stead/imaginary-alpine/) for image processing service [Imaginary](https://github.com/h2non/imaginary).

Built two different ways to find the most optimal.

- [Dockerfile](https://github.com/mikestead/docker-imaginary-alpine/blob/master/Dockerfile): Single Dockerfile which compiles binaries and then attempts to cleanup build dependencies.
- [Dockerfile.multi](https://github.com/mikestead/docker-imaginary-alpine/blob/master/Dockerfile.multi): Single [multi-stage](https://docs.docker.com/engine/userguide/eng-image/multistage-build) Dockerfile, compiling binaries and then copying them into clean Alpine image. Support for multi-part Dockerfile's coming in Docker 17.05.

#### Image Sizes

- [Dockerfile](https://github.com/mikestead/docker-imaginary-alpine/blob/master/Dockerfile): `111mb`, `62mb` compressed
- [Dockerfile.multi](https://github.com/mikestead/docker-imaginary-alpine/blob/master/Dockerfile.multi): `64mb`, `23mb` compressed

## Usage

See [Imaginary](https://github.com/h2non/imaginary#command-line-usage) readme for full usage.

    docker run --rm stead/imaginary-alpine:lite -h

#### Example

Start the server

    docker run --rm -p 9000:9000 stead/imaginary-alpine:lite -enable-url-source

Resize an image

    http://localhost:9000/resize?width=800&height=500&url=https://c1.staticflickr.com/1/409/18186063494_386acbe85c_k.jpg&type=webp

## Disclaimer

This has not been tested in production. Use at your own risk.
# vips-alpine
Minimal VIPS Docker image based on Alpine Linux
# hub.docker.com/tiredofit/nginx-ldap

# Introduction

This will build a container for [Nginx](https://www.nginx.org) w/ LDAP Authentication Enabled

*    Tracks Mainline release channel
*    Includes Zabbix Monitoring (nginx status) on port 73
*    Logrotate Included to roll over log files at 23:59, compress and retain for 7 days
*    Compile Options:
*    --with-threads
        --with-http_ssl_module 
        --with-http_realip_module 
        --with-http_addition_module 
        --with-http_sub_module 
        --with-http_dav_module 
        --with-http_flv_module 
        --with-http_mp4_module 
        --with-http_gunzip_module 
        --with-http_gzip_static_module 
        --with-http_random_index_module 
        --with-http_secure_link_module 
        --with-http_stub_status_module 
        --with-http_auth_request_module 
        --with-http_xslt_module=dynamic 
        --with-http_image_filter_module=dynamic 
        --with-http_geoip_module=dynamic 
        --with-http_perl_module=dynamic 
        --with-threads 
        --with-stream 
        --with-stream_ssl_module 
        --with-stream_ssl_preread_module 
        --with-stream_realip_module 
        --with-stream_geoip_module=dynamic 
        --with-http_slice_module 
        --with-mail 
        --with-mail_ssl_module 
        --with-compat 
        --with-file-aio 
        --with-http_v2_module 
        
This Container uses [tiredofit:alpine:3.4](https://hub.docker.com/r/tiredofit/alpine) as a base.


[Changelog](CHANGELOG.md)

# Authors

- [Dave Conroy](https://github.com/tiredofit)

# Table of Contents

- [Introduction](#introduction)
    - [Changelog](CHANGELOG.md)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
   - [References](#references)

# Prerequisites

This image assumes that you are using a reverse proxy such as [jwilder/nginx-proxy](https://github.com/jwilder/nginx-proxy) and optionally the [Let's Encrypt Proxy Companion @ https://github.com/JrCs/docker-letsencrypt-nginx-proxy-companion](https://github.com/JrCs/docker-letsencrypt-nginx-proxy-companion) in order to serve your pages. However, it will run just fine on it's own if you map appropriate ports.


# Installation

Automated builds of the image are available on [Docker Hub](https://hub.docker.com/tiredofit/nginx-ldap) and is the recommended method of installation.


```bash
docker pull hub.docker.com/tiredofit/nginx-ldap
```

# Quick Start

* The quickest way to get started is using [docker-compose](https://docs.docker.com/compose/). See the examples folder for a working [docker-compose.yml](examples/docker-compose.yml) that can be modified for development or production use.

* Set various [environment variables](#environment-variables) to understand the capabilities of this image.
* Map [persistent storage](#data-volumes) for access to configuration and data files for backup.
* Make [networking ports](#networking) available for public access if necessary



# Configuration

### Data-Volumes

The container starts up and reads from `/etc/nginx/nginx.conf` for some basic configuration and to listen on port 73 internally for Nginx Status responses. `/etc/nginx/conf.d` contains a sample configuration file that can be used to customize a nginx server block. The LDAP configuration resides in the `/etc/nginx/conf.d/01-ldap.conf` upon container start.


The following directories are used for configuration and can be mapped for persistent storage.

| Directory    | Description                                                 |
|--------------|-------------------------------------------------------------|
|  `/www/html` | Drop your Datafiles in this directory to be served by Nginx |
|  `/www/logs` | Logfiles for Nginx error and access                         |
      

### Environment Variables

Along with the Environment Variables from the [Base image](https://hub.docker.com/r/tiredofit/alpine), below is the complete list of available options that can be used to customize your installation.


| Parameter        | Description                            |
|------------------|----------------------------------------|
| `UPLOAD_MAX_SIZE` | Maximum Upload Size for Nginx (e.g 2G) |
| `LDAP_HOST` | Hostname and port number of LDAP Server (e.g. ldapserver:389) |
| `LDAP_BIND_DN` | User to Bind to LDAP (e.g. cn=admin,dc=orgname,dc=org) |
| `LDAP_BIND_PW` | Password for Above Bind User (e.g. password) |
| `LDAP_BASE_DN` | Base Distringuished Name (e =dc=hostname,dc=com |
| `LDAP_ATTRIBUTE` | Unique Identifier Attrbiute (e.g. uid) |
| `LDAP_SCOPE` |LDAP Scope for searching (e.g. sub) |
| `LDAP_FILTER` | Define what object that is searched for (e.g. objectClass=person) |
| `LDAP_GROUP_ATTRIBUTE` | If searching inside of a group what is the Group Attribute (e.g. uniquemember) |



### Networking

The following ports are exposed.

| Port      | Description |
|-----------|-------------|
| `80`      | HTTP        |
| `443`     | HTTPS       |


# Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it (whatever your container name is e.g. nginx-ldap) bash
```

# References

* https://nginx.org/
* https://github.com/kvspb/nginx-auth-ldap

<p align="center">
<img src="docs/img/traefik.logo.png" alt="Træfik" title="Træfik" />
</p>

[![Build Status SemaphoreCI](https://semaphoreci.com/api/v1/containous/traefik/branches/master/shields_badge.svg)](https://semaphoreci.com/containous/traefik)
[![Docs](https://img.shields.io/badge/docs-current-brightgreen.svg)](https://docs.traefik.io)
[![Go Report Card](https://goreportcard.com/badge/containous/traefik)](http://goreportcard.com/report/containous/traefik)
[![](https://images.microbadger.com/badges/image/traefik.svg)](https://microbadger.com/images/traefik)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/containous/traefik/blob/master/LICENSE.md)
[![Join the chat at https://traefik.herokuapp.com](https://img.shields.io/badge/style-register-green.svg?style=social&label=Slack)](https://traefik.herokuapp.com)
[![Twitter](https://img.shields.io/twitter/follow/traefikproxy.svg?style=social)](https://twitter.com/intent/follow?screen_name=traefikproxy)


Træfik (pronounced like [traffic](https://speak-ipa.bearbin.net/speak.cgi?speak=%CB%88tr%C3%A6f%C9%AAk)) is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease.
It supports several backends ([Docker](https://www.docker.com/), [Swarm mode](https://docs.docker.com/engine/swarm/), [Kubernetes](http://kubernetes.io), [Marathon](https://mesosphere.github.io/marathon/), [Consul](https://www.consul.io/), [Etcd](https://coreos.com/etcd/), [Rancher](https://rancher.com), [Amazon ECS](https://aws.amazon.com/ecs), and a lot more) to manage its configuration automatically and dynamically.

---

| **[Overview](#overview)** |
**[Features](#features)** |
**[Supported backends](#supported-backends)** |
**[Quickstart](#quickstart)** |
**[Web UI](#web-ui)** |
**[Test it](#test-it)** |
**[Documentation](#documentation)** |
**[Support](#support)** |
**[Release cycle](#release-cycle)** |

| **[Contributing](#contributing)** |
**[Maintainers](#maintainers)** |
**[Plumbing](#plumbing)** |
**[Credits](#credits)** |

---

## Overview

Imagine that you have deployed a bunch of microservices on your infrastructure. You probably used a service registry (like etcd or consul) and/or an orchestrator (swarm, Mesos/Marathon) to manage all these services.
If you want your users to access some of your microservices from the Internet, you will have to use a reverse proxy and configure it using virtual hosts or prefix paths:

- domain `api.domain.com` will point the microservice `api` in your private network
- path `domain.com/web` will point the microservice `web` in your private network
- domain `backoffice.domain.com` will point the microservices `backoffice` in your private network, load-balancing between your multiple instances

But a microservices architecture is dynamic... Services are added, removed, killed or upgraded often, eventually several times a day.

Traditional reverse-proxies are not natively dynamic. You can't change their configuration and hot-reload easily.

Here enters Træfik.

![Architecture](docs/img/architecture.png)

Træfik can listen to your service registry/orchestrator API, and knows each time a microservice is added, removed, killed or upgraded, and can generate its configuration automatically.
Routes to your services will be created instantly.

Run it and forget it!


## Features

- [It's fast](http://docs.traefik.io/benchmarks)
- No dependency hell, single binary made with go
- [Tiny](https://microbadger.com/images/traefik) [official](https://hub.docker.com/r/_/traefik/) official docker image
- Rest API
- Hot-reloading of configuration. No need to restart the process
- Circuit breakers, retry
- Round Robin, rebalancer load-balancers
- Metrics (Rest, Prometheus, Datadog, Statd)
- Clean AngularJS Web UI
- Websocket, HTTP/2, GRPC ready
- Access Logs (JSON, CLF)
- [Let's Encrypt](https://letsencrypt.org) support (Automatic HTTPS with renewal)
- [Proxy Protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt) support
- High Availability with cluster mode (beta)

## Supported backends

- [Docker](https://www.docker.com/) / [Swarm mode](https://docs.docker.com/engine/swarm/)
- [Kubernetes](http://kubernetes.io)
- [Mesos](https://github.com/apache/mesos) / [Marathon](https://mesosphere.github.io/marathon/)
- [Rancher](https://rancher.com) (API, Metadata)
- [Consul](https://www.consul.io/) / [Etcd](https://coreos.com/etcd/) / [Zookeeper](https://zookeeper.apache.org) / [BoltDB](https://github.com/boltdb/bolt)
- [Eureka](https://github.com/Netflix/eureka)
- [Amazon ECS](https://aws.amazon.com/ecs)
- [Amazon DynamoDB](https://aws.amazon.com/dynamodb)
- File
- Rest API

## Quickstart

You can have a quick look at Træfik in this [Katacoda tutorial](https://www.katacoda.com/courses/traefik/deploy-load-balancer) that shows how to load balance requests between multiple Docker containers. If you are looking for a more comprehensive and real use-case example, you can also check [Play-With-Docker](http://training.play-with-docker.com/traefik-load-balancing/) to see how to load balance between multiple nodes.

Here is a talk given by [Emile Vauge](https://github.com/emilevauge) at [GopherCon 2017](https://gophercon.com/).
You will learn Træfik basics in less than 10 minutes. 

[![Traefik GopherCon 2017](http://img.youtube.com/vi/RgudiksfL-k/0.jpg)](http://www.youtube.com/watch?v=RgudiksfL-k)

Here is a talk given by [Ed Robinson](https://github.com/errm) at [ContainerCamp UK](https://container.camp) conference.
You will learn fundamental Træfik features and see some demos with Kubernetes.

[![Traefik ContainerCamp UK](http://img.youtube.com/vi/aFtpIShV60I/0.jpg)](https://www.youtube.com/watch?v=aFtpIShV60I)


## Web UI

You can access the simple HTML frontend of Træfik.

![Web UI Providers](docs/img/web.frontend.png)
![Web UI Health](docs/img/traefik-health.png)


## Test it

- The simple way: grab the latest binary from the [releases](https://github.com/containous/traefik/releases) page and just run it with the [sample configuration file](https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml):

```shell
./traefik --configFile=traefik.toml
```

- Use the tiny Docker image and just run it with the [sample configuration file](https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml):

```shell
docker run -d -p 8080:8080 -p 80:80 -v $PWD/traefik.toml:/etc/traefik/traefik.toml traefik
```

- From sources:

```shell
git clone https://github.com/containous/traefik
```


## Documentation

You can find the complete documentation at [https://docs.traefik.io](https://docs.traefik.io).
A collection of contributions around Træfik can be found at [https://awesome.traefik.io](https://awesome.traefik.io). 


## Support

To get basic support, you can:
- join the Træfik community Slack channel: [![Join the chat at https://traefik.herokuapp.com](https://img.shields.io/badge/style-register-green.svg?style=social&label=Slack)](https://traefik.herokuapp.com) 
- use [Stack Overflow](https://stackoverflow.com/questions/tagged/traefik) (using the `traefik` tag)

If you prefer commercial support, please contact [containo.us](https://containo.us) by mail: <mailto:support@containo.us>.


## Release cycle

- Release: We try to release a new version every 2 months
  - i.e.: 1.3.0, 1.4.0, 1.5.0
- Release candidate: we do RC (1.**x**.0-rc**y**) before the final release (1.**x**.0)
  - i.e.: 1.1.0-rc1 -> 1.1.0-rc2 -> 1.1.0-rc3 -> 1.1.0-rc4 -> 1.1.0
- Bug-fixes: For each version we release bug fixes
  - i.e.: 1.1.1, 1.1.2, 1.1.3
  - those versions contain only bug-fixes
  - no additional features are delivered in those versions
- Each version is supported until the next one is released
  - i.e.: 1.1.x will be supported until 1.2.0 is out
- We use [Semantic Versioning](http://semver.org/)


## Contributing

Please refer to [contributing documentation](CONTRIBUTING.md).


### Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md).
By participating in this project you agree to abide by its terms.


## Maintainers

[Information about process and maintainers](MAINTAINER.md)


## Plumbing

- [Oxy](https://github.com/vulcand/oxy): an awesome proxy library made by Mailgun folks
- [Gorilla mux](https://github.com/gorilla/mux): famous request router
- [Negroni](https://github.com/urfave/negroni): web middlewares made simple
- [Lego](https://github.com/xenolf/lego): the best [Let's Encrypt](https://letsencrypt.org) library in go


## Credits

Kudos to [Peka](http://peka.byethost11.com/photoblog/) for his awesome work on the logo ![logo](docs/img/traefik.icon.png).
Traefik's logo licensed under the Creative Commons 3.0 Attributions license.

Traefik's logo was inspired by the gopher stickers made by Takuya Ueda (https://twitter.com/tenntenn).
The original Go gopher was designed by Renee French (http://reneefrench.blogspot.com/).
# How to generate the self-signed wildcard certificate

```bash
#!/usr/bin/env bash

# Specify where we will install
# the wildcard certificate
SSL_DIR="./ssl"

# Set the wildcarded domain
# we want to use
DOMAIN="*.acme.wtf"

# A blank passphrase
PASSPHRASE=""

# Set our CSR variables
SUBJ="
C=FR
ST=MP
O=
localityName=Toulouse
commonName=$DOMAIN
organizationalUnitName=Traefik
emailAddress=
"

# Create our SSL directory
# in case it doesn't exist
sudo mkdir -p "$SSL_DIR"

# Generate our Private Key, CSR and Certificate
sudo openssl genrsa -out "$SSL_DIR/wildcard.key" 2048
sudo openssl req -new -subj "$(echo -n "$SUBJ" | tr "\n" "/")" -key "$SSL_DIR/wildcard.key" -out "$SSL_DIR/wildcard.csr" -passin pass:$PASSPHRASE
sudo openssl x509 -req -days 3650 -in "$SSL_DIR/wildcard.csr" -signkey "$SSL_DIR/wildcard.key" -out "$SSL_DIR/wildcard.crt"
sudo rm -f "$SSL_DIR/wildcard.csr"
```# This is how the certs were created

```bash
openssl req -new -newkey rsa:2048 -x509 -days 3650 -extensions v3_ca -keyout ca1.pem -out ca1.crt
openssl req -new -newkey rsa:2048 -x509 -days 3650 -extensions v3_ca -keyout ca2.pem -out ca2.crt
openssl req -new -newkey rsa:2048 -x509 -days 3650 -extensions v3_ca -keyout ca3.pem -out ca3.crt
openssl rsa -in ca1.pem -out ca1.key
openssl rsa -in ca2.pem -out ca2.key
openssl rsa -in ca3.pem -out ca3.key
cat ca1.crt ca2.crt > ca1and2.crt
rm ca1.pem ca2.pem ca3.pem

openssl genrsa -out client1.key 2048
openssl genrsa -out client2.key 2048
openssl genrsa -out client3.key 2048

openssl req -key client1.key -new -out client1.csr
openssl req -key client2.key -new -out client2.csr
openssl req -key client3.key -new -out client3.csr

openssl x509 -req -days 3650 -in client1.csr -CA ca1.crt -CAkey ca1.key -CAcreateserial -out client1.crt
openssl x509 -req -days 3650 -in client2.csr -CA ca2.crt -CAkey ca2.key -CAcreateserial -out client2.crt
openssl x509 -req -days 3650 -in client3.csr -CA ca3.crt -CAkey ca3.key -CAcreateserial -out client3.crt

```
# TLS certificate description

## local.crt / local.key

Generate with
```bash
go run $GOROOT/src/crypto/tls/generate_cert.go  --rsa-bits 1024 --host 127.0.0.1,::1,localhost --ca --start-date "Jan 1 00:00:00 1970" --duration=1000000h
mv cert.pem local.cert
mv key.pem local.key
```
![](https://traefik.io/traefik.logo.svg)

[Træfɪk](https://github.com/containous/traefik) is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. It supports several backends ([Docker :whale:](https://www.docker.com/), [Swarm :whale::whale:](https://github.com/docker/swarm), [Mesos/Marathon](https://mesosphere.github.io/marathon/), [Consul](https://www.consul.io/), [Etcd](https://coreos.com/etcd/), [Zookeeper](https://zookeeper.apache.org), [BoltDB](https://github.com/boltdb/bolt), Rest API, file...) to manage its configuration automatically and dynamically.

# Example usage

Grab a [sample configuration file](https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml) and rename it to `traefik.toml`. Enable `docker` provider and web UI:

```toml
################################################################
# Web configuration backend
################################################################
[web]
address = ":8080"
################################################################
# Docker configuration backend
################################################################
[docker]
domain = "docker.local"
watch = true
```

Start Træfɪk:

```bash
docker run -d -p 8080:8080 -p 80:80 \
-v $PWD/traefik.toml:/etc/traefik/traefik.toml \
-v /var/run/docker.sock:/var/run/docker.sock \
traefik
```

Start a backend server, named `test`:

```bash
docker run -d --name test emilevauge/whoami
```

And finally, you can access to your `whoami` server throught Træfɪk, on the domain name `{containerName}.{configuredDomain}`:

```bash
curl --header 'Host: test.docker.local' 'http://localhost:80/'
Hostname: 117c5530934d
IP: 127.0.0.1
IP: ::1
IP: 172.17.0.3
IP: fe80::42:acff:fe11:3
GET / HTTP/1.1
Host: 172.17.0.3:80
User-Agent: curl/7.35.0
Accept: */*
Accept-Encoding: gzip
X-Forwarded-For: 172.17.0.1
X-Forwarded-Host: 172.17.0.3:80
X-Forwarded-Proto: http
X-Forwarded-Server: f2e05c433120

```

The web UI [http://localhost:8080](http://localhost:8080) will give you an overview of the frontends/backends and also a health dashboard.

![Web UI Providers](https://traefik.io/web.frontend.png)

# Documentation

You can find the complete documentation [here](https://docs.traefik.io).
# docker-opentsdb
[![](https://images.microbadger.com/badges/image/smizy/opentsdb:2.2.2-alpine.svg)](https://microbadger.com/images/smizy/opentsdb:2.2.2-alpine "Get your own version badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/opentsdb:2.2.2-alpine.svg)](https://microbadger.com/images/smizy/opentsdb:2.2.2-alpine "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-opentsdb.svg?style=svg&circle-token=03e3d264901a60ed454a2c296b3d243ad6f53305)](https://circleci.com/gh/smizy/docker-opentsdb)

OpenTSDB docker image based on alpine

## Run server

```
# load default env
eval $(docker-machine env default)

# network 
docker network create vnet

# make docker-compose.yml 
./make_docker_compose_yml.sh hdfs hbase tsdb > docker-compose.yml

# hadoop+hbase+tsdb startup (zookeeper, journalnode, namenode, datanode, hmaster, regionserver, tsdb)
docker-compose up -d

# tail logs for a while
docker-compose logs -f

# check ps
docker-compose ps

# check stats
docker ps --format {{.Names}} | xargs docker stats

# check web ui
open http://$(docker-machine ip default):4242
```

## Licenses
* Apache License 2.0

### mustache.sh License
* BSD License. See LICENSE.mustache.
* Source: https://github.com/rcrowley/mustache.sh
* Copyright 2011 Richard Crowley. All rights reserved.![cuckoo-logo](https://github.com/blacktop/docker-cuckoo/raw/master/docs/img/logo.png) Dockerfile-beta
======================================================================================================

[![CircleCI](https://circleci.com/gh/blacktop/docker-cuckoo.png?style=shield)](https://circleci.com/gh/blacktop/docker-cuckoo) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/cuckoo.svg)](https://hub.docker.com/r/blacktop/cuckoo/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/cuckoo.svg)](https://hub.docker.com/r/blacktop/cuckoo/) [![Docker Image](https://img.shields.io/badge/docker%20image-367MB%20MB-blue.svg)](https://hub.docker.com/r/blacktop/cuckoo/)

This repository contains a **Dockerfile** of [Cuckoo Sandbox](https://github.com/cuckoosandbox/cuckoo).

> :construction: WARNING: Currently only works with remote machinery: **esx, vsphere and xenserver**.

**Table of Contents**

-	[Dependencies](#dependencies)
-	[Image Tags](#image-tags)
-	[Installation](#installation)
-	[To Run on OSX](#to-run-on-osx)
-	[Getting Started](#getting-started)
-	[Documentation](#documentation)
-	[Known Issues](#known-issues)
-	[Issues](#issues)
-	[Todo](#todo)
-	[CHANGELOG](#changelog)
-	[Contributing](#contributing)
-	[License](#license)

### Dependencies

-	[blacktop/yara:3.5](https://hub.docker.com/r/blacktop/yara/)
-	[blacktop/volatility:2.6](https://hub.docker.com/r/blacktop/volatility/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/cuckoo     latest              367MB
blacktop/cuckoo     2.0                 367MB
blacktop/cuckoo     modified (WIP)      317.1 MB
blacktop/cuckoo     1.2                 258.6 MB
```

> **NOTE:** * tags **latest** and **2.0** contain all of `cuckoosandbox/community`  
>  * tag **modified** is the *awesome* **spender-sandbox** version of cuckoo and contains all of `spender-sandbox/community-modified`

### Installation

1.	Install [Docker](https://docs.docker.com).
2.	Install [docker-compose](https://docs.docker.com/compose/install/)
3.	Download [trusted build](https://hub.docker.com/r/blacktop/cuckoo/) from public [Docker Registry](https://hub.docker.com/): `docker pull blacktop/cuckoo`

### To Run on OSX

-	Install [Homebrew](http://brew.sh)

```bash
$ brew tap caskroom/cask
$ brew cask install virtualbox
$ brew install docker
$ brew install docker-machine
$ docker-machine create --driver virtualbox default
$ eval $(docker-machine env)
```

Or install [Docker for Mac](https://docs.docker.com/docker-for-mac/)

### Getting Started

```bash
$ curl -sL https://github.com/blacktop/docker-cuckoo/raw/master/docker-compose.yml > docker-compose.yml
$ docker-compose up -d
# For docker-machine
$ curl $(docker-machine ip):8000/cuckoo/status
# For Docker for Mac
$ curl localhost:8000/cuckoo/status
```

```json
{
  "cpuload": [
    0.01220703125,
    0.03515625,
    0.025390625
  ],
  "diskspace": {},
  "hostname": "195855fb100f",
  "machines": {
    "available": 0,
    "total": 0
  },
  "memory": 88.55692015425926,
  "tasks": {
    "completed": 0,
    "pending": 0,
    "reported": 0,
    "running": 0,
    "total": 0
  },
  "version": "2.0-dev"
}
```

##### Now Navigate To

-	With [docker-machine](https://docs.docker.com/machine/) : `http://$(docker-machine ip)`
-	With [Docker for Mac](https://docs.docker.com/engine/installation/mac/) : `http://localhost`

![cuckoo-dashboard](https://github.com/blacktop/docker-cuckoo/raw/master/docs/img/2.0/dashboard.png)

### Documentation

-	[Usage](https://github.com/blacktop/docker-cuckoo/blob/master/docs/usage.md)
-	[Available Subcommands](https://github.com/blacktop/docker-cuckoo/blob/master/docs/subcmd.md)
-	[Running Modified Version](https://github.com/blacktop/docker-cuckoo/blob/master/docs/modified.md)
-	[Tips and Tricks](https://github.com/blacktop/docker-cuckoo/blob/master/docs/tips-tricks.md)

### Known Issues

Currently won't work with VirtualBox, VMWare Workstation/Fusion or KVM/qemu, but I have an idea on how to do it. [:wink:](https://github.com/blacktop/vm-proxy)

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-cuckoo/issues/new) and I'll get right on it.

### Todo

-	[x] Install/Run Cuckoo Sandbox
-	[x] Break mongo out into a separate container using docker-compose
-	[x] Fix blacktop/yara and blacktop/volatility so I can use them as a base images for this image
-	[x] Create docker-entryporint.sh to use same container as daemon or web app or api or utility, etc
-	[ ] Figure out how to link to a analysis Windows VM (would be great if it was running in another container)
-	[x] Correctly link mongo/elasticsearch in confs or document how to do it at runtime (or use docker-entryporint BEST OPTION)
-	[x] add wait-for-it.sh to wait for postgres before API starts  
-	[ ] Web reverse proxy via Nginx with SSL
-	[ ] Add snort or suricata or both
-	[x] Get `modified` version of cuckoo to install/run in docker

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-cuckoo/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-cuckoo/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-cuckoo/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2015-2017 **blacktop**
![logstash-logo](https://raw.githubusercontent.com/blacktop/docker-logstash-alpine/master/logstash-logo.png)

docker-logstash-alpine
======================

[![CircleCI](https://circleci.com/gh/blacktop/docker-logstash-alpine.png?style=shield)](https://circleci.com/gh/blacktop/docker-logstash-alpine)
[![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/logstash.svg)](https://hub.docker.com/r/blacktop/logstash/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/logstash.svg)](https://hub.docker.com/r/blacktop/logstash/)
[![Docker Image](https://img.shields.io/badge/docker%20image-263MB-blue.svg)](https://hub.docker.com/r/blacktop/logstash/)

Alpine Linux based [Logstash](https://www.elastic.co/products/logstash) Docker Image

### Dependencies

-	[alpine:3.6](https://index.docker.io/_/gliderlabs/alpine/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/logstash   latest              263MB
blacktop/logstash   6.0                 275MB
blacktop/logstash   5.5                 263MB
blacktop/logstash   x-pack              263MB
blacktop/logstash   5.4                 263MB
blacktop/logstash   5.3                 289MB
blacktop/logstash   5.2                 289MB
blacktop/logstash   5.1                 289MB
blacktop/logstash   5.0                 312.2MB
blacktop/logstash   2.4                 257.2MB
blacktop/logstash   2.3                 255.8MB
blacktop/logstash   1.5                 253.5MB
```

### Getting Started

Start Logstash with configuration file

```bash
$ docker run -d -v "$PWD":/config-dir blacktop/logstash logstash -f /config-dir/logstash.conf
```

Start Logstash with commandline configuration. Download [metricbeat](https://www.elastic.co/downloads/beats/metricbeat)  

```bash
$ docker run -d --name elastic -p 9200:9200 blacktop/elasticsearch
$ docker run -d --name kibana --link elastic:elasticsearch -p 5601:5601 blacktop/kibana
$ docker run -d --name logstash -p 5044:5044 --link elastic:elasticsearch blacktop/logstash \
  logstash -e 'input {
                  beats {
                    port => 5044
                  }
               }

               output {
                 elasticsearch {
                   hosts => "elasticsearch:9200"
                   manage_template => false
                   index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
                   document_type => "%{[@metadata][type]}"
                 }
               }'
$ ./scripts/import_dashboards               
$ ./metricbeat -e -c metricbeat.yml               
```

> Navigate to [http://localhost:5601](http://localhost:5601)

Click on `metricbeat-*` and :star: **Set as default index**   

![index](https://raw.githubusercontent.com/blacktop/docker-logstash-alpine/master/docs/index.png)

Click on **Dashboard** -> **Open** -> `Metricbeat-cpu`  

![kibana](https://raw.githubusercontent.com/blacktop/docker-logstash-alpine/master/docs/kibana.png)

### Documentation

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-logstash-alpine/issues/new)

### Credits

Heavily (if not entirely) influenced by https://github.com/docker-library/logstash

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-logstash-alpine/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-logstash-alpine/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-logstash-alpine/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2016-2017 **blacktop**
![el-stack-logo](https://raw.githubusercontent.com/blacktop/docker-elastic-stack/master/docs/img/el_stack_logo.png)

Elastic Stack Dockerfile
========================

[![CircleCI](https://circleci.com/gh/blacktop/docker-elastic-stack.png?style=shield)](https://circleci.com/gh/blacktop/docker-elastic-stack) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/elastic-stack.svg)](https://hub.docker.com/r/blacktop/elastic-stack/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/elastic-stack.svg)](https://hub.docker.com/r/blacktop/elastic-stack/) [![Docker Image](https://img.shields.io/badge/docker%20image-525MB-blue.svg)](https://hub.docker.com/r/blacktop/elastic-stack/)

This repository contains a **Dockerfile** of the [Elastic Stack](https://www.elastic.co/products).

### Dependencies

-	[alpine:3.6](https://index.docker.io/_/gliderlabs/alpine/)
-	[openjdk8-jre](https://pkgs.alpinelinux.org/package/v3.4/community/x86_64/openjdk8-jre)
-	[Elasticsearch](https://www.elastic.co/products/elasticsearch)
-	[Logstash](https://www.elastic.co/products/logstash)
-	[Kibana](https://www.elastic.co/products/kibana)

### Image Tags

```bash
$ docker images

REPOSITORY                    TAG                 VIRTUAL SIZE
blacktop/elastic-stack        latest              525MB
blacktop/elastic-stack        6.0-alpha           529MB
blacktop/elastic-stack        5.5                 525MB
blacktop/elastic-stack        5.4                 539MB
blacktop/elastic-stack        5.3                 506MB
blacktop/elastic-stack        geoip               558MB
blacktop/elastic-stack        5.2                 537MB
blacktop/elastic-stack        4.6                 450.9MB
blacktop/elastic-stack        3.1                 363.3MB
```

> **NOTE:** tag **geoip** is the same as tag **latest**, but includes the *ingest-geoip* and the *ingest-user-agent* plugins.

### Getting Started

```bash
$ docker run -d --name elstack -p 80:80 -p 9200:9200 blacktop/elastic-stack
```

##### Now Navigate To

-	With [Docker for Mac](https://docs.docker.com/engine/installation/mac/) : `http://localhost`
-	With [docker-machine](https://docs.docker.com/machine/) : `http://$(docker-machine ip)`
-	With [docker-engine](https://docker.github.io/engine/installation/) : `$(docker inspect -f '{{ .NetworkSettings.IPAddress }}' elstack)`

![elk-logo](https://raw.githubusercontent.com/blacktop/docker-elk/master/docs/img/discover.png)

#### You can also use each part of the stack independently

-	[blacktop/elasticsearch](https://github.com/blacktop/docker-elasticsearch-alpine)
-	[blacktop/logstash](https://github.com/blacktop/docker-logstash-alpine)
-	[blacktop/kibana](https://github.com/blacktop/docker-kibana-alpine)

### Documentation

-	[Add some demo data](docs/add-data.md)
-	[Enable SSL](docs/ssl.md)
-	[Change nginx password](docs/change-pass.md)
-	[Build a multi-node Elastic Stack cluster](docs/mutil-node.md)

### Known Issues :warning:

I have noticed when running the new **5.0** version on a **linux** host you need to increase the memory map areas with the following command

```bash
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
sudo sysctl -w vm.max_map_count=262144
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-elastic-stack/issues/new)

### Credits

Heavily (if not entirely) influenced by all the elastic official docker images

### Todo

-	[x] Install/Run Elastic Stack
-	[x] Start Daemon and watch folder with supervisord
-	[x] Expose Logstash config folder as well as Nginx sites folder as Volumes
-	[x] Build ES test data docker image
-	[x] Add Nginx entrypoint to pass USER/PASS in as env vars
-	[x] Add SSL (auto-create certs if not found)
-	[x] Add back a 3.0 version of the stack (elk stack)
-	[ ] Integrate with Bro-IDS

### License

MIT Copyright (c) 2014-2017 **blacktop**
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
logstash-dfir
=============

Logstash configuration files for analyzing various types of logs. These configuration files are provided to analyze various types of log files using logstash, elasticsearch, and kibana.

Whether you are running a full-blown setup of ElasticSearch, Kibana, and log shippers, or a single instance for rapid analysis, these configuration files will help you quickly parse various log files found on system images.

Logstash
=============
[Logstash Website](http://www.logstash.net)

Other Resources
=============
[sysforensics GitHub](https://github.com/sysforensics/LogstashConfigs)

Related Posts
=============
[I'll take some Elasticsearch/Kibana with my Plaso (Windows edition)](http://blog.kiddaland.net/2014/06/ill-take-some-elasticsearchkibana-with.html)

[Finding the Needle in the Haystack with ELK](https://digital-forensics.sans.org/summit-archives/dfirprague14/Finding_the_Needle_in_the_Haystack_with_FLK_Christophe_Vandeplas.pdf)

[Rapid Log Analysis](http://www.505forensics.com/rapid-log-analysis/)

[Do you even Bro, bro?](http://www.505forensics.com/do-you-even-bro-bro/)

[Utilizing Dictionaries with Logstash](http://www.505forensics.com/utilizing-dictionaries-with-logstash/)

Changelog
=============
07 Jan 2015 - Uploaded logstash dictionaries for HTTP, FTP, and Bro IDS conn log status codes

04 Sep 2014 - Uploaded Bro IDS logs; thanks to team at http://www.appliednsm.com for laying the groundwork

02 Mar 2014 - Added log2timeline logstash config

01 Mar 2014 - Added apache-combined logstash config

22 Feb 2014 - Repository created; uploaded apache-common logstash config.
![kibana-logo](https://raw.githubusercontent.com/blacktop/docker-kibana-alpine/master/kibana-logo.png)

docker-kibana-alpine
====================

[![CircleCI](https://circleci.com/gh/blacktop/docker-kibana-alpine.png?style=shield)](https://circleci.com/gh/blacktop/docker-kibana-alpine) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/kibana.svg)](https://hub.docker.com/r/blacktop/kibana/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/kibana.svg)](https://hub.docker.com/r/blacktop/kibana/) [![Docker Image](https://img.shields.io/badge/docker%20image-189MB-blue.svg)](https://hub.docker.com/r/blacktop/kibana/)

Alpine Linux based [Kibana](https://www.elastic.co/products/kibana) Docker Image

**Table of Contents**

-	[Why?](#why)
-	[Dependencies](#dependencies)
-	[Image Tags](#image-tags)
-	[Getting Started](#getting-started)
-	[Documentation](#documentation)
-	[Issues](#issues)
-	[Credits](#credits)
-	[CHANGELOG](#changelog)
-	[Contributing](#contributing)
-	[License](#license)

### Why?

Compare Image Sizes:  
 - official kibana = 679 MB - blacktop/kibana = 146 MB

**Alpine version is 145 MB smaller !**

### Dependencies

-	[alpine:3.6](https://index.docker.io/_/gliderlabs/alpine/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/kibana     latest              189MB
blacktop/kibana     6.0                 187MB
blacktop/kibana     5.5                 189MB
blacktop/kibana     5.4                 203MB
blacktop/kibana     5.3                 145MB
blacktop/kibana     x-pack              404MB
blacktop/kibana     5.2                 246MB
blacktop/kibana     5.1                 246MB
blacktop/kibana     5.0                 245.8MB
blacktop/kibana     4.6                 229.7MB
```

> **NOTE:** tag **x-pack** is the same as tag **latest**, but includes the *x-pack* plugin.

### Getting Started

```bash
$ docker run --init -d --name elastic -p 9200:9200 blacktop/elasticsearch
$ docker run --init -d --name kibana --link elastic:elasticsearch -p 5601:5601 blacktop/kibana
```

### Documentation

#### To use your own elasticsearch address via `ELASTICSEARCH_URL`

```bash
$ docker run --init -d --name kibana -e ELASTICSEARCH_URL=http://some-elasticsearch:9200 -p 5601:5601 blacktop/kibana
```

For elasticsearch running on a OSX host it would be

```bash
$ docker run --init -d --name kibana \
  -p 5601:5601 \
  --net host \
  -e ELASTICSEARCH_URL="http://$(ipconfig getifaddr en0):9200" \
  blacktop/kibana
```

=OR=

```bash
$ docker run --init -d --name kibana \
  -p 5601:5601 \
  --net host \
  -e ELASTICSEARCH_URL=http://localhost:9200 \
  blacktop/kibana
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-kibana-alpine/issues/new)

### Credits

Heavily (if not entirely) influenced by https://github.com/docker-library/kibana

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-kibana-alpine/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-kibana-alpine/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-kibana-alpine/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2016-2017 **blacktop**
kibana-plugin-builder
=====================

[![Circle CI](https://circleci.com/gh/blacktop/kibana-plugin-builder.png?style=shield)](https://circleci.com/gh/blacktop/kibana-plugin-builder) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/kibana-plugin-builder.svg)](https://store.docker.com/community/images/blacktop/kibana-plugin-builder) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/kibana-plugin-builder.svg)](https://store.docker.com/community/images/blacktop/kibana-plugin-builder) [![Docker Image](https://img.shields.io/badge/docker%20image-1.09GB-blue.svg)](https://store.docker.com/community/images/blacktop/kibana-plugin-builder)

> Kibana Plugin Builder - **kibana** plugin development environment in a **docker** image

---

### Dependencies

-	[alpine:3.6](https://hub.docker.com/_/alpine/)

### Image Tags

```bash
REPOSITORY                         TAG                 SIZE
blacktop/kibana-plugin-builder     latest              1.09GB
blacktop/kibana-plugin-builder     5.5.2               1.09GB
blacktop/kibana-plugin-builder     node                54MB
```

> **NOTE:** tag `node` is the base image that has the appropriate version of **NodeJS** for the version of **Kibana** you are using to build your plugin (it defaults to the version needed for latest)

Getting Started
---------------

### Install [template-kibana-plugin](https://github.com/elastic/template-kibana-plugin/)

```bash
$ npm install -g sao template-kibana-plugin
```

### Run the generator

```bash
$ cd my-new-plugin
$ sao kibana-plugin
```

=OR=

### Use `kibana-plugin-builder` to create new plugin  

```bash
$ mkdir my-new-plugin
$ cd my-new-plugin
$ docker run --init --rm -ti -v `pwd`:/plugin -w /plugin blacktop/kibana-plugin-builder new-plugin
```

### Start Kibana Dev Environment

> **NOTE:** this assumes you have set a `version` in your `package.json`

*example:*

```json
  "kibana": {
    "version": "5.5.2",
    "templateVersion": "7.2.0"
  }
  ...
```

```bash
# Starting kibana elasticsearch...
$ docker run --init -d \
             --name kplug \
             -p 9200:9200 -p 5601:5601 \
             -v `pwd`:/plugin/my-new-plugin \
             blacktop/kibana-plugin-builder:$(jq -r '.version' package.json) elasticsearch
# Running kibana plugin...
$ docker exec -it kplug bash -c "cd ../my-new-plugin && npm start"
```

Build Image
-----------

To build a **kibana** dev env that uses version `5.5.2`

```bash
$ git clone https://github.com/blacktop/kibana-plugin-builder.git
$ cd kibana-plugin-builder
$ VERSION=5.5.2 make build
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/kibana-plugin-builder/issues/new)

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/kibana-plugin-builder/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/kibana-plugin-builder/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/kibana-plugin-builder/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2017 **blacktop**
![es-logo](https://raw.githubusercontent.com/blacktop/docker-elasticsearch-alpine/master/es-logo.png)

docker-elasticsearch-alpine
===========================

[![CircleCI](https://circleci.com/gh/blacktop/docker-elasticsearch-alpine.png?style=shield)](https://circleci.com/gh/blacktop/docker-elasticsearch-alpine) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/elasticsearch.svg)](https://hub.docker.com/r/blacktop/elasticsearch/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/elasticsearch.svg)](https://hub.docker.com/r/blacktop/elasticsearch/) [![Docker Image](https://img.shields.io/badge/docker%20image-123MB-blue.svg)](https://hub.docker.com/r/blacktop/elasticsearch/)

Alpine Linux based [Elasticsearch](https://www.elastic.co/products/elasticsearch) Docker Image

**Table of Contents**

-	[Dependencies](#dependencies)
-	[Image Tags](#image-tags)
-	[Getting Started](#getting-started)
-	[Documentation](#documentation)
	-	[To create an elasticsearch cluster](docs/create.md)
	-	[To increase the HEAP_SIZE to 2GB](docs/options.md)  
	-	[To monitor the clusters metrics using dockerbeat](docs/dockerbeat.md)
	-	[To run in production](docs/production.md)
-	[Issues](#issues)
-	[Credits](#credits)
-	[CHANGELOG](#changelog)
-	[Contributing](#contributing)
-	[License](#license)

### Why?

Compare Image Sizes:  
 - official elasticsearch = 206 MB - blacktop/elasticsearch = 123 MB

**blacktop version is 83 MB smaller !**

### Dependencies

-	[alpine:3.6](https://index.docker.io/_/gliderlabs/alpine/)

### Image Tags

```bash
REPOSITORY               TAG                 SIZE
blacktop/elasticsearch   latest              123MB
blacktop/elasticsearch   6.0                 117MB
blacktop/elasticsearch   5.5                 123MB
blacktop/elasticsearch   5.4                 123MB
blacktop/elasticsearch   5.3                 123MB
blacktop/elasticsearch   x-pack              255MB
blacktop/elasticsearch   geoip               156MB
blacktop/elasticsearch   5.2                 150MB
blacktop/elasticsearch   5.1                 149MB
blacktop/elasticsearch   5.0                 148.4MB
blacktop/elasticsearch   2.4                 116MB
blacktop/elasticsearch   kopf                122MB
blacktop/elasticsearch   2.3                 139.1MB
blacktop/elasticsearch   1.7                 142.7MB
```

> **NOTE:** * tag **x-pack** is the same as tag **latest**, but includes the *x-pack*, the *ingest-geoip* and the *ingest-user-agent* plugin.  
>  * tag **geoip** is the same as tag **latest**, but includes the *ingest-geoip* and the *ingest-user-agent* plugin.  
>  * tag **kopf** is the same as tag **2.4**, but includes the *kopf* plugin.

### Getting Started

```bash
$ docker run -d --name elastic -p 9200:9200 blacktop/elasticsearch
```

### Documentation

-	[To create an elasticsearch cluster](docs/create.md)
-	[To increase the HEAP_SIZE to 2GB](docs/options.md)
-	[To monitor the clusters metrics using dockerbeat](docs/dockerbeat.md)
-	[To run in production](docs/production.md)

### Known Issues :warning:

I have noticed when running the new **5.0+** version on a linux host you need to increase the memory map areas with the following command

```bash
sudo sysctl -w vm.max_map_count=262144
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-elasticsearch-alpine/issues/new)

### Credits

Heavily (if not entirely) influenced by https://github.com/docker-library/elasticsearch  
Production docs from https://stefanprodan.com/2016/elasticsearch-cluster-with-docker/

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-elasticsearch-alpine/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-elasticsearch-alpine/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-elasticsearch-alpine/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2016-2017 **blacktop**
* pg-sphinx

  [[https://travis-ci.org/andy128k/pg-sphinx][https://travis-ci.org/andy128k/pg-sphinx.svg?branch=master]]

  Pg-sphinx is an extension for PostgreSQL which allows to integrate Sphinx search engine.

** Installation

*** Compile

  #+BEGIN_SRC sh
  make
  #+END_SRC

*** Install
  
  #+BEGIN_SRC sh
  sudo make install
  #+END_SRC

*** Define sphinx functions in your database

  Superuser is required.

  #+BEGIN_SRC sh
  echo 'CREATE EXTENSION sphinx;' | psql -U postgres mydatabase
  #+END_SRC

*** Uninstall

  #+BEGIN_SRC sh
  sudo make uninstall
  #+END_SRC

** Configuration

   Extension can be configured by modifying corresponding rows in table sphinx_config.
   Following options are available: 'host', 'post', 'username', 'password', 'prefix'.

   'Prefix' is a string which is prepended to index names. This option is useful to simulate
   namespaces. For example, if prefix is 'test_' and index name in a request is 'blog_posts',
   real request will be addressed to index named 'test_blog_posts'.


** Functions

*** Search query

  #+BEGIN_SRC sql
  sphinx_select(
      /*index*/     varchar,
      /*query*/     varchar,
      /*condition*/ varchar,
      /*order*/     varchar,
      /*offset*/    int,
      /*limit*/     int,
      /*options*/   varchar)
  #+END_SRC

  Returns pairs (id, weight).

*** Update data

  #+BEGIN_SRC sql
  sphinx_replace(
      /*index*/     varchar,
      /*id*/        int,
      /*data*/      varchar[])
  #+END_SRC

  Updates document with specified id. Data array must have following format:
  ARRAY['key1', 'value2', ...]

*** Delete data

  #+BEGIN_SRC sql
  sphinx_delete(
      /*index*/     varchar,
      /*id*/        int)
  #+END_SRC

  Removes specified document.

*** Get snippet

  #+BEGIN_SRC sql
  sphinx_snippet(
      /*index*/     varchar,
      /*query*/     varchar,
      /*data*/      varchar,
      /*before*/    varchar,
      /*after*/     varchar)
  #+END_SRC

  Returns snippets for a given data and search query.

  Example:

  #+BEGIN_SRC sql
  SELECT sphinx_snippet('blog_posts', 'photo', 'There are photos from monday meeting', '<b>', '</b>')
  #+END_SRC

  This query will return following text:

  #+BEGIN_SRC sql
  'There are <b>photos</b> from monday meeting'
  #+END_SRC


  #+BEGIN_SRC sql
  sphinx_snippet_options(
      /*index*/     varchar,
      /*query*/     varchar,
      /*data*/      varchar,
      /*options*/   varchar[])
  #+END_SRC

  Returns snippets for a given data and search query. This function is similar to sphinx_snippet but it also accepts
  a list of additional options.

  Because of array type limitations all values must be represented as text.
  Integer values have to be represented as decimal values (e. g. '19').
  For boolean values '1', 't', 'true', 'y', and 'yes' are recognized as true, any other value is considered as false.

  Example (similar to previous one):

  #+BEGIN_SRC sql
  SELECT sphinx_snippet_options('blog_posts', 'photo', 'There are photos from monday meeting',
                                ARRAY['before_match', '<b>',
                                      'after_match', '</b>'])
  #+END_SRC

  One more example:

  #+BEGIN_SRC sql
  SELECT sphinx_snippet_options('blog_posts', 'photo', 'There are photos from monday meeting',
                                ARRAY['before_match', '<b>',
                                      'after_match', '</b>',
                                      'query_mode', 'yes'])
  #+END_SRC


![kafka-logo](https://raw.githubusercontent.com/blacktop/docker-kafka-alpine/master/docs/kafka-logo.png)

docker-kafka-alpine
===================

[![CircleCI](https://circleci.com/gh/blacktop/docker-kafka-alpine.png?style=shield)](https://circleci.com/gh/blacktop/docker-kafka-alpine) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/kafka.svg)](https://hub.docker.com/r/blacktop/kafka/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/kafka.svg)](https://hub.docker.com/r/blacktop/kafka/) [![Docker Image](https://img.shields.io/badge/docker%20image-226MB-blue.svg)](https://hub.docker.com/r/blacktop/kafka/)

Alpine Linux based [Kafka](http://kafka.apache.org/downloads.html) Docker Image

### Dependencies

-	[alpine:3.6](https://hub.docker.com/_/alpine/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/kafka      latest              226MB
blacktop/kafka      0.11                226MB
blacktop/kafka      0.10                223MB
blacktop/kafka      0.9                 238.6MB
blacktop/kafka      0.8                 227.5MB
```

### Getting Started

> **NOTE:** I am assuming use of Docker for Mac with these examples. (`KAFKA_ADVERTISED_HOST_NAME=localhost`\)

```
docker run -d \
           --name kafka \
           -p 9092:9092 \
           -e KAFKA_ADVERTISED_HOST_NAME=localhost \
           -e KAFKA_CREATE_TOPICS="test-topic:1:1" \
           blacktop/kafka
```

This will create a single-node kafka broker (*listening on localhost:9092*), a local zookeeper instance and create the topic `test-topic` with 1 `replication-factor` and 1 `partition`.

You can now test your new single-node kafka broker using [Shopify/sarama's](https://github.com/Shopify/sarama) **kafka-console-producer** and **kafka-console-consumer**

#### Required

 - [Golang](https://golang.org/doc/install)

```bash
$ go get github.com/Shopify/sarama/tools/kafka-console-consumer
$ go get github.com/Shopify/sarama/tools/kafka-console-producer
```

Now start a *consumer* in the background and then send some data to **kafka** via a *producer*

```bash
$ kafka-console-consumer --bootstrap-server=localhost:9092 --topic=test-topic &
$ echo "shrinky-dinks" | kafka-console-producer --broker-list=localhost:9092 --topic=test-topic
```

### Documentation

-	[Usage](https://github.com/blacktop/docker-kafka-alpine/blob/master/docs/usage.md)
-	[Tips and Tricks](https://github.com/blacktop/docker-kafka-alpine/blob/master/docs/tips.md)

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-kafka-alpine/issues/new)

### Credits

Heavily (if not entirely) influenced by https://github.com/wurstmeister/kafka-docker

### Todo

-	[x] Add ability to run a single node kafka broker when you don't supply a zookeeper link.

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-kafka-alpine/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-kafka-alpine/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-kafka-alpine/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2016-2017 **blacktop**
# Docker container for Spark
A Dockerimage repository for Spark 2.1.1 with the neo4j-driver included.

Based on the existing docker-spark-java-8 container by <a href="https://github.com/wimvanleuven">wimvanleuven</a>.
[![Build Status](https://travis-ci.org/CoorpAcademy/docker-pyspark.svg)](https://travis-ci.org/CoorpAcademy/docker-pyspark)

# docker-pyspark

This Docker image helps you to run Spark (on Docker) with the following
installed:

1. [Apache Spark](https://spark.apache.org/) 2.0.0
  + running on Hadoop 2.7.2 and Java openjdk version "1.8.0_92-internal"
2. Python 3.4.3
3. Spark's python interface [pyspark](
https://spark.apache.org/docs/1.5.2/programming-guide.html#linking-with-spark)

# How to install

## On Mac OS X

### 1. Install [homebrew](http://brew.sh)

### 2. Install docker and launch docker daemon
    brew cask install docker

Launch the Docker.app application, and make sure it displays "Docker is running".

## On other OSes

Follow the installation guide from the [official docker guide](
https://docs.docker.com/machine/install-machine/).

# Starting pyspark

## On any OS

### 1. Pull the docker image
    docker pull coorpacademy/docker-pyspark:latest

### 2. Start the container
Run the following command to start the container and get a bash prompt

    docker run -it coorpacademy/docker-pyspark:latest /bin/bash

### 3. Start pyspark
    ./bin/pyspark  # open an interactive python shell with SparkContext as sc

### 4. Verify installation
To verify pyspark, run the following example Spark program:

    sc.parallelize(range(1000)).count()

This should print a bunch of debugging output, and on the last line,
print the output, "1000".

To quit the interpreter, hit `<Ctrl> + D`.

# How to run a cluster of containers with [Docker Compose](http://docs.docker.com/compose)

## docker-compose.yml example files

    cd example
    docker-compose up  # launch cluster (Ctrl-C to stop)

The SparkUI will be running at `http://${YOUR_DOCKER_HOST}:8080` with one
worker listed. To run `pyspark`, exec into a container:

    docker exec -it example_master_1 /bin/bash
    bin/pyspark

# (OPTIONAL) Building the docker image yourself

You can build this docker image, by running the following command in
the same directory as this =README= file. The command will be slow (a
few minutes) the first time, since all dependencies need to be fetched and
compiled from source, but the result is then cached. This step should
only be necessary if you modify the `Dockerfile`.

    docker build -t docker-pyspark .

# Troubleshooting
If you are unable to access HDFS from pyspark, try running pyspark with the
`--master yarn` flag.

If you are unable to access the HTTP SparkUI, verify that the open ports are
redirected from your virtual machine to your host machine. Under VirtualBox,
see the machine's `Settings > Network > Port Forwarding`.
[![Build Status](https://travis-ci.org/CoorpAcademy/docker-scspark.svg)](https://travis-ci.org/CoorpAcademy/docker-scspark)

# docker-scspark

This Docker image helps you to run Spark (on Docker) with the following
installed:

1. [Apache Spark](https://spark.apache.org/) 2.0.0
  + running on Hadoop 2.7.2 and Java openjdk version "1.8.0_92-internal"
2. SBT 0.13.13

# How to install

## On Mac OS X

### 1. Install [homebrew](http://brew.sh)

### 2. Install docker and launch docker daemon
    brew cask install docker

Launch the Docker.app application, and make sure it displays "Docker is running".

## On other OSes

Follow the installation guide from the [official docker guide](
https://docs.docker.com/machine/install-machine/).

# Starting scspark

## On any OS

### 1. Pull the docker image
    docker pull coorpacademy/docker-scspark:latest

### 2. Start the container
Run the following command to start the container and get a bash prompt

    docker run -it coorpacademy/docker-scspark:latest /bin/bash

### 3. Start scspark
    ./bin/spark-shell  # open an interactive scala shell with SparkContext as sc

### 4. Verify installation
To verify scspark, run the following example Spark program:

    sc.parallelize(1 to 1000).count()

This should print: `res0: Long = 1000`.

To quit the interpreter, hit `<Ctrl> + D`.

# How to run a cluster of containers with [Docker Compose](http://docs.docker.com/compose)

## docker-compose.yml example files

    cd example
    docker-compose up  # launch cluster (Ctrl-C to stop)

The SparkUI will be running at `http://${YOUR_DOCKER_HOST}:8080` with one
worker listed. To run `spark-shell`, exec into a container:

    docker exec -it example_master_1 /bin/bash
    spark-shell

Another interesting way of running your script is to use:

    docker exec -it example_master_1 bash -c "sbt package && spark-submit target/scala-2.11/my-awesome-project_2.11-0.1.jar"

And in another terminal:

    docker exec -it example_master_1 nc -l -p 9999

Now whatever words you type in netcat (`nc`) are counted by spark.

# (OPTIONAL) Building the docker image yourself

You can build this docker image, by running the following command in
the same directory as this =README= file. The command will be slow (a
few minutes) the first time, since all dependencies need to be fetched and
compiled from source, but the result is then cached. This step should
only be necessary if you modify the `Dockerfile`.

    docker build -t docker-scspark .

# Troubleshooting
If you are unable to access HDFS from scspark, try running scspark with the
`--master yarn` flag.
# docker-hbase

[![](https://images.microbadger.com/badges/image/smizy/hbase:1.2.6-alpine.svg)](http://microbadger.com/images/smizy/hbase:1.2.6-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/hbase:1.2.6-alpine.svg)](http://microbadger.com/images/smizy/hbase:1.2.6-alpine "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-hbase.svg?style=svg&circle-token=c37476ccaf10f385fa251abd7a45c7e93817db0f)](https://circleci.com/gh/smizy/docker-hbase)

Apache HBase docker image based on alpine

## Small setup
```
# load default env as needed
eval $(docker-machine env default)

# network 
docker network create vnet

# make docker-compose.yml with small size (no redudency)
zookeeper=1 namenode=1 datanode=1 ./make_docker_compose_yml.sh hdfs hbase > docker-compose.yml

# or with default size(zookeeper=3, namenode=2, journalnode=3, datanode=3, hmaster=2, regionserver=3)  
./make_docker_compose_yml.sh hdfs hbase > docker-compose.yml

# hadoop+hbase startup
docker-compose up -d

# tail logs for a while
docker-compose logs -f

# check ps
docker-compose ps

     Name                   Command               State                  Ports                
---------------------------------------------------------------------------------------------
datanode-1       entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp     
hmaster-1        entrypoint.sh hmaster-1          Up      16000/tcp, 0.0.0.0:32771->16010/tcp 
namenode-1       entrypoint.sh namenode-1         Up      0.0.0.0:32770->50070/tcp, 8020/tcp  
regionserver-1   entrypoint.sh regionserver       Up      16020/tcp, 16030/tcp                
zookeeper-1      entrypoint.sh -server 1 1 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# check stats
docker ps --format {{.Names}} | xargs docker stats

# hbase shell
docker exec -it -u hbase regionserver-1 hbase shell
hbase(main):001:0> create 'test', 'cf'
hbase(main):002:0> list 'test'
hbase(main):003:0> put 'test', 'row1', 'cf:a', 'value1'
hbase(main):004:0> put 'test', 'row2', 'cf:b', 'value2'
hbase(main):005:0> put 'test', 'row3', 'cf:c', 'value3'
hbase(main):006:0> scan 'test'
hbase(main):007:0> get 'test', 'row1'
hbase(main):008:0> disable 'test'
hbase(main):009:0> drop 'test'
hbase(main):010:0> exit

# hadoop/hbase shutdown  
docker-compose stop

# cleanup container
docker-compose rm -v
```# spark1.6-jupyter4.1# docker-hadoop-base

[![](https://images.microbadger.com/badges/image/smizy/hadoop-base:2.7.4-alpine.svg)](http://microbadger.com/images/smizy/hadoop-base:2.7.4-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/hadoop-base:2.7.4-alpine.svg)](http://microbadger.com/images/smizy/hadoop-base:2.7.4-alpine "Get your own image badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-hadoop-base.svg?style=shield&circle-token=155cf7c34ea00da94d6d7848796b96d62d95de48)](https://circleci.com/gh/smizy/docker-hadoop-base)

Hadoop(Common/HDFS/YARN/MapReduce) docker image based on alpine

* Namenode is set to high availability mode with multiple namenode
* Non secure mode
* Alpine built native-hadoop library bundled
  *  Native netgroup mapping function missing
* One process per container as possible 
* No sshd setting. Cannot use utility script like start-dfs.sh and start-yarn.sh.  
* conf template applied by mustache.sh

This setup use FQDN with docker embedded DNS instead of editing /etc/hosts. 
Using FQDN on Hadoop require dns lookup and reverse lookup. 

You need set --name and --net (container_name.network_name as hostname) for dns lookup from other containers 
, and set --hostname(-h) for reverse lookup from container itself.


## Small setup  

```
# load default env as needed
eval $(docker-machine env default)

# network 
docker network create vnet

# make docker-compose.yml 
zookeeper=1 namenode=1 datanode=1 ./make_docker_compose_file.sh hdfs yarn > docker-compose.yml

# config test
docker-compose config

# hadoop startup
docker-compose up -d

# tail logs for a while
docker-compose logs -f

# check ps
docker-compose ps

      Name                     Command               State                                Ports                              
----------------------------------------------------------------------------------------------------------------------------
datanode-1          entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp                                 
historyserver-1     entrypoint.sh historyserver-1    Up      10020/tcp, 0.0.0.0:19888->19888/tcp                             
namenode-1          entrypoint.sh namenode-1         Up      0.0.0.0:32820->50070/tcp, 8020/tcp                              
nodemanager-1       entrypoint.sh nodemanager        Up      8040/tcp, 8041/tcp, 8042/tcp                                    
resourcemanager-1   entrypoint.sh resourcemana ...   Up      8030/tcp, 8031/tcp, 8032/tcp, 8033/tcp, 0.0.0.0:32819->8088/tcp 
zookeeper-1         entrypoint.sh -server 1 1 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# check stats
docker ps --format {{.Names}} | xargs docker stats

# run example data (pi calc)
docker exec -it -u hdfs datanode-1 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar pi 10 100

# view job history in web ui
open http://$(docker-machine ip default):19888

# hadoop shutdown  
docker-compose stop

# cleanup container
docker-compose rm -v

```# spark1.6-jupyter4.1# spark1.6-jupyter4.1# spark1.6-jupyter4.1# docker-phoenix

[![](https://images.microbadger.com/badges/image/smizy/apache-phoenix:4.11-alpine.svg)](http://microbadger.com/images/smizy/apache-phoenix:4.11-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/apache-phoenix:4.11-alpine.svg)](http://microbadger.com/images/smizy/apache-phoenix:4.11-alpine "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-apache-phoenix.svg?style=svg&circle-token=8171bd548172f815e994704c0c7f23ac3447371d)](https://circleci.com/gh/smizy/docker-apache-phoenix)

Apache Phoenix docker image based on alpine

## Small setup

```
# load default env as needed
eval $(docker-machine env default)

# network 
docker network create vnet

# hbase+phoenix startup
docker-compose up -d

# tail logs for a while
docker-compose logs -f

# check ps
docker-compose ps
    Name                   Command               State                  Ports                
---------------------------------------------------------------------------------------------
datanode-1       entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp     
hmaster-1        entrypoint.sh hmaster-1          Up      16000/tcp, 0.0.0.0:32781->16010/tcp 
namenode-1       entrypoint.sh namenode-1         Up      0.0.0.0:32780->50070/tcp, 8020/tcp  
regionserver-1   entrypoint.sh regionserver       Up      16020/tcp, 16030/tcp                
zookeeper-1      entrypoint.sh -server 1 1 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# Try Getting Started (http://phoenix.apache.org/installation.html)

docker-compose exec regionserver-1 sh
$ psql.py zookeeper-1.vnet examples/STOCK_SYMBOL.sql examples/STOCK_SYMBOL.csv
no rows upserted
Time: 0.01 sec(s)

1 row upserted
Time: 0.044 sec(s)

SYMBOL                                   COMPANY                                  
---------------------------------------- ---------------------------------------- 
CRM                                      SalesForce.com                           
Time: 0.044 sec(s)

csv columns from database.
CSV Upsert complete. 9 rows upserted
Time: 0.02 sec(s)

$ sqlline.py zookeeper-1.vnet
Connected to: Phoenix (version 4.11)
Driver: PhoenixEmbeddedDriver (version 4.11)
Autocommit status: true
Transaction isolation: TRANSACTION_READ_COMMITTED
Building list of tables and columns for tab-completion (set fastconnect to true to skip)...
88/88 (100%) Done
Done
sqlline version 1.2.0
0: jdbc:phoenix:zookeeper-1.vnet>
0: jdbc:phoenix:zookeeper-1.vnet> !table
0: jdbc:phoenix:zookeeper-1.vnet> select * from STOCK_SYMBOL;
+---------+-----------------------+
| SYMBOL  |        COMPANY        |
+---------+-----------------------+
| AAPL    | APPLE Inc.            |
| CRM     | SALESFORCE            |
| GOOG    | Google                |
| HOG     | Harlet-Davidson Inc.  |
| HPQ     | Hewlett Packard       |
| INTC    | Intel                 |
| MSFT    | Microsoft             |
| WAG     | Walgreens             |
| WMT     | Walmart               |
+---------+-----------------------+
9 rows selected (0.067 seconds)

0: jdbc:phoenix:zookeeper-1.vnet> !exit

# hbase/phoenix shutdown  
docker-compose stop

# cleanup container
docker-compose rm -v
```# spark1.6-jupyter4.1# docker-cassandra
[![](https://images.microbadger.com/badges/image/smizy/cassandra.svg)](https://microbadger.com/images/smizy/cassandra "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/cassandra.svg)](https://microbadger.com/images/smizy/cassandra "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-cassandra.svg?style=svg&circle-token=524cf9de6cdd8e1d44f2fbd1875d2138f223185a)](https://circleci.com/gh/smizy/docker-cassandra)

Apache Cassandra docker image based on alpine

Note that this image is unstable and under development.

## Small setup

```
# network 
docker network create vnet

# startup cassandra
docker run --net vnet --name cassandra -d smizy/cassandra:3.11-alpine 

# tail logs
docker logs -f cassandra

# cqlsh access
docker run -it --rm --net vnet smizy/cassandra:3.11-alpine cqlsh cassandra.vnet

Connected to Test Cluster at cassandra.vnet:9042.
[cqlsh 5.0.1 | Cassandra 3.11 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> 
cqlsh> SELECT release_version, cluster_name FROM system.local;

 release_version | cluster_name
-----------------+--------------
            3.11 | Test Cluster

(1 rows)
cqlsh> exit
```

## Cluster setup on single host

```
# startup cassandra
docker-compose up -d 

# tail logs for a while
docker-compose logs -f 

# check ps
docker-compose ps

   Name                Command             State                        Ports                       
---------------------------------------------------------------------------------------------------
cassandra-1   entrypoint.sh cassandra -f   Up      7000/tcp, 7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp 
cassandra-2   entrypoint.sh cassandra -f   Up      7000/tcp, 7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp 
cassandra-3   entrypoint.sh cassandra -f   Up      7000/tcp, 7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp

# check node status
docker-compose exec cassandra-1 nodetool status

Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens    Owns (effective)  Host ID                               Rack
UN  172.18.0.2  103.66 KiB  256      66.3%             30e50198-03ef-46dc-a521-9b77c11b185b  rack1
UN  172.18.0.3  100.4 KiB   256      62.5%             aa610862-ac91-4be7-9495-de54773752b3  rack1
UN  172.18.0.4  80.23 KiB   256      71.2%             d624fec9-1a5d-48ce-a229-20ad5a691757  rack1

# stop cassandra  
docker-compose stop

# cleanup container 
docker-compose rm -v

```

# docker-zookeeper

[![](https://images.microbadger.com/badges/image/smizy/zookeeper:3.4.10-alpine.svg)](http://microbadger.com/images/smizy/zookeeper:3.4.10-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/zookeeper:3.4.10-alpine.svg)](http://microbadger.com/images/smizy/zookeeper:3.4.10-alpine "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-zookeeper.svg?style=shield&circle-token=10770b46f0ce48d9f1de65b33f7e17f2ffa16c81)](https://circleci.com/gh/smizy/docker-zookeeper)

Apache Zookeeper docker image based on alpine

# yarn2.6-spark1.6-jupyter# spark1.6-jupyter4.1# docker-apache-drill

[![](https://images.microbadger.com/badges/image/smizy/apache-drill:1.11-alpine.svg)](http://microbadger.com/images/smizy/apache-drill:1.11-alpine "Get your own version badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/smizy/apache-drill:1.11-alpine.svg)](http://microbadger.com/images/smizy/apache-drill:1.11-alpine "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-apache-drill.svg?style=shield&circle-token=dfe0035c074dee01c850f7def4ccab95c392b696)](https://circleci.com/gh/smizy/docker-apache-drill)

Apache Drill docker image based on alpine

## Usage
### Small setup 
```
# network
docker network create vnet

# generate docker-compose.yml (zookeeper:1, drill:1)
zookeeper=1 drillbit=1 ./make_docker_compose_yml.sh drill  > docker-compose.yml

# config test
docker-compose config

# load docker env as needed
eval $(docker-machine env default)

# run containers
docker-compose up -d

$ docker-compose ps
   Name                  Command               State              Ports             
-----------------------------------------------------------------------------------
drillbit-1    entrypoint.sh drillbit           Up      0.0.0.0:32773->8047/tcp       
zookeeper-1   entrypoint.sh -server 1 1 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# run query from web ui (adjust drillbit exposed port)
open http://$(docker-machine ip default):32773/query
// Submit "SELECT * FROM cp.`employee.json` LIMIT 20" 

# run query from drill shell client
docker exec -it drillbit-1 drill-conf

0: jdbc:drill:> SELECT employee_id, full_name, position_id, position_title FROM cp.`employee.json` LIMIT 5;
+--------------+------------------+--------------+-------------------------+
| employee_id  |    full_name     | position_id  |     position_title      |
+--------------+------------------+--------------+-------------------------+
| 1            | Sheri Nowmer     | 1            | President               |
| 2            | Derrick Whelply  | 2            | VP Country Manager      |
| 4            | Michael Spence   | 2            | VP Country Manager      |
| 5            | Maya Gutierrez   | 2            | VP Country Manager      |
| 6            | Roberta Damstra  | 3            | VP Information Systems  |
+--------------+------------------+--------------+-------------------------+
5 rows selected (0.25 seconds)

0: jdbc:drill:> !quit

# cleanup
docker-compose stop
docker-compose rm -v

```

### Setup with HDFS

```
# generate docker-compose.yml 
./make_docker_compose_yml.sh hdfs drill  > docker-compose.yml

# config test
docker-compose config

# run containers
docker-compose up -d

$ docker-compose ps
    Name                   Command               State                 Ports                
-------------------------------------------------------------------------------------------
datanode-1      entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp    
datanode-2      entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp    
datanode-3      entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp    
drillbit-1      entrypoint.sh drillbit           Up      0.0.0.0:32774->8047/tcp             
journalnode-1   entrypoint.sh journalnode        Up      8480/tcp, 8485/tcp                 
journalnode-2   entrypoint.sh journalnode        Up      8480/tcp, 8485/tcp                 
journalnode-3   entrypoint.sh journalnode        Up      8480/tcp, 8485/tcp                 
namenode-1      entrypoint.sh namenode-1         Up      0.0.0.0:32771->50070/tcp, 8020/tcp 
namenode-2      entrypoint.sh namenode-2         Up      0.0.0.0:32772->50070/tcp, 8020/tcp 
zookeeper-1     entrypoint.sh -server 1 3 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp       
zookeeper-2     entrypoint.sh -server 2 3 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp       
zookeeper-3     entrypoint.sh -server 3 3 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# tail logs for a while
docker-compose logs -f

# Query json data on hdfs 
docker exec -it -u hdfs datanode-1 bash

bash-4.3$ hdfs dfs -mkdir -p /user/hdfs/output
bash-4.3$ echo '{ a:1, b:2, c:3}' | hdfs dfs -put - /user/hdfs/output/test.json
bash-4.3$ hdfs dfs -cat /user/hdfs/output/test.json
{ a:1, b:2, c:3}

# update dfs storage setting (adjust drillbit exposed port)
open http://$(docker-machine ip default):32774/storage/dfs

{
  "type": "file",
  "enabled": true,
  "connection": "hdfs://namenode-1.vnet:8020/",
  "config": null,
  "workspaces": {
    "root": {
      "location": "/user/hdfs",
      "writable": false,
      "defaultInputFormat": null
    },
    "tmp": {
      "location": "/tmp",
      "writable": true,
      "defaultInputFormat": null
    }
  },
  :
  :
  
# run query from web ui
select * from dfs.root.`output/test.json`

# run query from drill shell client
docker exec -it drillbit-1 drill-conf

0: jdbc:drill:> select * from dfs.root.`output/test.json`;
+----+----+----+
| a  | b  | c  |
+----+----+----+
| 1  | 2  | 3  |
+----+----+----+
1 row selected (0.448 seconds)

0: jdbc:drill:> !quit

# cleanup
docker-compose stop
docker-compose rm -v

```

* You can run multi-host distributed hdfs/drill cluster with overlay "vnet" network
(instead of bridge network) and swarm(v1.11) setup.
 

## mustache.sh LICENSE
* BSD License. See LICENSE.mustache.
* Source: https://github.com/rcrowley/mustache.sh
* Copyright 2011 Richard Crowley. All rights reserved.# docker-couchdb

Apache CouchDB docker image based on alpine

Note that this image is unstable and under testing.

## Cluster setup on single host

```
# build
docker build -t local/couchdb .

# network
docker network create vnet

# set admin pass 
export COUCHDB_ADMIN_PASS=xxxxxxxxxxxxx

# startup couchdb cluster (3 nodes)
docker-compose up -d 

# tail logs for a while (ignore [error] database_does_not_exist..)
docker-compose logs -f 

# check ps
docker-compose ps
  Name                 Command               State                           Ports                         
----------------------------------------------------------------------------------------------------------
couchdb-1   /sbin/tini -- entrypoint.s ...   Up      4369/tcp, 0.0.0.0:5984->5984/tcp, 5986/tcp, 9100/tcp 
couchdb-2   /sbin/tini -- entrypoint.s ...   Up      4369/tcp, 5984/tcp, 5986/tcp, 9100/tcp                
couchdb-3   /sbin/tini -- entrypoint.s ...   Up      4369/tcp, 5984/tcp, 5986/tcp, 9100/tcp

# cluster setup
docker exec -it couchdb-1 cluster-setup.sh couchdb-1.vnet couchdb-2.vnet couchdb-3.vnet
{"ok":true}
{"error":"conflict","reason":"Document update conflict."} 
{"ok":true}
{"ok":true}
{"ok":true}
{"ok":true}
{"ok":true}

# open admin page / verify
* open http://localhost:5984/_utils
* login 
* select [Verify] on left menu 
* do [Veriy Installation] 
TODO: Replication often failed.

# stop couchdb cluster  
docker-compose stop

# cleanup container 
docker-compose rm -v
```
[![Build Status](https://travis-ci.org/frol/flask-restplus-server-example.svg)](https://travis-ci.org/frol/flask-restplus-server-example)
[![Coverage Status](https://coveralls.io/repos/frol/flask-restplus-server-example/badge.svg?branch=master&service=github)](https://coveralls.io/github/frol/flask-restplus-server-example?branch=master)
[![Codacy Coverage Status](https://api.codacy.com/project/badge/coverage/b0fc91ce77d3437ea5f107c4b7ccfa26)](https://www.codacy.com/app/frolvlad/flask-restplus-server-example)
[![Codacy Quality Status](https://api.codacy.com/project/badge/grade/b0fc91ce77d3437ea5f107c4b7ccfa26)](https://www.codacy.com/app/frolvlad/flask-restplus-server-example)
[![Heroku](http://heroku-badge.herokuapp.com/?app=flask-restplus-example-server&root=api/v1/&style=flat&svg=1)](http://flask-restplus-example-server.herokuapp.com/api/v1/)


RESTful API Server Example
==========================

This project showcases my vision on how the RESTful API server should be
implemented.

The goals that were achived in this example:

* RESTful API server should be self-documented using OpenAPI (fka Swagger)
  specifications, so interactive documentation UI is in place;
* Authentication is handled with OAuth2 and using Resource Owner Password
  Credentials Grant (Password Flow) for first-party clients makes it usable
  not only for third-party "external" apps;
* Permissions are handled (and automaticaly documented);
* PATCH method can be handled accordingly to
  [RFC 6902](http://tools.ietf.org/html/rfc6902);
* Extensive testing with good code coverage.

I had to patch Flask-RESTplus (see `flask_restplus_patched` folder), so it can
handle Marshmallow schemas and Webargs arguments.

Here is how it looks at this point of time ([live demo](http://flask-restplus-example-server.herokuapp.com/api/v1/)):

![Flask RESTplus Example API](https://raw.githubusercontent.com/frol/flask-restplus-server-example/master/docs/static/Flask_RESTplus_Example_API.png)


Single File Example
-------------------

This example should give you a basic understanding of what you can get with
Flask, SQLAlchemy, Marshmallow, Flask-RESTplus (+ my patched extension), and
OpenAPI.

```python
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_restplus_patched import Api, Namespace, Resource, ModelSchema

# Extensions initialization
# =========================
app = Flask(__name__)
db = SQLAlchemy(app)
api = Api(app)


# Database table definition (SQLAlchemy)
# ======================================
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(), nullable=False)


# Serialization/Deserialization schema definition
# ===============================================
class UserSchema(ModelSchema):
    class Meta:
        model = User


# "Users" resource RESTful API definitions
# ========================================
users_api = Namespace('users')
api.add_namespace(users_api)

@users_api.route('/')
class UsersList(Resource):

    @users_api.response(UserSchema(many=True))
    def get(self):
        return User.query.all()


@users_api.route('/<int:user_id>')
@users_api.resolve_object('user', lambda kwargs: User.query.get_or_404(kwargs.pop('user_id')))
class UserByID(Resource):

    @users_api.response(UserSchema())
    def get(self, user):
        return user


# Run the RESTful API server
# ==========================
if __name__ == '__main__':
    db.create_all()
    with db.session.begin(nested=True):
        db.session.add(User(name='user1'))
        db.session.add(User(name='user2'))
    app.run()
```

Save it, install the dependencies, and run it:

```
$ pip install -r app/requirements.txt
$ python server.py
```

Open http://127.0.0.1:5000 and examine the interactive documentation for your
new RESTful API server! You can use any HTTP tools (e.g. `cURL`, `wget`,
Python `requests`, or just a web browser) to communicate with it, or generate
specialized API client libraries for many programming languages using
[Swagger Codegen](https://github.com/swagger-api/swagger-codegen) (learn more
in the [API Integration](#api-integration) section).

Note, this whole repo features much more than that; it demonstrates how I would
organize a production-ready RESTful API server *project*, so stay tunned.


Project Structure
-----------------

### Root folder

Folders:

* `app` - This RESTful API Server example implementation is here.
* `flask_restplus_patched` - There are some patches for Flask-RESTPlus (read
  more in *Patched Dependencies* section).
* `migrations` - Database migrations are stored here (see `invoke --list` to
  learn available commands, and learn more about PyInvoke usage below).
* `tasks` - [PyInvoke](http://www.pyinvoke.org/) commands are implemented here.
* `tests` - These are [pytest](http://pytest.org) tests for this RESTful API
  Server example implementation.
* `docs` - It contains just images for the README, so you can safely ignore it.
* `deploy` - It contains some application stack examples.

Files:

* `README.md`
* `config.py` - This is a config file of this RESTful API Server example.
* `conftest.py` - A top-most pytest config file (it is empty, but it [helps to
  have a proper PYTHON PATH](http://stackoverflow.com/a/20972950/1178806)).
* `.coveragerc` - [Coverage.py](http://coverage.readthedocs.org/) (code
  coverage) config for code coverage reports.
* `.travis.yml` - [Travis CI](https://travis-ci.org/) (automated continuous
  integration) config for automated testing.
* `.pylintrc` - [Pylint](https://www.pylint.org/) config for code quality
  checking.
* `Dockerfile` - Docker config file which is used to build a Docker image
  running this RESTful API Server example.
* `.dockerignore` - Lists files and file masks of the files which should be
  ignored while Docker build process.
* `.gitignore` - Lists files and file masks of the files which should not be
  added to git repository.
* `LICENSE` - MIT License, i.e. you are free to do whatever is needed with the
  given code with no limits.

### Application Structure

```
app/
├── requirements.txt
├── __init__.py
├── extensions
│   └── __init__.py
└── modules
    ├── __init__.py
    ├── api
    │   └── __init__.py
    ├── auth
    │   ├── __init__.py
    │   ├── models.py
    │   ├── parameters.py
    │   └── views.py
    ├── users
    │   ├── __init__.py
    │   ├── models.py
    │   ├── parameters.py
    │   ├── permissions.py
    │   ├── resources.py
    │   └── schemas.py
    └── teams
        ├── __init__.py
        ├── models.py
        ├── parameters.py
        ├── resources.py
        └── schemas.py
```

* `app/requirements.txt` - The list of Python (PyPi) requirements.
* `app/__init__.py` - The entrypoint to this RESTful API Server example
  application (Flask application is created here).
* `app/extensions` - All extensions (e.g. SQLAlchemy, OAuth2) are initialized
  here and can be used in the application by importing as, for example,
  `from app.extensions import db`.
* `app/modules` - All endpoints are expected to be implemented here in logicaly
  separated modules. It is up to you how to draw the line to separate concerns
  (e.g. you can implement a monolith `blog` module, or split it into
  `topics`+`comments` modules).

### Module Structure

Once you added a module name into `config.ENABLED_MODULES`, it is required to
have `your_module.init_app(app, **kwargs)` function. Everything else is
completely optional. Thus, here is the required minimum:

```
your_module/
└── __init__.py
```

, where `__init__.py` will look like this:

```python
def init_app(app, **kwargs):
    pass
```

In this example, however, `init_app` imports `resources` and registeres `api`
(an instance of (patched) `flask_restplus.Namespace`). Learn more about the
"big picture" in the next section.


Where to start reading the code?
--------------------------------

The easiest way to start the application is by using PyInvoke command `app.run`
implemented in [`tasks/app/run.py`](tasks/app/run.py):

```
$ invoke app.run
```

The command creates an application by running
[`app/__init__.py:create_app()`](app/__init__.py) function, which in its turn:

1. loads an application config;
2. initializes extensions:
   [`app/extensions/__init__.py:init_app()`](app/extensions/__init__.py);
3. initializes modules:
   [`app/modules/__init__.py:init_app()`](app/modules/__init__.py).

Modules initialization calls `init_app()` in every enabled module
(listed in `config.ENABLED_MODULES`).

Let's take `teams` module as an example to look further.
[`app/modules/teams/__init__.py:init_app()`](app/modules/teams/__init__.py)
imports and registers `api` instance of (patched) `flask_restplus.Namespace`
from `.resources`. Flask-RESTPlus `Namespace` is designed to provide similar
functionality as Flask `Blueprint`.

[`api.route()`](app/modules/teams/resources.py) is used to bind a
resource (classes inherited from `flask_restplus.Resource`) to a specific
route.

Lastly, every `Resource` should have methods which are lowercased HTTP method
names (i.e. `.get()`, `.post()`, etc). This is where users' requests end up.


Dependencies
------------

### Project Dependencies

* [**Python**](https://www.python.org/) 2.7, 3.3+ / pypy2 (2.5.0)
* [**flask-restplus**](https://github.com/noirbizarre/flask-restplus) (+
  [*flask*](http://flask.pocoo.org/))
* [**sqlalchemy**](http://www.sqlalchemy.org/) (+
  [*flask-sqlalchemy*](http://flask-sqlalchemy.pocoo.org/)) - Database ORM.
* [**sqlalchemy-utils**](https://sqlalchemy-utils.rtdf.org/) - for nice
  custom fields (e.g., `PasswordField`).
* [**alembic**](https://alembic.rtdf.org/) - for DB migrations.
* [**marshmallow**](http://marshmallow.rtfd.org/) (+
  [*marshmallow-sqlalchemy*](http://marshmallow-sqlalchemy.rtfd.org/),
  [*flask-marshmallow*](http://flask-marshmallow.rtfd.org/)) - for
  schema definitions. (*supported by the patched Flask-RESTplus*)
* [**webargs**](http://webargs.rtfd.org/) - for parameters (input arguments).
  (*supported by the patched Flask-RESTplus*)
* [**apispec**](http://apispec.rtfd.org/) - for *marshmallow* and *webargs*
  introspection. (*integrated into the patched Flask-RESTplus*)
* [**oauthlib**](http://oauthlib.rtfd.org/) (+
  [*flask-oauthlib*](http://flask-oauthlib.rtfd.org/)) - for authentication.
* [**flask-login**](http://flask-login.rtfd.org/) - for `current_user`
  integration only.
* [**bcrypt**](https://github.com/pyca/bcrypt/) - for password hashing (used as
  a backend by *sqlalchemy-utils.PasswordField*).
* [**permission**](https://github.com/hustlzp/permission) - for authorization.
* [**Swagger-UI**](https://github.com/swagger-api/swagger-ui) - for interactive
  RESTful API documentation.

### Build Dependencies

I use [`pyinvoke`](http://pyinvoke.org) with custom tasks to maintain easy and
nice command-line interface. Thus, it is required to have `invoke` Python
package installed, and optionally you may want to install `colorlog`, so your
life become colorful.

### Patched Dependencies

* **flask-restplus** is patched to handle marshmallow schemas and webargs
  input parameters
  ([GH #9](https://github.com/noirbizarre/flask-restplus/issues/9)).
* **swagger-ui** (*the bundle is automatically downloaded on the first run*)
  just includes a pull-request to support Resource Owner Password Credentials
  Grant OAuth2 (aka Password Flow)
  ([PR #1853](https://github.com/swagger-api/swagger-ui/pull/1853)).


Installation
------------

### Using Docker

It is very easy to start exploring the example using Docker:

```bash
$ docker run -it --rm --publish 5000:5000 frolvlad/flask-restplus-server-example
```

[![](https://images.microbadger.com/badges/image/frolvlad/flask-restplus-server-example.svg)](http://microbadger.com/images/frolvlad/flask-restplus-server-example "Get your own image badge on microbadger.com")


### From sources

#### Clone the Project

```bash
$ git clone https://github.com/frol/flask-restplus-server-example.git
```

#### Setup Environment

It is recommended to use virtualenv or Anaconda/Miniconda to manage Python
dependencies. Please, learn details yourself.

You will need `invoke` package to work with everything related to this project.

```bash
$ pip install -r tasks/requirements.txt
```


#### Run Server

NOTE: All dependencies and database migrations will be automatically handled,
so go ahead and turn the server ON! (Read more details on this in Tips section)

```bash
$ invoke app.run
```


Quickstart
----------

Open online interactive API documentation:
[http://127.0.0.1:5000/api/v1/](http://127.0.0.1:5000/api/v1/)

Autogenerated swagger config is always available from
[http://127.0.0.1:5000/api/v1/swagger.json](http://127.0.0.1:5000/api/v1/swagger.json)

`example.db` (SQLite) includes 2 users:

* Admin user `root` with password `q`
* Regular user `user` with password `w`

NOTE: Use On/Off switch in documentation to sign in.


Authentication Details
----------------------

This example server features OAuth2 Authentication protocol support, but don't
be afraid of it! If you learn it, OAuth2 will save you from a lot of troubles.

### Authentication with Login and Password (Resource Owner Password Credentials Grant)

Here is how you authenticate with user login and password credentials using cURL:

```
$ curl 'http://127.0.0.1:5000/auth/oauth2/token?grant_type=password&client_id=documentation&username=root&password=q'
{
    "token_type": "Bearer",
    "access_token": "oqvUpO4aKg5KgYK2EUY2HPsbOlAyEZ",
    "refresh_token": "3UTjLPlnomJPx5FvgsC2wS7GfVNrfH",
    "expires_in": 3600,
    "scope": "auth:read auth:write users:read users:write teams:read teams:write"
}
```

That is it!

Well, the above request uses query parameters to pass client ID, user login and
password which is not recommended (even discouraged) for production use since
most of the web servers logs the requested URLs in plain text and we don't want
to leak sensitive data this way.  Thus, in practice you would use form
parameters to pass credentials:

```
$ curl 'http://127.0.0.1:5000/auth/oauth2/token?grant_type=password' -F 'client_id=documentation' -F 'username=root' -F 'password=q'
```

, or even pass `client_id` as Basic HTTP Auth:

```
$ curl 'http://127.0.0.1:5000/auth/oauth2/token?grant_type=password' --user 'documentation:' -F 'username=root' -F 'password=q'
```

You grab the `access_token` and put it into `Authorization` header
to request "protected" resources:

```
$ curl --header 'Authorization: Bearer oqvUpO4aKg5KgYK2EUY2HPsbOlAyEZ' 'http://127.0.0.1:5000/api/v1/users/me'
{
    "id": 1,
    "username": "root",
    "email": "root@localhost",
    "first_name": "",
    "middle_name": "",
    "last_name": "",
    "is_active": true,
    "is_regular_user": true,
    "is_admin": true,
    "created": "2016-10-20T14:00:35.912576+00:00",
    "updated": "2016-10-20T14:00:35.912602+00:00"
}
```

Once the access token expires, you can refresh it with `refresh_token`. To do
that, OAuth2 RFC defines Refresh Token Flow (notice that there is no need to
store user credentials to do the refresh procedure):

```
$ curl 'http://127.0.0.1:5000/auth/oauth2/token?grant_type=refresh_token' --user 'documentation:' -F 'refresh_token=3UTjLPlnomJPx5FvgsC2wS7GfVNrfH'
{
    "token_type": "Bearer",
    "access_token": "FwaS90XWwBpM1sLeAytaGGTubhHaok",
    "refresh_token": "YD5Rc1FojKX1ZY9vltMSnFxhm9qpbb",
    "expires_in": 3600,
    "scope": "auth:read auth:write users:read users:write teams:read teams:write"
}
```

### Authentication with Client ID and Secret (Client Credentials Grant)

Here is how you authenticate with user login and password credentials using cURL:

```
$ curl 'http://127.0.0.1:5000/auth/oauth2/token?grant_type=client_credentials' --user 'documentation:KQ()SWK)SQK)QWSKQW(SKQ)S(QWSQW(SJ*HQ&HQW*SQ*^SSQWSGQSG'
{
    "token_type": "Bearer",
    "access_token": "oqvUpO4aKg5KgYK2EUY2HPsbOlAyEZ",
    "expires_in": 3600,
    "scope": "teams:read users:read users:write teams:write"
}
```

The same way as in the previous section, you can grab the `access_token` and
access protected resources.


API Integration
---------------

One of the key point of using OpenAPI (Swagger) specification is that it
enables automatic code generation.
[Swagger Codegen project](https://github.com/swagger-api/swagger-codegen)
implements client library and server stub generators for over 18
programming languages! There are also many other projects with OpenAPI
specification support, so if you lack anything in the official tooling,
search for third-party solutions.

I have had a need to work with my API servers from Python and JavaScript, so
I started with Swagger Codegen Python and JavaScript generators. Very soon I
realized that most (if not all) Swagger Codegen generators lack OAuth2 support,
but other than that, the client libraries look fine (I have contributed a few
changes to Python and JavaScript generators over the time, and the nice thing
all the clients benefit from contributions into a single project). Thus,
@khorolets and I implemented hacky OAuth2 support for Python client and even
more hacky out-of-client helpers for JavaScript (hopefully, one day OAuth2
support will be contributed into the Swagger Codegen upstream).

To use Swagger Codegen, you only need a `swagger.json` file describing your API
server. You can get one by accessing http://127.0.0.1:5000/api/v1/swagger.json,
or by running an Invoke task:

```bash
$ invoke app.swagger
```

NOTE: Use stdout rediction to save the output into a file.

To further simplify the codegeneration, there is another Invoke task:

```bash
$ invoke app.swagger.codegen --language python --version 1.0.0
```

To run that, however, you will need Docker installed on your machine since we
use Swagger Codegen as a Docker image. Once that is completed, you will have a
Python client in the `clients/python/dist/` folder. The `javascript` client can
be generated just the same way. Read the generated `clients/*/dist/README.md`
to learn how to use those clients.

NOTE: As mentioned above, a slightly modified Swagger Codegen version is used
to enable OAuth2 support in Python client.


Tips
----

Once you have invoke, you can learn all available commands related to this
project from:

```bash
$ invoke --list
```

Learn more about each command with the following syntax:

```bash
$ invoke --help <task>
```

For example:

```bash
$ invoke --help app.run
Usage: inv[oke] [--core-opts] app.run [--options] [other tasks here ...]

Docstring:
  Run DDOTS RESTful API Server.

Options:
  -d, --[no-]development
  -h STRING, --host=STRING
  -i, --[no-]install-dependencies
  -p, --port
  -u, --[no-]upgrade-db
```

Use the following command to enter ipython shell (`ipython` must be installed):

```bash
$ invoke app.env.enter
```

`app.run` and `app.env.enter` tasks automatically prepare all dependencies
(using `pip install`) and migrate database schema to the latest version.

Database schema migration is handled via `app.db.*` tasks group. The most
common migration commands are `app.db.upgrade` (it is automatically run on
`app.run`), and `app.db.migrate` (creates a new migration).

You can use [`better_exceptions`](https://github.com/Qix-/better-exceptions)
package to enable detailed tracebacks. Just add `better_exceptions` to the
`app/requirements.txt` and `import better_exceptions` in the `app/__init__.py`.

Useful Links
============

* "[The big Picture](https://identityserver.github.io/Documentation/docsv2/overview/bigPicture.html)" -
  short yet complete idea about how the modern apps should talk.
* "[Please. Don't PATCH Like An Idiot.](http://williamdurand.fr/2014/02/14/please-do-not-patch-like-an-idiot/)"
* "[A Concise RESTful API Design Guide](https://twincl.com/programming/*6af/rest-api-design)"
* "[Best Practices for Designing a Pragmatic RESTful API](http://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api)"
* "[My take on RESTful authentication](https://facundoolano.wordpress.com/2013/12/23/my-take-on-restful-authentication/)"
* "[Is it normal design to completely decouple backend and frontend web applications and allow them to communicate with (JSON) REST API?](http://softwareengineering.stackexchange.com/questions/337467/is-it-normal-design-to-completely-decouple-backend-and-frontend-web-applications)"
RESTful API Server Example deployments
==========================

This project showcases my vision on how the RESTful API server should be
implemented. See [api directory](../../api/) for detailed information.

Project Structure
-----------------

### Root folder

Folders:

* `stack1` - RESTful API Server Example behind an nginx reverse proxy, stack
managed using Docker Compose


Files:

* `README.md`


Installation
------------

Information provided in each stack.
Stack 1 - Docker Compose topology + nginx reverse proxy
==========================

This stack showcases an implementation of the RESTful API server in a
Docker Compose topology with an nginx reverse proxy.

Project Structure
-----------------

### Root folder

Folders:

* `revproxy` - Basic nginx configuration

Files:

* `README.md`
* `docker-compose.yml` - Docker Compose config file which is used to build a Docker
  images and run the topology (api and reverse proxy).


Dependencies
------------

### Project Dependencies

* [**RESTful API Server Example**](../../api/) 
* [**Docker**](https://www.docker.com/) 1.11+
* [**Docker Compose**](https://docs.docker.com/compose/overview/) 1.8+


Installation
------------

### From sources

#### Clone the Project

```bash
$ git clone https://github.com/frol/flask-restplus-server-example.git
```

#### Run the application topology

It is very easy to start exploring the example using Docker Compose:

```bash
$ cd deploy/stack1
$ docker-compose build
$ docker-compose up
```

To tear it down:
```bash
$ docker-compose kill
$ docker-compose rm -fv
```

Should you need to change the reverse proxy port, just change the following section
```
revproxy:
  restart: always
  build: ./revproxy
  ports:
    - "80:80"
  links:
    - api:api
```

to 

```
revproxy:
  restart: always
  build: ./revproxy
  ports:
    - "<CUSTOM_PORT>:80"
  links:
    - api:api
```

Quickstart
----------

Open reverse proxy home page
[http://127.0.0.1/](http://127.0.0.1/) or http://127.0.0.1:CUSTOM_PORT if you changed it

Open online interactive API documentation:
[http://127.0.0.1/api/v1/](http://127.0.0.1/api/v1/)

Autogenerated swagger config is always available from
[http://127.0.0.1:5000/api/v1/swagger.json](http://127.0.0.1:5000/api/v1/swagger.json)

Read the [RESTful API Server Example](../../api/) doc for more information about the API itself.

# Contributors

* Patrice LACHANCE (patrice at itisopen.net)

![YARA-logo](https://raw.githubusercontent.com/blacktop/docker-yara/master/logo.png)

Yara Dockerfile
===============

[![CircleCI](https://circleci.com/gh/blacktop/docker-yara.png?style=shield)](https://circleci.com/gh/blacktop/docker-yara) [![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/yara.svg)](https://hub.docker.com/r/blacktop/yara/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/yara.svg)](https://hub.docker.com/r/blacktop/yara/) [![Docker Image](https://img.shields.io/badge/docker%20image-53.9%20MB-blue.svg)](https://hub.docker.com/r/blacktop/yara/)

This repository contains a **Dockerfile** of [Yara](http://virustotal.github.io/yara/).

### Dependencies

-	[alpine:3.5](https://index.docker.io/_/gliderlabs/alpine/)

### Image Tags

```bash
REPOSITORY          TAG                 SIZE
blacktop/yara       latest              53.8MB
blacktop/yara       3.6                 53.8MB
blacktop/yara       3.5                 54.3MB
blacktop/yara       w-rules             60.4MB
blacktop/yara       no-py               15MB
blacktop/yara       3.4                 54.3MB
blacktop/yara       3.1.0               163.7MB (debian:jessie)
```

> NOTE: tag **no-py** is `yara:3.6` without yara-python  
> NOTE: tag **w-rules** is `yara:3.6` with some default yara rules included in the /rules directory.

### Installation

1.	Install [Docker](https://docs.docker.com).
2.	Download [trusted build](https://hub.docker.com/r/blacktop/yara/) from public [Docker Registry](https://hub.docker.com/): `docker pull blacktop/yara`

### Getting Started

```bash
$ docker run --rm -v /path/to/rules:/rules:ro \
                  -v /path/to/malware:/malware:ro \
                  blacktop/yara /rules/RULES_FILE FILE
```

```
YARA 3.6.0, the pattern matching swiss army knife.
Usage: yara [OPTION]... RULES_FILE FILE | DIR | PID

Mandatory arguments to long options are mandatory for short options too.

  -t,  --tag=TAG                   print only rules tagged as TAG
  -i,  --identifier=IDENTIFIER     print only rules named IDENTIFIER
  -n,  --negate                    print only not satisfied rules (negate)
  -D,  --print-module-data         print module data
  -g,  --print-tags                print tags
  -m,  --print-meta                print metadata
  -s,  --print-strings             print matching strings
  -L,  --print-string-length       print length of matched strings
  -e,  --print-namespace           print rules' namespace
  -p,  --threads=NUMBER            use the specified NUMBER of threads to scan a directory
  -l,  --max-rules=NUMBER          abort scanning after matching a NUMBER of rules
  -d VAR=VALUE                     define external variable
  -x MODULE=FILE                   pass FILE's content as extra data to MODULE
  -a,  --timeout=SECONDS           abort scanning after the given number of SECONDS
  -k,  --stack-size=SLOTS          set maximum stack size (default=16384)
  -r,  --recursive                 recursively search directories
  -f,  --fast-scan                 fast matching mode
  -w,  --no-warnings               disable warnings
       --fail-on-warnings          fail on warnings
  -v,  --version                   show version information
  -h,  --help                      show this help and exit

Send bug reports and suggestions to: vmalvarez@virustotal.com.
```

Add the following to your bash or zsh profile

```bash
alias yara='docker run -it --rm -v $(pwd):/malware:ro blacktop/yara $@'
```

### Documentation

#### Usage

```bash
$ yara [OPTION]... RULES_FILE FILE | DIR | PID
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-yara/issues/new) and I'll get right on it.

### License

MIT Copyright (c) 2014-2017 **blacktop**
# docker-hdocdb

[HDocDB](https://github.com/rayokota/hdocdb) (HBase as a JSON Document Database) docker images based on alpine

## Small setup
```
# build docker image
docker build --build-arg "VERSION=0.0.3" -t local/hdocdb .

# network 
docker network create vnet

# hadoop+hbase+hdocdb startup
docker-compose up -d

# tail logs for a while
docker-compose logs -f

# check ps
docker-compose ps

     Name                   Command               State                  Ports                
---------------------------------------------------------------------------------------------
datanode-1       entrypoint.sh datanode           Up      50010/tcp, 50020/tcp, 50075/tcp     
hmaster-1        entrypoint.sh hmaster-1          Up      16000/tcp, 0.0.0.0:32785->16010/tcp 
namenode-1       entrypoint.sh namenode-1         Up      0.0.0.0:32782->50070/tcp, 8020/tcp  
regionserver-1   entrypoint.sh regionserver       Up      16020/tcp, 16030/tcp                
zookeeper-1      entrypoint.sh -server 1 1 vnet   Up      2181/tcp, 2888/tcp, 3888/tcp

# hdocdb shell via nashorn
docker-compose exec regionserver-1 sh
> /usr/local/hdocdb # jrunscript -cp lib/hdocdb-0.0.3.jar -f lib/hdocdb.js -f -

nashorn> db.mycoll.insert( { _id: "jdoe", first_name: "John", last_name: "Doe" } )
nashorn> var doc = db.mycoll.find( { last_name: "Doe" } )[0]
nashorn> print(doc)
{"_id":"jdoe","first_name":"John","last_name":"Doe"}
nashorn> db.mycoll.update( { last_name: "Doe" }, { $set: { first_name: "Jim" } } )
nashorn> var doc = db.mycoll.find( { last_name: "Doe" } )[0]
nashorn> print(doc)
{"_id":"jdoe","first_name":"Jim","last_name":"Doe"}
nashorn> db.mycoll.delete( "jdoe" )

```Meta Package Manager
====================

CLI providing unifying interface to all package managers.

Stable release: |release| |versions| |license| |dependencies|

Development: |build| |docs| |coverage| |quality|

.. |release| image:: https://img.shields.io/pypi/v/meta-package-manager.svg
    :target: https://pypi.python.org/pypi/meta-package-manager
    :alt: Last release
.. |versions| image:: https://img.shields.io/pypi/pyversions/meta-package-manager.svg
    :target: https://pypi.python.org/pypi/meta-package-manager
    :alt: Python versions
.. |license| image:: https://img.shields.io/pypi/l/meta-package-manager.svg
    :target: https://www.gnu.org/licenses/gpl-2.0.html
    :alt: Software license
.. |dependencies| image:: https://img.shields.io/requires/github/kdeldycke/meta-package-manager/master.svg
    :target: https://requires.io/github/kdeldycke/meta-package-manager/requirements/?branch=master
    :alt: Requirements freshness
.. |build| image:: https://img.shields.io/travis/kdeldycke/meta-package-manager/develop.svg
    :target: https://travis-ci.org/kdeldycke/meta-package-manager
    :alt: Unit-tests status
.. |docs| image:: https://readthedocs.org/projects/meta-package-manager/badge/?version=develop
    :target: https://meta-package-manager.readthedocs.io/en/develop/
    :alt: Documentation Status
.. |coverage| image:: https://codecov.io/github/kdeldycke/meta-package-manager/coverage.svg?branch=develop
    :target: https://codecov.io/github/kdeldycke/meta-package-manager?branch=develop
    :alt: Coverage Status
.. |quality| image:: https://img.shields.io/scrutinizer/g/kdeldycke/meta-package-manager.svg
    :target: https://scrutinizer-ci.com/g/kdeldycke/meta-package-manager/?branch=develop
    :alt: Code Quality

.. figure:: https://imgs.xkcd.com/comics/universal_install_script.png
    :alt: Obligatory XKCD.
    :align: right

    Source: `XKCD #1654 <https://xkcd.com/1654/>`_.


Features
---------

* Search and list all package managers on the system.
* Supports macOS and Linux.
* List installed packages.
* Search for packages.
* List outdated packages.
* Sync local package infos.
* Upgrade all outdated packages.
* Apply commands per-package manager or to all of them.
* Export results in JSON or user-friendly tables.
* Provides a `BitBar plugin
  <https://meta-package-manager.readthedocs.io/en/develop/bitbar.html>`_ for
  friendly macOS integration.


Supported package managers
--------------------------

================ ========== ====== ====== ======== ========= ============== =========== ============ ============= ============
Package manager  Version    macOS  Linux  Windows  ``sync``  ``installed``  ``search``  ``install``  ``outdated``  ``upgrade``
================ ========== ====== ====== ======== ========= ============== =========== ============ ============= ============
|brew|__          >= 1.0.*  ✓                      ✓         ✓              ✓                        ✓             ✓
|cask|__          >= 1.0.*  ✓                      ✓         ✓              ✓                        ✓             ✓
|pip2|__                    ✓      ✓                         ✓              ✓                        ✓             ✓
|pip3|__                    ✓      ✓                         ✓              ✓                        ✓             ✓
|npm|__           >= 4.0.*  ✓      ✓                         ✓              ✓                        ✓             ✓
|apm|__                     ✓      ✓                         ✓              ✓                        ✓             ✓
|gem|__                     ✓      ✓                         ✓              ✓                        ✓             ✓
|mas|__           >= 1.3.1  ✓                                ✓              ✓                        ✓             ✓
================ ========== ====== ====== ======== ========= ============== =========== ============ ============= ============

.. |brew| replace::
   Homebrew
__ https://brew.sh
.. |cask| replace::
   Homebrew Cask
__ https://caskroom.github.io
.. |pip2| replace::
   Python 2 ``pip``
__ https://pypi.org
.. |pip3| replace::
   Python 3 ``pip``
__ https://pypi.org
.. |npm| replace::
   Node's ``npm``
__ https://www.npmjs.com
.. |apm| replace::
   Atom's ``apm``
__ https://atom.io/packages
.. |gem| replace::
   Ruby's ``gem``
__ https://rubygems.org
.. |mas| replace::
   Mac AppStore via ``mas``
__ https://github.com/argon/mas

If you're bored, feel free to add support for new package manager. See the
`list of good candidates
<https://en.wikipedia.org/wiki/List_of_software_package_management_systems>`_.


Usage
-----

Examples of the package's ``mpm`` CLI.

List global options and commands:

.. code-block:: shell-session

    $ mpm
    Usage: mpm [OPTIONS] COMMAND [ARGS]...

      CLI for multi-package manager upgrades.

    Options:
      -v, --verbosity LEVEL           Either CRITICAL, ERROR, WARNING, INFO or
                                      DEBUG. Defaults to INFO.
      -m, --manager [npm|mas|pip3|pip2|cask|apm|brew|gem]
                                      Restrict sub-command to one package manager.
                                      Defaults to all.
      -o, --output-format [simple|plain|json|fancy]
                                      Rendering mode of the output. Defaults to
                                      fancy.
      --version                       Show the version and exit.
      --help                          Show this message and exit.

    Commands:
      managers  List supported package managers and their location.
      outdated  List outdated packages.
      sync      Sync local package info.
      upgrade   Upgrade all packages.

List all supported package managers and their status on current system:

.. code-block:: shell-session

    $ mpm managers
    ╒═══════════════════╤══════╤═════════════╤════════════════════════╤══════════════╤═════════════╕
    │ Package manager   │ ID   │ Supported   │ CLI                    │ Executable   │ Version     │
    ╞═══════════════════╪══════╪═════════════╪════════════════════════╪══════════════╪═════════════╡
    │ Atom's apm        │ apm  │ ✓           │ ✓  /usr/local/bin/apm  │ ✓            │ ✓  1.12.9   │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Homebrew          │ brew │ ✓           │ ✓  /usr/local/bin/brew │ ✓            │ ✓  1.1.7    │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Homebrew Cask     │ cask │ ✓           │ ✓  /usr/local/bin/brew │ ✓            │ ✓  1.1.7    │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Ruby Gems         │ gem  │ ✓           │ ✓  /usr/bin/gem        │ ✓            │ ✓  2.0.14.1 │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Mac AppStore      │ mas  │ ✓           │ ✓  /usr/local/bin/mas  │ ✓            │ ✓  1.3.1    │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Node's npm        │ npm  │ ✓           │ ✓  /usr/local/bin/npm  │ ✓            │ ✓  4.0.5    │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Python 2's Pip    │ pip2 │ ✓           │ ✓  /usr/local/bin/pip2 │ ✓            │ ✓  9.0.1    │
    ├───────────────────┼──────┼─────────────┼────────────────────────┼──────────────┼─────────────┤
    │ Python 3's Pip    │ pip3 │ ✓           │ ✓  /usr/local/bin/pip3 │ ✓            │ ✓  9.0.1    │
    ╘═══════════════════╧══════╧═════════════╧════════════════════════╧══════════════╧═════════════╛
# Repology

[![Build Status](https://travis-ci.org/repology/repology.svg?branch=master)](https://travis-ci.org/repology/repology)
[![Coverage Status](https://coveralls.io/repos/github/repology/repology/badge.svg?branch=master)](https://coveralls.io/github/repology/repology?branch=master)
[![Code Health](https://landscape.io/github/repology/repology/master/landscape.svg?style=flat)](https://landscape.io/github/repology/repology/master)

![Example report](docs/screenshot.png)

Repology tracks and compares package versions along multiple
repositories including Arch, Chocolatey, Debian, Fedora, FreeBSD,
Gentoo, Mageia, OpenBSD, OpenSUSE, pkgsrc, Sisyphus, SlackBuilds,
Ubuntu and more.

## Uses

- **Users**:
  - Compare completeness and freshness of package repositories,
    choose most up to date distro
  - Find out what repositories contain newest versions of packages
    you need
- **Package/port maintainers**:
  - Another way to track new releases of software you package
  - Compete with other distros in keeping up to date
  - Find fellow maintainers to resolve packaging problems together
  - Keep package naming and versioning schemes in sync to other
    repos for your and your user's convenience
- **Software authors**:
  - Keep track of how well your project is packaged
  - Keep in touch with your product package maintainers
  - If you're not using [semantic versioning](http://semver.org/)
    yet, see how useful it is

## Status

Repology is currently in an early phase of development, with a goal
of creating usable utility in a quick and dirty way. For now, it is
usable in two modes: as a command line generator of single HTML
report and a static website generator for [repology.org](https://repology.org).

## Repository support

As much data as possible is parsed from each repo. Package name and
version are always parsed.

| Repository                       | Summary | Maint-r | Categ | WWW   | License | Download |
|----------------------------------|:-------:|:-------:|:-----:|:-----:|:-------:|:--------:|
| Alpine                           | ✔       | ✔       |       | ✔     | ✔       |          |
| ALT Sisyphus                     | ✔       | ✔       | ✔     |       |         |          |
| Arch, Parabola, Manjaro          | ✔       | ✔       |       | ✔     | ✔       |          |
| CentOS, Fedora, Mageia, OpenSUSE | ✔       |         | ✔     | ✔     | ✔       |          |
| Chocolatey                       | ✔       |         |       | ✔     |         |          |
| CPAN                             |         | ✔       |       | ✔ (2) |         |          |
| CRAN                             |         |         |       | ✔ (2) |         |          |
| CRUX                             | ✔       | ✔       |       | ✔     |         |          |
| Debian, Ubuntu, other deb-based  |         | ✔       | ✔     | ✔     |         |          |
| DistroWatch.com                  | ✔       |         |       | ✔     |         | ✔        |
| F-Droid                          |         |         | ✔     | ✔     | ✔       |          |
| FreeBSD                          | ✔       | ✔       | ✔     | ✔     |         |          |
| freshcode.club                   | ✔       |         |       | ✔     | ✔       |          |
| Gentoo                           | ✔       | ✔       | ✔     | ✔     | ✔ (1)   | ✔ (1)    |
| Guix                             | ✔       |         |       | ✔     | ✔       |          |
| GoboLinux                        | ✔       |         |       | ✔     | ✔       |          |
| Hackage                          |         |         |       | ✔ (2) |         |          |
| HaikuPorts                       |         |         | ✔     |       |         |          |
| Homebrew                         | ✔       |         |       | ✔     |         |          |
| MacPorts                         | ✔       | ✔       | ✔     | ✔     | ✔       |          |
| nixpkgs                          | ✔       | ✔       |       | ✔     | ✔       |          |
| OpenBSD                          | ✔       | ✔       | ✔     |       |         |          |
| OpenIndiana                      | ✔       |         | ✔     | ✔     |         | ✔        |
| PCLinuxOS                        | ✔       | ✔       | ✔     |       |         |          |
| pkgsrc                           | ✔       | ✔       | ✔     |       |         |          |
| PyPi                             | ✔       |         |       | ✔ (2) |         |          |
| Ravenports                       | ✔       |         | ✔     | ✔     |         |          |
| RubyGems                         |         |         |       | ✔ (2) |         |          |
| SlackBuilds                      |         | ✔       | ✔     | ✔     |         | ✔        |
| YACP                             |         |         |       |       |         |          |

(1) Gentoo support is not complete, complex cases like condional downloads and licenses
are ignored for now.

(2) WWWs are autogenerated for upstream package repos like CPAN, PyPi and Hackage

## Reading the report

Report is HTML table, columns correspond to repositories and rows
correspond to packages. Cells hold package versions, highlighted
as follows:

- ```cyan```: package is only present in a single repo: nothing to
              compare version to, and may be local artifact
- ```green```: package up to date
- ```red```: package outdated (there's newer version in some other repo)
- ```yellow```: there are multiple packages some of which are up to date
                and others are outdated
- ```gray```: version was manually ignored, likely because of broken
              versioning scheme

Note that there may be multiple packages of a same name in a single repo
(either naturally, or because of name transformations).

## Documentation

- How to [run](docs/RUNNING.md) repology tools on your own
- How to extend or fix [rules](docs/RULES.md) for package matching
- How repology [compares versions](https://github.com/repology/libversion/blob/master/doc/ALGORITHM.md)

## Author

* [Dmitry Marakasov](https://github.com/AMDmi3) <amdmi3@amdmi3.ru>

## License

GPLv3 or later, see [COPYING](COPYING).
================
ScanCode toolkit
================


Build and tests status
======================

+-------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
|Branch |                                        **Coverage**                                  |                         **Linux (Travis)**                                  |                         **MacOSX (Travis)**                                 |                         **Windows (AppVeyor)**                                                |
+=======+======================================================================================+=============================================================================+=============================================================================+===============================================================================================+
|       |.. image:: https://codecov.io/gh/nexB/scancode-toolkit/branch/master/graph/badge.svg  |.. image:: https://api.travis-ci.org/nexB/scancode-toolkit.png?branch=master |.. image:: https://api.travis-ci.org/nexB/scancode-toolkit.png?branch=master |.. image:: https://ci.appveyor.com/api/projects/status/4webymu0l2ip8utr/branch/master?png=true |
|Master |   :target: https://codecov.io/gh/nexB/scancode-toolkit/branch/master                 |   :target: https://travis-ci.org/nexB/scancode-toolkit                      |   :target: https://travis-ci.org/nexB/scancode-toolkit                      |   :target: https://ci.appveyor.com/project/nexB/scancode-toolkit                              |
|       |   :alt: Linux Master branch test coverage                                            |   :alt: Linux Master branch tests status                                    |   :alt: MacOSX Master branch tests status                                   |   :alt: Windows Master branch tests status                                                    |
+-------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
|       |.. image:: https://codecov.io/gh/nexB/scancode-toolkit/branch/develop/graph/badge.svg |.. image:: https://api.travis-ci.org/nexB/scancode-toolkit.png?branch=develop|.. image:: https://api.travis-ci.org/nexB/scancode-toolkit.png?branch=develop|.. image:: https://ci.appveyor.com/api/projects/status/4webymu0l2ip8utr/branch/develop?png=true|
|Develop|   :target: https://codecov.io/gh/nexB/scancode-toolkit/branch/develop                |   :target: https://travis-ci.org/nexB/scancode-toolkit                      |   :target: https://travis-ci.org/nexB/scancode-toolkit                      |   :target: https://ci.appveyor.com/project/nexB/scancode-toolkit                              |
|       |   :alt: Linux Develop branch test coverage                                           |   :alt: Linux Develop branch tests status                                   |   :alt: MacOSX Develop branch tests status                                  |   :alt: Windows Develop branch tests status                                                   |
+-------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+


ScanCode is a suite of utilities used to scan a codebase for license,
copyright, packages dependencies and other interesting information that
can be discovered in source and binary code files.

A typical software project often reuses hundreds of third-party
components. License and origin information is often scattered and not
easy to find: ScanCode discovers this data for you.

ScanCode provides accurate scan results and the line position
where each result is found. The results can be formatted as JSON or
HTML. ScanCode provides a simple HTML app for quick visualization of
scan results (see screenshot below), but you will have more robust analysis
options if you use AboutCode Manager to view a scan. AboutCode Manager is 
a desktop application available or Linux, OSX or Windows - go to 
https://github.com/nexB/aboutcode-manager to learn more or to download 
AboutCode Manager.

We are continuously working on new features, such as analysis of
dependencies or improving  performance for scanning of larger codebases.

See the roadmap for upcoming features:
https://github.com/nexB/scancode-toolkit/wiki/Roadmap

.. image:: samples/screenshot.png


Quick Start
===========

For Windows, please go to the 
`Comprehensive Installation <https://github.com/nexB/scancode-toolkit/wiki/Comprehensive-Installation>`_ 
section instead.

Make sure you have Python 2.7 installed:
 * Download and install Python 2.7 32 bits for Windows 
   https://www.python.org/ftp/python/2.7.13/python-2.7.13.msi
 * Download and install Python 2.7 for Mac 
   https://www.python.org/ftp/python/2.7.13/python-2.7.13-macosx10.6.pkg

On Linux install Python 2.7 "devel" and a few extra packages:
 
 * ``sudo apt-get install python-dev bzip2 xz-utils zlib1g libxml2-dev libxslt1-dev``
   for Ubuntu 12.04, 14.04 and 16.04

 * ``sudo apt-get install python-dev libbz2-1.0 xz-utils zlib1g libxml2-dev libxslt1-dev``
   for Debian and Debian-based distros
 
 * ``sudo yum install python-devel zlib bzip2-libs xz-libs libxml2-devel libxslt-devel``
   for RPM distros
 
 * ``sudo dnf install python-devel zlib bzip2-libs xz-libs libxml2-devel libxslt-devel``
   for Fedora 22 and later

 * See the Comprehensive Installation for additional details and other
   Linux installations: https://github.com/nexB/scancode-toolkit/wiki/Comprehensive-Installation

Next, download and extract the latest ScanCode release from::

    https://github.com/nexB/scancode-toolkit/releases/

Open a terminal, extract the downloaded release archive, then `cd` to
the extracted directory and run this command to display the command
help. ScanCode will self-configure if needed::

    ./scancode --help

Run a sample scan saved to the `samples.html` file::

    ./scancode --format html-app samples samples.html

Then open `samples.html` in your web browser to view the scan results. 

See more command examples::

    ./scancode --examples


Support
=======

If you have a problem, a suggestion or found a bug, please enter a ticket at:
https://github.com/nexB/scancode-toolkit/issues

For other questions, discussions, and chats, we have:

- a mailing list at https://lists.sourceforge.net/lists/listinfo/aboutcode-discuss

- an official Gitter channel at https://gitter.im/aboutcode-org/discuss
  Gitter also has an IRC bridge at https://irc.gitter.im/

- an official #aboutcode IRC channel on freenode (server chat.freenode.net)
  for scancode and other related tools. Note that this receives
  notifactions from repos so it can be a tad noisy. You can use your
  favorite IRC client or use the web chat at
  https://webchat.freenode.net/


About archives
==============

All code must be extracted before running ScanCode since ScanCode will
not extract files from tarballs, zip files, etc. However, the ScanCode
Toolkit comes with a utility called extractcode that does recursive
archive extraction. For example, this command will recursively extract
the mytar.tar.bz2 tarball in the mytar.tar.bz2-extract directory::

    ./extractcode mytar.tar.bz2


Source code
===========

* https://github.com/nexB/scancode-toolkit.git
* https://github.com/nexB/scancode-thirdparty-src.git


License
=======

* Apache-2.0 with an acknowledgement required to accompany the scan output.
* Public domain CC-0 for reference datasets.
* Multiple licenses (GPL2/3, LGPL, MIT, BSD, etc.) for third-party components. 

See the NOTICE file for more details.


Documentation & FAQ
===================

https://github.com/nexB/scancode-toolkit/wiki


Basic Usage
===========

Run this command for a list of options (On Windows use `scancode`
instead of `./scancode`)::

    ./scancode --help

Run this command for a list of command line examples::

    ./scancode --examples

To run a scan on sample data, first run this::

    ./scancode --format html-app samples samples.html

Then open samples.html in your web browser to see the results.
This directory contains miscellaneous scripts of some use with ScanCode.

    - json2csv: convert a scan JSON to a CSV.# scancode-thirdparty
Thirdparty dependencies repository

See .ABOUT files for provenance details

This test file was created this way:
 wget https://www.broadcom.com/collateral/pg/5756M-PG101-R.pdf
 pdfseparate -f 1 -l 1 5756M-PG101-R.pdf  5756M-PG101-R-p1.pdf
This keeps only the first page of the faulty document
This is a test for: https://github.com/euske/pdfminer/issues/118

This document is Copyright (c) Broadcom and used only as a small excerpt for bug testing
purpose
download_url: https://raw.githubusercontent.com/ffi/ffi/master/ext/ffi_c/libffi/configure

-----

Copyright (C) 2002 Free Software Foundation, Inc.

This file is part of GNU Bison.dhcpcd - DHCP client daemon
Copyright (c) 2006-2010 Roy Marples <roy@marples.name>


http://proguard.sourceforge.net/

Copyright (c) 2002-2009 Eric Lafortune (eric@graphics.cornell.edu)
 *
 *
 * Copyright (c) 1994
 * Hewlett-Packard Company
 *
 * Copyright (c) 1996-1999
 * Silicon Graphics Computer Systems, Inc.
 *
 * Copyright (c) 1997
 * Moscow Center for SPARC Technology
 *
 * Copyright (c) 1999-2003
 * Boris Fomitchev
 *
 * without fee, provided the above notices are retained on all copies.
 * Permission to modify the code and to distribute modified code is granted,
 * provided the above notices are retained, and a notice that the code was
 * modified is included with the above copyright notice.
 *
your needs.


Copyright and License
---------------------
This program has been written by Christophe Bothamy
The source code contains code ripped from rombios.c of plex86, written
by Kevin Lawton <kevin2001@yahoo.com>

The source code contains fonts from fntcol16.zip (c) by Joseph Gil avalable at :
ftp://ftp.simtel.net/pub/simtelnet/msdos/screen/fntcol16.zip
These fonts are public domain
  - Christophe
    . added lfb-mode numbers (patch from mathis)
    . updated the Makefile
    . removed display of copyrights.
    . changed the Copyright string to "LGPL VGABios developers"
  - Volker
    . set the cursor shape depending on the current font height
-----

Copyright (C) 1992, 1998, 1999, 2003, 2004, 2005 Free Software Foundation, Inc.

This file is part of Bison, the GNU Compiler Compiler.HERMES 1.2.4 (c)1998 Christian Nentwich (brn) (c.nentwich@cs.ucl.ac.uk)
and quite a few assembler routines (c) Glenn Fielder (gaffer@gaffer.org)

This library and all the files enclosed in this package are free software		* initial public release (rewrite of hhmalloc library)

=============================================================================
Copyright (c) 2004-2010, Alex Pankratov (ap@swapped.cc). All rights reserved.
Microsoft Developer Studio Project Files, Format Version 6.00 for zlib.

Copyright (C) 2000-2004 Simon-Pierre Cadieux.
Copyright (C) 2004 Cosmin Truta.
For conditions of distribution and use, see copyright notice in zlib.h.

lnstat - linux networking statistics
(C) 2004 Harald Welte <laforge@gnumonks.org
======================================================================
  people who reported problems and suggested various improvements in zlib; they
  are too numerous to cite here.

Copyright notice:

 (C) 1995-2010 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or impliedThis file contains the following sections:

OVERVIEW            General description of JPEG and the IJG software.
LEGAL ISSUES        Copyright, lack of warranty, terms of distribution.
REFERENCES          Where to learn more about JPEG.
ARCHIVE LOCATIONS   Where to find newer versions of this software.
fitness for a particular purpose.  This software is provided "AS IS", and you,
its user, assume the entire risk as to its quality and accuracy.

This software is copyright (C) 1991-1998, Thomas G. Lane.
All Rights Reserved except as specified below.

software (or portions thereof) for any purpose, without fee, subject to these
conditions:
(1) If any part of the source code for this software is distributed, then this
README file must be included, with this copyright and no-warranty notice
unaltered; and any additions, deletions, or changes to the original files
must be clearly indicated in accompanying documentation.


ansi2knr.c is included in this distribution by permission of L. Peter Deutsch,
sole proprietor of its copyright holder, Aladdin Enterprises of Menlo Park, CA.
ansi2knr.c is NOT covered by the above copyright and conditions, but instead
by the usual distribution terms of the Free Software Foundation; principally,
that you must include source code if you redistribute it.  (See the file
the foregoing paragraphs do.

The Unix configuration script "configure" was produced with GNU Autoconf.
It is copyright by the Free Software Foundation but is freely distributable.
The same holds for its supporting scripts (config.guess, config.sub,
ltconfig, ltmain.sh).  Another support script, install-sh, is copyright
by M.I.T. but is also freely distributable.

GIF decoders.

We are required to state that
    "The Graphics Interchange Format(c) is the Copyright property of
    CompuServe Incorporated.  GIF(sm) is a Service Mark property of
    CompuServe Incorporated."
available at ftp://ftp.uu.net/graphics/jpeg/wallace.ps.gz.  The file (actually
a preprint for an article that appeared in IEEE Trans. Consumer Electronics)
omits the sample images that appeared in CACM, but it includes corrections
and some added material.  Note: the Wallace article is copyright ACM and IEEE,
and it may not be used for commercial purposes.
 *
 * The Initial Developer of the Original Code is
 * Netscape Communications Corporation.
 * Portions created by the Initial Developer are Copyright (C) 1998
 * the Initial Developer. All Rights Reserved.
 *

// ***** BEGIN LICENSE BLOCK *****

Copyright (C) 2005 Apple Computer, Inc.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1.  Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer. 
2.  Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution. BlueZ - Bluetooth protocol stack for Linux
******************************************

Copyright (C) 2000-2002  Maxim Krasnyansky <maxk@qualcomm.com>
Copyright (C) 2003-2011  Marcel Holtmann <marcel@holtmann.org>

Bluetooth packet analyzerBlueZ - Bluetooth protocol stack for Linux
******************************************

Copyright (C) 2000-2001  Qualcomm Incorporated
Copyright (C) 2002-2003  Maxim Krasnyansky <maxk@qualcomm.com>
Copyright (C) 2002-2010  Marcel Holtmann <marcel@holtmann.org>

Fax: +49.30.31425156, Phone: +49.30.31424315

--
Copyright 1992 by Jutta Degener and Carsten Bormann, Technische
Universitaet Berlin.  See the accompanying file "COPYRIGHT" for
details.  THERE IS ABSOLUTELY NO WARRANTY FOR THIS SOFTWARE.Hyphen - hyphenation library to use converted TeX hyphenation patterns
 
(C) 1998 Raph Levien
(C) 2001 ALTLinux, Moscow
(C) 2006, 2007, 2008 László Németh
 
This was part of libHnj library by Raph Levien.Protocol Buffers - Google's data interchange format
Copyright 2008 Google Inc.
http://code.google.com/apis/protocolbuffers/
This is a port of GNU Bison @VERSION@ to MSDOS/DJGPP.

Copyright (C) 2005, 2006 Free Software Foundation, Inc.

This program is free software; you can redistribute it and/or modify
		libdes, Version 4.01 10-Jan-97

		Copyright (c) 1997, Eric Young
			  All rights reserved.

    This program is free software; you can redistribute it and/or modify
    it under the terms specified in COPYRIGHT.
    
--All the tests RPMs have been truncated to 10KB or less using dd to have
smaller tests files.

Their file type is correct and header is intact but are otherwise
incorrect/invalid rpms (incorrect checksum, corrupted archive, etc), 
they are not extractible with a regular rpm or cpio tool.

The dd command to create a file of 10000 bytes is:
$ dd if=mvlutils-2.8.4-7.0.2.0801061.src.rpm  of=mvlutils-2.8.4-7.0.2.0801061.src.rpm.1 bs=1 count=10000

And to verify the rpm headers are still readable:
$ dd if=mdv-rpm-summary-0.9.3-1mdv2010.0.noarch.rpm  of=mdv-rpm-summary-0.9.3-1mdv2010.0.noarch.rpm.1 bs=1 count=21000
$ rpm -qpi --nodigest --nosignature mdv-rp
m-summary-0.9.3-1mdv2010.0.noarch.rpm.1
21000+0 records in
21000+0 records out
21000 bytes (21 kB) copied, 0.179 s, 117 kB/s
Name        : mdv-rpm-summary              Relocations: (not relocateable)
Version     : 0.9.3                             Vendor: Mandriva
Release     : 1mdv2010.0                    Build Date: Thu Oct 29 11:31:32 2009
Install date: (not installed)               Build Host: klodia.mandriva.com
Group       : System/Internationalization   Source RPM: mdv-rpm-summary-0.9.3-1mdv2010.0.src.rpm
Size        : 6490616                          License: GPL
Signature   : DSA/SHA1, Thu Oct 29 22:14:02 2009, Key ID e7898ae070771ff3
Packager    : Anne Nicolas <anne.nicolas@mandriva.com>
Summary     : Localization files for packages summaries
Description :
This package includes translations of summaries the Mandriva Linux packages.
They are used by rpmdrake.
date-retrieved:2014-10-18 12:19:46+02:00
wget:-O package.json-no-dep https://raw.githubusercontent.com/coreos/etcd/v0.1.2/mod/dashboard/package.json
ScanCode license detection overview and key design elements
===========================================================

License detection is about finding common texts between the text of a query file
being scanned and the texts of the indexed license texts and rule texts. The process
strives to be correct first and fast second.

Ideally we want to find the best alignment possible between two texts so we know
exactly where they match: the scanned text and one or more of the many license texts.
We settle for good alignments rather than optimal alignments by still returning
accurate and correct matches in a reasonable amount of time.

Correctness is essential but efficiency too: both in terms of speed and memory usage.
One key to efficient matching is to process not characters but whole words and use
internally not strings but integers to represent a word.


Rules and licenses
------------------

The detection uses an index of reference license texts and a set of "rules" that are
common notices or mentions of these licenses. The things that makes detection
sometimes difficult is that a license reference can be very short as in "this is GPL"
or very long as a full license text for the GPLv3. To cope with this we use different
matching strategies and also compute the resemblance and containment of texts that
are matched.


Words as integers
-----------------

A dictionary mapping words to a unique integer is used to transform a scanned text
"query" words and reference indexed license texts and rules words to numbers.
This is possible because we have a limited number of words across all the license
texts (about 15K). We further assign these ids to words such that very common words
have a low id and less common, more discriminant words have a higher id. And define a
thresholds for this ids range such that very common words below that threshold cannot
possible form a license text or mention together.

Once that mapping is applied, the detection then only deal with integers in two
dimensions:

- the token ids (and whether they are in the high or low range).
- their positions in the query (qpos) and the indexed rule (ipos).

We also use an integer id for a rule.

All operations are from then on dealing with list, arrays or sets of integers in
defined ranges.

Matches are reduced to sets of integers we call "Spans":

- matched positions on the query side
- matched positions on the index side

By using integers in known ranges throughout, several operations are reduced to
integer and integer sets or lists comparisons and intersection. These operations
are faster and more readily optimizable.

With integers, we also use less memory:

- we can use arrays of unsigned 16 bits ints that store each number on two bytes
  rather than bigger lists of ints.
- we can replace dictionaries by sparse lists or arrays where the index is an integer key.
- we can use succinct, bit level representations (e.g. bitmaps) of integer sets.

Smaller data structures also means faster processing as the processors need to move
less data in memory.

With integers we can also be faster:
    
- a dict key lookup is slower than a list of array index lookup.
- processing large list of small structures is faster (such as bitmaps, etc).
- we can leverage libraries that speed up integer set operations.


Common/junk tokens
------------------

The quality and speed of detection is supported by classifying each word as either
good/discriminant or common/junk. Junk tokens are either very frequent of tokens that
taken together together cannot form some valid license mention or notice. When a
numeric id is assigned to a token during initial indexing, junk tokens are assigned a
lower id than good tokens. These are then called low or junk tokens and high or good
tokens.


Query processing
----------------

When a file is scanned, it is first converted to a query object which is a list of
integer token ids. A query is further broken down in slices (a.k.a. query runs) based
on heuristics.

While the query is processed a set of matched and matchable positions for for high
and low token ids is kept to track what is left to do in matching.


Matching pipeline
-----------------

The matching pipeline consist of:

- we start with matching the whole query at once against hashes on the whole text
  looked up agains a mapping of hash to license rule. We exit if we have a match.
 
- we are then subtracting from the query any exact match to a few "negative rules"
  which are texts that are not containing license information but may match otherwise
  to existing rules.

- then we match the whole query for exact matches using an automaton (Aho-Corasick).
  We exit if we have a match.

- then each query run is processed in sequence:

  - the best potentially matching rules are found with two rounds of approximate
    "set" matching.  This set matching uses a "bag of words" approach where the
    scanned text is transformed in a vector of integers based on the presence or
    absence of a word. It is compared against the index of vectors. This is similar
    conceptually to a traditional inverted index search for information retrieval.
    The best matches are ranked using a resemblance and containment comparison. A
    second round is performed on the best matches using multisets which are set where
    the number of occurrence of each word is also taken into account. The best matches
    are ranked again using a resemblance and containment comparison and is more
    accurate than the previous set matching.
    
  - using the ranked potential candidate matches from the two previous rounds, we
    then perform a pair-wise local sequence alignment between these candidates and
    the query run. This sequence alignment is essentially an optimized diff working
    on integer sequences and takes advantage of the fact that some very frequent
    words are considered less discriminant: this speeds up the sequence alignment
    significantly. The number of multiple local sequence alignments that are required
    in this step is also made much smaller by the pre-matching done using sets.
    
- finally all the collected matches are merged, refined and filtered to yield the
  final results. The merging considers the ressemblance, containment and overlap
  between scanned texts and the matched texts and several secondary factors.
  Filtering is based on the density and length of matches as well as the number of
  good or frequent tokens matched.
  Last, each match receives a score which is the based on the length of the rule text
  and how of this rule text was matched. Optionally we can also collect the exact
  matched texts and which part was not match for each match.


Comparison with other tools approaches
--------------------------------------

Most tools use regular expressions. The problem is that creating these expressions
requires a lot of intimate knowledge of the data set and the relation between each
license texts. The maintenance effort is high. And regex matches typically need a
complex second pass of disambiguation for similar matches.

Some tools use an index of pre-defined sentences and match these as regex and then
reassemble possible matches. They tend to suffer from the same issues as a pure regex
based approach and require an intimate knowledge of the license texts and how they
relate to each other.

Some tools use pair-wise comparisons like ScanCode. But in doing so they usually
perform poorly because a multiple local sequence alignment is an expensisve
computation. Say you scan 1000 files and you have 1000 reference texts. You would
need to recursively make multiple times 1000 comparisons with each scanned file very
quickly performing the equivalent 100 million diffs or more to process these files.
Because of the progressive matching pipeline used in ScanCode, sequence alignments
may not be needed at all in the common cases and when they are, only a few are
needed.

See also this list: https://wiki.debian.org/CopyrightReviewTools
extractcode is a universal archive extractor. It uses behind the scenes 
the Python standard library, a custom ctypes binding to libarchive and
the 7zip command line to extract a large number of common and
less common archives and compressed files. It tries to extract things
in the same way on all OSes, including auto-renaming files that would
not have valid names on certain filesystems or when there are multiple
copies of the same path in a given archive.
The extraction is driven from  a "voting" system that considers the
file extension(s) and name, the file type and mime type (using a ctypes
binding to libmagic) to select the most appropriate extractor or
uncompressor function. It can handle multi-level archives such as tar.gz.

See https://github.com/nexB/scancode-thirdparty-src for the corresponding source
code. See main NOTICE file for other details.

Precompiled binaries for supported target platforms are stored here.
We use the convention to match a binary based on the tuple returned by Python os.uname(). 
The first value is the OS and the last is the arch.
The precompiled binary for one platform is stored under ./<OS>/<ARCH>/mingw gcc
==============================================================================
This distribution is created from pristine source without patches.

Canonical homepage: http://gcc.gnu.org/
Canonical download: http://ftp.gnu.org/gnu/gcc/
License: LGPLv3+
Language: C, C++, OBJC, ADA, GFORTRAN

Build requirements:
  MinGW gcc, g++ and ada runtime environment
  MSYS shell environment and msys-xz

Build instructions:
  cd working-directory
  mingw-get source mingw32-gcc
  cd gcc-4.8.1-3-mingw32-src
  make clean package
  # See the output of make usage for a detailed list of targets
  # You can edit package.ini to modify the configuration items or to increment
  # the MPKGRLS value (the -3- after the mingw32).

When the job completes the release files will be generated in the rls directory
of the working directory.

  README.gcc-4.8.1-3-mingw32
  gcc-4.8.1-3-mingw32-lang.tar.lzma
  gcc-4.8.1-3-mingw32-src.tar.lzma
  gcc-ada-4.8.1-3-mingw32-bin.tar.lzma
  gcc-ada-4.8.1-3-mingw32-dev.tar.lzma
  gcc-ada-4.8.1-3-mingw32-dll.tar.lzma
  gcc-ada-4.8.1-3-mingw32-doc.tar.lzma
  gcc-ada-4.8.1-3-mingw32-info.tar.lzma
  gcc-c++-4.8.1-3-mingw32-bin.tar.lzma
  gcc-c++-4.8.1-3-mingw32-dev.tar.lzma
  gcc-c++-4.8.1-3-mingw32-dll.tar.lzma
  gcc-c++-4.8.1-3-mingw32-doc.tar.lzma
  gcc-c++-4.8.1-3-mingw32-man.tar.lzma
  gcc-core-4.8.1-3-mingw32-bin.tar.lzma
  gcc-core-4.8.1-3-mingw32-dev.tar.lzma
  gcc-core-4.8.1-3-mingw32-dll.tar.lzma
  gcc-core-4.8.1-3-mingw32-doc.tar.lzma
  gcc-core-4.8.1-3-mingw32-info.tar.lzma
  gcc-core-4.8.1-3-mingw32-lic.tar.lzma
  gcc-core-4.8.1-3-mingw32-man.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-bin.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-dev.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-dll.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-doc.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-info.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-man.tar.lzma
  gcc-objc-4.8.1-3-mingw32-dev.tar.lzma
  gcc-objc-4.8.1-3-mingw32-dll.tar.lzma
  gcc-objc-4.8.1-3-mingw32-doc.tar.lzma

Test results:
I am not equipped in my environment to execute the test suite delivered with the
source package.  However, I did use the results to rebuild the package itself.

Differences between -1- and -4-:
* The -1- release contained c++ files in both gcc-core and gcc-c++ that caused
mingw-get to spew errors.
* The -2- release has removed the duplication to silence the complaint of
mingw-get.
* The -3- release has no differences with the actual -2- package other than
it requires the correct mpc DLL numbered library of libmpc-3.dll in the
catalogue data for mingw-get.
* The -4- release added -disable-bootstrap, LDFLAGS=-2 and CFLAGS=-D_USE_32BIT_TIME_T to the configure options.  It also has a relocate.patch applied as
supplied by John E. / TDM <tdragon@tdragon.com> and added gcov.exe to the distributed gcc-core-bin file.
This directory contains the GNU Compiler Collection (GCC).

The GNU Compiler Collection is free software.  See the files whose
names start with COPYING for copying permission.  The manuals, and
some of the runtime libraries, are under different terms; see the
individual source files for details.

The directory INSTALL contains copies of the installation information
as HTML and plain text.  The source of this information is
gcc/doc/install.texi.  The installation information includes details
of what is included in the GCC sources and what files GCC installs.

See the file gcc/doc/gcc.texi (together with other files that it
includes) for usage and porting information.  An online readable
version of the manual is in the files gcc/doc/gcc.info*.

See http://gcc.gnu.org/bugs/ for how to report bugs usefully.

Copyright years on GCC source files may be listed using range
notation, e.g., 1987-2012, indicating that every year in the range,
inclusive, is a copyrightable year that could otherwise be listed
individually.

XZ Utils
========

    0. Overview
    1. Documentation
       1.1. Overall documentation
       1.2. Documentation for command-line tools
       1.3. Documentation for liblzma
    2. Version numbering
    3. Reporting bugs
    4. Translating the xz tool
    5. Other implementations of the .xz format
    6. Contact information


0. Overview
-----------

    XZ Utils provide a general-purpose data-compression library plus
    command-line tools. The native file format is the .xz format, but
    also the legacy .lzma format is supported. The .xz format supports
    multiple compression algorithms, which are called "filters" in the
    context of XZ Utils. The primary filter is currently LZMA2. With
    typical files, XZ Utils create about 30 % smaller files than gzip.

    To ease adapting support for the .xz format into existing applications
    and scripts, the API of liblzma is somewhat similar to the API of the
    popular zlib library. For the same reason, the command-line tool xz
    has a command-line syntax similar to that of gzip.

    When aiming for the highest compression ratio, the LZMA2 encoder uses
    a lot of CPU time and may use, depending on the settings, even
    hundreds of megabytes of RAM. However, in fast modes, the LZMA2 encoder
    competes with bzip2 in compression speed, RAM usage, and compression
    ratio.

    LZMA2 is reasonably fast to decompress. It is a little slower than
    gzip, but a lot faster than bzip2. Being fast to decompress means
    that the .xz format is especially nice when the same file will be
    decompressed very many times (usually on different computers), which
    is the case e.g. when distributing software packages. In such
    situations, it's not too bad if the compression takes some time,
    since that needs to be done only once to benefit many people.

    With some file types, combining (or "chaining") LZMA2 with an
    additional filter can improve the compression ratio. A filter chain may
    contain up to four filters, although usually only one or two are used.
    For example, putting a BCJ (Branch/Call/Jump) filter before LZMA2
    in the filter chain can improve compression ratio of executable files.

    Since the .xz format allows adding new filter IDs, it is possible that
    some day there will be a filter that is, for example, much faster to
    compress than LZMA2 (but probably with worse compression ratio).
    Similarly, it is possible that some day there is a filter that will
    compress better than LZMA2.

    XZ Utils doesn't support multithreaded compression or decompression
    yet. It has been planned though and taken into account when designing
    the .xz file format.


1. Documentation
----------------

1.1. Overall documentation

    README              This file

    INSTALL.generic     Generic install instructions for those not familiar
                        with packages using GNU Autotools
    INSTALL             Installation instructions specific to XZ Utils
    PACKAGERS           Information to packagers of XZ Utils

    COPYING             XZ Utils copyright and license information
    COPYING.GPLv2       GNU General Public License version 2
    COPYING.GPLv3       GNU General Public License version 3
    COPYING.LGPLv2.1    GNU Lesser General Public License version 2.1

    AUTHORS             The main authors of XZ Utils
    THANKS              Incomplete list of people who have helped making
                        this software
    NEWS                User-visible changes between XZ Utils releases
    ChangeLog           Detailed list of changes (commit log)
    TODO                Known bugs and some sort of to-do list

    Note that only some of the above files are included in binary
    packages.


1.2. Documentation for command-line tools

    The command-line tools are documented as man pages. In source code
    releases (and possibly also in some binary packages), the man pages
    are also provided in plain text (ASCII only) and PDF formats in the
    directory "doc/man" to make the man pages more accessible to those
    whose operating system doesn't provide an easy way to view man pages.


1.3. Documentation for liblzma

    The liblzma API headers include short docs about each function
    and data type as Doxygen tags. These docs should be quite OK as
    a quick reference.

    I have planned to write a bunch of very well documented example
    programs, which (due to comments) should work as a tutorial to
    various features of liblzma. No such example programs have been
    written yet.

    For now, if you have never used liblzma, libbzip2, or zlib, I
    recommend learning the *basics* of the zlib API. Once you know that,
    it should be easier to learn liblzma.

        http://zlib.net/manual.html
        http://zlib.net/zlib_how.html


2. Version numbering
--------------------

    The version number format of XZ Utils is X.Y.ZS:

      - X is the major version. When this is incremented, the library
        API and ABI break.

      - Y is the minor version. It is incremented when new features
        are added without breaking the existing API or ABI. An even Y
        indicates a stable release and an odd Y indicates unstable
        (alpha or beta version).

      - Z is the revision. This has a different meaning for stable and
        unstable releases:

          * Stable: Z is incremented when bugs get fixed without adding
            any new features. This is intended to be convenient for
            downstream distributors that want bug fixes but don't want
            any new features to minimize the risk of introducing new bugs.

          * Unstable: Z is just a counter. API or ABI of features added
            in earlier unstable releases having the same X.Y may break.

      - S indicates stability of the release. It is missing from the
        stable releases, where Y is an even number. When Y is odd, S
        is either "alpha" or "beta" to make it very clear that such
        versions are not stable releases. The same X.Y.Z combination is
        not used for more than one stability level, i.e. after X.Y.Zalpha,
        the next version can be X.Y.(Z+1)beta but not X.Y.Zbeta.


3. Reporting bugs
-----------------

    Naturally it is easiest for me if you already know what causes the
    unexpected behavior. Even better if you have a patch to propose.
    However, quite often the reason for unexpected behavior is unknown,
    so here are a few things to do before sending a bug report:

      1. Try to create a small example how to reproduce the issue.

      2. Compile XZ Utils with debugging code using configure switches
         --enable-debug and, if possible, --disable-shared. If you are
         using GCC, use CFLAGS='-O0 -ggdb3'. Don't strip the resulting
         binaries.

      3. Turn on core dumps. The exact command depends on your shell;
         for example in GNU bash it is done with "ulimit -c unlimited",
         and in tcsh with "limit coredumpsize unlimited".

      4. Try to reproduce the suspected bug. If you get "assertion failed"
         message, be sure to include the complete message in your bug
         report. If the application leaves a coredump, get a backtrace
         using gdb:
           $ gdb /path/to/app-binary   # Load the app to the debugger.
           (gdb) core core   # Open the coredump.
           (gdb) bt   # Print the backtrace. Copy & paste to bug report.
           (gdb) quit   # Quit gdb.

    Report your bug via email or IRC (see Contact information below).
    Don't send core dump files or any executables. If you have a small
    example file(s) (total size less than 256 KiB), please include
    it/them as an attachment. If you have bigger test files, put them
    online somewhere and include a URL to the file(s) in the bug report.

    Always include the exact version number of XZ Utils in the bug report.
    If you are using a snapshot from the git repository, use "git describe"
    to get the exact snapshot version. If you are using XZ Utils shipped
    in an operating system distribution, mention the distribution name,
    distribution version, and exact xz package version; if you cannot
    repeat the bug with the code compiled from unpatched source code,
    you probably need to report a bug to your distribution's bug tracking
    system.


4. Translating the xz tool
--------------------------

    The messages from the xz tool have been translated into a few
    languages. Before starting to translate into a new language, ask
    the author whether someone else hasn't already started working on it.

    Test your translation. Testing includes comparing the translated
    output to the original English version by running the same commands
    in both your target locale and with LC_ALL=C. Ask someone to
    proof-read and test the translation.

    Testing can be done e.g. by installing xz into a temporary directory:

        ./configure --disable-shared --prefix=/tmp/xz-test
        # <Edit the .po file in the po directory.>
        make -C po update-po
        make install
        bash debug/translation.bash | less
        bash debug/translation.bash | less -S  # For --list outputs

    Repeat the above as needed (no need to re-run configure though).

    Note especially the following:

      - The output of --help and --long-help must look nice on
        an 80-column terminal. It's OK to add extra lines if needed.

      - In contrast, don't add extra lines to error messages and such.
        They are often preceded with e.g. a filename on the same line,
        so you have no way to predict where to put a \n. Let the terminal
        do the wrapping even if it looks ugly. Adding new lines will be
        even uglier in the generic case even if it looks nice in a few
        limited examples.

      - Be careful with column alignment in tables and table-like output
        (--list, --list --verbose --verbose, --info-memory, --help, and
        --long-help):

          * All descriptions of options in --help should start in the
            same column (but it doesn't need to be the same column as
            in the English messages; just be consistent if you change it).
            Check that both --help and --long-help look OK, since they
            share several strings.

          * --list --verbose and --info-memory print lines that have
            the format "Description:   %s". If you need a longer
            description, you can put extra space between the colon
            and %s. Then you may need to add extra space to other
            strings too so that the result as a whole looks good (all
            values start at the same column).

          * The columns of the actual tables in --list --verbose --verbose
            should be aligned properly. Abbreviate if necessary. It might
            be good to keep at least 2 or 3 spaces between column headings
            and avoid spaces in the headings so that the columns stand out
            better, but this is a matter of opinion. Do what you think
            looks best.

      - Be careful to put a period at the end of a sentence when the
        original version has it, and don't put it when the original
        doesn't have it. Similarly, be careful with \n characters
        at the beginning and end of the strings.

      - Read the TRANSLATORS comments that have been extracted from the
        source code and included in xz.pot. If they suggest testing the
        translation with some type of command, do it. If testing needs
        input files, use e.g. tests/files/good-*.xz.

      - When updating the translation, read the fuzzy (modified) strings
        carefully, and don't mark them as updated before you actually
        have updated them. Reading through the unchanged messages can be
        good too; sometimes you may find a better wording for them.

      - If you find language problems in the original English strings,
        feel free to suggest improvements. Ask if something is unclear.

      - The translated messages should be understandable (sometimes this
        may be a problem with the original English messages too). Don't
        make a direct word-by-word translation from English especially if
        the result doesn't sound good in your language.

    In short, take your time and pay attention to the details. Making
    a good translation is not a quick and trivial thing to do. The
    translated xz should look as polished as the English version.


5. Other implementations of the .xz format
------------------------------------------

    7-Zip and the p7zip port of 7-Zip support the .xz format starting
    from the version 9.00alpha.

        http://7-zip.org/
        http://p7zip.sourceforge.net/

    XZ Embedded is a limited implementation written for use in the Linux
    kernel, but it is also suitable for other embedded use.

        http://tukaani.org/xz/embedded.html


6. Contact information
----------------------

    If you have questions, bug reports, patches etc. related to XZ Utils,
    contact Lasse Collin <lasse.collin@tukaani.org> (in Finnish or English).
    I'm sometimes slow at replying. If you haven't got a reply within two
    weeks, assume that your email has got lost and resend it or use IRC.

    You can find me also from #tukaani on Freenode; my nick is Larhzu.
    The channel tends to be pretty quiet, so just ask your question and
    someone may wake up.

The purpose of `packagedcode` is to:

- detect a package, 
- determine its dependencies, 
- detect its asserted license (at the metadata level) vs. its actual licensing (as scanned).


1. **detect the presence of a package** in a codebase based on its manifest, its file
or archive type. Typically it is a third party package but it may be your own too.
Taking Python as a main example a package can exist in multiple forms:

    1.1. as a **source checkout** (or some source archive such as a source
    distribution or an `sdist`) where the presence of a `setup.py` or some
    `requirements.txt` file is the key marker for Python. For Maven it would be a
    `pom.xml` or a `build.gradle` file, for Ruby a `Gemfile` or `Gemfile.lock`, the
    presence of autotools files, and so on, with the goal to eventually covering all
    the packages formats/types that are out there and commonly used.

    1.2. as an **installable archive or binary** such as a Pypi wheel `.whl` or
    `.egg`, a Maven `.jar`, a Ruby `.gem`, a `.nupkg` for a Nuget, a `.rpm` or `.deb`
    Linux package, etc... Here the type, shape and name structure of an archive as
    well as some its files content are the key markers for detection. The metadata
    may also be included in that archive as a file or as some headers (e.g. RPMs)

    1.3. as an **installed packaged** such as when you `pip install` a Python package
    or `bundle install` Ruby gems or `npm install` node modules. Here the key markers
    may be some combo of a typical or conventional directory layout and presence of
    specific files such as the metadata installed with a Python `wheel`, a `vendor`
    directory for Ruby, some `node_modules` directory tree for npms, or a certain
    file type with metadata such as Windows DLLs. Additional markers may also include
    "namespaces" such as Java or Python imports, C/C++ namespace declarations.

2. **parse and collect the package manifest(s)** metadata. For Python, this means
extracting name, version, authorship, asserted licensing and declared dependencies as
found in the any of the package descriptor files (e.g. a `setup.py` file,
`requirements` file(s) or any of the `*-dist-info` or `*-egg-info` dir files such as
a `metadata.json`). Other package formats have their own metatada that may be more or
less comprehensive in the breadth and depth of information they offer (e.g.
`.nuspec`, `package.json`, `bower.json`, Godeps, etc...). These metadata include the
declared dependencies (and in some cases the fully resolved dependencies too such as
with Gemfile.lock). Finally, all the different packages formats and data are
normalized and stored in a common data structure abstracting the small differences of
naming and semantics that may exists between all the different package formats.

Once collected, these data are then injected in a `packages` section of the scan. 

What code in `packagedcode` is not meant to do:

A. **download packages** from a thirdparty repository: there is code upcomming code in
another tool that will be specifically dealing with this and also handles collecting
the metadata as served by a package repository (which are in most cases --but not
always-- the same as what is declared in the manifests). 

B. **resolve dependencies**: the focus here is on a purely static analysis that does not
rely on any network access at runtime by design. To scan for actually used
dependencies the process is to instead scan for an as-built or as-installed or as-
deployed codebase where the dependencies have already been provisioned and installed
and there ScanCode would detect these. 
There are also some upcomming prototype for a dynamic multi-package dependencies
resolver that actually runs live the proper tool to resolve and collect dependencies
(e.g. effectively running Maven, bundler, pip, npm, gradle, bower, go get/dep, etc).
This will be a tool separate from ScanCode as this requires having several/all
package managers installed (and possibly multiple versions of each) and may run code
from the codebase (e.g. a setup.py) and access the network for fetching or resolving
dependencies. It could be also exposed as a web service that can take in a manifest
and package and run safely the dep resolution in an isolated environment (e.g. a
chroot jail or docker container) and return the collected deps.

C. **match packages** (and files) to actual repositories or registries, e.g. given a
scan detecting packages matching would be looking them up in a remote package
repository or a local index and possibly using A. and/or B. additionally if needed.
Here again there is some upcomming code and tool that will deal specifically with
this aspect and would handle also building an index of actual registries/repositories
and matching using hashes and fingerprints.

An now some answer to questions originally by @sschuberth:

> More concretely, this does not download the source code of a Python package to run
ScanCode over it.

Correct. The assumption with ScanCode proper (aside of the other in progress tools
that I mentioned above) is that the deps have been fetched in the code you scan if
you want to scan for deps. Packages will be detected with their declared deps but the
deps will neither be resolved nor fetched. Though, as a second step we could also
verify that all the declared deps are also present in the scanned code as detected
packages. 

> This should be made very clear as this means cases where the license from the
metadata is wrong compared to the LICENSE file in the source code will not get
detected.

Both the metadata and the file level licenses (such as a header comment or a
`LICENSE` file of sorts) are detected by ScanCode here: the license scan detect the
licenses while the package scan collect the asserted licensing in the metadata. The
interesting thing thanks to this combo is that eventual conflicts (or incomplete
data) can then be analyzed and a deduction should be doable automatically: given a
scan for packages and licenses and copyrights, do the package metadata
asserted/declared license match the actual detected licenses? If not this could be
reported as some "error" condition... Furthermore, this could be refined based on
classification of the files: a package may assert a top level `MIT` license and use a
GPL-licensed build script. By knowing that the build script is indeed a build script,
we could then report that the GPL detected in such script is not conflicting with the
overall asserted MIT license of the package.  The same could be done with test
scripts/code, or documentation code (such as doxygen-generated docs)

> Moreover, licenses from transitive dependencies are not taking into account.

If the transitive dependencies have been resolved and their code present in the
codebase, then they would be caught by a static ScanCode scan and eventually scanned
both for package metadata and/or license detection. There are some caveats that would
need to be dealt with of course as some tools (e.g. Maven) may not store locally
(e.g. side-by-side with a given checkout) the corresponding artifacts/Jars and use
instead a `~/user` "global" dot directory to store a cache.

Beyond this, actual dependency resolution of a single package or a complete manifest
would the topic of another tool as mentioned above.
Mac uses a more recent version of file and therefore uses its own magic BD.This package contains:

- The freedesktop.org shared MIME database spec.
- The merged GNOME and KDE databases, in the new format.
- The update-mime-database command, used to install new MIME data.


To install:

Do the usual:

	$ ./configure
	$ make
	$ make install

If you want to install to your home directory, you should instead do:

	$ ./configure --prefix=$HOME/.local
	$ make
	$ make install

You'll need to make sure that $HOME/.local/bin is in your PATH, of course.

See http://www.freedesktop.org/wiki/Standards/shared-mime-info-spec for more
information.


Please report bugs to the bugzilla, under the shared-mime-info product.

	http://www.freedesktop.org/wiki/GettingInvolved

Useful reference links:
IANA:
http://www.iana.org/assignments/media-types/
KDE's old mime-types:
http://websvn.kde.org/branches/KDE/3.5/kdelibs/mimetypes/

See https://github.com/nexB/scancode-thirdparty-src for the corresponding source
code. See main NOTICE file for other details.

Precompiled binaries for supported target platforms are stored here.
We use the convention to match a binary based on the tuple returned by Python os.uname(). 
The first value is the OS and the last is the arch.
The precompiled binary for one platform is stored under ./<OS>/<ARCH>/mingw gcc
==============================================================================
This distribution is created from pristine source without patches.

Canonical homepage: http://gcc.gnu.org/
Canonical download: http://ftp.gnu.org/gnu/gcc/
License: LGPLv3+
Language: C, C++, OBJC, ADA, GFORTRAN

Build requirements:
  MinGW gcc, g++ and ada runtime environment
  MSYS shell environment and msys-xz

Build instructions:
  cd working-directory
  mingw-get source mingw32-gcc
  cd gcc-4.8.1-3-mingw32-src
  make clean package
  # See the output of make usage for a detailed list of targets
  # You can edit package.ini to modify the configuration items or to increment
  # the MPKGRLS value (the -3- after the mingw32).

When the job completes the release files will be generated in the rls directory
of the working directory.

  README.gcc-4.8.1-3-mingw32
  gcc-4.8.1-3-mingw32-lang.tar.lzma
  gcc-4.8.1-3-mingw32-src.tar.lzma
  gcc-ada-4.8.1-3-mingw32-bin.tar.lzma
  gcc-ada-4.8.1-3-mingw32-dev.tar.lzma
  gcc-ada-4.8.1-3-mingw32-dll.tar.lzma
  gcc-ada-4.8.1-3-mingw32-doc.tar.lzma
  gcc-ada-4.8.1-3-mingw32-info.tar.lzma
  gcc-c++-4.8.1-3-mingw32-bin.tar.lzma
  gcc-c++-4.8.1-3-mingw32-dev.tar.lzma
  gcc-c++-4.8.1-3-mingw32-dll.tar.lzma
  gcc-c++-4.8.1-3-mingw32-doc.tar.lzma
  gcc-c++-4.8.1-3-mingw32-man.tar.lzma
  gcc-core-4.8.1-3-mingw32-bin.tar.lzma
  gcc-core-4.8.1-3-mingw32-dev.tar.lzma
  gcc-core-4.8.1-3-mingw32-dll.tar.lzma
  gcc-core-4.8.1-3-mingw32-doc.tar.lzma
  gcc-core-4.8.1-3-mingw32-info.tar.lzma
  gcc-core-4.8.1-3-mingw32-lic.tar.lzma
  gcc-core-4.8.1-3-mingw32-man.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-bin.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-dev.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-dll.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-doc.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-info.tar.lzma
  gcc-fortran-4.8.1-3-mingw32-man.tar.lzma
  gcc-objc-4.8.1-3-mingw32-dev.tar.lzma
  gcc-objc-4.8.1-3-mingw32-dll.tar.lzma
  gcc-objc-4.8.1-3-mingw32-doc.tar.lzma

Test results:
I am not equipped in my environment to execute the test suite delivered with the
source package.  However, I did use the results to rebuild the package itself.

Differences between -1- and -4-:
* The -1- release contained c++ files in both gcc-core and gcc-c++ that caused
mingw-get to spew errors.
* The -2- release has removed the duplication to silence the complaint of
mingw-get.
* The -3- release has no differences with the actual -2- package other than
it requires the correct mpc DLL numbered library of libmpc-3.dll in the
catalogue data for mingw-get.
* The -4- release added -disable-bootstrap, LDFLAGS=-2 and CFLAGS=-D_USE_32BIT_TIME_T to the configure options.  It also has a relocate.patch applied as
supplied by John E. / TDM <tdragon@tdragon.com> and added gcov.exe to the distributed gcc-core-bin file.
This directory contains the GNU Compiler Collection (GCC).

The GNU Compiler Collection is free software.  See the files whose
names start with COPYING for copying permission.  The manuals, and
some of the runtime libraries, are under different terms; see the
individual source files for details.

The directory INSTALL contains copies of the installation information
as HTML and plain text.  The source of this information is
gcc/doc/install.texi.  The installation information includes details
of what is included in the GCC sources and what files GCC installs.

See the file gcc/doc/gcc.texi (together with other files that it
includes) for usage and porting information.  An online readable
version of the manual is in the files gcc/doc/gcc.info*.

See http://gcc.gnu.org/bugs/ for how to report bugs usefully.

Copyright years on GCC source files may be listed using range
notation, e.g., 1987-2012, indicating that every year in the range,
inclusive, is a copyrightable year that could otherwise be listed
individually.
This is the regex functionality from glibc 2.5 extracted into a
separate library, for Win32.  It may be built, from the sources
provided, using the command sequence:--

  ./configure [--options...] && make

For a list of available configuration options, run:--

  ./configure --help

After building, as above, the resultant DLL, and optionally the
associated development kit, may be installed by:--

  make install

while redistributable binary DLL and development library kits may
be created by:--

  make dist


The original sources, on which this port is based, remain
copyright of their respective authors, or of the Free Software
Foundation Inc., as indicated in individual file headers; all are
redistributed with permission, as granted by the GNU Lesser
General Public License.

This is free software.  It is provided AS IS, in the hope that
it may be useful, but WITHOUT WARRANTY OF ANY KIND, not even an
IMPLIED WARRANTY of MERCHANTABILITY, nor of FITNESS FOR ANY
PARTICULAR PURPOSE.

Permission is granted to redistribute this software, either
"as is" or in modified form, under the terms of the GNU Lesser
General Public License, as published by the Free Software
Foundation; either version 2.1, or (at your option) any later
version.

You should have received a copy of the GNU Lesser General Public
License along with this software; see the file COPYING.LIB.  If
not, write to the Free Software Foundation, 51 Franklin St -
Fifth Floor, Boston, MA 02110-1301, USA.

The original port of this functionality was implemented by Tor
Lillqvist; I've adapted his work, to make it somewhat more MinGW
friendly.  I have *not* modified any of the `C' sources provided
by Tor; nor have I changed the naming conventions he adopted for
generated distributables.  I *have*:--

1) Replaced Tor's original `Makefile' with an autoconf generated
   configure script, and a backwardly compatible `Makefile.in';
   this provides a more flexible build procedure, which I find
   more convenient, when cross-compiling on a GNU/Linux host.

2) Added VPATH support, for `out of tree' builds.

3) Adapted the build procedure, to avoid a dependency on the `lib'
   program from Microsoft's MSVC tool chain.  This is achieved by
   providing an option to configure, which is disabled by default;
   it may by activated by specifying `--enable-msvc-implib' on the
   configure command line.  If this option is not activated, or if
   the MSVC `lib' tool is not present, the Makefile is configured
   without binding the rule for building an MSVC compatible import
   library, to the default target, (although the rule is left in
   place for explicit use).

   If the `--enable-msvc-implib' option is specified, but `lib' is
   not present, then configure will issue a warning message, and
   will again configure the Makefile without binding this rule to
   the default target.

   Only if the `--enable-msvc-implib' option is specified, *and*
   the `lib' tool is present, will building of an MSVC compatible
   import library be configured as a default deliverable.

4) Added `install', `install-dll' and `install-dev' targets, to
   support direct installation of the DLL, and its associated
   development kit.

5) Changed the default packaging format for distributables, from
   Tor's exclusive choice of `zip', to my own preferred `tar.gz';
   `zip' format remains available, as an option, by configuring
   with `--enable-dist=zip'.

6) Added `bindist', `devdist' and `srcdist' targets, for greater
   flexibility in building distribution kits.

The original text of Tor's README file will be found below.

--Keith Marshall  <keithmarshall@users.sourceforge.net>

I call the DLL libgnurx-0.dll which hopefully should be unique. At
least it isn't "regex.dll" which has been used by the
gnuwin32.sourceforge.net site for *two* incompatible DLLs. (That mess,
and the mess with their build of Henry Spencer's regex library, was
what lead me to build my own GNU regex library. See the
gnuwin32-users mailing list archives from December 2006.)

The "-0" is so that if at some point I build a release that isn't
binary compatible, I can then increment that and use a different name.

The import library for gcc is called libgnurx.dll.a, but I also
distribute a copy of it called libregex.a so that configure scripts
that look for -lregex will work.

Note that none of the wide-character and i18n functionality which is
built when this is part of glibc gets compiled. Thus things like
character classes most probably work only for single-byte codepages.

Compiling that stuff would drag in lots of glibc's locale handling
stuff which is completely incompatible with Microsoft's C library's
locale handling anyway. Also, I am not sure whether the GNU regex code
is prepared to handle a two-byte wchar_t, or does it assume that
wchar_t is int as it is on Linux? Hmm, actually there is lots of
sizeof(wchar_t) in glibc, so maybe it *is* prepared? Maybe
later... But anyway, it would presumably mean we should have not just
the regex functionality but a larger subset of glibc that would
include all locale, ctype, wchar, mbs, etc stuff, presumably ending up
with a very large part of glibc (not the system calls,
obviously). Indeed, something to save for later, or never...

--Tor Lillqvist <tml@iki.fi>, <tml@novell.com>
This directory contains a few sample files extracted from these two archives:

download_url: http://zlib.net/zlib-1.2.8.tar.gz
download_url: http://master.dl.sourceforge.net/project/javagroups/JGroups/2.10.0.GA/JGroups-2.10.0.GA.src.zip[![Build Status](https://travis-ci.org/matthieudelaro/nut.svg?branch=master)](https://travis-ci.org/matthieudelaro/nut)

### Whetting Your Appetite
Tired of hearing: "works on my machine"?
Ever experienced headache to install libraries and dependencies?
Ever had to deal with two incompatible versions of a program at once?
Ever wished to try out a new language first, and install it only if it pleases you?
Ever wished to develop for Linux when you use Mac OS or Windows?
Ever wished to develop in Go from the folder of your choice?
Ever wished to have a unified development tool, across all platforms, customizable to any languages?
Ever wished to simplify and share your research on neural networks in Docker, running on GPU, with reproducible results?

### Nut
**Nut** is a command line tool which offers a solution to common frustrations of developers. It hides the complexity of development environments, and extends them with customizable macros. Whether you develop in Swift, Go, Java, or C++, what you need is build/run/test the app. So just do it:
```bash
$ nut --init # create nut.yml file (equivalent of package.json for npm)
$ nut build
$ nut run
$ nut test
```

**Nut** mounts the current folder in a [Docker](https://www.docker.com/) container, and executes commands on your behalf, according to the project configuration. The configuration is read from `nut.yml` file, in the current/parent folder. You can choose the Docker image to use, declare volumes to mount, and define commands (called macros) such as *build*, *run*, and *test*. Nut also [synchronizes timezone](https://github.com/matthieudelaro/nut/wiki/Nut:-Under-The-Hood#synchronize-timezones)

Nut is in early stage of development. It has been tested on Ubuntu, on MacOS with *Docker for Mac* and *Docker Toolbox*, and on Windows with *Docker Toolbox*. Feedbacks and contributions to add features and to [improve Windows support](https://github.com/matthieudelaro/nut/issues/4) are welcome.

Check the [wiki](https://github.com/matthieudelaro/nut/wiki) to learn about Nut [implementation](https://github.com/matthieudelaro/nut/wiki/Nut:-Under-The-Hood), and to read some tutorials (GPU support, Caffe, TensorFlow, etc).

### Share and reuse environments
You can initialize **Nut** with an environment from a GitHub repository:
```bash
$ nut --init --github=matthieudelaro/nutfiles/golang1.6
```
This creates `nut.yml` file that inherites the configuration defined in the nut file at the root of the repository.
This configuration can be overloaded by defining/redefining docker image, macros, mounting points, ... It makes it easy for developers to use libraries and development tools that provide a nut file.

To inspect an environment, you can use `--exec` flag:
```bash
$ # --exec="command to run in the container"
$ nut --exec="pwd"  # will display the path in working directory of the container
$ nut --exec="ls"  # will display the files in the container working directory
$ nut --exec="echo hello && echo world!" --logs  # --logs will display the logs for developers
```
`--exec` flag can be really handy to build and test **Nut** on OSX:
```bash
$ nut build-osx && nut --exec="./nut --init --logs && ls -lah .nut"
```
### Getting Nut
#### Compiling from source
Provided that you use Docker, you don't need to install anything on your computer.
Not even Go!
```bash
# 1 - Download sources and dependencies
    # unix
    docker run --rm -v $PWD:/go/src/github.com/matthieudelaro/ -w /go/src/github.com/matthieudelaro/ matthieudelaro/golang:1.7-cross bash -c 'git clone https://github.com/matthieudelaro/nut.git  --progress && cd nut && govendor sync'

    # windows
    docker run --rm -v ${PWD}:/go/src/github.com/matthieudelaro/ -w /go/src/github.com/matthieudelaro/ matthieudelaro/golang:1.7-cross bash -c 'git clone https://github.com/matthieudelaro/nut.git  --progress && cd nut && govendor sync'

# 2 - Move to nut folder
    cd nut

# 3 - Build Nut
    # Build Nut for Linux, in a container
    docker run -i -t --rm -v $PWD:/go/src/github.com/matthieudelaro/nut -w /go/src/github.com/matthieudelaro/nut matthieudelaro/golang:1.7-cross go build -o nut

    # Build Nut for OSX, in a container
    docker run -i -t --rm -v $PWD:/go/src/github.com/matthieudelaro/nut -w /go/src/github.com/matthieudelaro/nut matthieudelaro/golang:1.7-cross env GOOS=darwin GOARCH=amd64 go build -o nut

    # Build Nut for Windows, in a container.
    docker run -i -t --rm -v ${PWD}:/go/src/github.com/matthieudelaro/nut -w /go/src/github.com/matthieudelaro/nut matthieudelaro/golang:1.7-cross env GOOS=windows GOARCH=amd64 go build -o nut.exe

# Run nut
./nut   # or .\nut.exe on Windows

# Try out Nut
./nut test # will compile and run the tests in a container, according to nut.yml

# Add nut to your PATH
    # Copy it in the path
    sudo cp nut /usr/local/bin/nut # on linux and osx

    # Or modify the path
    echo "PATH=`pwd`:\$PATH" >> ~/.bashrc  # on linux
    echo "PATH=`pwd`:\$PATH" >> ~/.bash_profile  # on osx
```

#### Using NPM
[@RnbWd](https://github.com/RnbWd) developed a npm package with Nut binaries: https://github.com/RnbWd/nut-bin

#### Download Binaries
Manually built binaries for Linux, OSX, and Windows are available in [release](https://github.com/matthieudelaro/nut/tree/manualbuild/release) folder. _It is a temporary solution._


### Nut File Syntax
#### Example
Here is an example of `nut.yml` to develop in Go. You can generate a sample configuration with:

`nut --init`
```yaml
# nut.yml
project_name: nut
enable_gui: true # forward X11 to run graphical application from within the container
                 # On OSX, you have to install an X11 server first : XQuartz (http://www.xquartz.org/) (and you may need to restart your terminal or to reboot, in order to initialize environment variables properly)
                 # On Ubuntu, depending on your config, you may need to run "xhost+" before running nut.

based_on: # configuration can be inherited from:
  github: matthieudelaro/nutfiles/golang1.6 # a GitHub repository
  nut_file_path: ../go1.5/nut.yml # a local file
  # You can inherite either from GitHub or from a file, not both.

docker_image: golang:1.6 # the Docker image. Will override image inherited from file or from GitHub

volumes: # declare folders to mount in the container
  main: # give each folder any name that you like
    host_path: .               # this folder (from your computer) will be mounted as
    container_path: /go/src/project # this folder (in the container)
  shared:
    volume_name: somevolume  # this docker volumes will be mounted as
    container_path: /tmp/shared # this folder (in the container)

macros: # macros define operations that Nut can perform
  build: # call this macro with "nut build"
    usage: build the project
    actions:  # a list of commands to run in the container
    - go build -o nut
    - echo Done
  run: # call this macro with "nut run"
    usage: run the project in the container
    # settings can be overridden for each macro
    enable_current_user: true # login as host user, instead of root
    actions:
    - ./nut
  test:
    usage: test the project
    actions:
    - go test

container_working_directory: /go/src/project # where macros will be executed
work_in_project_folder_as: /go/src/github.com/matthieudelaro/nut
  # Mount the folder of the project (where nut.yml is located) to the given
  # location inside the container. Also set the working directory to this
  # location. This is equivalent to container_working_directory + volume
syntax_version: "7" # Nut evolves quickly ; its configuration file syntax as well.
                    # So nut files are versioned to ensure backward compatibility.

# extra configuration:
privileged: true # run container with --privileged flag
environment: # set environment variables
  var_A: hello # equivalent to: export var_A=hello
  var_B: world
  # environment variables beginning with NUT_ should be reserved for internal usage.
ports: # open ports
  - "3000:3000"
  - 100:100
net: host # docker run --net
uts: host # docker run --uts
security_opts: # docker run --security-opt
  - seccomp=unconfined
devices: # expose devices to the container
  # On OSX and Windows, docker runs into a VM which does not support devices.
  # So Nut supports devices on linux only.
  first:
    host_path: "/dev/1"
    container_path: "/dev/1"
    options: "rw"

```

Here are other instructive [examples](https://github.com/matthieudelaro/nut/blob/master/examples/):
- [Dynamic folder name](https://github.com/matthieudelaro/nutfile_go1.5/blob/master/nut.yml)
- [GUI application](https://github.com/matthieudelaro/nut/blob/master/examples/geary/nut.yml)


#### Guidelines
Nut aims to unify development tools, not to replace compilers.
Nut aims to unify development processes, not to expose language specific requirements.

So, when creating a `nut.yml` file, one should standard names for macros, such as:
- build
- run
- test
- debug
- deploy

As opposed to:
- javac (should be generalized with *build*)
- make (duplicate of *build*)
- do (hum... *Do* what?)
This will keep Nut easy to integrate in text editors and IDEs.

### Support for [nvidia-docker](https://github.com/NVIDIA/nvidia-docker.git)
On Linux, Nut can leverage Nvidia GPUs for your environments. This is useful to use and develop deep learning frameworks, or even to run video games. Due to limitations of Docker on OSX and Windows, Nut does not support GPUs on those platforms.

GPU support relies on [nvidia-docker-plugin](https://github.com/NVIDIA/nvidia-docker/wiki/Using-nvidia-docker-plugin). If it is not running automatically on your machine after [installation](https://github.com/NVIDIA/nvidia-docker/wiki/Installation), you can run it [this way](https://github.com/NVIDIA/nvidia-docker/wiki/Using-nvidia-docker-plugin#usage):
```bash
# Add a system user nvidia-docker
adduser --system --home /var/lib/nvidia-docker nvidia-docker
# Register the plugin with the Docker daemon
mkdir -p /etc/docker/plugins
echo "unix:///var/lib/nvidia-docker/nvidia-docker.sock" > /etc/docker/plugins/nvidia-docker.spec
# Set the mandatory permission
setcap cap_fowner+pe /usr/bin/nvidia-docker-plugin

# Run nvidia-docker-plugin as the nvidia-docker user
sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker
```

nvidia-docker-plugin **MUST** be running when you call **Nut**. You can check with:
```bash
curl -s http://0.0.0.0:3476/v1.0/gpu/info  # query the REST API exposed by nvidia-docker-plugin

# should display something like
Driver version:          352.63
Supported CUDA version:  7.5

Device #0
  Model:         GeForce GTX TITAN X
  UUID:          GPU-7e7b6b05-764c-8e74-d867-9a87868d5a1f
  Path:          /dev/nvidia0
  Family:        Maxwell
  Arch:          5.2
  Cores:         3072
  Power:         250 W
  CPU Affinity:  NUMA node0
  PCI
    Bus ID:     0000:01:00.0
    BAR1:       256 MiB
    Bandwidth:  15760 MB/s
  Memory
    ECC:        false
    Global:     12287 MiB
    Constant:   64 KiB
    Shared:     96 KiB
    L2 Cache:   3072 KiB
    Bandwidth:  336480 MB/s
  Clocks
    Cores:        1391 MHz
    Memory:       3505 MHz
  P2P Available:  None
```


### What the Nut???
- build [Nut](https://github.com/matthieudelaro/nut/blob/master/nut.yml) within Nut (I never installed Go, and I'm never going to :)
- build [Docker](https://github.com/matthieudelaro/nut/blob/master/examples/docker/nut.yml)
- build and run [Caffe](https://github.com/matthieudelaro/nut/blob/master/examples/caffe/nut.yml) with `nut build`, `nut test`, `nut train-mnist`.
- compile CUDA code on a Mac Book Air, which hasn't got any Nvidia GPU. Just `nut build`
- test code in a whole infrastructure, by defining a macro running *docker-compose* in a container.
- run linux graphical applications such as [geary](https://github.com/matthieudelaro/nut/blob/master/examples/geary/nut.yml) and [chrome](https://github.com/matthieudelaro/nut/blob/master/examples/chrome/nut.yml) on your Mac after installing [XQuartz](http://www.xquartz.org/):

![Geary application on your Mac](https://camo.githubusercontent.com/b32c086f7da89f3365062f9a6a49b7f64377cb35/687474703a2f2f692e696d6775722e636f6d2f4b6650676d72322e676966)
![Chrome application on your Mac](https://pbs.twimg.com/media/Cl90rCuVYAATcsz.jpg:large)

### Milestones
- create container only once, and store its ID in .nut file
- improve support for Windows
- plugin for Sublime Text, to call `nut run`, `nut build`, and `nut test` from the editor
- create a registery for `nut.yml` files
- see [issues](https://github.com/matthieudelaro/nut/issues)

### Stay Tuned
Wanna receive updates? Or share your thoughts? You can post an issue or follow me on [Twitter](https://twitter.com/matthieudelaro).

### Authors and Contributors
@matthieudelaro and @gdevillele, as well as authors of PRs
### Modified path/filepath
This is a modified version of golang.org/pkg/path/filepath/

### README
**This README needs to be updated.**

Nut must remain backward compatible with respect to nut configuration files.
It must also be easy to add new features in nut files, without issues of
performance to parse the file, and without applying modifications to all
older versions to insure backward compatibility.

The chosen solution is that new features should be defined in the interface,
and default behavior/feature should be defined in base class (to be
accessible) to all syntaxes version. Then override this default behavior in
the new syntax version.

 Interfaces Names| Base Class Names    | Version 2         | Version 3         | ...
 ----------------+---------------------+-------------------+-------------------+----
 Macro           | MacroBase           | MacroV2           | MacroV3           | ...
 MountingPoint   | MountingPointBase   | MountingPointV2   | MountingPointV3   | ...
 BaseEnvironment | BaseEnvironmentBase | BaseEnvironmentV2 | BaseEnvironmentV3 | ...
 Project         | ProjectBase         | ProjectV2         | ProjectV3         | ...

For each new syntax, copy/paste a project_vXXX.go file (ie the latest one)
and change implementation to suit new requirements.
For any new feature, create a virtual method in the interface Project
(or its components Macro, MountingPoint, BaseEnvironment, etc) and
implement a method in ProjectBase class (or its components MacroBase, etc)
to define a default behavior for older versions.

Old syntax versions should never be modified. Only the interface, the
base class, and the new syntax should be updated.

Note: When working with structs and embedding, everything is STATICALLY LINKED. All references are resolved at compile time.
See https://github.com/luciotato/golang-notes/blob/master/OOP.md for more details.
All this image does it track docker:dnid, add zabbix, install s6 add a few packages for easier navigation and update the Docker Daemon to commence in experimental mode to support --squash --compress and prune functions and switch to overlay2 filesystem.

However, there are some environment variables:

Below is the complete list of available options that can be used to customize your installation.

| Parameter         | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `DEBUG_MODE`      | Enable Debug Mode - Default: `FALSE`                            |
| `ENABLE_CRON`     | Enable Cron - Default: `TRUE`                                   |
| `ENABLE_SMTP`     | Enable SMTP services - Default: `TRUE`						|
| `ENABLE_ZABBIX`   | Enable Zabbix Agent - Default: `TRUE`                           |
| `TIMEZONE`        | Set Timezone - Default: `America/Vancouver`                     |

If you wish to have this send mail, set `ENABLE_SMTP=TRUE` and configure the following environment variables. See the [MSMTP Configuration Options](http://msmtp.sourceforge.net/doc/msmtp.html) for further information on options to configure MSMTP

| Parameter         | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `SMTP_HOST`      | Hostname of SMTP Server - Default: `postfix-relay`                            |
| `SMTP_PORT`      | Port of SMTP Server - Default: `25`                            |
| `SMTP_DOMAIN`     | HELO Domain - Default: `docker`                                   |
| `SMTP_MAILDOMAIN`     | Mail Domain From - Default: `example.org`						|
| `SMTP_AUTHENTICATION`     | SMTP Authentication - Default: `none`                                   |
| `SMTP_USER`     | Enable SMTP services - Default: `user`						|
| `SMTP_PASS`   | Enable Zabbix Agent - Default: `password`                           |
| `SMTP_TLS`        | Use TLS - Default: `off`                     |
| `SMTP_STARTTLS`   | Start TLS from within Dession - Default: `off` |
| `SMTP_TLSCERTCHECK` | Check remote certificate - Default: `off` |

See The [Official Zabbix Agent Documentation](https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd) for information about the following Zabbix values

| Zabbix Parameters | Description                                                    |
|-------------------|----------------------------------------------------------------|
| `ZABBIX_LOGFILE` | Logfile Location - Default: `/var/log/zabbix/zabbix_agentd.log` |
| `ZABBIX_LOGFILESIZE` | Logfile Size - Default: `1` |
| `ZABBIX_DEBUGLEVEL` | Debug Level - Default: `1` |
| `ZABBIX_REMOTECOMMANDS` | Enable Remote Commands (0/1) - Default: `1` |
| `ZABBIX_REMOTECOMMANDS_LOG` | Enable Remote Commands Log (0/1)| - Default: `1` |
| `ZABBIX_SERVER` | Allow connections from Zabbix Server IP - Default: `0.0.0.0/0` |
| `ZABBIX_LISTEN_PORT` | Zabbix Agent Listening Port - Default: `10050` |
| `ZABBIX_LISTEN_IP` | Zabbix Agent Listening IP - Default: `0.0.0.0` |
| `ZABBIX_START_AGENTS` | How many Zabbix Agents to Start - Default: `3 | 
| `ZABBIX_SERVER_ACTIVE` | Server for Active Checks - Default: `zabbix-proxy` |
| `ZABBIX_HOSTNAME` | Container hostname to report to server - Default: `docker` |
| `ZABBIX_REFRESH_ACTIVE_CHECKS` | Seconds to refresh Active Checks - Default: `120` |
| `ZABBIX_BUFFER_SEND` | Buffer Send - Default: `5` |
| `ZABBIX_BUFFER_SIZE` | Buffer Size - Default: `100` |
| `ZABBIX_MAXLINES_SECOND` | Max Lines Per Second - Default: `20` |
| `ZABBIX_ALLOW_ROOT` | Allow running as root - Default: `1` |
| `ZABBIX_USER` | Zabbix user to start as - Default: `zabbix` |


# Crane
Lift containers with ease - [www.craneup.tech](https://www.craneup.tech?utm_source=github&utm_medium=web&utm_campaign=readme&utm_content=header)


## Overview
Crane is a Docker orchestration tool similar to Docker Compose with extra
features and (arguably) smarter behaviour. It works by reading in some
configuration (JSON or YAML) which describes how to run containers. Crane is
ideally suited for development environments or continuous integration.

## Features

* Extensive support of Docker run flags
* Simple configuration with 1:1 mapping to Docker run flags
* `docker-compose` compatible
* **ultra-fast bind-mounts via Unison on Mac** in the PRO version
* Grouping of containers
* Excluding / limiting containers via CLI
* Smart detach / attach behaviour
* Verbose output which shows exact Docker commands
* Hooks
* ... and much more!

## Documentation & Usage

Please see [www.craneup.tech](https://www.craneup.tech?utm_source=github&utm_medium=web&utm_campaign=readme&utm_content=docs).

## Installation

The latest release is 3.2.0 and requires Docker >= 1.10.
Please have a look at the [changelog](https://github.com/michaelsauter/crane/blob/master/CHANGELOG.md) when upgrading.

The free version can be installed via:

```
bash -c "`curl -sL https://raw.githubusercontent.com/michaelsauter/crane/v3.2.0/download.sh`" && \
mv crane /usr/local/bin/crane
```

**If you are on Mac, check out the PRO version which seamlessly integrates
[ultra-fast bind-mounts via Unison](https://www.craneup.tech?utm_source=github&utm_medium=web&utm_campaign=readme&utm_content=pro)**.

---

Copyright © 2013-2017 Michael Sauter. See the LICENSE file for details.

---

[![GoDoc](https://godoc.org/github.com/michaelsauter/crane?status.png)](https://godoc.org/github.com/michaelsauter/crane)
[![Build Status](https://travis-ci.org/michaelsauter/crane.svg?branch=master)](https://travis-ci.org/michaelsauter/crane)
Docker REMOTE API Caller with curl + jq

# 概要

Docker Machineで作られたDockerホストへのDocker Remote API呼び出し用イメージです。

# Usage

### 書式/引数/環境変数
```bash

$ docker run (options) yamamotofebc/docker-api-client [APIエンドポイント] [jqコマンド引数]

or

$ docker-compose run (options) docker-api-client [APIエンドポイント] [jqコマンド引数]

```


  - `APIエンドポイント` : 必須。`/images/json`など。詳細は[こちら](https://docs.docker.com/engine/reference/api/docker_remote_api/)を参照ください。
  - `jqコマンド引数` : オプション。デフォルト`.`。

#### 環境変数

  - `$DOCKER_HOST` : TLS保護付きTCPポートURL(docker-machine envコマンドで出力されるもの)
  - `$DOCKER_SOCKET` : $DOCKER_HOSTが未指定の場合に利用される。dockerのunixドメインソケットパス。デフォルト`/var/run/docker.sock`
  - `$DOCKER_CURL_OPTION` : curlコマンドに追加指定されるコマンド引数。(デフォルトで-sSfkは指定済み。それ以外の追加オプション(-xとか)を指定)

#### TLS接続する場合のvolumeの割り当て

以下にTLS関連ファイルが格納されていますので、/etc/dockerへvolumeを割り当ててください。

  - virtualboxなどのローカルドライバで作ったマシンの場合: `~/.docker/machine/machines/対象マシン`
  - sakuracloudなどのクラウドドライバで作ったマシンの場合: `/etc/docker`

イメージ内の`/etc/docker`配下のTLS関連ファイル名は以下のようになっている必要があります。

  - `証明書(--cert)` : `server.pem`
  - `秘密鍵(--key)` :  `server-key.pem`

### docker runで実行する場合(TLS接続)

```bash

# docker-machineコマンドで環境変数を設定しておく
$ eval $(docker-machine env 対象マシン)

$ docker run -it --rm -e DOCKER_HOST \
             -v /etc/docker:/etc/docker \
             yamamotofebc/docker-api-client /images/json

```

### docker runで実行する場合(unixドメインソケット)

```bash

# docker-machineコマンドで環境変数を設定しておく(アンセット)
$ eval $(docker-machine env -u)

$ docker run -it -v /var/run/docker.sock:/var/run/docker.sock \
             --rm yamamotofebc/docker-api-client /images/json

# DOCKER_SOCKET環境変数を指定していないため、/var/run/docker.sockが使われる

```


### docker-composeで実行する場合

#### 準備

```bash

$ git clone https://github.com/yamamoto-febc/docker-api-client.git
$ cd docker-api-client

```

#### 実行

```bash

$ docker-compose run --rm docker-api-client /images/json

```

#### 実行例(/images/jsonにてIDのみ抜き出し)

```bash

$ docker-compose run --rm docker-api-client /images/json ".[].Id"

```





# ETCD v3 FaaS操作实例

本实例使用FaaS的方法操作etcd v3版本。

## Requirements

- Functions API
- fn 
- ETCD v3 server([好雨部署](etcd_v3_server.md))

## Development

### 构建本地镜像

```
# 修改func.yaml文件，将name改成你自己的镜像名称。
# build it

fn build

```
### 本地测试
```
fn run

```

### 部署应用到仓库

```
fn deploy etcd_v3
```

## 在平台运行

### 首先设置必须的环境变量

```
# Set your Function server address
# Eg. api.faas.pro

FUNCAPI=YOUR_FUNCTIONS_ADDRESS

# ETCD服务端地址需要先部署etcd,参考： (Requirements)[#Requirements]

ETCD_SERVER=""
```

### Running with Functions

创建应用

```
curl -X POST --data '{
    "app": {
        "name": "etcd_v3",
        "config": { 
            "ETCD_SERVER": "'$ETCD_SERVER'",
        }
    }
}' http://$FUNCAPI/v1/apps
```

创建路由

```
curl -X POST --data '{
    "route": {
        "image": "<镜像名>",
        "path": "/command",
    }
}' http://$FUNCAPI/v1/apps/etcd_v3/routes
```

#### 云端运行试试？

```
curl -X POST --data '{"method": "put","key":"/hello","value":"hello word"}' http://$FUNCAPI/r/etcd_v3/command
curl -X POST --data '{"method": "get","key":"/hello"}' http://$FUNCAPI/r/etcd_v3/command

```# Twitter Function Image

This function exemplifies an authentication in Twitter API and get latest tweets of an account.

## Requirements

- Functions API
- fn 
- Configure a [Twitter App](https://apps.twitter.com/) and [configure Customer Access and Access Token](https://dev.twitter.com/oauth/overview/application-owner-access-tokens).

## Development

### 构建本地镜像

```
# 修改func.yaml文件，将name改成你自己的镜像名称。
# build it

fn build

```
### 本地测试
```
fn run
```

### 上传到镜像仓库

```
docker push <镜像名>
```

## 在平台运行

### 首先设置必须的环境变量

```
# Set your Function server address
# Eg. api.faas.pro

FUNCAPI=YOUR_FUNCTIONS_ADDRESS

# 以下信息在 apps.twitter.com 申请和获取 (Requirements)[#Requirements]
CUSTOMER_KEY="XXXXXX"
CUSTOMER_SECRET="XXXXXX"
ACCESS_TOKEN="XXXXXX"
ACCESS_SECRET="XXXXXX"
```

### Running with Functions

创建应用

```
curl -X POST --data '{
    "app": {
        "name": "twitter",
        "config": { 
            "CUSTOMER_KEY": "'$CUSTOMER_KEY'",
            "CUSTOMER_SECRET": "'$CUSTOMER_SECRET'", 
            "ACCESS_TOKEN": "'$ACCESS_TOKEN'",
            "ACCESS_SECRET": "'$ACCESS_SECRET'"
        }
    }
}' http://$FUNCAPI/v1/apps
```

创建路由

```
curl -X POST --data '{
    "route": {
        "image": "<镜像名>",
        "path": "/tweets",
    }
}' http://$FUNCAPI/v1/apps/twitter/routes
```

#### 云端运行试试？

```
curl -X POST --data '{"username": "zengqingguo"}' http://$FUNCAPI/r/twitter/tweets
```This repository holds supplementary Go networking libraries.

To submit changes to this repository, see http://golang.org/doc/contribute.html.
The *.dat files in this directory are copied from The WebKit Open Source
Project, specifically $WEBKITROOT/LayoutTests/html5lib/resources.
WebKit is licensed under a BSD style license.
http://webkit.org/coding/bsd-license.html says:

Copyright (C) 2009 Apple Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

These test cases come from
http://www.w3.org/International/tests/repository/html5/the-input-byte-stream/results-basics

Distributed under both the W3C Test Suite License
(http://www.w3.org/Consortium/Legal/2008/04-testsuite-license)
and the W3C 3-clause BSD License
(http://www.w3.org/Consortium/Legal/2008/03-bsd-license).
To contribute to a W3C Test Suite, see the policies and contribution
forms (http://www.w3.org/2004/10/27-testcases).
This is a work-in-progress HTTP/2 implementation for Go.

It will eventually live in the Go standard library and won't require
any changes to your code to use.  It will just be automatic.

Status:

* The server support is pretty good. A few things are missing
  but are being worked on.
* The client work has just started but shares a lot of code
  is coming along much quicker.

Docs are at https://godoc.org/golang.org/x/net/http2

Demo test server at https://http2.golang.org/

Help & bug reports welcome!

Contributing: https://golang.org/doc/contribute.html
Bugs:         https://golang.org/issue/new?title=x/net/http2:+
# h2i

**h2i** is an interactive HTTP/2 ("h2") console debugger. Miss the good ol'
days of telnetting to your HTTP/1.n servers? We're bringing you
back.

Features:
- send raw HTTP/2 frames
 - PING
 - SETTINGS
 - HEADERS
 - etc
- type in HTTP/1.n and have it auto-HPACK/frame-ify it for HTTP/2
- pretty print all received HTTP/2 frames from the peer (including HPACK decoding)
- tab completion of commands, options

Not yet features, but soon:
- unnecessary CONTINUATION frames on short boundaries, to test peer implementations 
- request bodies (DATA frames)
- send invalid frames for testing server implementations (supported by underlying Framer)

Later:
- act like a server

## Installation

```
$ go get golang.org/x/net/http2/h2i
$ h2i <host>
```

## Demo

```
$ h2i
Usage: h2i <hostname>
  
  -insecure
        Whether to skip TLS cert validation
  -nextproto string
        Comma-separated list of NPN/ALPN protocol names to negotiate. (default "h2,h2-14")

$ h2i google.com
Connecting to google.com:443 ...
Connected to 74.125.224.41:443
Negotiated protocol "h2-14"
[FrameHeader SETTINGS len=18]
  [MAX_CONCURRENT_STREAMS = 100]
  [INITIAL_WINDOW_SIZE = 1048576]
  [MAX_FRAME_SIZE = 16384]
[FrameHeader WINDOW_UPDATE len=4]
  Window-Increment = 983041
  
h2i> PING h2iSayHI
[FrameHeader PING flags=ACK len=8]
  Data = "h2iSayHI"
h2i> headers
(as HTTP/1.1)> GET / HTTP/1.1
(as HTTP/1.1)> Host: ip.appspot.com
(as HTTP/1.1)> User-Agent: h2i/brad-n-blake
(as HTTP/1.1)>  
Opening Stream-ID 1:
 :authority = ip.appspot.com
 :method = GET
 :path = /
 :scheme = https
 user-agent = h2i/brad-n-blake
[FrameHeader HEADERS flags=END_HEADERS stream=1 len=77]
  :status = "200"
  alternate-protocol = "443:quic,p=1"
  content-length = "15"
  content-type = "text/html"
  date = "Fri, 01 May 2015 23:06:56 GMT"
  server = "Google Frontend"
[FrameHeader DATA flags=END_STREAM stream=1 len=15]
  "173.164.155.78\n"
[FrameHeader PING len=8]
  Data = "\x00\x00\x00\x00\x00\x00\x00\x00"
h2i> ping  
[FrameHeader PING flags=ACK len=8]  
  Data = "h2i_ping"  
h2i> ping  
[FrameHeader PING flags=ACK len=8]
  Data = "h2i_ping"
h2i> ping
[FrameHeader GOAWAY len=22]
  Last-Stream-ID = 1; Error-Code = PROTOCOL_ERROR (1)

ReadFrame: EOF
```

## Status

Quick few hour hack. So much yet to do. Feel free to file issues for
bugs or wishlist items, but [@bmizerany](https://github.com/bmizerany/)
and I aren't yet accepting pull requests until things settle down.


Client:
 -- Firefox nightly with about:config network.http.spdy.enabled.http2draft set true
 -- Chrome: go to chrome://flags/#enable-spdy4, save and restart (button at bottom)

Make CA:
$ openssl genrsa -out rootCA.key 2048
$ openssl req -x509 -new -nodes -key rootCA.key -days 1024 -out rootCA.pem
... install that to Firefox

Make cert:
$ openssl genrsa -out server.key 2048
$ openssl req -new -key server.key -out server.csr
$ openssl x509 -req -in server.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server.crt -days 500


This is a fork of the encoding/xml package at ca1d6c4, the last commit before
https://go.googlesource.com/go/+/c0d6d33 "encoding/xml: restore Go 1.4 name
space behavior" made late in the lead-up to the Go 1.5 release.

The list of encoding/xml changes is at
https://go.googlesource.com/go/+log/master/src/encoding/xml

This fork is temporary, and I (nigeltao) expect to revert it after Go 1.6 is
released.

See http://golang.org/issue/11841
# Docker ECR

Dockerfile with the tools needed to create and release a Docker image to ECR.

https://hub.docker.com/r/stead/docker-ecr
# hub.docker.com/tiredofit/piwik

# Introduction

Dockerfile to build a [piwik](https://www.piwik.org/) container image. It is an Open Source Google Analytics Replacement.

It will automatically download the latest Piwik release upon build, and if you have set correct environment variables, will autocreate a database if you wish.

This Container uses Alpine:Edge as a base.
Additional Components are PHP7 w/ APC, OpCache, LDAP Support - MySQL Client is also available.


[Changelog](CHANGELOG.md)

# Authors

- [Dave Conroy](https://github.com/tiredofit)

# Table of Contents

- [Introduction](#introduction)
    - [Changelog](CHANGELOG.md)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Database](#database)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
   - [References](#references)

# Prerequisites

This image relies on an external MySQL Server, external SMTP Server, external Redis Server and is meant to be run behind a reverse SSL Proxy such as nginx-proxy.


# Installation

Automated builds of the image are available on [Docker Hub](https://hub.docker.com/tiredofit/piwik) and is the recommended method of installation.


```bash
docker pull hub.docker.com/tiredofit/piwik
```

# Quick Start

* The quickest way to get started is using [docker-compose](https://docs.docker.com/compose/). See the examples folder for a working [docker-compose.yml](examples/docker-compose.yml) that can be modified for development or production use.

* Set various [environment variables](#environment-variables) to understand the capabilities of this image.
* Map [persistent storage](#data-volumes) for access to configuration and data files for backup.

# Configuration

### Data-Volumes

The following directories are used for configuration and can be mapped for persistent storage.

| Directory | Description |
|-----------|-------------|
| `/www/piwik` | Root piwik Directory |
| `/www/logs` | Nginx and php-fpm logfiles |

### Database

This container requires the usage of an external database. Set one up accordingly.

```sql
CREATE USER 'piwik'@'%.%.%.%' IDENTIFIED BY 'password';
CREATE DATABASE IF NOT EXISTS `piwik` DEFAULT CHARACTER SET `utf8` COLLATE `utf8_unicode_ci`;
GRANT ALL PRIVILEGES ON `piwik`.* TO 'piwik'@'%.%.%.%';
```

### Environment Variables

Along with the Environment Variables from the [Base image](https://hub.docker.com/r/tiredofit/alpine), and the [Nginx+PHP-FPM Engine](https://hub.docker.com/r/tiredofit/nginx-php-fpm) below is the complete list of available options that can be used to customize your installation.


| Parameter | Description |
|-----------|-------------|
| `DB_HOST` | MySQL external container hostname (e.g. piwik1-db)
| `DB_NAME` | MySQL database name i.e. (e.g. piwik)
| `DB_USER` | MySQL username for database (e.g. piwik)
| `DB_PASS` | MySQL password for database (e.g. userpassword)


### Networking

The following ports are exposed.

| Port      | Description |
|-----------|-------------|
| `80` | HTTP |

# Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it (whatever your container name is e.g. piwik) bash
```

# References

* https://www.piwik.org

# hub.docker.com/tiredofit/backuppc

# Introduction

Dockerfile to build a [BackupPC](https://backuppc.sourceforge.net/) 4.x (stable) container image.

This Container uses [Alpine 3.5](http://www.alpinelinux.org) as a base, along with served via LIghttpd.

[Changelog](CHANGELOG.md)

# Authors

- [Dave Conroy](https://github.com/tiredofit)

# Table of Contents

- [Introduction](#introduction)
    - [Changelog](CHANGELOG.md)
- [Prerequisites](#prerequisites)
- [Dependencies](#dependendcies)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
    - [Additional](#additional)   
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
- [References](#references)


# Prerequisites

None

# Dependencies

Make sure there is adequate storage available to perform deduplicated backups!

# Installation

Automated builds of the image are available on [Docker Hub](https://tiredofit/backuppc) and is the recommended method of installation.


```bash
docker pull tiredofit/backuppc
```

# Quick Start

* The quickest way to get started is using [docker-compose](https://docs.docker.com/compose/). See the examples folder for a working [docker-compose.yml](examples/docker-compose.yml) that can be modified for development or production use.

* Set various [environment variables](#environment-variables) to understand the capabilities of this image.
* Map [persistent storage](#data-volumes) for access to configuration and data files for backup.

Start openldap-fusiondirectory using:

```bash
docker-compose up
```

Point your browser to `https://YOURHOSTNAME

__NOTE__: It is highly recommended this be run through a SSL proxy, with authentication, as by default there is none required!

## Data-Volumes

The following directories are used for configuration and can be mapped for persistent storage.

| Directory | Description |
|-----------|-------------|
| `/var/lib/backuppc` | The backed up Data |
| `/etc/backuppc` | Configuration Files |
| `/home/backuppc` | Home Directory for Backuppc (SSH Keys) |
| `/www/logs` | Logfiles for Lighttpd, Supervisord, BackupPC, Zabbix |



## Environment Variables

Along with the Environment Variables from the [Base image](https://hub.docker.com/r/tiredofit/alpine), below is the complete list of available options that can be used to customize your installation.

| Variable | Description |
|-----------|-------------|
| `BACKUPPC_ADMIN_USER` | The Admin User for Logging in |
| `BACKUPPC_ADMIN_PASS` | The Admin Pass for Logging in |
| `BACKUPPC_UUID` | The uid for the backuppc user e.g. 10000 |
| `BACKUPPC_GUID` | The gid for the backuppc user e.g. 10000 |

## Networking

The following ports are exposed and available to public interfaces

| Port | Description |
|-----------|-------------|
| `80` | HTTP |

__NOTE__: It is highly recommended this be run through a SSL proxy, or via localhost and tunnel via SSH.

## Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it backuppc bash
```

# References

* http://backuppc.sourceforge.net/

# hub.docker.com/tiredofit/tinc

# Introduction

Dockerfile to build a [tinc](https://www.tinc.org/) container image.

* Latest Release automatically downloaded and compiled (1.1 test series)
* Automatically downloads peer configuration files from git server based on network name.
* Configurable Options for resyncing information from git server
* Configurable Options to enable various types of compression or enable debugging for troubleshooting.
* Logrotate Installed and will rotate logs daily and hold for 7 days.

** Do NOT use a public git server to host your repository, as it will reveal personal details of your network! You have been warned **

This Container uses Alpine:Edge as a base.

[Changelog](CHANGELOG.md)

# Authors

- [Dave Conroy](https://github.com/tiredofit)

# Table of Contents

- [Introduction](#introduction)
    - [Changelog](CHANGELOG.md)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
    - [Data Volumes](#data-volumes)
    - [Environment Variables](#environmentvariables)   
    - [Networking](#networking)
- [Maintenance](#maintenance)
    - [Shell Access](#shell-access)
   - [References](#references)

# Prerequisites

This image relies on a private Git Repository to store configuration data. Create a private repo and user account in git before proceeding.


# Installation

Automated builds of the image are available on [Docker Hub](https://hub.docker.com/tiredofit/tinc) and is the recommended method of installation.


```bash
docker pull tiredofit/tinc
```

# Quick Start

* The quickest way to get started is using [docker-compose](https://docs.docker.com/compose/). See the examples folder for a working [docker-compose.yml](examples/docker-compose.yml) that can be modified for development or production use.

* Set various [environment variables](#environment-variables) to understand the capabilities of this image.
* Map [persistent storage](#data-volumes) for access to configuration and data files for backup.
* Alter Firewall Configuration to allow access to [network ports](#networking)

# Configuration

### Data-Volumes

The following directories are used for configuration and can be mapped for persistent storage.

| Directory | Description |
|-----------|-------------|
| `/etc/tinc/` | Root tinc Directory |



### Environment Variables

Along with the Environment Variables from the [Base image](https://hub.docker.com/r/tiredofit/alpine), below is the complete list of available options that can be used to customize your installation.

| Parameter | Description |
|-----------|-------------|
| `GIT_URL` | GIT Repository URL (ie `https://github.com/username/repo`)
| `GIT_USER` | Username to Authenticate to git server (e.g. `username`) |
| `GIT_PASS` | Password for above user (e.g. `password`) |
| `CRON_PERIOD` | Adjustable time to check GIT Server for any updates (Default: `5`) |
| `DEBUG` | Adjustable Debug level as per tinc documentation (e.g 5 Deault: `0`) |
| `NETWORK` | The VPN name -  (e.g. `securenetwork`) |
| `NODE` | The unique hostname of the machine joining the VPN (e.g. `hostname`) |
| `PUBLIC_IP` | The public IP you wish to listen on (e.g. `137.233.212.121`) |
| `PRIVATE_IP` | The private IP that is assigned to this machine on the VPN (e.g. `172.16.23.13`) |
| `INTERFACE` | Which Interface to use (relies on /dev/tun) (e.g. `tun0`) |
| `PEERS` | Which server should be used to contact first to create the mesh VPN (e.g. `host1.hostname.com` `host2.hostname.com`) |
| `COMPRESSION` | Level of LZO Compression (e.g. 9) (Default: `1`) |


### Networking

The following ports are exposed.

| Port      | Description |
|-----------|-------------|
| `655` | Tinc |

> **NOTE**: You must also allow capabilities for `NET_ADMIN` to docker to be able to have access to the IP Stack. Also, you must create `/dev/tun` as a device. If you want to make the Docker Host be able to be accessible you also must add `network:host` as an option otherwise only the containers will be accessible. See the working docker-compose.yml example as shown above.


# Maintenance
#### Shell Access

For debugging and maintenance purposes you may want access the containers shell. 

```bash
docker exec -it (whatever your container name is e.g. tinc) bash
```

# References

* https://www.tinc-vpn.org


[![Docker Stars](https://img.shields.io/docker/stars/frolvlad/alpine-rust.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-rust/)
[![Docker Pulls](https://img.shields.io/docker/pulls/frolvlad/alpine-rust.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-rust/)


Rust Docker image
=================

This image is based on Alpine Linux image, which is only a 5MB image, and contains
[Rust](https://www.rust-lang.org/) with [GCC](https://gcc.gnu.org/).

This image download size is:

[![](https://badge.imagelayers.io/frolvlad/alpine-rust.svg)](https://imagelayers.io/?images=frolvlad/alpine-rust 'Get your own badge on imagelayers.io')


Usage Example
-------------

```bash
$ echo -e 'fn main() { println!("Hello world"); }' > qq.rs
$ docker run --rm -v "$(pwd):/tmp" --workdir /tmp frolvlad/alpine-rust rustc -C target-feature=+crt-static ./qq.rs
```

Once you have run these commands you will have `qq` executable in your current directory and if you
execute it, you will get printed 'Hello World'!
[![Docker Stars](https://img.shields.io/docker/stars/frolvlad/alpine-mono.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-mono/)
[![Docker Pulls](https://img.shields.io/docker/pulls/frolvlad/alpine-mono.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-mono/)


Mono (C#) Docker image
======================

This image is based on Alpine Linux image, which is only a 5MB image, and contains
[Mono](http://www.mono-project.com/).

Download size of this image is only:

[![](https://images.microbadger.com/badges/image/frolvlad/alpine-mono.svg)](http://microbadger.com/images/frolvlad/alpine-mono "Get your own image badge on microbadger.com")

WARNING: This mono was compiled for Arch Linux and is ported to Alpine via
[glibc hack](https://github.com/gliderlabs/docker-alpine/issues/11)! Ideally,
Mono package should be added to Alpine repo, but I don't have much time now.


Usage Example
-------------

```bash
$ echo 'using System; class MainClass { public static void Main (string[] args) { Console.WriteLine ("Hello World"); } }' > qq.mono
$ docker run --rm -v "$(pwd)":/mnt frolvlad/alpine-mono sh -c "mcs -out:/mnt/qq.exe /mnt/qq.mono && mono /mnt/qq.exe"
```

Once you have run these commands you will have `qq.exe` mono-executable in your
current directory, and you will get printed 'Hello World' from Mono!
# alpine-python

[![Docker Stars](https://img.shields.io/docker/stars/jfloff/alpine-python.svg)][hub]
[![Docker Pulls](https://img.shields.io/docker/pulls/jfloff/alpine-python.svg)][hub]

[hub]: https://hub.docker.com/r/jfloff/alpine-python/

A small Python Docker image based on [Alpine Linux](http://alpinelinux.org/).

<!-- MDTOC maxdepth:6 firsth1:0 numbering:0 flatten:0 bullets:1 updateOnSave:1 -->

- [Supported tags](#supported-tags)   
- [Why?](#why)   
- [Details](#details)   
- [Usage](#usage)   
- [Usage of onbuild images](#usage-of-onbuild-images)   
- [Usage of slim images](#usage-of-slim-images)   
   - [Via `docker run`](#via-docker-run)   
   - [Pip Dependencies](#pip-dependencies)   
   - [Run-Time Dependencies](#run-time-dependencies)   
   - [Build-Time Dependencies](#build-time-dependencies)   
   - [Creating Images](#creating-images)   
   - [Debugging](#debugging)   
- [License](#license)
- [TODO](#todo)

<!-- /MDTOC -->

## Supported tags
* **`2.7` ([2.7/Dockerfile](https://github.com/jfloff/alpine-python/blob/master/2.7/Dockerfile))**
* **`2.7-onbuild` ([2.7/onbuild/Dockerfile](https://github.com/jfloff/alpine-python/blob/master/2.7/onbuild/Dockerfile))**
* **`2.7-slim` ([2.7/slim/Dockerfile](https://github.com/jfloff/alpine-python/blob/master/2.7/slim/Dockerfile))**
* **`3.4` ([3.4/Dockerfile](https://github.com/jfloff/alpine-python/blob/master/3.4/Dockerfile))**
* **`3.4-onbuild` ([3.4/onbuild/Dockerfile](https://github.com/jfloff/alpine-python/blob/master/3.4/onbuild/Dockerfile))**
* **`3.4-slim` ([3.4/slim/Dockerfile](https://github.com/jfloff/alpine-python/blob/master/3.4/slim/Dockerfile))**
* **`latest` ([Dockerfile](https://github.com/jfloff/alpine-python/blob/master/3.4/Dockerfile))**
* **`latest-onbuild` ([Dockerfile](https://github.com/jfloff/alpine-python/blob/master/3.4/onbuild/Dockerfile))**
* **`latest-slim` ([Dockerfile](https://github.com/jfloff/alpine-python/blob/master/3.4/slim/Dockerfile))**

**NOTE:** `onbuild` images install the `requirements.txt` of your project from the get go. This allows you to cache your requirements right in the build. _Make sure you are in the same directory of your `requirements.txt` file_.

## Why?
The default docker python images are too [big](https://github.com/docker-library/python/issues/45), much larger than they need to be. Hence I built this simple image based on [docker-alpine](https://github.com/gliderlabs/docker-alpine), that has everything needed for the most common python projects - including `python3-dev` (which is not common in most minimal alpine python packages).

```
REPOSITORY                TAG             SIZE
jfloff/alpine-python      2.7-slim        52.86 MB
python                    2.7-slim        180.8 MB

jfloff/alpine-python      2.7             234.2 MB
python                    2.7             676.2 MB

jfloff/alpine-python      3.4-slim        110.4 MB
python                    3.4-slim        193.9 MB

jfloff/alpine-python      3.4             280 MB
python                    3.4             681.5 MB

jfloff/alpine-python      latest          248.8 MB
python                    3.5             685.4 MB

jfloff/alpine-python      latest-slim     79.11 MB
python                    3.5-slim        197.8 MB
```

Perhaps this could be even smaller, but I'm not an Alpine guru. **Feel free to post a PR.**

## Details
* Installs `build-base` and `python-dev`, allowing the use of more advanced packages such as `gevent`
* Installs `bash` allowing interaction with the container
* Just like the main `python` docker image, it creates useful symlinks that are expected to exist, e.g. `python3.4` > `python`, `pip2.7` > `pip`, etc.)
* Added `testing` and `community` repositories to Alpine's `/etc/apk/repositories` file

## Usage

This image runs `python` command on `docker run`. You can either specify your own command, e.g:
```shell
docker run --rm -ti jfloff/alpine-python python hello.py
```

You can also access `bash` inside the container:
```shell
docker run --rm -ti jfloff/alpine-python bash
```

## Usage of `onbuild` images

These images can be used to bake your dependencies into an image by extending the plain python images. To do so, create a custom `Dockerfile` like this:
```dockerfile
FROM jfloff/alpine-python:3.4-onbuild

# for a flask server
EXPOSE 5000
CMD python manage.py runserver
```

Don't forget to build that `Dockerfile`:
```shell
docker build --rm=true -t jfloff/app .

docker run --rm -t jfloff/app
```

Personally, I build an extended `Dockerfile` version (like shown above), and mount my specific application inside the container:
```shell
docker run --rm -v "$(pwd)":/home/app -w /home/app -p 5000:5000 -ti jfloff/app
```

## Usage of `slim` images

These images are very small to download, and can install requirements at run-time via flags. The install only happens the first time the container is run, and dependencies can be baked in (see Creating Images).

#### Via `docker run`
These images can be run in multiple ways. With no arguments, it will run `python` interactively:
```shell
docker run --rm -ti jfloff/alpine-python:2.7-slim
```

If you specify a command, they will run that:
```shell
docker run --rm -ti jfloff/alpine-python:2.7-slim python hello.py
```

#### Pip Dependencies
Pip dependencies can be installed by the `-p` switch, or a `requirements.txt` file.

If the file is at `/requirements.txt` it will be automatically read for dependencies. If not, use the `-P` or `-r` switch to specify a file.
```shell
# This runs interactive Python with 'simplejson' and 'requests' installed
docker run --rm -ti jfloff/alpine-python:2.7-slim -p simplejson -p requests

# Don't forget to add '--' after your dependencies to run a custom command:
docker run --rm -ti jfloff/alpine-python:2.7-slim -p simplejson -p requests -- python hello.py

# This accomplishes the same thing by mounting a requirements.txt in:
echo 'simplejson' > requirements.txt
echo 'requests' > requirements.txt
docker run --rm -ti \
  -v requirements.txt:/requirements.txt \
  jfloff/alpine-python:2.7-slim python hello.py

# This does too, but with the file somewhere else:
echo 'simplejson requests' > myapp/requirements.txt
docker run --rm -ti \
  -v myapp:/usr/src/app \
  jfloff/alpine-python:2.7-slim \
    -r /usr/src/app/requirements.txt \
    -- python /usr/src/app/hello.py
```

#### Run-Time Dependencies
Alpine package dependencies can be installed by the `-a` switch, or an `apk-requirements.txt` file.

If the file is at `/apk-requirements.txt` it will be automatically read for dependencies. If not, use the `-A` switch to specify a file.

You can also try installing some Python modules via this method, but it is possible for Pip to interfere if it detects a version problem.
```shell
# Unknown why you'd need to do this, but you can!
docker run --rm -ti jfloff/alpine-python:2.7-slim -a openssl -- python hello.py

# This installs libxml2 module faster than via Pip, but then Pip reinstalls it because Ajenti's dependencies make it think it's the wrong version.
docker run --rm -ti jfloff/alpine-python:2.7-slim -a py-libxml2 -p ajenti
```

#### Build-Time Dependencies
Build-time Alpine package dependencies (such as compile headers) can be installed by the `-b` switch, or a `build-requirements.txt` file. They will be removed after the dependencies are installed to save space.

If the file is at `/build-requirements.txt` it will be automatically read for dependencies. If not, use the `-B` switch to specify a file.

`build-base`, `linux-headers` and `python-dev` are always build dependencies, you don't need to include them.
```shell
docker run --rm -ti jfloff/alpine-python:2.7-slim \
  -p gevent \
  -p libxml2 \
  -b libxslt-dev \
  -b libxml-dev \
  -- python hello.py
```

#### Creating Images
Similar to the onbuild images, dependencies can be baked into a new image by using a custom `Dockerfile`, e.g:
```dockerfile
FROM jfloff/alpine-python:2.7-slim
RUN /entrypoint.sh \
  -p ajenti-panel \
  -p ajenti.plugin.dashboard \
  -p ajenti.plugin.settings \
  -p ajenti.plugin.plugins \
  -b libxml2-dev \
  -b libxslt-dev \
  -b libffi-dev \
  -b openssl-dev \
&& echo
CMD ["ajenti-panel"]
# you won't be able to add more dependencies later though-- see 'Debugging'
```

#### Debugging
The `/entrypoint.sh` script that manages dependencies in the slim images creates an empty file, `/requirements.installed`, telling the script not to install any dependencies after the container's first run. Removing this file will allow the script to work again if it is needed.

You can also access `bash` inside the container:
```shell
docker run --rm -ti jfloff/alpine-python:2.7-slim bash
```

## License
The code in this repository, unless otherwise noted, is MIT licensed. See the `LICENSE` file in this repository.

## TODO
At this moment with Alpine APK we are not able to install previous packages versions, i.e., its virtually impossible to provide multiple versions of Python. This is limits us to only provide the latest `python3` package that's available in Alpine APK.

While I was able to provide a 3.4 tag (due to existence of `python3.4` packages), this is a temporary fix. Ideally we would support a solution like the official `python` images does (see example [here](https://github.com/docker-library/python/blob/master/3.4/alpine/Dockerfile)), where the specific Python version is downloaded and compiled. Yet, I don't want to follow the pitfall of copy-pasting that solution, otherwise we end up with almost the same image. ***I'm requesting PRs on this issue, either by optimizing official solution, or other.***
# docker-scikit-learn
[![](https://images.microbadger.com/badges/image/smizy/scikit-learn.svg)](https://microbadger.com/images/smizy/scikit-learn "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/scikit-learn.svg)](https://microbadger.com/images/smizy/scikit-learn "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-scikit-learn.svg?style=svg&circle-token=0142f1f1188bf3bd4407cd860c1e8280f7315f60)](https://circleci.com/gh/smizy/docker-scikit-learn)

Python3 scikit-learn with Jupyter docker image based on alpine 


```
# run Jupyter Notebook container 
docker run  -p 8888:8888 -v $(pwd):/code  -d smizy/scikit-learn

# open browser
open http://$(docker-machine ip default):8888
```
[![Docker Stars](https://img.shields.io/docker/stars/frolvlad/alpine-python-machinelearning.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-python-machinelearning/)
[![Docker Pulls](https://img.shields.io/docker/pulls/frolvlad/alpine-python-machinelearning.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-python-machinelearning/)


Python Machine Learning tools Docker image
==========================================

This image is based on
[Alpine Linux Python 3.5 image](https://hub.docker.com/r/frolvlad/alpine-python3/),
which is only a 60MB image, and contains popular Machine Leaning tools:

* numpy
* pandas
* scipy
* scikit-learn

Download size of this image is only:

[![](https://images.microbadger.com/badges/image/frolvlad/alpine-python-machinelearning.svg)](http://microbadger.com/images/frolvlad/alpine-python-machinelearning "Get your own image badge on microbadger.com")


Usage Example
-------------

```bash
$ docker run --rm frolvlad/alpine-python-machinelearning python3 -c 'import numpy; print(numpy.arange(3))'
```

Once you have run this command you will get printed `array([0, 1, 2])` from Python!
# docker-kotlin

[Kotlin](http://kotlinlang.org) docker image based on alpine for local development.

```
# build
docker build -t local/kotlin .

# enter shell
docker run -it --rm -v $(pwd):/code -w /code local/kotlin sh

$ cat <<EOS > hello.kt
fun main(args: Array<String>) {
    println("Hello, World!")
}
EOS

$ kotlinc hello.kt -include-runtime -d hello.jar
$ kotlin hello.jar
Hello, World!
$ java -jar hello.jar
Hello, World!

```[![Docker Stars](https://img.shields.io/docker/stars/frolvlad/alpine-gxx.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-gxx/)
[![Docker Pulls](https://img.shields.io/docker/pulls/frolvlad/alpine-gxx.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-gxx/)


C/C++ (GCC) Docker image
========================

This image is based on Alpine Linux image, which is only a 5MB image, and contains
[C/C++ compilers](https://gcc.gnu.org/) (gcc/g++ packages).

Download size of this image is only:

[![](https://images.microbadger.com/badges/image/frolvlad/alpine-gxx.svg)](http://microbadger.com/images/frolvlad/alpine-gxx "Get your own image badge on microbadger.com")


Usage Example
-------------

```bash
$ echo -e '#include <iostream>\nint main() { std::cout << "Hello World\\n"; }' > qq.cpp
$ docker run --rm -v `pwd`:/tmp frolvlad/alpine-gxx c++ --static /tmp/qq.cpp -o /tmp/qq
```

Once you have run these commands you will have `qq` executable in your current directory and if you
execute it, you will get printed 'Hello World'!
![NSRL logo](https://raw.githubusercontent.com/blacktop/docker-nsrl/master/logo.png)

NSRL Dockerfile
===============

[![CircleCI](https://circleci.com/gh/blacktop/docker-nsrl.png?style=shield)](https://circleci.com/gh/blacktop/docker-nsrl)
[![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org) [![Docker Stars](https://img.shields.io/docker/stars/blacktop/nsrl.svg)](https://hub.docker.com/r/blacktop/nsrl/) [![Docker Pulls](https://img.shields.io/docker/pulls/blacktop/nsrl.svg)](https://hub.docker.com/r/blacktop/nsrl/) [![Docker Image](https://img.shields.io/badge/docker image-141.7 MB-blue.svg)](https://hub.docker.com/r/blacktop/nsrl/)

This takes the **5.43GB** NSRL minimal set and converts it into a **96M** [bloom filter](https://en.wikipedia.org/wiki/Bloom_filter).

This repository contains a **Dockerfile** of the [NSRL Database](http://www.nsrl.nist.gov/Downloads.htm).

### Dependencies

-	[gliderlabs/alpine:3.4](https://index.docker.io/_/gliderlabs/alpine/)

### Image Tags

```bash
$ docker images

REPOSITORY          TAG                 VIRTUAL SIZE
blacktop/nsrl       latest              142 MB
blacktop/nsrl       sha1                142 MB
blacktop/nsrl       name                142 MB
blacktop/nsrl       error_0.001         192 MB
```

> **NOTE:** There are **3** other versions of this image:
- [sha1](https://github.com/blacktop/docker-nsrl/tree/sha1) tag allows you to search the NSRL DB by **sha-1** hash  
- [name](https://github.com/blacktop/docker-nsrl/tree/name) tag allows you to search the NSRL DB by **filename**  
- [error_0.001](https://github.com/blacktop/docker-nsrl/tree/error_0.001) tag searches by **md5** hash and has a much **lower error_rate** threshold. It does, however, grow the size of the bloomfilter by 50MB.  

### Installation

1.	Install [Docker](https://docs.docker.com).
2.	Download [trusted build](https://hub.docker.com/r/blacktop/nsrl/) from public [Docker Registry](https://hub.docker.com/): `docker pull blacktop/nsrl`

### Getting Started

```bash
$ docker run --rm blacktop/nsrl
```

```
usage: blacktop/nsrl [-h] [-v] MD5 [MD5 ...]

positional arguments:
  MD5            a md5 hash to search for.

optional arguments:
  -h, --help     show this help message and exit
  -v, --verbose  Display verbose output message
```

### Documentation

#### Usage Example (with `-v` option):

```bash
$ docker run --rm blacktop/nsrl -v 60B7C0FEAD45F2066E5B805A91F4F0FC
```

```bash
Hash 60B7C0FEAD45F2066E5B805A91F4F0FC found in NSRL Database.
```

#### To read from a **hash-list** file:

```bash
$ cat hash-list.txt
60B7C0FEAD45F2066E5B805A91F4F0FC
AABCA0896728846A9D5B841617EBE746
AABCA0896728846A9D5B841617EBE745

$ cat hash-list.txt | xargs docker run --rm blacktop/nsrl
True
True
False
```

#### Optional Build Options

You can use different **NSRL databases** or **error-rates** for the bloomfilter (*which will increase it's accuracy*\)

1.	To use your own [NSRL](http://www.nsrl.nist.gov/Downloads.htm) database simply download the ZIP and place it in the `nsrl` folder and build the image like so: `docker build -t my_nsrl .`
2.	To decrease the error-rate of the bloomfilter simply change the value of `ERROR_RATE` in the file `nsrl/shrink_nsrl.sh` and build as above.

##### Use **blacktop/nsrl** like a host binary

Add the following to your bash or zsh profile

```bash
alias nsrl='docker run --rm blacktop/nsrl $@'
```

### Issues

Find a bug? Want more features? Find something missing in the documentation? Let me know! Please don't hesitate to [file an issue](https://github.com/blacktop/docker-nsrl/issues/new)

### Credits

Inspired by https://github.com/bigsnarfdude/Malware-Probabilistic-Data-Structres

### CHANGELOG

See [`CHANGELOG.md`](https://github.com/blacktop/docker-nsrl/blob/master/CHANGELOG.md)

### Contributing

[See all contributors on GitHub](https://github.com/blacktop/docker-nsrl/graphs/contributors).

Please update the [CHANGELOG.md](https://github.com/blacktop/docker-nsrl/blob/master/CHANGELOG.md) and submit a [Pull Request on GitHub](https://help.github.com/articles/using-pull-requests/).

### License

MIT Copyright (c) 2014-2016 **blacktop**
# About
 **ops-bot** helps teams to implement Chat Ops on Spark & Hubot.
 It would assist me in doing ops work

## Minimal requirements
* Ensure you are connected to the Internet (required to download open source libraries etc.)
* Ensure you have a valid [Cisco Spark Account](https://web.ciscospark.com/)


## Prerequisites
The developer environment relies on the components shown below.
This project is tested with the following versions see below.

Local Laptop
---

| S.No | Software            | Version | 
|------|---------------------|---------|
|  1   | [Node]              | v6.10.3 | 
|  2   | [NPM]               | v4.2.0  | 
|  3   | [Git Client]        | v2.13.0 | 
|  3   | [ngrok]             | v2.2.3  | 


## Phase-1 : Preparation
0. Setup Workspace
	```
	mkdir -p ~/Workspace;cd Workspace;git clone https://github.com/rajasoun/ops-bot;cd ops-bot;npm install;cp app.env.sample app.env
	```
1. Add Bot in Cisco [Spark](https://developer.ciscospark.com/add-bot.html)
2. Start [Ngrok] in Port 8080 with predefined public url. 
  ```
    ngrok http 8080 --subdomain spark-ops-bot
  ```
3. Edit app.env with [Bot's Details](https://developer.ciscospark.com/apps.html#)  Access token,Bot ID, Webhook URL, 
    #### Customize Following Values 
    - BOT_NAME   
    - SPARK_ROOM_NAME
    - SPARK_WEBHOOK_NAME
   #### Fill in Following Values
    - BOT_ID
    - SPARK_ACCESS_TOKEN 
    - HUBOT_SPARK_ACCESS_TOKEN
    - WEBHOOK_URL   - NGROK URL
4. Run the Setup Script to Create a Room, Add teh Bot to the Room & Create WebHook
  ```
    ./ops-bot.sh setup
  ```
5. Run the Setup Script to check Room, Webhook are created
  ```
    ./ops-bot.sh log
  ```  
6. Open Spark - You should see the Room with the Bot   
  
## Phase-2 : Start Hubot
  ```
    ./ops-bot.sh start
  ```

## Phase-3 : Teardown
  ```
    ./ops-bot.sh teardown
  ```
  
[Node]: https://nodejs.org/en/
[NPM]: https://www.npmjs.com/
[Git Client]: https://git-scm.com/downloads
[ngrok]: https://ngrok.com/download


# searx admin

A web based management interface for searx.

![engineslist](docs/images/engineslist.png)

## Features

- Edit configuration
- Update from remote repository
- Manage process (start, stop, reload)

## Installation & usage

### Install dependencies

Searx-admin depends on `git` and `uwsgi` tools and implemented in python.

Please make sure that dependencies of searx are installed in the same virtualenv or on the
same host searx admin is ran.

```
virtualenv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Edit config

Edit `admin/config.yml`


### Start application

```
source venv/bin/activate
python admin/webapp.py
```
![Krakend logo](docs/images/krakend.png)

# KrakenD

[![Travis-CI](https://travis-ci.org/devopsfaith/krakend.svg?branch=master)](https://travis-ci.org/devopsfaith/krakend) [![Go Report Card](https://goreportcard.com/badge/github.com/devopsfaith/krakend)](https://goreportcard.com/report/github.com/devopsfaith/krakend) [![Coverage Status](https://coveralls.io/repos/github/devopsfaith/krakend/badge.svg?branch=master)](https://coveralls.io/github/devopsfaith/krakend?branch=master) [![GoDoc](https://godoc.org/github.com/devopsfaith/krakend?status.svg)](https://godoc.org/github.com/devopsfaith/krakend)

Ultra performant API Gateway with middlewares

## Motivation

Consumers of REST API content (specially in microservices) often query backend services that weren't coded for the UI implementation. This is of course a good practice, but the UI consumers need to do implementations that suffer a lot of complexity and burden with the sizes of their microservices responses.

KrakenD is an **API Gateway** builder and proxy generator that sits between the client and all the source servers, adding a new layer that removes all the complexity to the clients, providing them only the information that the UI needs. KrakenD acts as an **aggregator** of many sources into single endpoints and allows you to group, wrap, transform and shrink responses. Additionally it supports a myriad of middelwares and plugins that allow you to extend the functionality, such as adding Oauth authorization or security layers.

KrakenD not only supports HTTP(S), but because it is a set of generic libraries you can build all type of API Gateways and proxies, including for instance, a RPC gateway.

### Practical Example

Fred Calamari is a mobile developer that needs to construct a single front page that requires data from several calls to their backend services, e.g:

    1) api.store.server/products
    2) api.store.server/marketing-promos
    3) api.users.server/users/{id_user}
    4) api.users.server/shopping-cart/{id_user}

The screen is very simple and _only_ needs to retrieve data from 4 different sources, wait for the round trip and then pick only a few fields of the response. Instead of thing these calls, the mobile could call a single endpoint to KrakenD:

    1) krakend.server/frontpage/{id_user}

And this is how it would look like:

![Gateway](docs/images/krakend-gateway.png)

The difference in size in this example would be because KrakenD server would have removed unneeded attributes from the responses.

## What's in this repository?
The source code on which the [KrakenD](http://www.krakend.io) service core is built on. It is designed to work with your own middleware and extend the functionality by using small, independent, reusable components following the Unix philosophy.

**This repository is only for those who want to build from source a Krakend service** or for those who will reuse any of the components in another application.

If you just want to use the server, please [download the binary for your architecture](http://www.krakend.io/download).


## Library Usage
Krakend is presented as a **go library** that you can include in your own go application to build a powerful proxy or API gateway. In order to get you started several examples of implementations are included in the `examples` folder.

Of course you will need [go installed](https://golang.org/doc/install) in your system to compile the code.
There is a `Makefile` in every example that will download library dependencies and compile a binary for you to test. Just run:

    $ cd examples/gin
    $ make

Or, if you want to build all the examples, from the root of the project

    $ make

For the lazy, a ready to use example:

    package main

    import (
        "flag"
        "log"
        "os"

        "github.com/devopsfaith/krakend/config/viper"
        "github.com/devopsfaith/krakend/logging/gologging"
        "github.com/devopsfaith/krakend/proxy"
        "github.com/devopsfaith/krakend/router/gin"
    )

    func main() {
        port := flag.Int("p", 0, "Port of the service")
        logLevel := flag.String("l", "ERROR", "Logging level")
        debug := flag.Bool("d", false, "Enable the debug")
        configFile := flag.String("c", "/etc/krakend/configuration.json", "Path to the configuration filename")
        flag.Parse()

        parser := viper.New()
        serviceConfig, err := parser.Parse(*configFile)
        if err != nil {
            log.Fatal("ERROR:", err.Error())
        }
        serviceConfig.Debug = serviceConfig.Debug || *debug
        if *port != 0 {
            serviceConfig.Port = *port
        }

        logger := gologging.NewLogger(*logLevel, os.Stdout, "[KRAKEND]")

        routerFactory := gin.DefaultFactory(proxy.DefaultFactory(logger), logger)

        routerFactory.New().Run(serviceConfig)
    }

Visit the [framework overview](/docs/OVERVIEW.md) for more details about the components of the KrakenD.

### Examples

1. [gin router](/examples/gin/README.md)
2. [mux router](/examples/mux/README.md)
3. [gorilla router](/examples/gorilla/README.md)
4. [negroni middlewares](/examples/negroni/README.md)
5. [dns srv service discovery](/examples/dns/README.md)
6. [rss backends](/examples/rss/README.md)
7. [jwt middlewares](/examples/jwt/README.md)
8. [httpcache based proxies](/examples/httpcache/README.md)
9. [etcd service discovery](/examples/httpcache/README.md)

## Configuration file

[KrakenD config file](/docs/CONFIG.md)

## Benchmarks

Check out the [benchmark results](/docs/BENCHMARKS.md) of several KrakenD components

## Contributing
We are always happy to receive contributions. If you have questions, suggestions, bugs please open an issue.
If you want to submit the code, create the issue and send us a pull request for review.

Enjoy the KrakenD!
Krakend MUX example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_mux_example
	Usage of ./krakend_mux_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service
Krakend GIN example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_gin_example
	Usage of ./krakend_gin_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service
Krakend DNS SRV example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_dns_example
	Usage of ./krakend_dns_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service
Krakend JWT example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI.

Notice this example starts a dedicated service just for issuing signed JWT.

	$ ./krakend_jwt_example
	Usage of ./krakend_jwt_example_92cc18c:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -cors-headers string
	    	Comma-separated list of CORS allowed headers (default "Origin,Authorization,Content-Type")
	  -cors-headers-exposed string
	    	Comma-separated list of CORS exposed headers (default "Content-Length")
	  -cors-methods string
	    	Comma-separated list of CORS allowed methods (default "HEAD,GET,POST,PUT,PATCH,DELETE")
	  -cors-origins string
	    	Comma-separated list of CORS allowed origins (default "http://example.com")
	  -cors-ttl duration
	    	Max age for the CORS layer (default 12h0m0s)
	  -d	Enable the debug
	  -hosts string
	    	Comma-separated list of allowed hosts (default "127.0.0.1:8080,example.com,ssl.example.com")
	  -jwt-issuer string
	    	Issuer for the jwt (default "http://example.com/")
	  -jwt-port int
	    	Port for the jwt generator api (default 8090)
	  -jwt-secret string
	    	Secret for signing jwt (default "KrakenDrulez123.4567890!")
	  -jwt-ttl duration
	    	Expiration for the JWT (default 1h0m0s)
	  -l string
	    	Logging level (default "ERROR")
	  -p int
	    	Port of the service
Krakend ETCD example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_etcd_example
	Usage of ./krakend_etcd_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -etcd string
	    	Comma-separated list of etcd servers (with port and schema) (default "http://192.168.99.100:4001")
	  -l string
	    	Logging level (default "ERROR")
	  -p int
	    	Port of the service
Krakend RSS example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_rss_example
	Usage of ./krakend_rss_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service

This is an example of a suitable json configuration

	{
	    "version": 1,
	    "name": "TV shows gateway",
	    "port": 8080,
	    "cache_ttl": 3600,
	    "timeout": "3s",
	    "endpoints": [
	        {
	            "endpoint": "/showrss/{id}",
	            "backend": [
	                {
	                    "host": [
	                        "http://showrss.info/"
	                    ],
	                    "url_pattern": "/user/schedule/{id}.rss",
	                    "encoding": "rss",
	                    "group": "schedule",
	                    "whitelist": ["items", "title"]
	                },
	                {
	                    "host": [
	                        "http://showrss.info/"
	                    ],
	                    "url_pattern": "/user/{id}.rss",
	                    "encoding": "rss",
	                    "group": "available",
	                    "whitelist": ["items", "title"]
	                }
	            ]
	        }
	    ]
	}

You can see the resulting output with this simple curl command (replace `<YOUR_SHOWRSS_USER_ID>` with your own id)

	$ curl -i http://127.0.0.1:8080/showrss/<YOUR_SHOWRSS_USER_ID>Krakend GORILLA example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_gorilla_example
	Usage of ./krakend_gorilla_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service
Krakend NEGRONI example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_negroni_example
	Usage of ./krakend_negroni_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service
Krakend httpcache example
====

Simple example of how to extend the basic `proxy.BackendFactory` in order to implement all kinds of features, like client-side http cache so the KrakenD is able to cache the responses from the backends and re-use them in other compositions.

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_httpcache_example
	Usage of ./krakend_httpcache_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service
![Krakend logo](images/krakend.png)

# KrakenD

## How to use it

Visit the [framework overview](/docs/OVERVIEW.md) for details about the components of the KrakenD.

### Examples

1. [gin router](/examples/gin/README.md)
2. [mux router](/examples/mux/README.md)

## Configuration file

[KrakenD config file](/docs/CONFIG.md).
 
## Benchmarks

Check out the [benchmark results](/docs/BENCHMARKS.md) of several KrakenD components.

## Contributing![KrakenD Playground logo](logo.png)

KrakenD Playground
====

The KrakenD Playground is a running environment with different versions of [KrakenD](http://wwww.krakend.io), commercial and [Open Source](https://github.com/devopsfaith/krakend), feeding from a demonstration API that you can change at your will.

You can expand this static API just by storing more XML or JSON files in the `data`
folder.

The KrakenD configuration is stored under `krakend/krakend.json` and you can
drag this file anytime to the [KrakenD designer](http://www.krakend.io/designer/) and resume the edition from there.

## Start!

In order to start all the services just run:

    docker-compose up

## Play!

Fire up your browser, curl, postman, httpie or anything else you like to interact with any of the following ports.

Different versions of KrakenD:

- Free version (based on commercial enterprise) runs in the port [8080](http://localhost:8080)
- Open source using **Gin** runs in the port [8081](http://localhost:8081)
- Open source using **Mux** runs in the port [8082](http://localhost:8082)
- Open source using **Gorilla** runs in the port [8083](http://localhost:8083)
- Open source using **Negroni** runs in the port [8084](http://localhost:8084)
- Open source using **Gin** + **JWT** runs in the port [8085](http://localhost:8085) (the token issuer is exposed here: http://localhost:8090/token/random_user_id)

The backend data ([LWAN](https://github.com/lpereira/lwan) server):

- All local datasource endpoints under port [8000](http://localhost:8000)

![KrakenD Playground logo](playground.jpg)

If you use `docker-machine` you will need to access the services using something like `http://192.168.99.100:PORT` instead of `http://localhost:PORT`.

## Editing the endpoints

Initially the different KrakenD gateways present the following endpoints:

* /splash composes responses from several local datasources
* /showrss/{id} composes responses from two RSS feeds
* /nick/{nick} composes responses from actual github and bitbucket api endpoints

To add more, edit the file `krakend/krakend.json`, the easiest way is to **drag this file anytime to the [KrakenD designer](http://www.krakend.io/designer/)** and download the resulting file.

To change the data in the static server (simulating your backend) edit, add or delete files in the **`data`** folder. 

## Available demos

### KrakenD Free

This demo uses the [KrakenD free version](https://hub.docker.com/r/devopsfaith/krakend/), and is limited to 1000rps and 2 backend endpoints per KrakenD endpoint (if you add more they are ignored)

	$ curl -i http://${DOCKER_IP}:8080/splash

### OS KrakenD Gin

This demo uses the [Gin example](https://github.com/devopsfaith/krakend/blob/master/examples/gin/main.go) from the KrakenD OS

	$ curl -i -H'Host: ssl.example.com' http://${DOCKER_IP}:8081/splash

### OS KrakenD Mux

This demo uses the [Mux example](https://github.com/devopsfaith/krakend/blob/master/examples/mux/main.go) from the KrakenD OS

	$ curl -i -H'Host: ssl.example.com' http://${DOCKER_IP}:8082/splash

### OS KrakenD Gorilla

This demo uses the [Gorilla example](https://github.com/devopsfaith/krakend/blob/master/examples/gorilla/main.go) from the KrakenD OS

	$ curl -i -H'Host: ssl.example.com' http://${DOCKER_IP}:8083/splash

### OS KrakenD Negroni

This demo uses the [Negroni example](https://github.com/devopsfaith/krakend/blob/master/examples/negroni/main.go) from the KrakenD OS

	$ curl -i -H'Host: ssl.example.com' http://${DOCKER_IP}:8084/splash

### OS KrakenD Gin + JWT

This demo uses the [JWT example](https://github.com/devopsfaith/krakend/blob/master/examples/jwt/main.go) from the KrakenD OS

	$ curl -i -H'Host: ssl.example.com' http://${DOCKER_IP}:8085/splash
	HTTP/1.1 401 Unauthorized
	Content-Security-Policy: default-src 'self'
	Strict-Transport-Security: max-age=315360000; includeSubdomains
	X-Content-Type-Options: nosniff
	X-Frame-Options: DENY
	X-Xss-Protection: 1; mode=block
	Date: Sat, 24 Jun 2017 12:19:14 GMT
	Content-Length: 0
	Content-Type: text/plain; charset=utf-8

If you want to access an endpoint on this service, you must add an `Authorization` header with a valid token issued by the dummy issuer. The following requests demonstrates how the flow works:

	$ curl -i http://$(docker-machine ip dev):8090/token/myUser
	HTTP/1.1 200 OK
	Content-Type: application/json; charset=utf-8
	Date: Sat, 24 Jun 2017 12:22:15 GMT
	Content-Length: 174

	{"token":"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJJZCI6Im15VXNlciIsImV4cCI6MTQ5ODMxMDUzNSwiaXNzIjoiaHR0cDovL2V4YW1wbGUuY29tLyJ9.YJgp2qLaPkQ0DVxqGAJ95RBL3e6rEMEY_L-jlqWNrxU"}

	$ curl -iH'Host: ssl.example.com' -H'Authorization: bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJJZCI6Im15VXNlciIsImV4cCI6MTQ5ODMxMDUzNSwiaXNzIjoiaHR0cDovL2V4YW1wbGUuY29tLyJ9.YJgp2qLaPkQ0DVxqGAJ95RBL3e6rEMEY_L-jlqWNrxU'  http://${DOCKER_IP}:8085/splash
	HTTP/1.1 200 OK
	Cache-Control: public, max-age=300
	Content-Security-Policy: default-src 'self'
	Content-Type: application/json; charset=utf-8
	Strict-Transport-Security: max-age=315360000; includeSubdomains
	X-Content-Type-Options: nosniff
	X-Frame-Options: DENY
	X-Krakend: Version undefined
	X-Xss-Protection: 1; mode=block
	Date: Sat, 24 Jun 2017 12:24:12 GMT
	Transfer-Encoding: chunked

## Add your demo endpoints and middleware integrations!

Do you want to add a new router? Just some other endpoints that might help others? Please sends a pull request!

Thanks!
Krakend RSS example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_rss_example
	Usage of ./krakend_rss_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service

This is an example of a suitable json configuration

	{
	    "version": 1,
	    "name": "TV shows gateway",
	    "port": 8080,
	    "cache_ttl": 3600,
	    "timeout": "3s",
	    "endpoints": [
	        {
	            "endpoint": "/showrss/{id}",
	            "backend": [
	                {
	                    "host": [
	                        "http://showrss.info/"
	                    ],
	                    "url_pattern": "/user/schedule/{id}.rss",
	                    "encoding": "rss",
	                    "group": "schedule",
	                    "whitelist": ["items", "title"]
	                },
	                {
	                    "host": [
	                        "http://showrss.info/"
	                    ],
	                    "url_pattern": "/user/{id}.rss",
	                    "encoding": "rss",
	                    "group": "available",
	                    "whitelist": ["items", "title"]
	                }
	            ]
	        }
	    ]
	}

You can see the resulting output with this simple curl command (replace `<YOUR_SHOWRSS_USER_ID>` with your own id)

	$ curl -i http://127.0.0.1:8080/showrss/<YOUR_SHOWRSS_USER_ID>Krakend ETCD example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_etcd_example
	Usage of ./krakend_etcd_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -etcd string
	    	Comma-separated list of etcd servers (with port and schema) (default "http://192.168.99.100:4001")
	  -l string
	    	Logging level (default "ERROR")
	  -p int
	    	Port of the serviceKrakend GIN example
====

## Build

Go 1.8 is a requirement

	$ make

## Run

Running it as a common executable, logs are send to the stdOut and some options are available at the CLI

	$ ./krakend_gin_example
	Usage of ./krakend_gin_example:
	  -c string
	    	Path to the configuration filename (default "/etc/krakend/configuration.json")
	  -d	Enable the debug
	  -p int
	    	Port of the service# StreamSets Data Collector

You must accept the [Oracle Binary Code License Agreement for Java SE](http://www.oracle.com/technetwork/java/javase/terms/license/index.html) to use this image.

The Docker image for Data Collector starting from version 2.4.1.0 now uses the form type of file-based authentication by default.
As a result, you must use a Data Collector user account to log in to the Data Collector.
If you haven't set up custom user accounts, you can use the admin account shipped with the Data Collector.
The default login is: admin / admin.
Earlier versions of the Docker image required no authentication.

Basic Usage
-----------
`docker run --restart on-failure -p 18630:18630 -d --name streamsets-dc streamsets/datacollector`

Detailed Usage
--------------
*   You can specify a custom configs by mounting them as a volume to /etc/sdc or /etc/sdc/<specific config>
*   Configuration properties in `sdc.properties` can also be overridden at runtime by specifying them env vars prefixed
    with SDC_CONF
*   For example http.port would be set as SDC_CONF_HTTP_PORT=12345
*   You *should at a minimum* specify a data volume for the data directory and stage libraries. The default configured
    location is /data for $SDC_DATA. You can override this location by passing a different value to the environment
    variable SDC_DATA. Creating a volume for additional stage libraries is described in more detail below.
*   You can also specify your own explicit port mappings, or arguments to the streamsets command.

For example to run with a customized sdc.properties file, a local filsystem path to store pipelines, and statically map
the default UI port you could use the following:

`docker run --restart on-failure -v $PWD/sdc.properties:/etc/sdc/sdc.properties:ro -v $PWD/sdc-data:/data:rw -p 18630:18630 -d streamsets/datacollector dc`

Creating a Data Volumes
-----------------------
To create a dedicated data volume for the pipeline store issue the following command:

`docker volume create --name sdc-data`

You can then use the `-v` (volume) argument to mount it when you start the data collector.

`docker run -v sdc-data:/data -P -d streamsets/datacollector dc`

**Note:** There are two different methods for managing data in Docker. The above is using *data volumes* which are empty
when created. You can also use *data containers* which are derived from an image. These are useful when you want to
modify and persist a path starting with existing files from a base container, such as for configuration files. We'll use
both in the example below. See [Manage data in containers](https://docs.docker.com/engine/tutorials/dockervolumes/) for
more detailed documentation.

Pre-configuring Data Collector
-----------------------------

#### Option 1 - Volumes (Recommended)

First we create a data container for our configuration. We'll call ours `sdc-conf`

`docker create -v /etc/sdc --name sdc-conf streamsets/datacollector`
`docker run --rm -it --volumes-from sdc-conf ubuntu bash`

**Tip:** You can substitute `ubuntu` for your favorite base image. This is only
a temporary container for editing the base configuration files.

Edit the configuration of SDC to your liking by modifying the files in `/etc/sdc`

You can choose to create separate data containers using the above procedure for
`$SDC_DATA` (`/data`) and other locations, or you can add all of the volumes to the
same container. For multiple volumes in a single data container you could use the following syntax:

`docker create -v /etc/sdc -v /data -v --name sdc-volumes streamsets/datacollector`

If you find it easier to edit the configuration files locally you can, instead
of starting the temporary container above, use the `docker cp` command to
copy the configuration files back and forth from the data container.

To install stage libs using the CLI or Package Manager UI you'll need to create a volume for the stage libs directory.
It's also recommended to use a volume for the data directory at a minimum.

`docker volume create --name sdc-stagelibs`
(If you didn't create a data container for `/data` then run the command below)
`docker volume create --name sdc-data`

The volume needs to then be mounted to the correct directory when launching the container. The example below is for
Data Collector version 2.2.1.0.

`docker run --name sdc -d -v sdc-stagelibs:/opt/streamsets-datacollector-2.2.1.0/streamsets-libs -v sdc-data:/data -P streamsets/datacollector dc -verbose`

To get a list of available libs you could do:

`docker run --rm streamsets/datacollector:2.2.1.0 stagelibs -list`

For example, to install the JDBC lib into the sdc-stagelibs volume you created above, you would run:

`docker run --rm -v sdc-stagelibs:/opt/streamsets-datacollector-2.2.1.0/streamsets-libs streamsets/datacollector:2.2.1.0 stagelibs -install=streamsets-datacollector-jdbc-lib`

#### Option 2 - Deriving a new image

One disadvantage of the first method is that we can't commit data in a volume
and distribute it via a docker registry. Instead we must create the volume,
backup the data, restore the data if we need to recreate or move the container
to another host.

This second option will allow us to make modifications to the original base
image, creating a new one which can be pushed to a docker registry and easily
distributed.

The simplest and recommended way is simply to create your own Dockerfile with
the official streamsets/datacollector image as the base! This provides a
repeatable process for building derived images.

For example this derived Dockerfile:

```
FROM streamsets/datacollector:2.2.1.0
# My custom configured sdc.properties
COPY sdc.properties /etc/sdc/sdc.properties
```

`docker build -t mycompany/datacollector:2.2.1.0-abc .`
`docker push mycompany/datacollector:2.2.1.0-abc`

I've now created a new image with a customized sdc.properties file and
am able to distribute it from a docker registry with ease!

You can also launch a default container, modify it while it is running and
use the `docker commit` command, but this isn't recommended.
# QubeStash / Jenkins

[![TravisCI Status Widget]][TravisCI Status] [![Coverage Status Widget]][Coverage Status]

[TravisCI Status]: https://travis-ci.org/qubestash/jenkins
[TravisCI Status Widget]: https://travis-ci.org/qubestash/jenkins.svg?branch=master
[Coverage Status]: https://coveralls.io/r/qubestash/jenkins
[Coverage Status Widget]: https://coveralls.io/repos/github/qubestash/jenkins/badge.svg?branch=master

Based on official docker [jenkins](https://hub.docker.com/_/jenkins/) image. Please read full documentation before running 
this image. 

## Supported tags

* `2.32.1`, `latest` (use `make build`)
* `2.32.1-alpine`, `alpine` (use `make build-alpine`)

## Additional Environment Variables

### JENKINS_THEME_MATERIAL_COLOR

> Default value: `deep-purple`

Will allow Jenkins to load [Material Theme](http://afonsof.com/jenkins-material-theme/) with a certain color.

> Please note theme may only be seen at second docker image restart. Otherwise, please use them install instructions.

### JENKINS_INSTALL_PLUGINS 

> Default value: ``
# docker-fasttext

fastText command docker image based on alpine

```
# run command (echo usage)
docker run  -it --rm smizy/fasttext fasttext
```
# docker-word2vec

word2vec command docker image based on alpine

```
# run command (echo usage)
docker run  -it --rm smizy/word2vec word2vec


```
# docker-gensim
[![](https://images.microbadger.com/badges/image/smizy/gensim.svg)](https://microbadger.com/images/smizy/gensim "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/gensim.svg)](https://microbadger.com/images/smizy/gensim "Get your own version badge on microbadger.com")
<!--[![CircleCI](https://circleci.com/gh/smizy/docker-gensim.svg?style=svg&circle-token=2b58baee340c55eef2cbece97ab66da62f5bef7e)](https://circleci.com/gh/smizy/docker-gensim)-->

Gensim docker image based on alpine

## run jupyter notebook
docker run -it --rm  -p 8888:8888 -v $(pwd)/data:/code  smizy/gensim:1.0-alpine# docker-sentencepiece

[Google SentencePiece](https://github.com/google/sentencepiece) docker image based on alpine

## End-to-End Example

```
docker run -it --rm smizy/sentencepiece sh

$ spm_train --input=data/botchan.txt --model_prefix=m --vocab_size=1000

$ echo "I saw a girl with a telescope." | spm_encode --model=m.model
▁I ▁saw ▁a ▁girl ▁with ▁a ▁ te le s c o pe .

$ echo "I saw a girl with a telescope." | spm_encode --model=m.model --output_format=id
9 459 11 939 44 11 4 142 82 8 28 21 132 6

$ echo "9 459 11 939 44 11 4 142 82 8 28 21 132 6" | spm_decode --model=m.model --input_format=id
I saw a girl with a telescope.
```[![Build Status](https://travis-ci.org/deepcortex/dockers.svg?branch=master)](https://travis-ci.org/deepcortex/dockers)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/4e7b52b9b6ae4ae9be6c5eb6c6a0db76)](https://www.codacy.com/app/ssemichev_2/dockers?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=deepcortex/dockers&amp;utm_campaign=Badge_Grade)

## Image inheritance diagram

[![Image inheritance diagram](internal/inherit-diagram.png)](http://interactive.blockdiag.com/?compression=deflate&src=eJyFjk0OwiAQhfc9xVyAlRuTRq9ihpZaKmUqHUys8e5Cg4Gk_iy_9z4eIw01l1bjGR4VADmtLCNrsnCAiRw71FyHplUdesOnjizPelGh3sd8wBsKL71lD-IIc4PmzbEuOfaLcjRexRrnHs2krVrfS04U29LO69Ode7K7T0LJyfu-434tSM7npQ83F4jRbKSQRS-TGHwA5f6ZfdvNdfV8AfwShrs)

# dockers
A set of docker images

## dcos-rabbitmq
[![](https://images.microbadger.com/badges/image/deepcortex/dcos-rabbitmq.svg)](https://microbadger.com/images/deepcortex/dcos-rabbitmq "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/deepcortex/dcos-rabbitmq.svg)](https://microbadger.com/images/deepcortex/dcos-rabbitmq "Get your own version badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/dcos-rabbitmq.svg)](https://microbadger.com/images/deepcortex/dcos-rabbitmq "Get your own commit badge on microbadger.com")

## java-ubuntu
[![](https://images.microbadger.com/badges/image/deepcortex/java-ubuntu.svg)](https://microbadger.com/images/deepcortex/java-ubuntu "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/java-ubuntu.svg)](https://microbadger.com/images/deepcortex/java-ubuntu "Get your own commit badge on microbadger.com")

## scala-alpine
[![](https://images.microbadger.com/badges/image/deepcortex/scala-alpine.svg)](https://microbadger.com/images/deepcortex/scala-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-alpine.svg)](https://microbadger.com/images/deepcortex/scala-alpine "Get your own commit badge on microbadger.com")

## sbt-alpine
[![](https://images.microbadger.com/badges/image/deepcortex/sbt-alpine.svg)](https://microbadger.com/images/deepcortex/sbt-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/sbt-alpine.svg)](https://microbadger.com/images/deepcortex/sbt-alpine "Get your own commit badge on microbadger.com")

## scala-ubuntu
[![](https://images.microbadger.com/badges/image/deepcortex/scala-ubuntu.svg)](https://microbadger.com/images/deepcortex/scala-ubuntu "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/deepcortex/scala-ubuntu.svg)](https://microbadger.com/images/deepcortex/scala-ubuntu "Get your own version badge on microbadger.com")

## zeromq-sbt
[![](https://images.microbadger.com/badges/image/deepcortex/zeromq-sbt.svg)](https://microbadger.com/images/deepcortex/zeromq-sbt "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/zeromq-sbt.svg)](https://microbadger.com/images/deepcortex/zeromq-sbt "Get your own commit badge on microbadger.com")

## zeromq-scala
[![](https://images.microbadger.com/badges/image/deepcortex/zeromq-scala.svg)](https://microbadger.com/images/deepcortex/zeromq-scala "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/zeromq-scala.svg)](https://microbadger.com/images/deepcortex/zeromq-scala "Get your own commit badge on microbadger.com")

## zeromq-scala-python
[![](https://images.microbadger.com/badges/image/deepcortex/zeromq-scala-python.svg)](https://microbadger.com/images/deepcortex/zeromq-scala-python "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/zeromq-scala-python.svg)](https://microbadger.com/images/deepcortex/zeromq-scala-python "Get your own commit badge on microbadger.com")

## scala-python3

[![](https://images.microbadger.com/badges/image/deepcortex/scala-python3.svg)](https://microbadger.com/images/deepcortex/scala-python3 "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-python3.svg)](https://microbadger.com/images/deepcortex/scala-python3 "Get your own commit badge on microbadger.com")

## scala-python3-ml

[![](https://images.microbadger.com/badges/image/deepcortex/scala-python3-ml.svg)](https://microbadger.com/images/deepcortex/scala-python3-ml "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-python3-ml.svg)](https://microbadger.com/images/deepcortex/scala-python3-ml "Get your own commit badge on microbadger.com")

## scala-r

[![](https://images.microbadger.com/badges/image/deepcortex/scala-r.svg)](https://microbadger.com/images/deepcortex/scala-r "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-r.svg)](https://microbadger.com/images/deepcortex/scala-r "Get your own commit badge on microbadger.com")
[![](https://images.microbadger.com/badges/image/deepcortex/zeromq-sbt.svg)](https://microbadger.com/images/deepcortex/zeromq-sbt "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/zeromq-sbt.svg)](https://microbadger.com/images/deepcortex/zeromq-sbt "Get your own commit badge on microbadger.com")

# zeromq-sbt
Docker image providing ZeroMQ + JZMQ + Scala + SBT

FROM zeromq-scala:latest

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/scala-python3.svg)](https://microbadger.com/images/deepcortex/scala-python3 "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-python3.svg)](https://microbadger.com/images/deepcortex/scala-python3 "Get your own commit badge on microbadger.com")

# scala-python3
Docker image providing ZeroMQ + JZMQ + Scala + Python 3.5.2 environment

FROM deepcortex/zeromq-scala:latest

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/scala-r.svg)](https://microbadger.com/images/deepcortex/scala-r "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-r.svg)](https://microbadger.com/images/deepcortex/scala-r "Get your own commit badge on microbadger.com")

# scala-r
Docker image providing ZeroMQ + JZMQ + Scala + R

FROM deepcortex/zeromq-scala:latest

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/python3-hdfs.svg)](https://microbadger.com/images/deepcortex/python3-hdfs "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/python3-hdfs.svg)](https://microbadger.com/images/deepcortex/python3-hdfs "Get your own commit badge on microbadger.com")

# python3-hdfs
Docker image providing ZeroMQ + JZMQ + Scala + Python 3.5.2 environment + ML Libs + HDFS Python Client + Hadoop

numpy, scipy, scikit-learn, pandas, tensorflow, h5py, keras

FROM deepcortex/scala-python3-ml:latest

# Install HDFS
```
wget "http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz"
tar xvfz hadoop-2.6.5.tar.gz
mv hadoop-2.6.5 /usr/local/hadoop
ln -s /usr/local/hadoop /opt/hadoop
```

# Modify hadoop files  

/opt/hadoop/etc/hadoop/hdfs-site.xml

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

/opt/hadoop/etc/hadoop/core-site.xml

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://10.200.10.1:9000</value>
    </property>
</configuration>
```

/opt/hadoop/etc/hadoop/yarn-site.xml

```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

/opt/hadoop/etc/hadoop/mapred-site.xml

```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

# Install Ignite

```
wget "http://download.nextag.com/apache//ignite/2.0.0/apache-ignite-hadoop-2.0.0-bin.zip"
unzip apache-ignite-hadoop-2.0.0-bin.zip
mv apache-ignite-hadoop-2.0.0-bin /opt
ln -s /opt/apache-ignite-hadoop-2.0.0-bin /opt/ignite
```

# Add paths to ~/.bash_profile

```
export IGNITE_HOME=/opt/ignite
export PATH=$PATH:$IGNITE_HOME/bin
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
```

# Start HDFS

```
/opt/hadoop/sbin/start-dfs.sh
/opt/hadoop/sbin/start-yarn.sh
```

# Start Ignite 

```
/opt/ignite/bin/ignite.sh cortex-prototypes/cortex-ignite/src/main/configs/igfs-hadoop-fs-cache/igfs-hadoop-fs-cache-config.xml
```

# Make localhost alias

```
sudo ifconfig lo0 alias 10.200.10.1/24
```

# Start docker image 

```
make run
```

# To run HDFS commands in Docker

```
hdfs --config /etc/hadoop/conf/ dfs <command> <arguments>
```

# python3-ml-jupyter
Docker image providing ZeroMQ + JZMQ + Scala + Python 3.5.2 environment + ML Libs + Jupyter Notebook

numpy, scipy, scikit-learn, pandas, tensorflow, h5py, keras

FROM deepcortex/python3-ml-jupyter:latest

To run locally ```make && make run```

To run as a docker image ```docker run --rm -it -p 8888:8888 -v <local-projects-path>:/home/projects deepcortex/python3-ml-jupyter```[![](https://images.microbadger.com/badges/image/deepcortex/mesos-java.svg)](https://microbadger.com/images/deepcortex/mesos-java "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/image/deepcortex/mesos-java.svg)](https://microbadger.com/images/deepcortex/mesos-java "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/mesos-java.svg)](https://microbadger.com/images/deepcortex/mesos-java "Get your own commit badge on microbadger.com")

# mesos-java
Docker image providing Mesos and Java

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/zeromq-scala-python.svg)](https://microbadger.com/images/deepcortex/zeromq-scala-python "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/zeromq-scala-python.svg)](https://microbadger.com/images/deepcortex/zeromq-scala-python "Get your own commit badge on microbadger.com")

# zeromq-scala-python
Docker image providing ZeroMQ + JZMQ + Scala + Python 2.7

FROM scala-ubuntu:latest

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/dcos-rabbitmq.svg)](https://microbadger.com/images/deepcortex/dcos-rabbitmq "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/version/deepcortex/dcos-rabbitmq.svg)](https://microbadger.com/images/deepcortex/dcos-rabbitmq "Get your own version badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/dcos-rabbitmq.svg)](https://microbadger.com/images/deepcortex/dcos-rabbitmq "Get your own commit badge on microbadger.com")

# dcos-rabbitmq
Docker image providing RabbitMQ cluster for DC/OS

FROM rabbitmq:3.6.9-management-alpine

To run ```make run```[![](https://images.microbadger.com/badges/image/deepcortex/scala-alpine.svg)](https://microbadger.com/images/deepcortex/scala-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-alpine.svg)](https://microbadger.com/images/deepcortex/scala-alpine "Get your own commit badge on microbadger.com")

# scala-alpine
Docker image providing Scala

FROM openjdk:8-jre-alpine

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/sbt-alpine.svg)](https://microbadger.com/images/deepcortex/sbt-alpine "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/sbt-alpine.svg)](https://microbadger.com/images/deepcortex/sbt-alpine "Get your own commit badge on microbadger.com")

# sbt-alpine
Docker image providing SBT and Scala

FROM deepcortex/scala-alpine

To run ```make run```[![](https://images.microbadger.com/badges/image/deepcortex/zeromq-scala.svg)](https://microbadger.com/images/deepcortex/zeromq-scala "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/zeromq-scala.svg)](https://microbadger.com/images/deepcortex/zeromq-scala "Get your own commit badge on microbadger.com")

# zeromq-scala
Docker image providing ZeroMQ + JZMQ + Scala

FROM scala-ubuntu:latest

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/scala-python3-ml.svg)](https://microbadger.com/images/deepcortex/scala-python3-ml "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-python3-ml.svg)](https://microbadger.com/images/deepcortex/scala-python3-ml "Get your own commit badge on microbadger.com")

# scala-python3-ml
Docker image providing ZeroMQ + JZMQ + Scala + Python 3.5.2 environment + ML Libs

numpy, scipy, scikit-learn, pandas, tensorflow, h5py, keras

FROM deepcortex/scala-python3:latest

To run ```make run```
[![](https://images.microbadger.com/badges/image/deepcortex/scala-ubuntu.svg)](https://microbadger.com/images/deepcortex/scala-ubuntu "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/scala-ubuntu.svg)](https://microbadger.com/images/deepcortex/scala-ubuntu "Get your own commit badge on microbadger.com")

# scala-ubuntu
Docker image providing Scala

FROM java-ubuntu:latest

To run ```make run```[![](https://images.microbadger.com/badges/image/deepcortex/java-ubuntu.svg)](https://microbadger.com/images/deepcortex/java-ubuntu "Get your own image badge on microbadger.com")
[![](https://images.microbadger.com/badges/commit/deepcortex/java-ubuntu.svg)](https://microbadger.com/images/deepcortex/java-ubuntu "Get your own commit badge on microbadger.com")

# java-ubuntu
Docker image providing Java

FROM ubuntu:16.4

To run ```make run```
# docker-floyd-cli

[FloydHub](https://www.floydhub.com) floyd client docker image based on alpine.

## Build floyd client as docker image
```sh
$ docker build -t local/floyd --no-cache  .
```

## Create a project on floydhub UI
```sh
$ open https://www.floydhub.com
```

## Login
```sh
$ mkdir ~/.floyd
$ docker run -it --rm -v $HOME/.floyd:/root -v $(pwd):/code -w /code local/floyd sh
> $ floyd login
> Authentication token page will now open in your browser. Continue? [Y/n]: Y
# copy and paste token from floyd admin 

```

## Initialize project
```sh
$ floyd init <project-name>
```

## Run a Jupyter notebook with Tensorflow environment on GPU
```sh
$ floyd run --mode jupyter --env tensorflow --gpu
 :
 :
Setting up your instance and waiting for Jupyter notebook to become available ......................

Path to jupyter notebook: https://www.floydhub.com:8000/<RUN ID>

To view logs enter:
    floyd logs <RUN ID>

```

## Run a Tensorflow job on GPU
```sh
$ floyd run "python test.py" --gpu
$ floyd status
``` 

## Cleanup
```sh
$ floyd status 
$ floyd stop <ID>

$ floyd data status
$ floyd data delete <DATA ID>
```

# docker-mecab

[MeCab](http://taku910.github.io/mecab/) docker image based on alpine


## Usage

```
$ echo "MeCabは 京都大学情報学研究科−日本電信電話株式会社コミュニケーション科学基礎研究所 共同研究ユニットプロジェクトを通じて開発されたオープンソース 形態素解析エンジンです。" \
 | docker run --rm -i -a STDIN -a STDOUT smizy/mecab:0.996-alpine 
MeCab	名詞,固有名詞,一般,*,*,*,MeCab,メカブ,メカブ
は	助詞,係助詞,*,*,*,*,は,ハ,ワ
京都大	名詞,固有名詞,組織,*,*,*,京都大,キョウトダイ,キョートダイ
学	名詞,接尾,一般,*,*,*,学,ガク,ガク
情報学研究科	名詞,固有名詞,一般,*,*,*,情報学研究科,ジョウホウガクケンキュウカ,ジョーホーガクケンキューカ
−	記号,一般,*,*,*,*,−,ヒク,ヒク
日本電信電話株式会社	名詞,固有名詞,組織,*,*,*,日本電信電話,カブシキガイシャ,カブシキガイシャ
コミュニケーション	名詞,一般,*,*,*,*,コミュニケーション,コミュニケーション,コミュニケーション
科学	名詞,一般,*,*,*,*,科学,カガク,カガク
基礎	名詞,一般,*,*,*,*,基礎,キソ,キソ
研究所	名詞,一般,*,*,*,*,研究所,ケンキュウジョ,ケンキュージョ
共同	名詞,サ変接続,*,*,*,*,共同,キョウドウ,キョードー
研究	名詞,サ変接続,*,*,*,*,研究,ケンキュウ,ケンキュー
ユニット	名詞,一般,*,*,*,*,ユニット,ユニット,ユニット
プロジェクト	名詞,一般,*,*,*,*,プロジェクト,プロジェクト,プロジェクト
を通じて	助詞,格助詞,連語,*,*,*,を通じて,ヲツウジテ,ヲツージテ
開発	名詞,サ変接続,*,*,*,*,開発,カイハツ,カイハツ
さ	動詞,自立,*,*,サ変・スル,未然レル接続,する,サ,サ
れ	動詞,接尾,*,*,一段,連用形,れる,レ,レ
た	助動詞,*,*,*,特殊・タ,基本形,た,タ,タ
オープンソース	名詞,固有名詞,一般,*,*,*,Open Source,オープンソース,オープンソース
形態素解析	名詞,固有名詞,一般,*,*,*,形態素解析,ケイタイソカイセキ,ケイタイソカイセキ
エンジン	名詞,一般,*,*,*,*,エンジン,エンジン,エンジン
です	助動詞,*,*,*,特殊・デス,基本形,です,デス,デス
。	記号,句点,*,*,*,*,。,。,。
EOS 
```# docker-mecab-api

MeCab API docker images based on alpine

## Usage

```
# run dev server
docker run --rm -p 8080:8080 smizy/mecab-api

# with gunicorn
docker run --rm -p 8080:8080 smizy/mecab-api gunicorn -w 4 -b 0.0.0.0:8080 main:app

# api call (change "localhost" as your docker env)
curl -s "http://localhost:8080/parse?q=%E5%A4%96%E5%9B%BD%E4%BA%BA%E5%8F%82%E6%94%BF%E6%A8%A9%0A" |
python -c 'import sys,json;print json.dumps(json.loads(sys.stdin.read()),indent=4,ensure_ascii=False)'

# decoded json
{
    "result": [
        {
            "features": [
                "名詞", 
                "固有名詞", 
                "人名", 
                "一般", 
                "*", 
                "*", 
                "外国人参政権", 
                "ガイコクジンサンセイケン", 
                "ガイコクジンサンセイケン"
            ], 
            "surface": "外国人参政権"
        }
    ]
}

```[![Docker Stars](https://img.shields.io/docker/stars/frolvlad/alpine-miniconda2.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-miniconda2/)
[![Docker Pulls](https://img.shields.io/docker/pulls/frolvlad/alpine-miniconda2.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-miniconda2/)


Miniconda Python 2.7 Docker image
=================================

This image is based on Alpine Linux image, which is only a 5MB image, and contains
[Python 2.7](https://www.python.org/) packaged by Continuum with Conda package manager.

Download size of this image is only:

[![](https://images.microbadger.com/badges/image/frolvlad/alpine-miniconda2.svg)](http://microbadger.com/images/frolvlad/alpine-miniconda2 "Get your own image badge on microbadger.com")

NOTE: Conda repositories contain only Glibc linked packaged binaries for Linux,
so we have to use
[glibc workaround](https://github.com/gliderlabs/docker-alpine/issues/11) on
Alpine.


Usage Example
-------------

```bash
$ docker run --rm frolvlad/alpine-miniconda2 python -c 'print u"Hello World"'
```

Once you have run this command you will get printed 'Hello World' from Python!

NOTE: `conda` and `pip` are also available in this image.
# docker-jupyter

Jupyter docker image based on alpine 

```
docker build -t local/jupyter .
docker build -f Dockerfile.nlp -t local/jupyter:nlp .

# run Jupyter Notebook container 
docker run  -p 8888:8888 -v $(pwd):/code  -d local/jupyter:nlp

# open browser
open http://$(docker-machine ip default):8888
```
# docker-octave

Octave + Jupyter Notebook docker image based on alpine

```
# run jupyter
docker run -it --rm -p 8888:8888 -v $(pwd):/code  smizy/octave:4.2.0-alpine

# open browser (see token in log)
open http://$(docker-machine ip default):8888?token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# create a notebook selecting "Octave" from [New] pulldown  

# run cell
x = rand(10,1)
plot(x)

```# alpine-python3-tensorflow
python 3.6.1 with

bleach==1.5.0
certifi==2017.7.27.1
chardet==3.0.4
cycler==0.10.0
Cython==0.26
decorator==4.1.2
FALCONN==2.0.0
html5lib==0.9999999
idna==2.6
Keras==2.0.6
Markdown==2.6.9
matplotlib==2.0.2
networkx==1.11
numpy==1.13.1
olefile==0.44
pandas==0.20.3
Pillow==4.2.1
protobuf==3.4.0
pyparsing==2.2.0
python-dateutil==2.6.1
pytz==2017.2
PyWavelets==0.5.2
PyYAML==3.12
requests==2.18.4
scikit-image==0.13.0
scikit-learn==0.19.0
scipy==0.19.1
six==1.10.0
tensorflow==1.3.0
tensorflow-tensorboard==0.1.4
tensorlayer==1.6.1
Theano==0.9.0
urllib3==1.22
Werkzeug==0.12.2


This is heavily inspired by https://github.com/tatsushid/docker-alpine-py3-tensorflow-jupyter/ Docker image.
# docker-keras-tensorflow
[![](https://images.microbadger.com/badges/image/smizy/keras.svg)](https://microbadger.com/images/smizy/keras "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/keras.svg)](https://microbadger.com/images/smizy/keras "Get your own version badge on microbadger.com")
[![build status](https://gitlab.com/smizy/docker-keras-tensorflow/badges/master/build.svg)](https://gitlab.com/smizy/docker-keras-tensorflow/commits/master)

Python3 Keras(Tensorflow backended) with Jupyter docker image based on alpine 

* numpy, scipy, pandas, scikit-learn, seaborn, tensorflow, keras installed via pip. See `pip list --format=columns` for detail.
* CPU only

## Usage
```
docker run -it --rm -v $(pwd):/data -w /data -p 8888:8888 smizy/keras:2.0-cpu-alpine
```Docker image with alpine + Python3 + TensolFlow + Jupyter
=========================================================

This provides a Docker image with

- Alpine
- Python3
- TensolFlow
- Juptyer

This is heavily inspired by https://hub.docker.com/r/enakai00/jupyter_tensorflow/
Docker image.

## Supported tags and respective `Dockerfile` links

- [`1.3.0`, `latest`](https://github.com/tatsushid/docker-alpine-py3-tensorflow-jupyter/blob/master/Dockerfile)

## How to use this image

Please run the following

```shellsession
docker run -itd -p 8888:8888 -e PASSWORD=foobar tatsushid/alpine-py3-tensorflow-jupyter
```

and access to `http://{docker host}:8888/`. It opens Jupyter's login panel so
please enter the password which you specified as `PASSWORD` environment value

This repository also provides Docker Compose example so you can boot a
container of this image by running

```shellsession
docker-compose up
```

in `docker_compose_example` directory.

## License
This alpine-py3-tensorflow-jupyter Docker image is under MIT License. See the
[LICENSE][license] file for details.

[license]: https://github.com/tatsushid/docker-alpine-py3-tensorflow-jupyter/blob/master/LICENSE
Docker Compose Example
======================

This is Docker Compose example. You can boot a container based on this
repository image with Jupyter port 8888, its password 'foobar' and sharing
files in `notebook` directory with the container by just running

```shellsession
docker-compose up
```

To stop it, please press 'Ctrl-C'.
Notebook Directory
==================

If you put files into this directory and boot a container by docker-compose
with an example `docker-compose.yml` in a parent directory, all files in this
directory are shared with the container and placed at `/root/notebook/my_notes`
in the container. It can be also used from Jupyter as `my_notes`.
# docker-keras-theano
[![](https://images.microbadger.com/badges/image/smizy/keras-theano.svg)](https://microbadger.com/images/smizy/keras-theano "Get your own image badge on microbadger.com") 
[![](https://images.microbadger.com/badges/version/smizy/keras-theano.svg)](https://microbadger.com/images/smizy/keras-theano "Get your own version badge on microbadger.com")
[![CircleCI](https://circleci.com/gh/smizy/docker-keras-theano.svg?style=svg&circle-token=3d06a409dacb17ef9c99bb4597492887ec9b2050)](https://circleci.com/gh/smizy/docker-keras-theano)

Python3 Keras(Theano backended) with Jupyter docker image based on alpine.

See (https://hub.docker.com/r/smizy/keras/) for Thensorflow backend version

* numpy, scipy, pandas, scikit-learn, seaborn, theano, keras installed via pip. See ` pip list --format=columns` for detail.
* CPU only

## Usage
```
docker run -it --rm -v $(pwd):/data -w /data -p 8888:8888 smizy/keras-theano:1.2.2-cpu-alpine
```[![Docker Stars](https://img.shields.io/docker/stars/frolvlad/alpine-miniconda3.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-miniconda3/)
[![Docker Pulls](https://img.shields.io/docker/pulls/frolvlad/alpine-miniconda3.svg?style=flat-square)](https://hub.docker.com/r/frolvlad/alpine-miniconda3/)


Miniconda Python 3.5 Docker image
=================================

This image is based on Alpine Linux image, which is only a 5MB image, and contains
[Python 3.5](https://www.python.org/) packaged by Continuum with Conda package manager.

Download size of this image is only:

[![](https://images.microbadger.com/badges/image/frolvlad/alpine-miniconda3.svg)](http://microbadger.com/images/frolvlad/alpine-miniconda3 "Get your own image badge on microbadger.com")

NOTE: Conda repositories contain only Glibc linked packaged binaries for Linux,
so we have to use
[glibc workaround](https://github.com/gliderlabs/docker-alpine/issues/11) on
Alpine.


Usage Example
-------------

```bash
$ docker run --rm frolvlad/alpine-miniconda3 python -c 'print("Hello World")'
```

Once you have run this command you will get printed 'Hello World' from Python!

NOTE: `conda` and `pip` are also available in this image.
## DeepDetect : Open Source Deep Learning Server & API

[![Join the chat at https://gitter.im/beniz/deepdetect](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/beniz/deepdetect?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/beniz/deepdetect.png)](https://travis-ci.org/beniz/deepdetect)

DeepDetect (http://www.deepdetect.com/) is a machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.

DeepDetect relies on external machine learning libraries through a very generic and flexible API. At the moment it has support for:

- the deep learning library [Caffe](https://github.com/BVLC/caffe)
- distributed gradient boosting library [XGBoost](https://github.com/dmlc/xgboost)
- the deep learning and other usages library [Tensorflow](https://tensorflow.org)
- clustering with [T-SNE](https://github.com/DmitryUlyanov/Multicore-TSNE)

#### Machine Learning functionalities per library (current):

|            | Training | Prediction | Classification | Object Detection | Segmentation | Regression | Autoencoder |
|------------|----------|------------|----------------|-----------|-----------|------------|-------------|
| Caffe      | Y        | Y          | Y              | Y         |   Y       |   Y        | Y           |
| XGBoost    | Y        | Y          | Y              | N         |   N       |   Y        | N/A         |
| Tensorflow | N        | Y          | Y              | N         |   N       |   N        | N           |
| T-SNE      | Y        | N/A        | N/A            | N/A       |   N/A     |   N/A      | N/A         |


#### GPU support per library

|            | Training | Prediction |
|------------|----------|------------|
| Caffe      | Y        | Y          |
| XGBoost    | Y        | N          |
| Tensorflow | Y        | Y          |
| T-SNE      | N        | N          |

#### Input data support per library (current):

|            | CSV | SVM | Text words | Text characters | Images |
|------------|-----|-----|------------|-----------------|--------|
| Caffe      | Y   | Y   | Y          | Y               | Y      |
| XGBoost    | Y   | Y   | Y          | N               | N      |
| Tensorflow | N   | N   | N          | N               | Y      |
| T-SNE      | Y   | N   | N          | N               | Y      | (*)
(*) more input support for T-SNE is pending

#### Main functionalities

DeepDetect implements support for supervised and unsupervised deep learning of images, text and other data, with focus on simplicity and ease of use, test and connection into existing applications. It supports classification, object detection, segmentation, regression, autoencoders, ...

#### Support

Please join either the community on [Gitter](https://gitter.im/beniz/deepdetect) or on IRC Freenode #deepdetect, where we help users get through with installation, API, neural nets and connection to external applications.

#### Supported Platforms

The reference platforms with support are **Ubuntu 14.04 LTS** and **Ubuntu 16.04 LTS**.

Supported images that come with pre-trained image classification deep (residual) neural nets:

- **docker images** for CPU and GPU machines are available at https://hub.docker.com/r/beniz/deepdetect_cpu/ and https://hub.docker.com/r/beniz/deepdetect_gpu/ respectively. See https://github.com/beniz/deepdetect/tree/master/docker/README.md for details on how to use them.

- For **Amazon AMI** see official builds documentation at https://deepdetect.com/products/ami/, and direct links to [GPU AMI](https://aws.amazon.com/marketplace/pp/B01N4D483M) and [CPU AMI](https://aws.amazon.com/marketplace/pp/B01N1RGWQZ).

#### Performances

See https://github.com/jolibrain/dd_performances for a report on performances on NVidia Desktop and embedded GPUs, along with Raspberry Pi 3.

#### Quickstart
Setup an image classifier API service in a few minutes:
http://www.deepdetect.com/tutorials/imagenet-classifier/

#### Tutorials
List of tutorials, training from text, data and images, setup of prediction services, and export to external software (e.g. ElasticSearch): http://www.deepdetect.com/tutorials/tutorials/

#### Features and Documentation
Current features include:

- high-level API for machine learning and deep learning
- Support for Caffe, Tensorflow, XGBoost and T-SNE
- classification, regression, autoencoders, object detection, segmentation
- JSON communication format
- remote Python client library
- dedicated server with support for asynchronous training calls
- high performances, benefit from multicore CPU and GPU
- connector to handle large collections of images with on-the-fly data augmentation (e.g. rotations, mirroring)
- connector to handle CSV files with preprocessing capabilities
- connector to handle text files, sentences, and character-based models
- connector to handle SVM file format for sparse data
- range of built-in model assessment measures (e.g. F1, multiclass log loss, ...)
- no database dependency and sync, all information and model parameters organized and available from the filesystem
- flexible template output format to simplify connection to external applications
- templates for the most useful neural architectures (e.g. Googlenet, Alexnet, ResNet, convnet, character-based convnet, mlp, logistic regression)
- support for sparse features and computations on both GPU and CPU

##### Documentation

- Full documentation is available from http://www.deepdetect.com/overview/introduction/
- API documentation is available from http://www.deepdetect.com/api/
- FAQ is available from http://www.deepdetect.com/overview/faq/

##### Clients

- Python client:
  - REST client: https://github.com/beniz/deepdetect/tree/master/clients/python
  - 'a la scikit' bindings: https://github.com/ArdalanM/pyDD
- Java client: https://github.com/kfadhel/deepdetect-api-java
- Early C# client: https://github.com/beniz/deepdetect/pull/98

##### Dependencies

- C++, gcc >= 4.8 or clang with support for C++11 (there are issues with Clang + Boost)
- [eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) for all matrix operations;
- [glog](https://code.google.com/p/google-glog/) for logging events and debug;
- [gflags](https://code.google.com/p/gflags/) for command line parsing;
- OpenCV >= 2.4
- [cppnetlib](http://cpp-netlib.org/)
- Boost
- [curl](http://curl.haxx.se/)
- [curlpp](http://www.curlpp.org/)
- [utfcpp](http://utfcpp.sourceforge.net/)
- [gtest](https://code.google.com/p/googletest/) for unit testing (optional);

##### Caffe Dependencies

- CUDA 8 or 7.5 is recommended for GPU mode.
- BLAS via ATLAS, MKL, or OpenBLAS.
- [protobuf](https://github.com/google/protobuf)
- IO libraries hdf5, leveldb, snappy, lmdb

##### XGBoost Dependencies

None outside of C++ compiler and make
- CUDA 8 or 7.5 is recommended for GPU mode.

#### Tensorflow Dependencies

- Cmake > 3
- [Bazel](https://www.bazel.io/versions/master/docs/install.html#install-on-ubuntu)

##### Caffe version

By default DeepDetect automatically relies on a modified version of Caffe, https://github.com/beniz/caffe/tree/master
This version includes many improvements over the original Caffe, such as sparse input data support, exception handling, class weights, object detection, segmentation, and various additional losses and layers.

##### Implementation

The code makes use of C++ policy design for modularity, performance and putting the maximum burden on the checks at compile time. The implementation uses many features from C++11.

##### Demo

- Image classification Web interface:
HTML and javascript classification image demo in [demo/imgdetect](https://github.com/beniz/deepdetect/tree/master/demo/imgdetect)

- Image similarity search:
Python script for indexing and searching images is in [demo/imgsearch](https://github.com/beniz/deepdetect/tree/master/demo/imgsearch)

- Image object detection:
Python script for object detection within images is in [demo/objdetect](https://github.com/beniz/deepdetect/tree/master/demo/objdetect)

- Image segmentation:
Python script for image segmentation is in [demo/segmentation](https://github.com/beniz/deepdetect/tree/master/demo/segmentation)

##### Examples

- List of examples, from MLP for data, text, multi-target regression to CNN and GoogleNet, finetuning, etc...:
http://www.deepdetect.com/overview/examples/

##### Models

|                          | Caffe | Tensorflow | Source        | Top-1 Accuracy (ImageNet) |
|--------------------------|-------|------------|---------------|---------------------------|
| AlexNet                  | Y     | N          | BVLC          |          57.1%                 |
| SqueezeNet               | Y     | N          | DeepScale              |       59.5%                    | 
| Inception v1 / GoogleNet | [Y](https://deepdetect.com/models/ggnet/bvlc_googlenet.caffemodel)     | [Y](https://deepdetect.com/models/tf/inception_v1.pb)          | BVLC / Google |             67.9%              |
| Inception v2             | N     | [Y](https://deepdetect.com/models/tf/inception_v2.pb)          | Google        |     72.2%                      |
| Inception v3             | N     | [Y](https://deepdetect.com/models/tf/inception_v3.pb)          | Google        |         76.9%                  |
| Inception v4             | N     | [Y](https://deepdetect.com/models/tf/inception_v4.pb)          | Google        |         80.2%                  |
| ResNet 50                | [Y](https://deepdetect.com/models/resnet/ResNet-50-model.caffemodel)     | [Y](https://deepdetect.com/models/tf/resnet_v1_50/resnet_v1_50.pb)          | MSR           |      75.3%                     |
| ResNet 101               | [Y](https://deepdetect.com/models/resnet/ResNet-101-model.caffemodel)     | [Y](https://deepdetect.com/models/tf/resnet_v1_101/resnet_v1_101.pb)          | MSR           |        76.4%                   |
| ResNet 152               | [Y](https://deepdetect.com/models/resnet/ResNet-152-model.caffemodel)     | [Y](https://deepdetect.com/models/tf/resnet_v1_152/resnet_v1_152.pb)         | MSR           |               77%            |
| Inception-ResNet-v2      | N     | [Y](https://deepdetect.com/models/tf/inception_resnet_v2.pb)          | Google        |       79.79%                    |
| VGG-16                   | [Y](https://deepdetect.com/models/vgg_16/VGG_ILSVRC_16_layers.caffemodel)     | [Y](https://deepdetect.com/models/tf/vgg_16/vgg_16.pb)          | Oxford        |               70.5%            |
| VGG-19                   | [Y](https://deepdetect.com/models/vgg_19/VGG_ILSVRC_19_layers.caffemodel)     | [Y](https://deepdetect.com/models/tf/vgg_19/vgg_19.pb)          | Oxford        |               71.3%            |
| ResNext 50                | [Y](https://deepdetect.com/models/resnext/resnext_50)     | N          | https://github.com/terrychenism/ResNeXt           |      76.9%                     |
| ResNext 101                | [Y](https://deepdetect.com/models/resnext/resnext_101)     | N          | https://github.com/terrychenism/ResNeXt           |      77.9%                     |
| ResNext 152               | [Y](https://deepdetect.com/models/resnext/resnext_152)     | N          | https://github.com/terrychenism/ResNeXt           |      78.7%                     |
| DenseNet-121                   | [Y](https://deepdetect.com/models/densenet/densenet_121_32/)     | N          | https://github.com/shicai/DenseNet-Caffe        |               74.9%            |
| DenseNet-161                   | [Y](https://deepdetect.com/models/densenet/densenet_161_48/)     | N          | https://github.com/shicai/DenseNet-Caffe        |               77.6%            |
| DenseNet-169                   | [Y](https://deepdetect.com/models/densenet/densenet_169_32/)     | N          | https://github.com/shicai/DenseNet-Caffe        |               76.1%            |
| DenseNet-201                   | [Y](https://deepdetect.com/models/densenet/densenet_201_32/)     | N          | https://github.com/shicai/DenseNet-Caffe        |               77.3%            |
| VOC0712 (object detection) | [Y](https://deepdetect.com/models/voc0712_dd.tar.gz) | N | https://github.com/weiliu89/caffe/tree/ssd | 71.2 mAP |

More models:

- List of free, even for commercial use, deep neural nets for image classification, and character-based convolutional nets for text classification: http://www.deepdetect.com/applications/list_models/

#### Templates

DeepDetect comes with a built-in system of neural network templates (Caffe backend only at the moment). This allows the creation of custom networks based on recognized architectures, for images, text and data, and with much simplicity.

Usage:
- specify `template` to use, from `mlp`, `convnet` and `resnet`
- specify the architecture with the `layers` parameter:
  - for `mlp`, e.g. `[300,100,10]`
  - for `convnet`, e.g. `["1CR64","1CR128","2CR256","1024","512"], where the main pattern is `xCRy` where `y` is the number of outputs (feature maps), `CR` stands for Convolution + Activation (with `relu` as default), and `x` specifies the number of chained `CR` blocks without pooling. Pooling is applied between all `xCRy`
- for `resnets`:
   - with images, e.g. `["Res50"]` where the main pattern is `ResX` with X the depth of the Resnet
   - with character-based models (text), use the `xCRy` pattern of convnets instead, with the main difference that `x` now specifies the number of chained `CR` blocks within a resnet block
   - for Resnets applied to CSV or SVM (sparse data), use the `mlp` pattern. In this latter case, at the moment, the `resnet` is built with blocks made of two layers for each specified layer after the first one. Here is an example: `[300,100,10]` means that a first hidden layer of size `300` is applied followed by a `resnet` block made of two `100` fully connected layer, and another block of two `10` fully connected layers. This is subjected to future changes and more control.

### Authors
DeepDetect is designed and implemented by Emmanuel Benazera <beniz@droidnik.fr>.

### Build

Below are instructions for Ubuntu 14.04 LTS. For other Linux and Unix systems, steps may differ, CUDA, Caffe and other libraries may prove difficult to setup. If you are building on 16.04 LTS, look at https://github.com/beniz/deepdetect/issues/126 that tells you how to proceed.

Beware of dependencies, typically on Debian/Ubuntu Linux, do:
```
sudo apt-get install build-essential libgoogle-glog-dev libgflags-dev libeigen3-dev libopencv-dev libcppnetlib-dev libboost-dev libboost-iostreams-dev libcurlpp-dev libcurl4-openssl-dev protobuf-compiler libopenblas-dev libhdf5-dev libprotobuf-dev libleveldb-dev libsnappy-dev liblmdb-dev libutfcpp-dev cmake libgoogle-perftools-dev unzip
```

#### Default build with Caffe
For compiling along with Caffe:
```
mkdir build
cd build
cmake ..
make
```

If you are building for one or more GPUs, you may need to add CUDA to your ld path:
```
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64
```

If you would like to build with cuDNN, your `cmake` line should be:
```
cmake .. -DUSE_CUDNN=ON
```

To target the build of underlying Caffe to a specific CUDA architecture (e.g. Pascal), you can use:
```
cmake .. -DCUDA_ARCH="-gencode arch=compute_61,code=sm_61"
```

If you would like a CPU only build, use:
```
cmake .. -DUSE_CPU_ONLY=ON
```

If you would like to constrain Caffe to CPU only, use:
```
cmake .. -DUSE_CAFFE_CPU_ONLY=ON
```

#### Build with XGBoost support

If you would like to build with XGBoost, include the `-DUSE_XGBOOST=ON` parameter to `cmake`:
```
cmake .. -DUSE_XGBOOST=ON
```

If you would like to build the GPU support for XGBoost (experimental from DMLC), use the `-DUSE_XGBOOST_GPU=ON` parameter to `cmake`:
```
cmake .. -DUSE_XGBOOST=ON -DUSE_XGBOOST_GPU=ON
```


#### Build with Tensorflow support
First you must install [Bazel](https://www.bazel.io/versions/master/docs/install.html#install-on-ubuntu) and Cmake with version > 3.

And other dependencies:
```
sudo apt-get install python-numpy swig python-dev python-wheel unzip
```

If you would like to build with Tensorflow, include the `-DUSE_TF=ON` paramter to `cmake`:
```
cmake .. -DUSE_TF=ON
```

If you would like to constrain Tensorflow to CPU, use:
```
cmake .. -DUSE_TF=ON -DUSE_TF_CPU_ONLY=ON
```

You can combine with XGBoost support with:
```
cmake .. -DUSE_TF=ON -DUSE_XGBOOST=ON
```

#### Build with T-SNE support

Simply specify the option via cmake command line:
```
cmake .. -DUSE_TSNE=ON
```

### Run tests

Note: running tests requires the automated download of ~75Mb of datasets, and computations may take around thirty minutes on a CPU-only machines.

To prepare for tests, compile with:
```
cmake -DBUILD_TESTS=ON ..
make
```
Run tests with:
```
ctest
```

### Start the server

```
cd build/main
./dede

DeepDetect [ commit 73d4e638498d51254862572fe577a21ab8de2ef1 ]
Running DeepDetect HTTP server on localhost:8080
```

Main options are:
- `-host` to select which host to run on, default is `localhost`, use `0.0.0.0` to listen on all interfaces
- `-port` to select which port to listen to, default is `8080`
- `-nthreads` to select the number of HTTP threads, default is `10`

To see all options, do:
```
./dede --help
```

### Pure command line JSON API

To use deepdetect without the client/server architecture while passing the exact same JSON messages from the API:

```
./dede --jsonapi 1 <other options>
```

where `<other options>` stands for the command line parameters from the command line JSON API:

```
-info (/info JSON call) type: bool default: false
-service_create (/service/service_name call JSON string) type: string default: ""
-service_delete (/service/service_name DELETE call JSON string) type: string default: ""
-service_name (service name string for JSON call /service/service_name) type: string default: ""
-service_predict (/predict POST call JSON string) type: string default: ""
-service_train (/train POST call JSON string) type: string default: ""
-service_train_delete (/train DELETE call JSON string) type: string default: ""
-service_train_status (/train GET call JSON string) type: string default: ""
							  
```

The options above can be obtained from running

```
./dede --help
```

Example of creating a service then listing it:

```
./dede --jsonapi 1 --service_name test --service_create '{"mllib":"caffe","description":"classification service","type":"supervised","parameters":{"input":{"connector":"image"},"mllib":{"template":"googlenet","nclasses":10}},"model":{"templates":"/path/to/deepdetect/templates/caffe/","repository":"/path/to/model/"}}'
```

Note that in command line mode the `--service_xxx` calls are executed sequentially, and synchronously. Also note the logs are those from the server, the JSON API response is not available in pure command line mode.

### Run examples

See tutorials from http://www.deepdetect.com/tutorials/tutorials/

### References

- DeepDetect (http://www.deepdetect.com/)
- Caffe (https://github.com/BVLC/caffe)
- XGBoost (https://github.com/dmlc/xgboost)
- T-SNE (https://github.com/DmitryUlyanov/Multicore-TSNE)
http://www.kaggle.com/c/forest-cover-type-prediction/data20 newsgroups dataset, no email headerFarm ads, https://archive.ics.uci.edu/ml/datasets/Farm+Adshttp://www.kaggle.com/c/datasciencebowl/dataTo use the Python DeepDetect client, install `python-requests`. On Ubuntu:

```
sudo apt-get install python-requests
```

Then either copy it into your Python code repository or point your `PYTHONPATH` to it:

```
export PYTHONPATH=/path/to/deepdetect/clients/python/:$PYTHONPATH
```### Object detection demo

This is a small demo of object detection via the Python client.

It uses a pre-trained VOC0712 model to detect 21 different classes of objects in images. A bounding box is returned for every detected object, along with its class and a confidence.

To run the code on your own image:

- Install the pre-trained model:

```
mkdir model
cd model
wget https://deepdetect.com/models/voc0712_dd.tar.gz
tar xvzf voc0712_dd.tar.gz
cd ..
```

- Start a DeepDetect server:

```
./dede
```

- Try object detection on an image

```
python objdetect.py --image /path/to/yourimage.jpg --confidence-threshold 0.1
```

Notes:

- The VOC0712 model originates from https://github.com/weiliu89/caffe/tree/ssd and may not be very accurate on standard pictures

- You can predict over batches of images with only slight modifications of the Python code of this demo
### Image similary search demo

This is a small Python demo of an image similarity search application.

It does two things:

- use a DeepDetect image classification service in order to generate a numerical or binary code for every image
- indexes images with [annoy](https://github.com/spotify/annoy), an approximate nearest neighbors C++/Python library
- search by images, even for new images, not previously indexed, and return the closest images

To run the code on your own collection of images:

- install Annoy:
  ```
  pip install annoy
  ```
  or go look at https://github.com/spotify/annoy

- create a model repository with the pre-trained image classification network of your choice. Here we are using a pre-trained GoogleNet, but you can also use a built-in ResNet or [other provided models](http://www.deepdetect.com/applications/model/):
  ```
  mkdir model
  cd model
  wget http://www.deepdetect.com/models/ggnet/bvlc_googlenet.caffemodel
  ```
  
  **make sure that the `model` repository is in the same repository as the script `imgsearch.py`**

- start a DeepDetect server:
  ```
  ./dede
  ```

- index your collection of images:
  ```
  python imgsearch.py --index /path/to/your/images --index-batch-size 64
  ```
  Here `index-batch-size` controls the number of images that are processed at once.
  The index file is then `index.ann` in the repository. `names.bin` indexes the filenames.
  
  **Index and name files are erased upon every new indexing call**

- search for similar images:
  ```
  python imgsearch.py --search /path/your/image.png --search-size 10
  ```
  Here `search-size` controls the number of approximate neighbors.

Notes:

- The search uses a deep convolutional net layer as a code for every image. Using top layers (e.g. `loss3/classifier` with GoogleNet) uses high level features and thus image similarity is based on high level concepts such as whether the image contains a lakeshore, a bottle, etc... Using bottom or mid-range layers (e.g. `pool5/7x7_s1` with GoogleNet) makes image similarity based on lower level, potentially invariant, universal features such as lightning conditions, basic shapes, etc... Experiment and see what is best for your application.

- Annoy is a nice piece of code but in experiments the index building step becomes very memory inefficient and time-consuming around a million of images. If this is an issue, get in touch, as they are other, more complicated, ways to index and perform the search and scale.

- The code in `imgsearch.py` allows for more options such as whether to use `binarized` codes, `angular` or `euclidean` metric for similar image retrieval, and control of the accuracy of the search through `ntrees`.
### Clustering with T-SNE demo

This is a small demo of clustering the MNIST dataset test set via the Python client. The dataset contains 10000 images in CSV format. The final clustering thus has 10000 points.

To run the code:
- Start a DeepDetect server:

```
./dede
```

- Try the clustering:
```
python demo_tsne.py
```

The script downloads the dataset, and that can take a few seconds, then starts the clustering. Expect around a minute total before the scatter plot appears on screen:

![alt tag](https://deepdetect.com/dd/examples/tsne_mnist_test.png)
### Segmentation demo

This is a small demo of image segmentation via the Python client to DeepDetect server.

It uses a custom pre-trained model that segments 150 different classes of objects in images. A class is predicted for every pixel in the input image.

To run the code on your own image:

- Install the pre-trained model:

```
mkdir model
wget https://deepdetect.com/models/model_deeplab_ade20k.tar.gz
tar xvzf model_deeplab_ade20k.tar.gz
cd ..
```

- Start a DeepDetect server:

```
./dede
```

- Try image segmentation on an image

```
python segment.py --nclasses 150 --model-dir /path/to/model/model_deeplab_ade20k/ --image /path/to/image.jpg
```

In order for this demo to work, you'll need to serve the static
index.html file in a webserver, and redirect a request to your
deepdetect server.

## image classification service setup

Follow instructions from http://www.deepdetect.com/tutorials/imagenet-classifier/

This should look like this ![dd_sshot](https://cloud.githubusercontent.com/assets/3530657/13314070/4ea6aad6-dba3-11e5-889c-120cfe15ce6f.png)

## nginx configuration

Here is an nginx configuration example you can use to serve the
index.html file and redirect api request to your deepdetect server:

    server {
      listen 80 default_server;
      listen [::]:80 default_server;
    
      root /home/alx/code/deepdetect/demo/imgdetect;
      index index.html
      server_name _;
    
      location / {
        try_files $uri $uri/ =404;
      }
    
      location /api {
        rewrite ^/api(.*)$  $1  break;
        proxy_pass         http://127.0.0.1:8080;
        proxy_set_header   Host                   $http_host;
        proxy_redirect off;
      }
    }

If you find a 'bad gateway' error after this modification, you can try
to bind deepdetect server to 127.0.0.1 host :

     ./dede -host 127.0.0.1

## DeepDetect Docker images

This repository contains the Dockerfiles for building the CPU and GPU images for deepdetect.

Also see https://hub.docker.com/u/beniz/starred/ for pre-built images

The docker images contain:
- a running `dede` server ready to be used, no install required
- `googlenet` and `resnet_50` pre-trained image classification models, in `/opt/models/`

This allows to run the container and set an image classification model based on deep (residual) nets in two short command line calls.

### Getting and running official images

```
docker pull beniz/deepdetect_cpu
```
or
```
docker pull beniz/deepdetect_gpu
```

#### Running the CPU image

```
docker run -d -p 8080:8080 beniz/deepdetect_cpu
```

`dede` server is now listening on your port `8080`:

```
curl http://localhost:8080/info

{"status":{"code":200,"msg":"OK"},"head":{"method":"/info","version":"0.1","branch":"master","commit":"c8556f0b3e7d970bcd9861b910f9eae87cfd4b0c","services":[]}}
```

Here is how to do a simple image classification service and prediction test:
- service creation
```
curl -X PUT "http://localhost:8080/services/imageserv" -d "{\"mllib\":\"caffe\",\"description\":\"image classification service\",\"type\":\"supervised\",\"parameters\":{\"input\":{\"connector\":\"image\"},\"mllib\":{\"nclasses\":1000}},\"model\":{\"repository\":\"/opt/models/ggnet/\"}}"

{"status":{"code":201,"msg":"Created"}}
```
- image classification
```
curl -X POST "http://localhost:8080/predict" -d "{\"service\":\"imageserv\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":3},\"mllib\":{\"gpu\":false}},\"data\":[\"http://i.ytimg.com/vi/0vxOhd4qlnA/maxresdefault.jpg\"]}"

{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","time":852.0,"service":"imageserv"},"body":{"predictions":{"uri":"http://i.ytimg.com/vi/0vxOhd4qlnA/maxresdefault.jpg","classes":[{"prob":0.2255125343799591,"cat":"n03868863 oxygen mask"},{"prob":0.20917612314224244,"cat":"n03127747 crash helmet"},{"last":true,"prob":0.07399296760559082,"cat":"n03379051 football helmet"}]}}}
```

#### Running the GPU image

This requires [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) in order for the local GPUs to be made accessible by the container.

The following steps are required:

- install `nvidia-docker`: https://github.com/NVIDIA/nvidia-docker
- run with
```
nvidia-docker run -d -p 8080:8080 beniz/deepdetect_gpu
```

Notes:
- `nvidia-docker` requires docker >= 1.9

To test on image classification on GPU:
```
curl -X PUT "http://localhost:8080/services/imageserv" -d "{\"mllib\":\"caffe\",\"description\":\"image classification service\",\"type\":\"supervised\",\"parameters\":{\"input\":{\"connector\":\"image\"},\"mllib\":{\"nclasses\":1000}},\"model\":{\"repository\":\"/opt/models/ggnet/\"}}"
{"status":{"code":201,"msg":"Created"}}
```
and
```
curl -X POST "http://localhost:8080/predict" -d "{\"service\":\"imageserv\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":3},\"mllib\":{\"gpu\":true}},\"data\":[\"http://i.ytimg.com/vi/0vxOhd4qlnA/maxresdefault.jpg\"]}"
```

Try the `POST` call twice: first time loads the net so it takes slightly below a second, then second call should yield a `time` around 100ms as reported in the output JSON.

#### Access to server logs

To look at server logs, use 
```
docker logs -f <container name>
```
where <container name> can be obtained via `docker ps`

Example:


- start container and server:
```
> docker run -d -p 8080:8080 beniz/deepdetect_cpu
```

- look for container:
```
> docker ps
CONTAINER ID        IMAGE                  COMMAND                  CREATED              STATUS              PORTS                    NAMES
d9944734d5d6        beniz/deepdetect_cpu   "/bin/sh -c './dede -"   17 seconds ago       Up 16 seconds       0.0.0.0:8080->8080/tcp   loving_shaw
```

- access server logs:
```
> docker logs -f loving_shaw 

DeepDetect [ commit 4e2c9f4cbd55eeba3a93fae71d9d62377e91ffa5 ]
Running DeepDetect HTTP server on 0.0.0.0:8080
```

- share a volume with the image:
```
docker run -d -p 8080:8080 -v /path/to/volume:/opt/deepdetect beniz/deepdetect_cpu
```
where `path/to/volume` is the path to your local volume that you'd like to attach to `/opt/deepdetect/`. This is useful for sharing / saving models, etc...

#### Building an image

Example goes with the CPU image:
```
cd cpu
docker build -t beniz/deepdetect_cpu --no-cache .
```
[![Build Status](https://drone.io/github.com/tleyden/elastic-thought/status.png)](https://drone.io/github.com/tleyden/elastic-thought/latest) [![GoDoc](https://godoc.org/github.com/tleyden/elastic-thought?status.png)](https://godoc.org/github.com/tleyden/elastic-thought) [![Coverage Status](https://coveralls.io/repos/tleyden/elastic-thought/badge.svg?branch=master)](https://coveralls.io/r/tleyden/elastic-thought?branch=master) [![Join the chat at https://gitter.im/tleyden/elastic-thought](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tleyden/elastic-thought?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Scalable REST API wrapper for the [Caffe](http://caffe.berkeleyvision.org) deep learning framework. 

## The problem

Caffe is an awesome deep learning framework, but running it on a single laptop or desktop computer isn't nearly as productive as running it in the cloud at scale.

ElasticThought gives you the ability to:

* Run multiple Caffe training jobs in parallel
* Queue up training jobs
* Tune the number of workers that process jobs on the queue 
* Interact with it via a REST API (and later build Web/Mobile apps on top of it)
* Multi-tenancy to allow multiple users to interact with it, each having access to only their own data

## Components

![ElasticThought Components](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-components.png)


* [Caffe](http://caffe.berkeleyvision.org/) - core deep learning framework
* [Couchbase Server](http://www.couchbase.com/nosql-databases/couchbase-server) - Distributed document database used as an object store ([source code](https://github.com/couchbase/manifest))
* [Sync Gateway](https://github.com/couchbase/sync_gateway) - REST adapter layer for Couchbase Server + Mobile Sync gateway
* [CBFS](https://github.com/couchbaselabs/cbfs) - Couchbase Distributed File System used as blob store
* [NSQ](http://nsq.io/) - Distributed message queue
* [ElasticThought REST Service](https://github.com/tleyden/elastic-thought/) - REST API server written in Go

## Deployment Architecture

Here is what a typical cluster might look like:

![ElasticThought Deployment](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-stack.png) 

If running on AWS, each [CoreOS](https://coreos.com/) instance would be running on its own EC2 instance.

Although not shown, all components would be running inside of [Docker](https://www.docker.com/) containers.

It would be possible to start more nodes which only had Caffe GPU workers running.

## Roadmap

*Current Status: everything under heavy construction, not ready for public consumption yet*

1. **[done]** Working end-to-end with IMAGE_DATA caffe layer using a single test set with a single training set, and ability to query trained set.
1. **[done]** Support LEVELDB / LMDB data formats, to run mnist example.
1. **[in progress]** Package everything up to make it easy to deploy locally or in the cloud
1. Support the majority of caffe use cases
1. Ability to auto-scale worker instances up and down based on how many jobs are in the message queue.
1. Attempt to add support for other deep learning frameworks: pylearn2, cuda-convnet, etc.
1. Build a Web App on top of the REST API that leverages [PouchDB](https://github.com/pouchdb/pouchdb)
1. Build Android and iOS mobile apps on top of the REST API that leverages [Couchbase Mobile](https://github.com/couchbase/couchbase-lite-android)


## Design goals

* 100% Open Source (Apache 2 / BSD), including all components used.
* Architected to enable *warehouse scale* computing
* No IAAS lockin -- easily migrate between AWS, GCE, or your own private data center
* Ability to scale *down* as well as up

## Documentation 

* [REST API](http://docs.elasticthought.apiary.io/)
* [Godocs](http://godoc.org/github.com/tleyden/elastic-thought)
* This README

## System Requirements

ElasticThought requires CoreOS to run.

If you want to access the GPU, you will need to do extra work to get [CoreOS working with Nvidia CUDA GPU Drivers](http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/)

## Installing elastic-thought on a single CoreOS host (Development mode)

If you are on OSX, you'll first need to install Vagrant, VirtualBox, and CoreOS.  See [CoreOS on Vagrant](https://coreos.com/docs/running-coreos/platforms/vagrant/) for instructions.  

Here's what will be created:

                                                                          
                                                                          
               ┌─────────────────────────────────────────────────────────┐
               │                       CoreOS Host                       │
               │  ┌──────────────────────────┐  ┌─────────────────────┐  │
               │  │     Docker Container     │  │  Docker Container   │  │
               │  │   ┌───────────────────┐  │  │    ┌────────────┐   │  │
               │  │   │  Elastic Thought  │  │  │    │Sync Gateway│   │  │
               │  │   │      Server       │  │  │    │  Database  │   │  │
               │  │   │   ┌───────────┐   │  │  │    │            │   │  │
               │  │   │   │In-process │   │◀─┼──┼───▶│            │   │  │
               │  │   │   │   Caffe   │   │  │  │    │            │   │  │
               │  │   │   │  worker   │   │  │  │    │            │   │  │
               │  │   │   └───────────┘   │  │  │    └────────────┘   │  │
               │  │   └───────────────────┘  │  └─────────────────────┘  │
               │  └──────────────────────────┘                           │
               └─────────────────────────────────────────────────────────┘
	       

Run the following commands on your CoreOS box (to get in, you may need to `vagrant ssh core-01`)

**Start Sync Gateway Database**

```
$ docker run -d --name sync-gateway -P couchbase/sync-gateway:1.1.0-forestdb_bucket sync_gateway https://gist.githubusercontent.com/tleyden/8051567cf62dfa8f89ca/raw/43d4abc9ef64cef7b4bbbdf6cb8ce80c456efd1f/gistfile1.txt
```

**Start ElasticThought REST API server**

```
$ docker run -d --name elastic-thought -p 8080:8080 --link sync-gateway:sync-gateway tleyden5iwx/elastic-thought-cpu-develop bash -c 'refresh-elastic-thought; elastic-thought --sync-gw http://sync-gateway:4984/elastic-thought'
```

It's also a good idea to check the logs of both containers to look for any errors:

```
$ docker logs sync-gateway 
$ docker logs -f elastic-thought
```

At this point you can [test the API](http://docs.elasticthought.apiary.io/) via curl. 


## Installing elastic-thought on AWS (Production mode)

It should be possible to install elastic-thought anywhere that CoreOS is supported.  Currently, there are instructions for AWS and Vagrant (below).

### Launch EC2 instances via CloudFormation script

*Note: the instance will launch in **us-east-1***.  If you want to launch in another region, please [file an issue](https://github.com/tleyden/elastic-thought/issues).

* [Launch CPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_cpu.template) or [Launch GPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_gpu.template) 
* Choose 3 node cluster with m3.medium or g2.2xlarge (GPU case) instance type
* All other values should be default

### Verify CoreOS cluster

Run:

```
$ fleetctl list-machines
```

Which should show all the CoreOS machines in your cluster.  (this uses etcd under the hood, so will also validate that etcd is setup correctly).

### Kick off ElasticThought

Ssh into one of the machines (doesn't matter which): `ssh -A core@ec2-54-160-96-153.compute-1.amazonaws.com`

```
$ wget https://raw.githubusercontent.com/tleyden/elastic-thought/master/docker/scripts/elasticthought-cluster-init.sh
$ chmod +x elasticthought-cluster-init.sh
$ ./elasticthought-cluster-init.sh -v 3.0.1 -n 3 -u "user:passw0rd" -p gpu 
```

Once it launches, verify your cluster by running `fleetctl list-units`.  

It should look like this:

```
UNIT						MACHINE				ACTIVE	SUB
cbfs_announce@1.service                         2340c553.../10.225.17.229       active	running
cbfs_announce@2.service                         fbd4562e.../10.182.197.145      active	running
cbfs_announce@3.service                         0f5e2e11.../10.168.212.210      active	running
cbfs_node@1.service                             2340c553.../10.225.17.229       active	running
cbfs_node@2.service                             fbd4562e.../10.182.197.145      active	running
cbfs_node@3.service                             0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node.service                0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node_announce.service       0f5e2e11.../10.168.212.210      active	running
couchbase_node.1.service                        2340c553.../10.225.17.229       active	running
couchbase_node.2.service                        fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@1.service                   2340c553.../10.225.17.229       active	running
elastic_thought_gpu@2.service                   fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@3.service                   0f5e2e11.../10.168.212.210      active	running
sync_gw_announce@1.service                      2340c553.../10.225.17.229       active	running
sync_gw_announce@2.service                      fbd4562e.../10.182.197.145      active	running
sync_gw_announce@3.service                      0f5e2e11.../10.168.212.210      active	running
sync_gw_node@1.service                          2340c553.../10.225.17.229       active	running
sync_gw_node@2.service                          fbd4562e.../10.182.197.145      active	running
sync_gw_node@3.service                          0f5e2e11.../10.168.212.210      active	running
```

At this point you should be able to access the [REST API](http://docs.elasticthought.apiary.io/) on the public ip any of the three Sync Gateway machines.

	
## Installing elastic-thought on Vagrant (Staging mode)

This mode tries to replicate the Production mode described above, but on Vagrant instead of AWS.

### Update Vagrant

Make sure you're running a current version of Vagrant, otherwise the plugin install below may [fail](https://github.com/mitchellh/vagrant/issues/3769).

```
$ vagrant -v
1.7.1
```

### Install CoreOS on Vagrant

Clone the coreos/vagrant fork that has been customized for running ElasticThought.

```
$ cd ~/Vagrant 
$ git clone git@github.com:tleyden/coreos-vagrant.git
$ cd coreos-vagrant
$ cp config.rb.sample config.rb
$ cp user-data.sample user-data
```

By default this will run a **two node** cluster, if you want to change this, update the `$num_instances` variable in the `config.rb` file.

### Run CoreOS

```
$ vagrant up
```

Ssh in:

```
$ vagrant ssh core-01 -- -A
```

If you see:

```
Failed Units: 1
  user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service
```

Jump to **Workaround CoreOS + Vagrant issues** below.

Verify things started up correctly:

```
core@core-01 ~ $ fleectctl list-machines
```

If you get errors like:

```
2015/03/26 16:58:50 INFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2015/03/26 16:58:50 ERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100ms
```

Jump to **Workaround CoreOS + Vagrant issues** below.

### Workaround CoreOS + Vagrant issues:

First exit out of CoreOS:

```
core@core-01 ~ $ exit
```

On your OSX workstation, try the following workaround:

```
$ sed -i '' 's/420/0644/' user-data
$ sed -i '' 's/484/0744/' user-data
$ vagrant reload --provision
```

Ssh back in:

```
$ vagrant ssh core-01 -- -A
```

Verify it worked:

```
core@core-01 ~ $ fleectctl list-machines
```

You should see:

```
MACHINE		IP		METADATA
ce0fec18...	172.17.8.102	-
d6402b24...	172.17.8.101	-
```

I filed [CoreOS cloudinit issue 328](https://github.com/coreos/coreos-cloudinit/issues/328) to figure out why this error is happening (possibly related issues: [CoreOS cloudinit issue 261](https://github.com/coreos/coreos-cloudinit/issues/261) or [CoreOS cloudinit issue 190](https://github.com/coreos/bugs/issues/190))


### Continue steps above 

Scroll up to the **Installing elastic-thought on AWS** section and start with **Verify CoreOS cluster**

## FAQ

* Is this useful for grid computing / distributed computation?  **Ans**:  No, this is not trying to be a grid computing (aka distributed computation) solution.  You may want to check out [Caffe Issue 876](https://github.com/BVLC/caffe/issues/876) or [ParameterServer](http://parameterserver.org/)
  
## Related Projects

* [DeepDetect - REST Api wrapper for Caffe](https://github.com/beniz/deepdetect) 
* [caffe-docker-classifier - Set up your own classifier API based on pre-trained GoogleNet model and Caffe.](https://github.com/irony/caffe-docker-classifier)


## License

Apache 2

[![Build Status](https://drone.io/github.com/tleyden/elastic-thought/status.png)](https://drone.io/github.com/tleyden/elastic-thought/latest) [![GoDoc](https://godoc.org/github.com/tleyden/elastic-thought?status.png)](https://godoc.org/github.com/tleyden/elastic-thought) [![Coverage Status](https://coveralls.io/repos/tleyden/elastic-thought/badge.svg?branch=master)](https://coveralls.io/r/tleyden/elastic-thought?branch=master) [![Join the chat at https://gitter.im/tleyden/elastic-thought](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tleyden/elastic-thought?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Scalable REST API wrapper for the [Caffe](http://caffe.berkeleyvision.org) deep learning framework. 

## The problem

Caffe is an awesome deep learning framework, but running it on a single laptop or desktop computer isn't nearly as productive as running it in the cloud at scale.

ElasticThought gives you the ability to:

* Run multiple Caffe training jobs in parallel
* Queue up training jobs
* Tune the number of workers that process jobs on the queue 
* Interact with it via a REST API (and later build Web/Mobile apps on top of it)
* Multi-tenancy to allow multiple users to interact with it, each having access to only their own data

## Components

![ElasticThought Components](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-components.png)


* [Caffe](http://caffe.berkeleyvision.org/) - core deep learning framework
* [Couchbase Server](http://www.couchbase.com/nosql-databases/couchbase-server) - Distributed document database used as an object store ([source code](https://github.com/couchbase/manifest))
* [Sync Gateway](https://github.com/couchbase/sync_gateway) - REST adapter layer for Couchbase Server + Mobile Sync gateway
* [CBFS](https://github.com/couchbaselabs/cbfs) - Couchbase Distributed File System used as blob store
* [NSQ](http://nsq.io/) - Distributed message queue
* [ElasticThought REST Service](https://github.com/tleyden/elastic-thought/) - REST API server written in Go

## Deployment Architecture

Here is what a typical cluster might look like:

![ElasticThought Deployment](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-stack.png) 

If running on AWS, each [CoreOS](https://coreos.com/) instance would be running on its own EC2 instance.

Although not shown, all components would be running inside of [Docker](https://www.docker.com/) containers.

It would be possible to start more nodes which only had Caffe GPU workers running.

## Roadmap

*Current Status: everything under heavy construction, not ready for public consumption yet*

1. **[done]** Working end-to-end with IMAGE_DATA caffe layer using a single test set with a single training set, and ability to query trained set.
1. **[done]** Support LEVELDB / LMDB data formats, to run mnist example.
1. **[in progress]** Support the majority of caffe use cases
1. Package everything up to make it easy to deploy  <-- initial release
1. Ability to auto-scale worker instances up and down based on how many jobs are in the message queue.
1. Attempt to add support for other deep learning frameworks: pylearn2, cuda-convnet, etc.
1. Build a Web App on top of the REST API that leverages [PouchDB](https://github.com/pouchdb/pouchdb)
1. Build Android and iOS mobile apps on top of the REST API that leverages [Couchbase Mobile](https://github.com/couchbase/couchbase-lite-android)


## Design goals

* 100% Open Source (Apache 2 / BSD), including all components used.
* Architected to enable *warehouse scale* computing
* No IAAS lockin -- easily migrate between AWS, GCE, or your own private data center
* Ability to scale *down* as well as up

## Documentation 

* [REST API](http://docs.elasticthought.apiary.io/)
* [Godocs](http://godoc.org/github.com/tleyden/elastic-thought)
* This README

## System Requirements

ElasticThought requires CoreOS to run.

If you want to access the GPU, you will need to do extra work to get [CoreOS working with Nvidia CUDA GPU Drivers](http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/)


## Installing elastic-thought on AWS (Production mode)

It should be possible to install elastic-thought anywhere that CoreOS is supported.  Currently, there are instructions for AWS and Vagrant (below).

### Launch EC2 instances via CloudFormation script

*Note: the instance will launch in **us-east-1***.  If you want to launch in another region, please [file an issue](https://github.com/tleyden/elastic-thought/issues).

* [Launch CPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_cpu.template) or [Launch GPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_gpu.template) 
* Choose 3 node cluster with m3.medium or g2.2xlarge (GPU case) instance type
* All other values should be default

### Verify CoreOS cluster

Run:

```
$ fleetctl list-machines
```

Which should show all the CoreOS machines in your cluster.  (this uses etcd under the hood, so will also validate that etcd is setup correctly).

### Kick off ElasticThought

Ssh into one of the machines (doesn't matter which): `ssh -A core@ec2-54-160-96-153.compute-1.amazonaws.com`

```
$ wget https://raw.githubusercontent.com/tleyden/elastic-thought/master/docker/scripts/elasticthought-cluster-init.sh
$ chmod +x elasticthought-cluster-init.sh
$ ./elasticthought-cluster-init.sh -v 3.0.1 -n 3 -u "user:passw0rd" -p gpu 
```

Once it launches, verify your cluster by running `fleetctl list-units`.  

It should look like this:

```
UNIT						MACHINE				ACTIVE	SUB
cbfs_announce@1.service                         2340c553.../10.225.17.229       active	running
cbfs_announce@2.service                         fbd4562e.../10.182.197.145      active	running
cbfs_announce@3.service                         0f5e2e11.../10.168.212.210      active	running
cbfs_node@1.service                             2340c553.../10.225.17.229       active	running
cbfs_node@2.service                             fbd4562e.../10.182.197.145      active	running
cbfs_node@3.service                             0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node.service                0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node_announce.service       0f5e2e11.../10.168.212.210      active	running
couchbase_node.1.service                        2340c553.../10.225.17.229       active	running
couchbase_node.2.service                        fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@1.service                   2340c553.../10.225.17.229       active	running
elastic_thought_gpu@2.service                   fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@3.service                   0f5e2e11.../10.168.212.210      active	running
sync_gw_announce@1.service                      2340c553.../10.225.17.229       active	running
sync_gw_announce@2.service                      fbd4562e.../10.182.197.145      active	running
sync_gw_announce@3.service                      0f5e2e11.../10.168.212.210      active	running
sync_gw_node@1.service                          2340c553.../10.225.17.229       active	running
sync_gw_node@2.service                          fbd4562e.../10.182.197.145      active	running
sync_gw_node@3.service                          0f5e2e11.../10.168.212.210      active	running
```

At this point you should be able to access the [REST API](http://docs.elasticthought.apiary.io/) on the public ip any of the three Sync Gateway machines.

## Installing elastic-thought on a single CoreOS host (Development mode)

If you are on OSX, you'll first need to install Vagrant, VirtualBox, and CoreOS.  See [CoreOS on Vagrant](https://coreos.com/docs/running-coreos/platforms/vagrant/) for instructions.  

Here's what will be created:

                                                                          
                                                                          
               ┌─────────────────────────────────────────────────────────┐
               │                       CoreOS Host                       │
               │  ┌──────────────────────────┐  ┌─────────────────────┐  │
               │  │     Docker Container     │  │  Docker Container   │  │
               │  │   ┌───────────────────┐  │  │    ┌────────────┐   │  │
               │  │   │  Elastic Thought  │  │  │    │Sync Gateway│   │  │
               │  │   │      Server       │  │  │    │  Database  │   │  │
               │  │   │   ┌───────────┐   │  │  │    │            │   │  │
               │  │   │   │In-process │   │◀─┼──┼───▶│            │   │  │
               │  │   │   │   Caffe   │   │  │  │    │            │   │  │
               │  │   │   │  worker   │   │  │  │    │            │   │  │
               │  │   │   └───────────┘   │  │  │    └────────────┘   │  │
               │  │   └───────────────────┘  │  └─────────────────────┘  │
               │  └──────────────────────────┘                           │
               └─────────────────────────────────────────────────────────┘
	       

```
$ vagrant ssh core-01
$ docker run --name sync-gateway -P couchbase/sync-gateway sync-gw-start -c feature/forestdb_bucket -g http://git.io/vfF4b
$ docker run --name elastic-thought -P --link sync-gateway:sync-gateway tleyden5iwx/elastic-thought-cpu-develop bash -c 'refresh-elastic-thought; elastic-thought --sync-gw http://sync-gateway:4984'
```
	
## Installing elastic-thought on Vagrant

### Update Vagrant

Make sure you're running a current version of Vagrant, otherwise the plugin install below may [fail](https://github.com/mitchellh/vagrant/issues/3769).

```
$ vagrant -v
1.7.1
```

### Install CoreOS on Vagrant

Clone the coreos/vagrant fork that has been customized for running ElasticThought.

```
$ cd ~/Vagrant 
$ git clone git@github.com:tleyden/coreos-vagrant.git
$ cd coreos-vagrant
$ cp config.rb.sample config.rb
$ cp user-data.sample user-data
```

By default this will run a **two node** cluster, if you want to change this, update the `$num_instances` variable in the `config.rb` file.

### Run CoreOS

```
$ vagrant up
```

Ssh in:

```
$ vagrant ssh core-01 -- -A
```

If you see:

```
Failed Units: 1
  user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service
```

Jump to **Workaround CoreOS + Vagrant issues** below.

Verify things started up correctly:

```
core@core-01 ~ $ fleectctl list-machines
```

If you get errors like:

```
2015/03/26 16:58:50 INFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2015/03/26 16:58:50 ERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100ms
```

Jump to **Workaround CoreOS + Vagrant issues** below.

### Workaround CoreOS + Vagrant issues:

First exit out of CoreOS:

```
core@core-01 ~ $ exit
```

On your OSX workstation, try the following workaround:

```
$ sed -i '' 's/420/0644/' user-data
$ sed -i '' 's/484/0744/' user-data
$ vagrant reload --provision
```

Ssh back in:

```
$ vagrant ssh core-01 -- -A
```

Verify it worked:

```
core@core-01 ~ $ fleectctl list-machines
```

You should see:

```
MACHINE		IP		METADATA
ce0fec18...	172.17.8.102	-
d6402b24...	172.17.8.101	-
```

I filed [CoreOS cloudinit issue 328](https://github.com/coreos/coreos-cloudinit/issues/328) to figure out why this error is happening (possibly related issues: [CoreOS cloudinit issue 261](https://github.com/coreos/coreos-cloudinit/issues/261) or [CoreOS cloudinit issue 190](https://github.com/coreos/bugs/issues/190))


### Continue steps above 

Scroll up to the **Installing elastic-thought on AWS** section and start with **Verify CoreOS cluster**

## FAQ

* Is this useful for grid computing / distributed computation?  **Ans**:  No, this is not trying to be a grid computing (aka distributed computation) solution.  You may want to check out [Caffe Issue 876](https://github.com/BVLC/caffe/issues/876) or [ParameterServer](http://parameterserver.org/)
  
## License

Apache 2
[![Build Status](https://drone.io/github.com/tleyden/elastic-thought/status.png)](https://drone.io/github.com/tleyden/elastic-thought/latest) [![GoDoc](https://godoc.org/github.com/tleyden/elastic-thought?status.png)](https://godoc.org/github.com/tleyden/elastic-thought) [![Coverage Status](https://coveralls.io/repos/tleyden/elastic-thought/badge.svg?branch=master)](https://coveralls.io/r/tleyden/elastic-thought?branch=master) [![Join the chat at https://gitter.im/tleyden/elastic-thought](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tleyden/elastic-thought?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Scalable REST API wrapper for the [Caffe](http://caffe.berkeleyvision.org) deep learning framework. 

## The problem

Caffe is an awesome deep learning framework, but running it on a single laptop or desktop computer isn't nearly as productive as running it in the cloud at scale.

ElasticThought gives you the ability to:

* Run multiple Caffe training jobs in parallel
* Queue up training jobs
* Tune the number of workers that process jobs on the queue 
* Interact with it via a REST API (and later build Web/Mobile apps on top of it)
* Multi-tenancy to allow multiple users to interact with it, each having access to only their own data

## Components

![ElasticThought Components](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-components.png)


* [Caffe](http://caffe.berkeleyvision.org/) - core deep learning framework
* [Couchbase Server](http://www.couchbase.com/nosql-databases/couchbase-server) - Distributed document database used as an object store ([source code](https://github.com/couchbase/manifest))
* [Sync Gateway](https://github.com/couchbase/sync_gateway) - REST adapter layer for Couchbase Server + Mobile Sync gateway
* [CBFS](https://github.com/couchbaselabs/cbfs) - Couchbase Distributed File System used as blob store
* [NSQ](http://nsq.io/) - Distributed message queue
* [ElasticThought REST Service](https://github.com/tleyden/elastic-thought/) - REST API server written in Go

## Deployment Architecture

Here is what a typical cluster might look like:

![ElasticThought Deployment](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-stack.png) 

If running on AWS, each [CoreOS](https://coreos.com/) instance would be running on its own EC2 instance.

Although not shown, all components would be running inside of [Docker](https://www.docker.com/) containers.

It would be possible to start more nodes which only had Caffe GPU workers running.

## Roadmap

*Current Status: everything under heavy construction, not ready for public consumption yet*

1. **[done]** Working end-to-end with IMAGE_DATA caffe layer using a single test set with a single training set, and ability to query trained set.
1. **[done]** Support LEVELDB / LMDB data formats, to run mnist example.
1. **[in progress]** Support the majority of caffe use cases
1. Package everything up to make it easy to deploy  <-- initial release
1. Ability to auto-scale worker instances up and down based on how many jobs are in the message queue.
1. Attempt to add support for other deep learning frameworks: pylearn2, cuda-convnet, etc.
1. Build a Web App on top of the REST API that leverages [PouchDB](https://github.com/pouchdb/pouchdb)
1. Build Android and iOS mobile apps on top of the REST API that leverages [Couchbase Mobile](https://github.com/couchbase/couchbase-lite-android)


## Design goals

* 100% Open Source (Apache 2 / BSD), including all components used.
* Architected to enable *warehouse scale* computing
* No IAAS lockin -- easily migrate between AWS, GCE, or your own private data center
* Ability to scale *down* as well as up

## Documentation 

* [REST API](http://docs.elasticthought.apiary.io/)
* [Godocs](http://godoc.org/github.com/tleyden/elastic-thought)
* This README

## System Requirements

ElasticThought requires CoreOS to run.

If you want to access the GPU, you will need to do extra work to get [CoreOS working with Nvidia CUDA GPU Drivers](http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/)


## Installing elastic-thought on AWS (Production mode)

It should be possible to install elastic-thought anywhere that CoreOS is supported.  Currently, there are instructions for AWS and Vagrant (below).

### Launch EC2 instances via CloudFormation script

*Note: the instance will launch in **us-east-1***.  If you want to launch in another region, please [file an issue](https://github.com/tleyden/elastic-thought/issues).

* [Launch CPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_cpu.template) or [Launch GPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_gpu.template) 
* Choose 3 node cluster with m3.medium or g2.2xlarge (GPU case) instance type
* All other values should be default

### Verify CoreOS cluster

Run:

```
$ fleetctl list-machines
```

Which should show all the CoreOS machines in your cluster.  (this uses etcd under the hood, so will also validate that etcd is setup correctly).

### Kick off ElasticThought

Ssh into one of the machines (doesn't matter which): `ssh -A core@ec2-54-160-96-153.compute-1.amazonaws.com`

```
$ wget https://raw.githubusercontent.com/tleyden/elastic-thought/master/docker/scripts/elasticthought-cluster-init.sh
$ chmod +x elasticthought-cluster-init.sh
$ ./elasticthought-cluster-init.sh -v 3.0.1 -n 3 -u "user:passw0rd" -p gpu 
```

Once it launches, verify your cluster by running `fleetctl list-units`.  

It should look like this:

```
UNIT						MACHINE				ACTIVE	SUB
cbfs_announce@1.service                         2340c553.../10.225.17.229       active	running
cbfs_announce@2.service                         fbd4562e.../10.182.197.145      active	running
cbfs_announce@3.service                         0f5e2e11.../10.168.212.210      active	running
cbfs_node@1.service                             2340c553.../10.225.17.229       active	running
cbfs_node@2.service                             fbd4562e.../10.182.197.145      active	running
cbfs_node@3.service                             0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node.service                0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node_announce.service       0f5e2e11.../10.168.212.210      active	running
couchbase_node.1.service                        2340c553.../10.225.17.229       active	running
couchbase_node.2.service                        fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@1.service                   2340c553.../10.225.17.229       active	running
elastic_thought_gpu@2.service                   fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@3.service                   0f5e2e11.../10.168.212.210      active	running
sync_gw_announce@1.service                      2340c553.../10.225.17.229       active	running
sync_gw_announce@2.service                      fbd4562e.../10.182.197.145      active	running
sync_gw_announce@3.service                      0f5e2e11.../10.168.212.210      active	running
sync_gw_node@1.service                          2340c553.../10.225.17.229       active	running
sync_gw_node@2.service                          fbd4562e.../10.182.197.145      active	running
sync_gw_node@3.service                          0f5e2e11.../10.168.212.210      active	running
```

At this point you should be able to access the [REST API](http://docs.elasticthought.apiary.io/) on the public ip any of the three Sync Gateway machines.

## Installing elastic-thought on a single CoreOS host (Development mode)

If you are on OSX, you'll first need to install Vagrant, VirtualBox, and CoreOS.  See [CoreOS on Vagrant](https://coreos.com/docs/running-coreos/platforms/vagrant/) for instructions.  

Here's what will be created:

                                                                          
                                                                          
               ┌─────────────────────────────────────────────────────────┐
               │                       CoreOS Host                       │
               │  ┌──────────────────────────┐  ┌─────────────────────┐  │
               │  │     Docker Container     │  │  Docker Container   │  │
               │  │   ┌───────────────────┐  │  │    ┌────────────┐   │  │
               │  │   │  Elastic Thought  │  │  │    │Sync Gateway│   │  │
               │  │   │      Server       │  │  │    │  Database  │   │  │
               │  │   │   ┌───────────┐   │  │  │    │            │   │  │
               │  │   │   │In-process │   │◀─┼──┼───▶│            │   │  │
               │  │   │   │   Caffe   │   │  │  │    │            │   │  │
               │  │   │   │  worker   │   │  │  │    │            │   │  │
               │  │   │   └───────────┘   │  │  │    └────────────┘   │  │
               │  │   └───────────────────┘  │  └─────────────────────┘  │
               │  └──────────────────────────┘                           │
               └─────────────────────────────────────────────────────────┘
	       

```
$ vagrant ssh core-01
$ docker run --name sync-gateway -P couchbase/sync-gateway sync-gw-start -c feature/forestdb_bucket -g http://git.io/vfF4b
$ docker run --name elastic-thought -P --link sync-gateway:sync-gateway tleyden5iwx/elastic-thought-cpu-develop bash -c 'refresh-elastic-thought; elastic-thought --sync-gw http://sync-gateway:4984'
```
	
## Installing elastic-thought on Vagrant

### Update Vagrant

Make sure you're running a current version of Vagrant, otherwise the plugin install below may [fail](https://github.com/mitchellh/vagrant/issues/3769).

```
$ vagrant -v
1.7.1
```

### Install CoreOS on Vagrant

Clone the coreos/vagrant fork that has been customized for running ElasticThought.

```
$ cd ~/Vagrant 
$ git clone git@github.com:tleyden/coreos-vagrant.git
$ cd coreos-vagrant
$ cp config.rb.sample config.rb
$ cp user-data.sample user-data
```

By default this will run a **two node** cluster, if you want to change this, update the `$num_instances` variable in the `config.rb` file.

### Run CoreOS

```
$ vagrant up
```

Ssh in:

```
$ vagrant ssh core-01 -- -A
```

If you see:

```
Failed Units: 1
  user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service
```

Jump to **Workaround CoreOS + Vagrant issues** below.

Verify things started up correctly:

```
core@core-01 ~ $ fleectctl list-machines
```

If you get errors like:

```
2015/03/26 16:58:50 INFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2015/03/26 16:58:50 ERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100ms
```

Jump to **Workaround CoreOS + Vagrant issues** below.

### Workaround CoreOS + Vagrant issues:

First exit out of CoreOS:

```
core@core-01 ~ $ exit
```

On your OSX workstation, try the following workaround:

```
$ sed -i '' 's/420/0644/' user-data
$ sed -i '' 's/484/0744/' user-data
$ vagrant reload --provision
```

Ssh back in:

```
$ vagrant ssh core-01 -- -A
```

Verify it worked:

```
core@core-01 ~ $ fleectctl list-machines
```

You should see:

```
MACHINE		IP		METADATA
ce0fec18...	172.17.8.102	-
d6402b24...	172.17.8.101	-
```

I filed [CoreOS cloudinit issue 328](https://github.com/coreos/coreos-cloudinit/issues/328) to figure out why this error is happening (possibly related issues: [CoreOS cloudinit issue 261](https://github.com/coreos/coreos-cloudinit/issues/261) or [CoreOS cloudinit issue 190](https://github.com/coreos/bugs/issues/190))


### Continue steps above 

Scroll up to the **Installing elastic-thought on AWS** section and start with **Verify CoreOS cluster**

## FAQ

* Is this useful for grid computing / distributed computation?  **Ans**:  No, this is not trying to be a grid computing (aka distributed computation) solution.  You may want to check out [Caffe Issue 876](https://github.com/BVLC/caffe/issues/876) or [ParameterServer](http://parameterserver.org/)
  
## License

Apache 2
[![Build Status](https://drone.io/github.com/tleyden/elastic-thought/status.png)](https://drone.io/github.com/tleyden/elastic-thought/latest) [![GoDoc](https://godoc.org/github.com/tleyden/elastic-thought?status.png)](https://godoc.org/github.com/tleyden/elastic-thought) [![Coverage Status](https://coveralls.io/repos/tleyden/elastic-thought/badge.svg?branch=master)](https://coveralls.io/r/tleyden/elastic-thought?branch=master) [![Join the chat at https://gitter.im/tleyden/elastic-thought](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tleyden/elastic-thought?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Scalable REST API wrapper for the [Caffe](http://caffe.berkeleyvision.org) deep learning framework. 

## The problem

Caffe is an awesome deep learning framework, but running it on a single laptop or desktop computer isn't nearly as productive as running it in the cloud at scale.

ElasticThought gives you the ability to:

* Run multiple Caffe training jobs in parallel
* Queue up training jobs
* Tune the number of workers that process jobs on the queue 
* Interact with it via a REST API (and later build Web/Mobile apps on top of it)
* Multi-tenancy to allow multiple users to interact with it, each having access to only their own data

## Components

![ElasticThought Components](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-components.png)


* [Caffe](http://caffe.berkeleyvision.org/) - core deep learning framework
* [Couchbase Server](http://www.couchbase.com/nosql-databases/couchbase-server) - Distributed document database used as an object store ([source code](https://github.com/couchbase/manifest))
* [Sync Gateway](https://github.com/couchbase/sync_gateway) - REST adapter layer for Couchbase Server + Mobile Sync gateway
* [CBFS](https://github.com/couchbaselabs/cbfs) - Couchbase Distributed File System used as blob store
* [NSQ](http://nsq.io/) - Distributed message queue
* [ElasticThought REST Service](https://github.com/tleyden/elastic-thought/) - REST API server written in Go

## Deployment Architecture

Here is what a typical cluster might look like:

![ElasticThought Deployment](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-stack.png) 

If running on AWS, each [CoreOS](https://coreos.com/) instance would be running on its own EC2 instance.

Although not shown, all components would be running inside of [Docker](https://www.docker.com/) containers.

It would be possible to start more nodes which only had Caffe GPU workers running.

## Roadmap

*Current Status: everything under heavy construction, not ready for public consumption yet*

1. **[done]** Working end-to-end with IMAGE_DATA caffe layer using a single test set with a single training set, and ability to query trained set.
1. **[done]** Support LEVELDB / LMDB data formats, to run mnist example.
1. **[in progress]** Support the majority of caffe use cases
1. Package everything up to make it easy to deploy  <-- initial release
1. Ability to auto-scale worker instances up and down based on how many jobs are in the message queue.
1. Attempt to add support for other deep learning frameworks: pylearn2, cuda-convnet, etc.
1. Build a Web App on top of the REST API that leverages [PouchDB](https://github.com/pouchdb/pouchdb)
1. Build Android and iOS mobile apps on top of the REST API that leverages [Couchbase Mobile](https://github.com/couchbase/couchbase-lite-android)


## Design goals

* 100% Open Source (Apache 2 / BSD), including all components used.
* Architected to enable *warehouse scale* computing
* No IAAS lockin -- easily migrate between AWS, GCE, or your own private data center
* Ability to scale *down* as well as up

## Documentation 

* [REST API](http://docs.elasticthought.apiary.io/)
* [Godocs](http://godoc.org/github.com/tleyden/elastic-thought)
* This README

## System Requirements

ElasticThought requires CoreOS to run.

If you want to access the GPU, you will need to do extra work to get [CoreOS working with Nvidia CUDA GPU Drivers](http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/)


## Installing elastic-thought on AWS (Production mode)

It should be possible to install elastic-thought anywhere that CoreOS is supported.  Currently, there are instructions for AWS and Vagrant (below).

### Launch EC2 instances via CloudFormation script

*Note: the instance will launch in **us-east-1***.  If you want to launch in another region, please [file an issue](https://github.com/tleyden/elastic-thought/issues).

* [Launch CPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_cpu.template) or [Launch GPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_gpu.template) 
* Choose 3 node cluster with m3.medium or g2.2xlarge (GPU case) instance type
* All other values should be default

### Verify CoreOS cluster

Run:

```
$ fleetctl list-machines
```

Which should show all the CoreOS machines in your cluster.  (this uses etcd under the hood, so will also validate that etcd is setup correctly).

### Kick off ElasticThought

Ssh into one of the machines (doesn't matter which): `ssh -A core@ec2-54-160-96-153.compute-1.amazonaws.com`

```
$ wget https://raw.githubusercontent.com/tleyden/elastic-thought/master/docker/scripts/elasticthought-cluster-init.sh
$ chmod +x elasticthought-cluster-init.sh
$ ./elasticthought-cluster-init.sh -v 3.0.1 -n 3 -u "user:passw0rd" -p gpu 
```

Once it launches, verify your cluster by running `fleetctl list-units`.  

It should look like this:

```
UNIT						MACHINE				ACTIVE	SUB
cbfs_announce@1.service                         2340c553.../10.225.17.229       active	running
cbfs_announce@2.service                         fbd4562e.../10.182.197.145      active	running
cbfs_announce@3.service                         0f5e2e11.../10.168.212.210      active	running
cbfs_node@1.service                             2340c553.../10.225.17.229       active	running
cbfs_node@2.service                             fbd4562e.../10.182.197.145      active	running
cbfs_node@3.service                             0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node.service                0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node_announce.service       0f5e2e11.../10.168.212.210      active	running
couchbase_node.1.service                        2340c553.../10.225.17.229       active	running
couchbase_node.2.service                        fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@1.service                   2340c553.../10.225.17.229       active	running
elastic_thought_gpu@2.service                   fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@3.service                   0f5e2e11.../10.168.212.210      active	running
sync_gw_announce@1.service                      2340c553.../10.225.17.229       active	running
sync_gw_announce@2.service                      fbd4562e.../10.182.197.145      active	running
sync_gw_announce@3.service                      0f5e2e11.../10.168.212.210      active	running
sync_gw_node@1.service                          2340c553.../10.225.17.229       active	running
sync_gw_node@2.service                          fbd4562e.../10.182.197.145      active	running
sync_gw_node@3.service                          0f5e2e11.../10.168.212.210      active	running
```

At this point you should be able to access the [REST API](http://docs.elasticthought.apiary.io/) on the public ip any of the three Sync Gateway machines.

## Installing elastic-thought on a single CoreOS host (Development mode)

If you are on OSX, you'll first need to install Vagrant, VirtualBox, and CoreOS.  See [CoreOS on Vagrant](https://coreos.com/docs/running-coreos/platforms/vagrant/) for instructions.  

Here's what will be created:

                                                                          
                                                                          
               ┌─────────────────────────────────────────────────────────┐
               │                       CoreOS Host                       │
               │  ┌──────────────────────────┐  ┌─────────────────────┐  │
               │  │     Docker Container     │  │  Docker Container   │  │
               │  │   ┌───────────────────┐  │  │    ┌────────────┐   │  │
               │  │   │  Elastic Thought  │  │  │    │Sync Gateway│   │  │
               │  │   │      Server       │  │  │    │  Database  │   │  │
               │  │   │   ┌───────────┐   │  │  │    │            │   │  │
               │  │   │   │In-process │   │◀─┼──┼───▶│            │   │  │
               │  │   │   │   Caffe   │   │  │  │    │            │   │  │
               │  │   │   │  worker   │   │  │  │    │            │   │  │
               │  │   │   └───────────┘   │  │  │    └────────────┘   │  │
               │  │   └───────────────────┘  │  └─────────────────────┘  │
               │  └──────────────────────────┘                           │
               └─────────────────────────────────────────────────────────┘
	       

```
$ vagrant ssh core-01
$ docker run --name sync-gateway -P couchbase/sync-gateway sync-gw-start -c feature/forestdb_bucket -g http://git.io/vfF4b
$ docker run --name elastic-thought -P --link sync-gateway:sync-gateway tleyden5iwx/elastic-thought-cpu-develop bash -c 'refresh-elastic-thought; elastic-thought --sync-gw http://sync-gateway:4984'
```
	
## Installing elastic-thought on Vagrant

### Update Vagrant

Make sure you're running a current version of Vagrant, otherwise the plugin install below may [fail](https://github.com/mitchellh/vagrant/issues/3769).

```
$ vagrant -v
1.7.1
```

### Install CoreOS on Vagrant

Clone the coreos/vagrant fork that has been customized for running ElasticThought.

```
$ cd ~/Vagrant 
$ git clone git@github.com:tleyden/coreos-vagrant.git
$ cd coreos-vagrant
$ cp config.rb.sample config.rb
$ cp user-data.sample user-data
```

By default this will run a **two node** cluster, if you want to change this, update the `$num_instances` variable in the `config.rb` file.

### Run CoreOS

```
$ vagrant up
```

Ssh in:

```
$ vagrant ssh core-01 -- -A
```

If you see:

```
Failed Units: 1
  user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service
```

Jump to **Workaround CoreOS + Vagrant issues** below.

Verify things started up correctly:

```
core@core-01 ~ $ fleectctl list-machines
```

If you get errors like:

```
2015/03/26 16:58:50 INFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2015/03/26 16:58:50 ERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100ms
```

Jump to **Workaround CoreOS + Vagrant issues** below.

### Workaround CoreOS + Vagrant issues:

First exit out of CoreOS:

```
core@core-01 ~ $ exit
```

On your OSX workstation, try the following workaround:

```
$ sed -i '' 's/420/0644/' user-data
$ sed -i '' 's/484/0744/' user-data
$ vagrant reload --provision
```

Ssh back in:

```
$ vagrant ssh core-01 -- -A
```

Verify it worked:

```
core@core-01 ~ $ fleectctl list-machines
```

You should see:

```
MACHINE		IP		METADATA
ce0fec18...	172.17.8.102	-
d6402b24...	172.17.8.101	-
```

I filed [CoreOS cloudinit issue 328](https://github.com/coreos/coreos-cloudinit/issues/328) to figure out why this error is happening (possibly related issues: [CoreOS cloudinit issue 261](https://github.com/coreos/coreos-cloudinit/issues/261) or [CoreOS cloudinit issue 190](https://github.com/coreos/bugs/issues/190))


### Continue steps above 

Scroll up to the **Installing elastic-thought on AWS** section and start with **Verify CoreOS cluster**

## FAQ

* Is this useful for grid computing / distributed computation?  **Ans**:  No, this is not trying to be a grid computing (aka distributed computation) solution.  You may want to check out [Caffe Issue 876](https://github.com/BVLC/caffe/issues/876) or [ParameterServer](http://parameterserver.org/)
  
## License

Apache 2
[![Build Status](https://drone.io/github.com/tleyden/elastic-thought/status.png)](https://drone.io/github.com/tleyden/elastic-thought/latest) [![GoDoc](https://godoc.org/github.com/tleyden/elastic-thought?status.png)](https://godoc.org/github.com/tleyden/elastic-thought) [![Coverage Status](https://coveralls.io/repos/tleyden/elastic-thought/badge.svg?branch=master)](https://coveralls.io/r/tleyden/elastic-thought?branch=master) [![Join the chat at https://gitter.im/tleyden/elastic-thought](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tleyden/elastic-thought?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Scalable REST API wrapper for the [Caffe](http://caffe.berkeleyvision.org) deep learning framework. 

## The problem

Caffe is an awesome deep learning framework, but running it on a single laptop or desktop computer isn't nearly as productive as running it in the cloud at scale.

ElasticThought gives you the ability to:

* Run multiple Caffe training jobs in parallel
* Queue up training jobs
* Tune the number of workers that process jobs on the queue 
* Interact with it via a REST API (and later build Web/Mobile apps on top of it)
* Multi-tenancy to allow multiple users to interact with it, each having access to only their own data

## Components

![ElasticThought Components](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-components.png)


* [Caffe](http://caffe.berkeleyvision.org/) - core deep learning framework
* [Couchbase Server](http://www.couchbase.com/nosql-databases/couchbase-server) - Distributed document database used as an object store ([source code](https://github.com/couchbase/manifest))
* [Sync Gateway](https://github.com/couchbase/sync_gateway) - REST adapter layer for Couchbase Server + Mobile Sync gateway
* [CBFS](https://github.com/couchbaselabs/cbfs) - Couchbase Distributed File System used as blob store
* [NSQ](http://nsq.io/) - Distributed message queue
* [ElasticThought REST Service](https://github.com/tleyden/elastic-thought/) - REST API server written in Go

## Deployment Architecture

Here is what a typical cluster might look like:

![ElasticThought Deployment](http://tleyden-misc.s3.amazonaws.com/blog_images/elasticthought-stack.png) 

If running on AWS, each [CoreOS](https://coreos.com/) instance would be running on its own EC2 instance.

Although not shown, all components would be running inside of [Docker](https://www.docker.com/) containers.

It would be possible to start more nodes which only had Caffe GPU workers running.

## Roadmap

*Current Status: everything under heavy construction, not ready for public consumption yet*

1. **[done]** Working end-to-end with IMAGE_DATA caffe layer using a single test set with a single training set, and ability to query trained set.
1. **[done]** Support LEVELDB / LMDB data formats, to run mnist example.
1. **[in progress]** Support the majority of caffe use cases
1. Package everything up to make it easy to deploy  <-- initial release
1. Ability to auto-scale worker instances up and down based on how many jobs are in the message queue.
1. Attempt to add support for other deep learning frameworks: pylearn2, cuda-convnet, etc.
1. Build a Web App on top of the REST API that leverages [PouchDB](https://github.com/pouchdb/pouchdb)
1. Build Android and iOS mobile apps on top of the REST API that leverages [Couchbase Mobile](https://github.com/couchbase/couchbase-lite-android)


## Design goals

* 100% Open Source (Apache 2 / BSD), including all components used.
* Architected to enable *warehouse scale* computing
* No IAAS lockin -- easily migrate between AWS, GCE, or your own private data center
* Ability to scale *down* as well as up

## Documentation 

* [REST API](http://docs.elasticthought.apiary.io/)
* [Godocs](http://godoc.org/github.com/tleyden/elastic-thought)
* This README

## System Requirements

ElasticThought requires CoreOS to run.

If you want to access the GPU, you will need to do extra work to get [CoreOS working with Nvidia CUDA GPU Drivers](http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/)


## Installing elastic-thought on AWS (Production mode)

It should be possible to install elastic-thought anywhere that CoreOS is supported.  Currently, there are instructions for AWS and Vagrant (below).

### Launch EC2 instances via CloudFormation script

*Note: the instance will launch in **us-east-1***.  If you want to launch in another region, please [file an issue](https://github.com/tleyden/elastic-thought/issues).

* [Launch CPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_cpu.template) or [Launch GPU Stack](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/elastic-thought/cloudformation/elastic_thought_gpu.template) 
* Choose 3 node cluster with m3.medium or g2.2xlarge (GPU case) instance type
* All other values should be default

### Verify CoreOS cluster

Run:

```
$ fleetctl list-machines
```

Which should show all the CoreOS machines in your cluster.  (this uses etcd under the hood, so will also validate that etcd is setup correctly).

### Kick off ElasticThought

Ssh into one of the machines (doesn't matter which): `ssh -A core@ec2-54-160-96-153.compute-1.amazonaws.com`

```
$ wget https://raw.githubusercontent.com/tleyden/elastic-thought/master/docker/scripts/elasticthought-cluster-init.sh
$ chmod +x elasticthought-cluster-init.sh
$ ./elasticthought-cluster-init.sh -v 3.0.1 -n 3 -u "user:passw0rd" -p gpu 
```

Once it launches, verify your cluster by running `fleetctl list-units`.  

It should look like this:

```
UNIT						MACHINE				ACTIVE	SUB
cbfs_announce@1.service                         2340c553.../10.225.17.229       active	running
cbfs_announce@2.service                         fbd4562e.../10.182.197.145      active	running
cbfs_announce@3.service                         0f5e2e11.../10.168.212.210      active	running
cbfs_node@1.service                             2340c553.../10.225.17.229       active	running
cbfs_node@2.service                             fbd4562e.../10.182.197.145      active	running
cbfs_node@3.service                             0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node.service                0f5e2e11.../10.168.212.210      active	running
couchbase_bootstrap_node_announce.service       0f5e2e11.../10.168.212.210      active	running
couchbase_node.1.service                        2340c553.../10.225.17.229       active	running
couchbase_node.2.service                        fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@1.service                   2340c553.../10.225.17.229       active	running
elastic_thought_gpu@2.service                   fbd4562e.../10.182.197.145      active	running
elastic_thought_gpu@3.service                   0f5e2e11.../10.168.212.210      active	running
sync_gw_announce@1.service                      2340c553.../10.225.17.229       active	running
sync_gw_announce@2.service                      fbd4562e.../10.182.197.145      active	running
sync_gw_announce@3.service                      0f5e2e11.../10.168.212.210      active	running
sync_gw_node@1.service                          2340c553.../10.225.17.229       active	running
sync_gw_node@2.service                          fbd4562e.../10.182.197.145      active	running
sync_gw_node@3.service                          0f5e2e11.../10.168.212.210      active	running
```

At this point you should be able to access the [REST API](http://docs.elasticthought.apiary.io/) on the public ip any of the three Sync Gateway machines.

## Installing elastic-thought on a single CoreOS host (Development mode)

If you are on OSX, you'll first need to install Vagrant, VirtualBox, and CoreOS.  See [CoreOS on Vagrant](https://coreos.com/docs/running-coreos/platforms/vagrant/) for instructions.  

Here's what will be created:

                                                                          
                                                                          
               ┌─────────────────────────────────────────────────────────┐
               │                       CoreOS Host                       │
               │  ┌──────────────────────────┐  ┌─────────────────────┐  │
               │  │     Docker Container     │  │  Docker Container   │  │
               │  │   ┌───────────────────┐  │  │    ┌────────────┐   │  │
               │  │   │  Elastic Thought  │  │  │    │Sync Gateway│   │  │
               │  │   │      Server       │  │  │    │  Database  │   │  │
               │  │   │   ┌───────────┐   │  │  │    │            │   │  │
               │  │   │   │In-process │   │◀─┼──┼───▶│            │   │  │
               │  │   │   │   Caffe   │   │  │  │    │            │   │  │
               │  │   │   │  worker   │   │  │  │    │            │   │  │
               │  │   │   └───────────┘   │  │  │    └────────────┘   │  │
               │  │   └───────────────────┘  │  └─────────────────────┘  │
               │  └──────────────────────────┘                           │
               └─────────────────────────────────────────────────────────┘
	       

```
$ vagrant ssh core-01
$ docker run --name sync-gateway -P couchbase/sync-gateway sync-gw-start -c feature/forestdb_bucket -g http://git.io/vfF4b
$ docker run --name elastic-thought -P --link sync-gateway:sync-gateway tleyden5iwx/elastic-thought-cpu-develop bash -c 'refresh-elastic-thought; elastic-thought --sync-gw http://sync-gateway:4984'
```
	
## Installing elastic-thought on Vagrant

### Update Vagrant

Make sure you're running a current version of Vagrant, otherwise the plugin install below may [fail](https://github.com/mitchellh/vagrant/issues/3769).

```
$ vagrant -v
1.7.1
```

### Install CoreOS on Vagrant

Clone the coreos/vagrant fork that has been customized for running ElasticThought.

```
$ cd ~/Vagrant 
$ git clone git@github.com:tleyden/coreos-vagrant.git
$ cd coreos-vagrant
$ cp config.rb.sample config.rb
$ cp user-data.sample user-data
```

By default this will run a **two node** cluster, if you want to change this, update the `$num_instances` variable in the `config.rb` file.

### Run CoreOS

```
$ vagrant up
```

Ssh in:

```
$ vagrant ssh core-01 -- -A
```

If you see:

```
Failed Units: 1
  user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service
```

Jump to **Workaround CoreOS + Vagrant issues** below.

Verify things started up correctly:

```
core@core-01 ~ $ fleectctl list-machines
```

If you get errors like:

```
2015/03/26 16:58:50 INFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2015/03/26 16:58:50 ERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100ms
```

Jump to **Workaround CoreOS + Vagrant issues** below.

### Workaround CoreOS + Vagrant issues:

First exit out of CoreOS:

```
core@core-01 ~ $ exit
```

On your OSX workstation, try the following workaround:

```
$ sed -i '' 's/420/0644/' user-data
$ sed -i '' 's/484/0744/' user-data
$ vagrant reload --provision
```

Ssh back in:

```
$ vagrant ssh core-01 -- -A
```

Verify it worked:

```
core@core-01 ~ $ fleectctl list-machines
```

You should see:

```
MACHINE		IP		METADATA
ce0fec18...	172.17.8.102	-
d6402b24...	172.17.8.101	-
```

I filed [CoreOS cloudinit issue 328](https://github.com/coreos/coreos-cloudinit/issues/328) to figure out why this error is happening (possibly related issues: [CoreOS cloudinit issue 261](https://github.com/coreos/coreos-cloudinit/issues/261) or [CoreOS cloudinit issue 190](https://github.com/coreos/bugs/issues/190))


### Continue steps above 

Scroll up to the **Installing elastic-thought on AWS** section and start with **Verify CoreOS cluster**

## FAQ

* Is this useful for grid computing / distributed computation?  **Ans**:  No, this is not trying to be a grid computing (aka distributed computation) solution.  You may want to check out [Caffe Issue 876](https://github.com/BVLC/caffe/issues/876) or [ParameterServer](http://parameterserver.org/)
  
## License

Apache 2
<p align="center"><img src="https://raw.githubusercontent.com/pavlovai/match/master/resources/logo.png" alt="logo" width="220" /></p>

<p align="center"><strong>Scalable reverse image search</strong><br /><em>built on <a href="http://kubernetes.io/">Kubernetes</a> and <a href="https://www.elastic.co/">Elasticsearch</a></em></p>

<p align="center"><a href="https://github.com/pavlovai/match/stargazers"><img src="https://img.shields.io/github/stars/pavlovai/match.svg?style=flat" alt="GitHub stars" /></a> <a href="https://hub.docker.com/r/pavlov/match/"><img src="https://img.shields.io/docker/pulls/pavlov/match.svg" alt="Docker Pulls" /></a> <a href="http://kubernetes.io"><img src="https://img.shields.io/badge/kubernetes-ready-brightgreen.svg?style=flat" alt="Kubernetes shield" /></a> <a href="https://app.fossa.io/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Fpavlovai%2Fmatch?ref=badge_shield" alt="FOSSA Status"><img src="https://app.fossa.io/api/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Fpavlovai%2Fmatch.svg?type=shield"/></a></p>

**Pavlov Match** makes it easy to search for images that look similar to each other. Using a state-of-the-art perceptual hash, it is invariant to scaling and 90 degree rotations. Its HTTP API is quick to integrate and flexible for a number of reverse image search applications. Kubernetes and Elasticsearch allow Match to scale to billions of images with ease while giving you full control over where your data is stored. Match uses the awesome [ascribe/image-match](https://github.com/ascribe/image-match) under the hood for most of the image search legwork.

1. [Getting Started](#getting-started)
2. [API](#api)
3. [Development](#development)
4. [License and Acknowledgements](#license-and-acknowledgements)

## Getting Started

If you already have ElasticSearch running:
```
$ docker run -e ELASTICSEARCH_URL=https://daisy.us-west-1.es.amazonaws.com -it pavlov/match
```

If you want to run ElasticSearch locally as well, have [`docker-compose`](https://docs.docker.com/compose/) installed on your system, clone this repository and type:
```
$ make dev
```

Match is packaged as a Docker container ([pavlov/match](https://hub.docker.com/r/pavlov/match/) on Docker Hub), making it highly portable and scalable to billions of images. You can configure a few options using environment variables:

* **WORKER_COUNT** *(default: `4`)*

  The number of gunicorn workers to spin up.

* **ELASTICSEARCH_URL** *(default: `elasticsearch:9200`)*

  A URL pointing to the Elasticsearch database where image signatures are to be stored. If you don't want to host your own Elasticsearch cluster, consider using [AWS Elasticsearch Service](https://aws.amazon.com/elasticsearch-service/). That's what we use.

* **ELASTICSEARCH_INDEX** *(default: images)*

  The index in the Elasticsearch database where image signatures are to be stored.

* **ELASTICSEARCH_DOC_TYPE** *(default: images)*

  The doc type used for storing image signatures.


### Using in your own Kubernetes cluster

You can configure the service, replication controller, and secret like so:

```yaml
# match-service.yml
apiVersion: v1
kind: Service
metadata:
  name: match
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
  selector:
    app: match
```

```yaml
# match-rc.yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: match
spec:
  replicas: 1
  selector:
    app: match
  template:
    metadata:
      labels:
        app: match
    spec:
      containers:
      - name: match
        image: pavlov/match:latest
        ports:
        - containerPort: 80
        env:
        - name: WORKER_COUNT
          value: "4"
        - name: ELASTICSEARCH_URL
          valueFrom:
            secretKeyRef:
              name: match
              key: elasticsearch.url
        - name: ELASTICSEARCH_INDEX
          valueFrom:
            secretKeyRef:
              name: match
              key: elasticsearch.index
        - name: ELASTICSEARCH_DOC_TYPE
          valueFrom:
            secretKeyRef:
              name: match
              key: elasticsearch.doc-type
```

```yaml
# match-secret.yml
apiVersion: v1
kind: Secret
metadata:
  name: match
data:
  # https://daisy.us-west-1.es.amazonaws.com (change me)
  elasticsearch.url: aHR0cHM6Ly9kYWlzeS51cy13ZXN0LTEuZXMuYW1hem9uYXdzLmNvbQ==

  # images
  elasticsearch.index: aW1hZ2Vz

  # images
  elasticsearch.doc-type: aW1hZ2Vz
```

## API

Match has a simple HTTP API. All request parameters are specified via `application/x-www-form-urlencoded` or `multipart/form-data`.

* [POST `/add`](#post-add)
* [DELETE `/delete`](#delete-delete)
* [POST `/search`](#post-search)
* [POST `/compare`](#post-compare)
* [GET `/count`](#get-count)
* [GET `/list`](#get-list)
* [GET `/ping`](#get-ping)

---

### POST `/add`

Adds an image signature to the database.

#### Parameters

* **url** or **image** *(required)*

  The image to add to the database. It may be provided as a URL via `url` or as a `multipart/form-data` file upload via `image`.

* **filepath** *(required)*

  The path to save the image to in the database. If another image already exists at the given path, it will be overwritten.

* **metadata** *(default: None)*

  An arbitrary JSON object featuring meta data to attach to the image.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "add",
  "result": []
}
```

---

### DELETE `/delete`

Deletes an image signature from the database.

#### Parameters

* **filepath** *(required)*

  The path of the image signature in the database.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "delete",
  "result": []
}
```

---

### POST `/search`

Searches for a similar image in the database. Scores range from 0 to 100, with 100 being a perfect match.

#### Parameters

* **url** or **image** *(required)*

  The image to add to the database. It may be provided as a URL via `url` or as a `multipart/form-data` file upload via `image`.

* **all_orientations** *(default: true)*

  Whether or not to search for similar 90 degree rotations of the image.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "search",
  "result": [
    {
      "score": 99.0,
      "filepath": "http://static.wixstatic.com/media/0149b5_345c8f862e914a80bcfcc98fcd432e97.jpg_srz_614_709_85_22_0.50_1.20_0.00_jpg_srz"
    }
  ]
}
```

---

### POST `/compare`

Compares two images, returning a score for their similarity. Scores range from 0 to 100, with 100 being a perfect match.

#### Parameters

* **url1** or **image1**, **url2** or **image2** *(required)*

  The images to compare. They may be provided as a URL via `url1`/`url2` or as a `multipart/form-data` file upload via `image1`/`image2`.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "compare",
  "result": [
    {
      "score": 99.0
    }
  ]
}
```

---

### GET `/count`

Count the number of image signatures in the database.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "list",
  "result": [420]
}
```

---

### GET `/list`

Lists the file paths for the image signatures in the database.

#### Parameters

* **offset** *(default: 0)*

  The location in the database to begin listing image paths.

* **limit** *(default: 20)*

  The number of image paths to retrieve.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "list",
  "result": [
    "http://img.youtube.com/vi/iqPqylKy-bY/0.jpg",
    "https://i.ytimg.com/vi/zbjIwBggt2k/hqdefault.jpg",
    "https://s-media-cache-ak0.pinimg.com/736x/3d/67/6d/3d676d3f7f3031c9fd91c10b17d56afe.jpg"
  ]
}
```

---

### GET `/ping`

Check for the health of the server.

#### Example Response

```json
{
  "status": "ok",
  "error": [],
  "method": "ping",
  "result": []
}
```

## Development

    $ export ELASTICSEARCH_URL=https://daisy.us-west-1.es.amazonaws.com
    $ make build
    $ make run
    $ make push

## License and Acknowledgements

Match is based on [ascribe/image-match](https://github.com/ascribe/image-match), which is in turn based on the paper [_An image signature for any kind of image_, Goldberg et al](http://www.cs.cmu.edu/~hcwong/Pdfs/icip02.ps). There is an existing [reference implementation](https://www.pureftpd.org/project/libpuzzle) which may be more suited to your needs.

Match itself is released under the [BSD 3-Clause license](https://github.com/pavlovai/match/blob/master/LICENSE). `ascribe/image-match` is released under the Apache 2.0 license.
[![GoDoc](http://godoc.org/github.com/tleyden/open-ocr?status.png)](http://godoc.org/github.com/tleyden/open-ocr) 
[![Join the chat at https://gitter.im/tleyden/open-ocr](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tleyden/open-ocr?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)


OpenOCR makes it simple to host your own OCR REST API.

The heavy lifting OCR work is handled by [Tesseract OCR](https://code.google.com/p/tesseract-ocr/).

[Docker](http://www.docker.io) is used to containerize the various components of the service.

![screenshot](http://tleyden-misc.s3.amazonaws.com/blog_images/openocr-architecture.png)

# Features

* Scalable message passing architecture via RabbitMQ.
* Platform independence via Docker containers.
* [Kubernetes support](https://github.com/tleyden/open-ocr/tree/master/kubernetes): workers can run in a Kubernetes Replication Controller
* Supports 31 languages in addition to English 
* Ability to use an image pre-processing chain.  An example using [Stroke Width Transform](https://github.com/tleyden/open-ocr/wiki/Stroke-Width-Transform) is provided.
* Pass arguments to Tesseract such as character whitelist and page segment mode.
* [REST API docs](http://docs.openocr.apiary.io/)
* A [Go REST client](http://github.com/tleyden/open-ocr-client) is available.


# Launching OpenOCR on a Docker PAAS

OpenOCR can easily run on any PAAS that supports Docker containers.  Here are the instructions for a few that have already been tested:

* [Launch on Google Container Engine GKE - Kubernetes](https://github.com/tleyden/open-ocr/wiki/Installation-on-Google-Container-Engine)
* [Launch on AWS with CoreOS](https://github.com/tleyden/open-ocr/wiki/Installation-on-CoreOS-Fleet)
* [Launch on Google Compute Engine](https://github.com/tleyden/open-ocr/wiki/Installation-on-Google-Compute-Engine)

If your preferred PAAS isn't listed, please open a [Github issue](https://github.com/tleyden/open-ocr/issues) to request instructions.

# Launching OpenOCR on Ubuntu 14.04

OpenOCR can be launched on anything that supports Docker, such as Ubuntu 14.04.  

Here's how to install it from scratch and verify that it's working correctly.

## Install Docker

See [Installing Docker on Ubuntu](https://docs.docker.com/installation/ubuntulinux/) instructions.

## Find out your host address

```
$ ifconfig
eth0      Link encap:Ethernet  HWaddr 08:00:27:43:40:c7
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          ...
```

The ip address `10.0.2.15` will be used as the `RABBITMQ_HOST` env variable below.


# Launching OpenOCR command run.sh

 * [Install docker](https://docs.docker.com/installation/)
 * [Install docker-compose](https://docs.docker.com/compose/)
 * `git clone https://github.com/tleyden/open-ocr.git`
 * `cd open-ocr/docker-compose`
 * Type ```./run.sh ``` (in case you don't have execute right type ```sudo chmod +x run.sh```
 * The runner will ask you if you want to delete the images (choose y or n for each)
 * The runner will ask you to choose between version 1 and 2
   * Version 1 is using the ocr Tesseract 3.04. The memory usage is light. It is pretty fast and not costly in term of size (a simple aws instance with 1GB of ram and 8GB of storage is sufficiant). Result are acceptable
   * Version 2 is using the ocr Tesseract 4.00. The memory usage is light. It is less fast than tesseract 3 and more costly in term of size (an simple aws instance with 1GB of ram is sufficient but with an EBS of 16GB of storage). Result are really better compared to version 3.04.
   * To see a comparative you can have a look to the [official page of tesseract](https://github.com/tesseract-ocr/tesseract/wiki/4.0-Accuracy-and-Performance)


**You can use the docker-compose without the run.sh. For this just do:**

```
# for v1
export OPEN_OCR_INSTANCE=open-ocr

# for v2
export OPEN_OCR_INSTANCE=open-ocr-2

# then up (with -d to start it as deamon)
docker-compose up

```

Docker Compose will start four docker instances

* [RabbitMQ](https://index.docker.io/u/tutum/rabbitmq/)
* [OpenOCR Worker](https://index.docker.io/u/tleyden5iwx/open-ocr/)
* [OpenOCR HTTP API Server](https://index.docker.io/u/tleyden5iwx/open-ocr/)
* [OpenOCR Transform Worker](https://registry.hub.docker.com/u/tleyden5iwx/open-ocr-preprocessor/)

You are now ready to decode images → text via your REST API.

# Launching OpenOCR with Docker Compose on OSX

 * [Install docker](https://docs.docker.com/installation/)
 * [Install docker toolbox](https://www.docker.com/products/docker-toolbox)
 * Checkout OpenOCR repository 
 * `cd docker-compose directory`
 * `docker-machine start default`
 * `docker-machine env` 
 * Look at the Docker host IP address
 * Run  `docker-compose up -d` to run containers as daemons or `docker-compose up` to see the log in console
 

## How to test the REST API after turning on the docker-compose up

Where `IP_ADDRESS_OF_DOCKER_HOST` is what you saw when you run `docker-machine env` (e.g. 192.168.99.100)
and where `HTTP_POST` is the port number inside the `.yml` file inside the docker-compose directory presuming it should be the same 9292.

**Request**

```
$ curl -X POST -H "Content-Type: application/json" -d '{"img_url":"http://bit.ly/ocrimage","engine":"tesseract"}' http://IP_ADDRESS_OF_DOCKER_HOST:HTTP_PORT/ocr
```

Assuming the values are (192.168.99.100 and 9292 respectively)

```
$ curl -X POST -H "Content-Type: application/json" -d '{"img_url":"http://bit.ly/ocrimage","engine":"tesseract"}' http://192.168.99.100:9292/ocr
```

**Response**

It will return the decoded text for the [test image](http://bit.ly/ocrimage):

```
< HTTP/1.1 200 OK
< Date: Tue, 13 May 2014 16:18:50 GMT
< Content-Length: 283
< Content-Type: text/plain; charset=utf-8
<
You can create local variables for the pipelines within the template by
preﬁxing the variable name with a “$" sign. Variable names have to be
composed of alphanumeric characters and the underscore. In the example
below I have used a few variations that work for variable names.

```
 
# Test the REST API 

## With image url

**Request**

```
$ curl -X POST -H "Content-Type: application/json" -d '{"img_url":"http://bit.ly/ocrimage","engine":"tesseract"}' http://10.0.2.15:$HTTP_PORT/ocr
```

**Response**

It will return the decoded text for the [test image](http://bit.ly/ocrimage):

```
< HTTP/1.1 200 OK
< Date: Tue, 13 May 2014 16:18:50 GMT
< Content-Length: 283
< Content-Type: text/plain; charset=utf-8
<
You can create local variables for the pipelines within the template by
preﬁxing the variable name with a “$" sign. Variable names have to be
composed of alphanumeric characters and the underscore. In the example
below I have used a few variations that work for variable names.

```

## With image base64


**Request**

```
$ curl -X POST -H "Content-Type: application/json" -d '{"img_base64":"<YOUR BASE 64 HERE>","engine":"tesseract"}' http://10.0.2.15:$HTTP_PORT/ocr
```


## The REST API also supports:

* Uploading the image content via `multipart/related`, rather than passing an image URL.  (example client code provided in the [Go REST client](http://github.com/tleyden/open-ocr-client))
* Tesseract config vars (eg, equivalent of -c arguments when using Tesseract via the command line) and Page Seg Mode 
* Ability to use an image pre-processing chain, eg [Stroke Width Transform](https://github.com/tleyden/open-ocr/wiki/Stroke-Width-Transform).
* Non-English languages

See the [REST API docs](http://docs.openocr.apiary.io/) and the [Go REST client](http://github.com/tleyden/open-ocr-client) for details.


# Uploading local files using curl

The supplied `docs/upload-local-file.sh` provides an example of how to upload a local file using curl with `multipart/related` encoding of the json and image data:
* usage: `docs/upload-local-file.sh <urlendpoint> <file> [mimetype]`
* download the example ocr image `wget http://bit.ly/ocrimage`
* example: `docs/upload-local-file.sh http://10.0.2.15:$HTTP_PORT/ocr-file-upload ocrimage` 


# Community

* Follow [@OpenOCR](https://twitter.com/openocr) on Twitter
* Checkout the [Github issue tracker](https://github.com/tleyden/open-ocr/issues)

# Client Libraries

* **Go** [open-ocr-client](https://github.com/tleyden/open-ocr-client)
* **C#** [open-ocr-dotnet](https://github.com/alex-doe/open-ocr-dotnet)

# License

OpenOCR is Open Source and available under the Apache 2 License.

OpenOCR runs on Kubernetes!

* [Instructions to run on Google Container Engine](https://github.com/tleyden/open-ocr/wiki/Installation-on-Google-Container-Engine)

If you want to run it on a different Kubernetes provider, particularly on ones that don't offer the `Type: LoadBalancer` support for Kubernetes Service definitions, you will need to change the [open-ocr-httpd service](https://github.com/tleyden/open-ocr/blob/master/kubernetes/services/open_ocr_httpd.yml) accordingly.

# Quick Start

## Create RabbitMQ password

You will want to replace the `YOUR_RABBITMQ_PASS` below with something more secure.

```
printf "YOUR_RABBITMQ_PASS" > ./password
kubectl create secret generic rabbit-mq-password --from-file=./password

```

## Clone OpenOCR repo

```
git clone https://github.com/tleyden/open-ocr.git
```

## Launch RabbitMQ 

```
kubectl create -f kubernetes/pods/rabbitmq.yaml
kubectl create -f kubernetes/services/rabbitmq.yml
```

Wait until it launches by checking:

```
kubectl describe pod rabbitmq
```

and make sure the state is `RUNNING`

# Launch REST API Server

```
kubectl create -f kubernetes/pods/open_ocr_httpd.yml
kubectl create -f kubernetes/services/open_ocr_httpd.yml
```

# Launch OCR Worker

```
kubectl create -f kubernetes/replication-controllers/open-ocr-worker.yaml
```

# Verify

**Find the external IP**

```
kubectl describe service open-ocr-httpd-service
```

and look for the `LoadBalancer Ingress` value.  That is the publicly available IP address.

**Run curl against the REST API**

Replace the IP below with *your* `LoadBalancer Ingress` returned above in this command:

```
curl -X POST -H "Content-Type: application/json" -d '{"img_url":"http://bit.ly/ocrimage","engine":"tesseract"}' http://104.197.33.5/ocr
```

and you should get the output:

```
You can create local variables for the pipelines within the template by
preﬁxing the variable name with a “$" sign. Variable names have to be
composed of alphanumeric characters and the underscore. In the example
below I have used a few variations that work for variable names.
```
# Deepcut

[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](https://github.com/rkcosmos/deepcut/blob/master/LICENSE)

A Thai word tokenization library using Deep Neural Network.

## What's new?

* v0.5.2.0: Better weight matrix
* v0.5.1.0: Faster tokenization by code refactorization from our new contributor: Titipat Achakulvisut

## Performance

The Convolutional Neural network is trained from 90% of NECTEC's BEST corpus
(consists of 4 sections, article, news, novel and encyclopedia) and test on the rest 10%.
It is a binary classification model trying to predict whether a character is the beginning of word or not.
The results calculated from only 'true' class are as follow

* f1 score:  98.1%
* precision score:  97.8%
* recall score:  98.5%

## Installation

Install using `pip` for stable release,

```bash
pip install deepcut
```

For latest development release,

```bash
pip install git+git://github.com/rkcosmos/deepcut.git
```

Or clone the repository and install using `setup.py`

```bash
python setup.py install
```

Make sure you are using `tensorflow` backend in `Keras` by making sure `~/.keras/keras.json` is as follows (see also https://keras.io/backend/)

```bash
{
  "floatx": "float32",
  "epsilon": 1e-07,
  "backend": "tensorflow",
  "image_data_format": "channels_last"
}
```

We do not add `tensorflow` in automatic installation process because it has cpu and gpu version. Installing cpu version to everyone might break those who already have gpu version installed. So please install tensorflow yourself following this guildline https://www.tensorflow.org/install/.

### Docker

Install Docker on your machine 

For Linux:
```bash
curl -sSL https://get.docker.com | sudo sh
docker build -t deepcut .
```

For other OS: see https://docs.docker.com/engine/installation/

To run this Docker image:

```bash
docker run --rm -it deepcut
```

It will open a shell for us to play with deepcut.

## Usage

```python
import deepcut
deepcut.tokenize('ตัดคำได้ดีมาก')
```

Output will be in list format

```bash
['ตัด','คำ','ได้','ดี','มาก']
```

## Notes

Some texts might not be segmented as we would expected (e.g. 'โรงเรียน' -> ['โรง', 'เรียน']), this is because of

* BEST corpus (training data) tokenizes word this way (They use 'Compound words' as a criteria for segmentation)

* They are unseen/new words -> Ideally, this would be cured by having better corpus but it's not very practical so I am thinking of doing semi-supervised learning to incorporate new examples.

Any suggestion and comment are welcome, please post it in issue section.

## Contributors

* [Rakpong Kittinaradorn](https://github.com/rkcosmos)
* [Korakot Chaovavanich](https://github.com/korakot)
* [Titipat Achakulvisut](https://github.com/titipata)
* [Chanwit Kaewkasi](https://github.com/chanwit)

## Partner Organizations

* True Corporation

And we are open for contribution and collaboration.
# Research Notebook

This folder contains Jupyter notebook for training and testing performance of `deepcut`.
![Coverage](https://codecov.io/gh/maqquettex/sps_async/branch/dev/graph/badge.svg)
![TravisBuild](https://travis-ci.org/maqquettex/sps_async.svg?branch=dev)
# sps_async
Song Party Service v2 (powered by asyncio and aiohttp)

# Usage for my dear frontender

## Requirements:

 * docker ( ``` sudo pacman -S docker ``` )
 * docker-compose ( ``` sudo pip3 install docker-compose ``` )

Run:

 * ``` make conf_dev ``` for copying conf files to root dir (conf files in root dir are in .gitignore, so you  could change them to make own configuration)
 * ``` make ``` for running server (0.0.0.0:4000)
 * ``` make restart ``` if python backend changes and you want to reload it
 * ``` make stop ``` to stop all services
 * ``` make clean ``` to stop all services and clean images/volumes etc

Polls (demo for aiohttp)
========================

Example of polls project using aiohttp_, aiopg_ and aiohttp_jinja2_,
similar to django one.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/polls
    $ pip install -e .

Create database for your project with fake data::

    make docker_start_pg
    make fake_data

Run application::

    $ python -m aiohttpdemo_polls


Open browser::

    http://127.0.0.1:9002/admin


Requirements
============
* aiohttp_
* aiopg_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _aiopg: https://github.com/aio-libs/aiopg
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
Blog
====
Example of polls project using aiohttp_, aiopg_ and aiohttp_jinja2_,
similar to django one.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/blog
    $ pip install -e .

Create database for your project::

    make docker_start_pg
    make fake_data


Run application::

    $ make run

Open browser::

    http://127.0.0.1:9001/admin


Requirements
============
* aiohttp_
* aiopg_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _aiopg: https://github.com/aio-libs/aiopg
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
Motortwit Demo
==============

Example of mongo project using aiohttp_, motor_ and aiohttp_jinja2_,
similar to flask one. Here I assume you have *python3.5* and *docker* installed.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/motortwit
    $ pip install -e .

Create database for your project::

    make docker_start_mongo
    make fake_data


Run application::

    $ make run

Open browser::

    http://127.0.0.1:9001/admin


Requirements
============
* aiohttp_
* motor_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _motor: https://github.com/mongodb/motor
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
Motortwit Demo
==============

Example of mongo project using aiohttp_, motor_ and aiohttp_jinja2_,
similar to flask one. Here I assume you have *python3.5* and *docker* installed.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/motortwit
    $ pip install -e .

Create database for your project::

    make docker_start_mongo
    make fake_data


Run application::

    $ make run

Open browser::

    http://127.0.0.1:9001/admin


Requirements
============
* aiohttp_
* motor_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _motor: https://github.com/mongodb/motor
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
Motortwit Demo
==============

Example of mongo project using aiohttp_, motor_ and aiohttp_jinja2_,
similar to flask one. Here I assume you have *python3.5* and *docker* installed.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/motortwit
    $ pip install -e .

Create database for your project::

    make docker_start_mongo
    make fake_data


Run application::

    $ make run

Open browser::

    http://127.0.0.1:9001/admin


Requirements
============
* aiohttp_
* motor_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _motor: https://github.com/mongodb/motor
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
Motortwit Demo
==============

Example of mongo project using aiohttp_, motor_ and aiohttp_jinja2_,
similar to flask one. Here I assume you have *python3.5* and *docker* installed.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/motortwit
    $ pip install -e .

Create database for your project::

    make docker_start_mongo
    make fake_data


Run application::

    $ make run

Open browser::

    http://127.0.0.1:9001/admin


Requirements
============
* aiohttp_
* motor_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _motor: https://github.com/mongodb/motor
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
Motortwit Demo
==============

Example of mongo project using aiohttp_, motor_ and aiohttp_jinja2_,
similar to flask one. Here I assume you have *python3.5* and *docker* installed.

Installation
============

Clone repo and install library::

    $ git clone git@github.com:aio-libs/aiohttp_admin.git
    $ cd aiohttp_admin
    $ pip install -e .
    $ pip install -r requirements-dev.txt

Install the app::

    $ cd demos/motortwit
    $ pip install -e .

Create database for your project::

    make docker_start_mongo
    make fake_data


Run application::

    $ make run

Open browser::

    http://127.0.0.1:9001/admin


Requirements
============
* aiohttp_
* motor_
* aiohttp_jinja2_


.. _Python: https://www.python.org
.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _motor: https://github.com/mongodb/motor
.. _aiohttp_jinja2: https://github.com/aio-libs/aiohttp_jinja2
# docker-react-yarn

Make a react template app with yarn command 

```
docker volume create --driver local --name node_modules
docker run -it --rm -v $(pwd):/code -v node_modules:/code/node_modules smizy/nodejs sh
> /code # yarn add --dev react-scripts
> /code # yarn add react react-dom
> /code # exit

# Run dev server
docker run -it --rm -v $(pwd):/code -v node_modules:/code/node_modules -p 3000:3000 smizy/nodejs yarn start  

# Run build
docker run --rm -v $(pwd):/code -v node_modules:/code/node_modules smizy/nodejs yarn build 

# Run built page
docker run --rm -v $(pwd):/code -v node_modules:/code/node_modules smizy/nodejs yarn add --dev http-server
docker run -it --rm -v $(pwd):/code -v node_modules:/code/node_modules -w /code/build -p 8080:8080 smizy/nodejs hs

```# docker-mosquitto
# docker-scala-zeromq
Docker image providing Scala, ZeroMQ and Protobuf

FROM deepcortex/scala

To run ```make run```
<img src="https://avatars3.githubusercontent.com/u/12463357?v=3" />

# hepic-elasticfence
Docker container running the Elasticfence Stack + HEPIC Dashboards

- Elasticsearch 2.4.1 
- Kibi 4.6.4 + Siren 2.4.1
- Elasticfence Auth _(root/elasticFence)_
- Kibana-auth-elasticfence
- KiBrand 0.4.5
- Sentinl 4.x Snapshot
- Sense/Timelion

#### Usage

Install mixed ES container on new host w/ authentication (default: root/elasticFence)
```
docker pull qxip/docker-elasticfence
```
Create stateful data volume
```
docker volume create -o size=20GB --name esdata
```
Run container and map ports
```
docker run -tid --name elk -p 9200:9200 -p 5606:5606 -v esdata:/usr/share/elasticsearch qxip/docker-elasticfence
```
Connect shell to container
```
docker exec -ti elk /bin/bash
```

##### External ES Connector

Run the image using remote Elastic instance
```
$ docker run -i -t -e ELASTICSEARCH_URL=http://192.168.10.20:9200 -p 5601:5606 qxip/docker-kibi
```
# docker-telekibi
TeleKibi: Kibana/Kibi Supercharged with Elasticfence Plugins and much more!

## Usage

Run the image using local Elastic instance
```
$ docker run -i -t -p 9200:9200 -p 5606:5606 qxip/docker-telekibi
```

Run the image using remote Elastic instance
```
$ docker run -i -t -e ELASTICSEARCH_URL=http://192.168.10.20:9200 -p 5606:5606 qxip/docker-telekibi
```
