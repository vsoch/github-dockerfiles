#!/bin/sh

# Crude script to convert plain READMEs to HTML

echo '<!doctype html public "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html4/loose.dtd">

<html>

<head>

<title>Title Here</title>

<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">

</head>

<body>

<h1>Title Here</h1>'

sed '
	s/&/\&amp;/g
	s/</\&lt;/g
	s/>/\&gt;/g
' "$@" | awk '
/^====+$/ { print "<h2>" line "</h2>"; line = ""; getline; next }
NF == 0   { print line; print $0; print "<p>"; line = $0; next }
	  { print line; line = $0 }
END	  { print line }
'

echo '
</body>

</html>'
# $NetBSD: readme.mk,v 1.3 2014/07/20 22:58:02 tron Exp $

PFIX_README_FILES=	ADDRESS_CLASS_README \
	ADDRESS_REWRITING_README ADDRESS_VERIFICATION_README \
	BACKSCATTER_README BASIC_CONFIGURATION_README BUILTIN_FILTER_README \
	CONNECTION_CACHE_README CONTENT_INSPECTION_README \
	DATABASE_README DB_README DEBUG_README DSN_README ETRN_README \
	FILTER_README FORWARD_SECRECY_README IPV6_README LDAP_README \
	LOCAL_RECIPIENT_README MAILDROP_README \
	MEMCACHE_README MILTER_README MULTI_INSTANCE_README \
	NFS_README OVERVIEW POSTSCREEN_README RESTRICTION_CLASS_README \
	SASL_README SCHEDULER_README SMTPD_ACCESS_README SMTPD_POLICY_README \
	SMTPD_PROXY_README SOHO_README SQLITE_README \
	STANDARD_CONFIGURATION_README STRESS_README TLS_LEGACY_README \
	TLS_README TUNING_README UUCP_README VERP_README \
	VIRTUAL_README XCLIENT_README XFORWARD_README

#	ASCII only:
#	AAAREADME, RELEASE_NOTES

#	Not installed:
#	CDB_README, CYRUS_README, INSTALL, LINUX_README,  LMDB_README,
#	MYSQL_README, PACKAGE_README, PCRE_README, PGSQL_README, QMQP_README,
#	QSHAPE_README, ULTRIX_README
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Lua 5.3 readme</TITLE>
<LINK REL="stylesheet" TYPE="text/css" HREF="lua.css">
<META HTTP-EQUIV="content-type" CONTENT="text/html; charset=iso-8859-1">
<STYLE TYPE="text/css">
blockquote, .display {
	border: solid #a0a0a0 2px ;
	border-radius: 8px ;
	padding: 1em ;
	margin: 0px ;
}

.display {
	word-spacing: 0.25em ;
}

dl.display dd {
	padding-bottom: 0.2em ;
}

tt, kbd, code {
	font-size: 12pt ;
}
</STYLE>
</HEAD>

<BODY>

<H1>
<A HREF="http://www.lua.org/"><IMG SRC="logo.gif" ALT="Lua"></A>
Welcome to Lua 5.3
</H1>

<DIV CLASS="menubar">
<A HREF="#about">about</A>
&middot;
<A HREF="#install">installation</A>
&middot;
<A HREF="#changes">changes</A>
&middot;
<A HREF="#license">license</A>
&middot;
<A HREF="contents.html">reference manual</A>
</DIV>

<H2><A NAME="about">About Lua</A></H2>
<P>
Lua is a powerful, fast, lightweight, embeddable scripting language
developed by a
<A HREF="http://www.lua.org/authors.html">team</A>
at
<A HREF="http://www.puc-rio.br/">PUC-Rio</A>,
the Pontifical Catholic University of Rio de Janeiro in Brazil.
Lua is
<A HREF="#license">free software</A>
used in many products and projects around the world.

<P>
Lua's
<A HREF="http://www.lua.org/">official web site</A>
provides complete information
about Lua,
including
an
<A HREF="http://www.lua.org/about.html">executive summary</A>
and
updated
<A HREF="http://www.lua.org/docs.html">documentation</A>,
especially the
<A HREF="http://www.lua.org/manual/5.3/">reference manual</A>,
which may differ slightly from the
<A HREF="contents.html">local copy</A>
distributed in this package.

<H2><A NAME="install">Installing Lua</A></H2>
<P>
Lua is distributed in
<A HREF="http://www.lua.org/ftp/">source</A>
form.
You need to build it before using it.
Building Lua should be straightforward
because
Lua is implemented in pure ANSI C and compiles unmodified in all known
platforms that have an ANSI C compiler.
Lua also compiles unmodified as C++.
The instructions given below for building Lua are for Unix-like platforms.
See also
<A HREF="#other">instructions for other systems</A>
and
<A HREF="#customization">customization options</A>.

<P>
If you don't have the time or the inclination to compile Lua yourself,
get a binary from
<A HREF="http://lua-users.org/wiki/LuaBinaries">LuaBinaries</A>.
Try also
<A HREF="http://luadist.org/">LuaDist</A>,
a multi-platform distribution of Lua that includes batteries.

<H3>Building Lua</H3>
<P>
In most Unix-like platforms, simply do "<KBD>make</KBD>" with a suitable target.
Here are the details.

<OL>
<LI>
Open a terminal window and move to
the top-level directory, which is named <TT>lua-5.3.x</TT>.
The <TT>Makefile</TT> there controls both the build process and the installation process.
<P>
<LI>
  Do "<KBD>make</KBD>" and see if your platform is listed.
  The platforms currently supported are:
<P>
<P CLASS="display">
   aix bsd c89 freebsd generic linux macosx mingw posix solaris
</P>
<P>
  If your platform is listed, just do "<KBD>make xxx</KBD>", where xxx
  is your platform name.
<P>
  If your platform is not listed, try the closest one or posix, generic,
  c89, in this order.
<P>
<LI>
The compilation takes only a few moments
and produces three files in the <TT>src</TT> directory:
lua (the interpreter),
luac (the compiler),
and liblua.a (the library).
<P>
<LI>
  To check that Lua has been built correctly, do "<KBD>make test</KBD>"
  after building Lua. This will run the interpreter and print its version.
</OL>
<P>
If you're running Linux and get compilation errors,
make sure you have installed the <TT>readline</TT> development package
(which is probably named <TT>libreadline-dev</TT> or <TT>readline-devel</TT>).
If you get link errors after that,
then try "<KBD>make linux MYLIBS=-ltermcap</KBD>".

<H3>Installing Lua</H3>
<P>
  Once you have built Lua, you may want to install it in an official
  place in your system. In this case, do "<KBD>make install</KBD>". The official
  place and the way to install files are defined in the <TT>Makefile</TT>. You'll
  probably need the right permissions to install files.

<P>
  To build and install Lua in one step, do "<KBD>make xxx install</KBD>",
  where xxx is your platform name.

<P>
  To install Lua locally, do "<KBD>make local</KBD>".
  This will create a directory <TT>install</TT> with subdirectories
  <TT>bin</TT>, <TT>include</TT>, <TT>lib</TT>, <TT>man</TT>, <TT>share</TT>,
  and install Lua as listed below.

  To install Lua locally, but in some other directory, do
  "<KBD>make install INSTALL_TOP=xxx</KBD>", where xxx is your chosen directory.
  The installation starts in the <TT>src</TT> and <TT>doc</TT> directories,
  so take care if <TT>INSTALL_TOP</TT> is not an absolute path.

<DL CLASS="display">
<DT>
    bin:
<DD>
    lua luac
<DT>
    include:
<DD>
    lua.h luaconf.h lualib.h lauxlib.h lua.hpp
<DT>
    lib:
<DD>
    liblua.a
<DT>
    man/man1:
<DD>
    lua.1 luac.1
</DL>

<P>
  These are the only directories you need for development.
  If you only want to run Lua programs,
  you only need the files in <TT>bin</TT> and <TT>man</TT>.
  The files in <TT>include</TT> and <TT>lib</TT> are needed for
  embedding Lua in C or C++ programs.

<H3><A NAME="customization">Customization</A></H3>
<P>
  Three kinds of things can be customized by editing a file:
<UL>
    <LI> Where and how to install Lua &mdash; edit <TT>Makefile</TT>.
    <LI> How to build Lua &mdash; edit <TT>src/Makefile</TT>.
    <LI> Lua features &mdash; edit <TT>src/luaconf.h</TT>.
</UL>

<P>
  You don't actually need to edit the Makefiles because you may set the
  relevant variables in the command line when invoking make.
  Nevertheless, it's probably best to edit and save the Makefiles to
  record the changes you've made.

<P>
  On the other hand, if you need to customize some Lua features, you'll need
  to edit <TT>src/luaconf.h</TT> before building and installing Lua.
  The edited file will be the one installed, and
  it will be used by any Lua clients that you build, to ensure consistency.
  Further customization is available to experts by editing the Lua sources.

<H3><A NAME="other">Building Lua on other systems</A></H3>
<P>
  If you're not using the usual Unix tools, then the instructions for
  building Lua depend on the compiler you use. You'll need to create
  projects (or whatever your compiler uses) for building the library,
  the interpreter, and the compiler, as follows:

<DL CLASS="display">
<DT>
library:
<DD>
lapi.c lcode.c lctype.c ldebug.c ldo.c ldump.c lfunc.c lgc.c llex.c
lmem.c lobject.c lopcodes.c lparser.c lstate.c lstring.c ltable.c
ltm.c lundump.c lvm.c lzio.c
lauxlib.c lbaselib.c lbitlib.c lcorolib.c ldblib.c liolib.c
lmathlib.c loslib.c lstrlib.c ltablib.c lutf8lib.c loadlib.c linit.c
<DT>
interpreter:
<DD>
  library, lua.c
<DT>
compiler:
<DD>
  library, luac.c
</DL>

<P>
  To use Lua as a library in your own programs you'll need to know how to
  create and use libraries with your compiler. Moreover, to dynamically load
  C libraries for Lua you'll need to know how to create dynamic libraries
  and you'll need to make sure that the Lua API functions are accessible to
  those dynamic libraries &mdash; but <EM>don't</EM> link the Lua library
  into each dynamic library. For Unix, we recommend that the Lua library
  be linked statically into the host program and its symbols exported for
  dynamic linking; <TT>src/Makefile</TT> does this for the Lua interpreter.
  For Windows, we recommend that the Lua library be a DLL.
  In all cases, the compiler luac should be linked statically.

<P>
  As mentioned above, you may edit <TT>src/luaconf.h</TT> to customize
  some features before building Lua.

<H2><A NAME="changes">Changes since Lua 5.2</A></H2>
<P>
Here are the main changes introduced in Lua 5.3.
The
<A HREF="contents.html">reference manual</A>
lists the
<A HREF="manual.html#8">incompatibilities</A> that had to be introduced.

<H3>Main changes</H3>
<UL>
<LI> integers (64-bit by default)
<LI> official support for 32-bit numbers
<LI> bitwise operators
<LI> basic utf-8 support
<LI> functions for packing and unpacking values

</UL>

Here are the other changes introduced in Lua 5.3:
<H3>Language</H3>
<UL>
<LI> userdata can have any Lua value as uservalue
<LI> floor division
<LI> more flexible rules for some metamethods
</UL>

<H3>Libraries</H3>
<UL>
<LI> <CODE>ipairs</CODE> and the table library respect metamethods
<LI> strip option in <CODE>string.dump</CODE>
<LI> table library respects metamethods
<LI> new function <CODE>table.move</CODE>
<LI> new function <CODE>string.pack</CODE>
<LI> new function <CODE>string.unpack</CODE>
<LI> new function <CODE>string.packsize</CODE>
</UL>

<H3>C API</H3>
<UL>
<LI> simpler API for continuation functions in C
<LI> <CODE>lua_gettable</CODE> and similar functions return type of resulted value
<LI> strip option in <CODE>lua_dump</CODE>
<LI> new function: <CODE>lua_geti</CODE>
<LI> new function: <CODE>lua_seti</CODE>
<LI> new function: <CODE>lua_isyieldable</CODE>
<LI> new function: <CODE>lua_numbertointeger</CODE>
<LI> new function: <CODE>lua_rotate</CODE>
<LI> new function: <CODE>lua_stringtonumber</CODE>
</UL>

<H3>Lua standalone interpreter</H3>
<UL>
<LI> can be used as calculator; no need to prefix with '='
<LI> <CODE>arg</CODE> table available to all code
</UL>

<H2><A NAME="license">License</A></H2>
<P>
<A HREF="http://www.opensource.org/docs/definition.php">
<IMG SRC="osi-certified-72x60.png" ALIGN="right" ALT="[osi certified]" STYLE="padding-left: 30px ;">
</A>
Lua is free software distributed under the terms of the
<A HREF="http://www.opensource.org/licenses/mit-license.html">MIT license</A>
reproduced below;
it may be used for any purpose, including commercial purposes,
at absolutely no cost without having to ask us.

The only requirement is that if you do use Lua,
then you should give us credit by including the appropriate copyright notice somewhere in your product or its documentation.

For details, see
<A HREF="http://www.lua.org/license.html">this</A>.

<BLOCKQUOTE STYLE="padding-bottom: 0em">
Copyright &copy; 1994&ndash;2017 Lua.org, PUC-Rio.

<P>
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

<P>
The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

<P>
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
</BLOCKQUOTE>
<P>

<P CLASS="footer">
Last update:
Thu Dec 22 18:22:57 BRST 2016
</P>
<!--
Last change: revised for Lua 5.3.4
-->

</BODY>
</HTML>
Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast(), for 64 bits x86 (both AMD64 and Intel EM64t),
for use with Microsoft Macro Assembler (x64) for AMD64 and Microsoft C++ 64 bits.

gvmat64.asm is written by Gilles Vollant (2005), by using Brian Raiter 686/32 bits
   assembly optimized version from Jean-loup Gailly original longest_match function

inffasx64.asm and inffas8664.c were written by Chris Anderson, by optimizing
   original function from Mark Adler

Use instructions
----------------
Assemble the .asm files using MASM and put the object files into the zlib source
directory.  You can also get object files here:

     http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

define ASMV and ASMINF in your project. Include inffas8664.c in your source tree,
and inffasx64.obj and gvmat64.obj as object to link.


Build instructions
------------------
run bld_64.bat with Microsoft Macro Assembler (x64) for AMD64 (ml64.exe)

ml64.exe is given with Visual Studio 2005, Windows 2003 server DDK

You can get Windows 2003 server DDK with ml64 and cl for AMD64 from
  http://www.microsoft.com/whdc/devtools/ddk/default.mspx for low price)

This directory contains a Pascal (Delphi, Kylix) interface to the
zlib data compression library.


Directory listing
=================

zlibd32.mak     makefile for Borland C++
example.pas     usage example of zlib
zlibpas.pas     the Pascal interface to zlib
readme.txt      this file


Compatibility notes
===================

- Although the name "zlib" would have been more normal for the
  zlibpas unit, this name is already taken by Borland's ZLib unit.
  This is somehow unfortunate, because that unit is not a genuine
  interface to the full-fledged zlib functionality, but a suite of
  class wrappers around zlib streams.  Other essential features,
  such as checksums, are missing.
  It would have been more appropriate for that unit to have a name
  like "ZStreams", or something similar.

- The C and zlib-supplied types int, uInt, long, uLong, etc. are
  translated directly into Pascal types of similar sizes (Integer,
  LongInt, etc.), to avoid namespace pollution.  In particular,
  there is no conversion of unsigned int into a Pascal unsigned
  integer.  The Word type is non-portable and has the same size
  (16 bits) both in a 16-bit and in a 32-bit environment, unlike
  Integer.  Even if there is a 32-bit Cardinal type, there is no
  real need for unsigned int in zlib under a 32-bit environment.

- Except for the callbacks, the zlib function interfaces are
  assuming the calling convention normally used in Pascal
  (__pascal for DOS and Windows16, __fastcall for Windows32).
  Since the cdecl keyword is used, the old Turbo Pascal does
  not work with this interface.

- The gz* function interfaces are not translated, to avoid
  interfacing problems with the C runtime library.  Besides,
    gzprintf(gzFile file, const char *format, ...)
  cannot be translated into Pascal.


Legal issues
============

The zlibpas interface is:
  Copyright (C) 1995-2003 Jean-loup Gailly and Mark Adler.
  Copyright (C) 1998 by Bob Dellaca.
  Copyright (C) 2003 by Cosmin Truta.

The example program is:
  Copyright (C) 1995-2003 by Jean-loup Gailly.
  Copyright (C) 1998,1999,2000 by Jacques Nomssi Nzali.
  Copyright (C) 2003 by Cosmin Truta.

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the author be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

Building instructions for the DLL versions of Zlib 1.2.8
========================================================

This directory contains projects that build zlib and minizip using
Microsoft Visual C++ 9.0/10.0.

You don't need to build these projects yourself. You can download the
binaries from:
  http://www.winimage.com/zLibDll

More information can be found at this site.





Build instructions for Visual Studio 2008 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Compile assembly code (with Visual Studio Command Prompt) by running:
   bld_ml64.bat (in contrib\masmx64)
   bld_ml32.bat (in contrib\masmx86)
- Open contrib\vstudio\vc9\zlibvc.sln with Microsoft Visual C++ 2008
- Or run: vcbuild /rebuild contrib\vstudio\vc9\zlibvc.sln "Release|Win32"

Build instructions for Visual Studio 2010 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc10\zlibvc.sln with Microsoft Visual C++ 2010

Build instructions for Visual Studio 2012 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc11\zlibvc.sln with Microsoft Visual C++ 2012


Important
---------
- To use zlibwapi.dll in your application, you must define the
  macro ZLIB_WINAPI when compiling your application's source files.


Additional notes
----------------
- This DLL, named zlibwapi.dll, is compatible to the old zlib.dll built
  by Gilles Vollant from the zlib 1.1.x sources, and distributed at
    http://www.winimage.com/zLibDll
  It uses the WINAPI calling convention for the exported functions, and
  includes the minizip functionality. If your application needs that
  particular build of zlib.dll, you can rename zlibwapi.dll to zlib.dll.

- The new DLL was renamed because there exist several incompatible
  versions of zlib.dll on the Internet.

- There is also an official DLL build of zlib, named zlib1.dll. This one
  is exporting the functions using the CDECL convention. See the file
  win32\DLL_FAQ.txt found in this zlib distribution.

- There used to be a ZLIB_DLL macro in zlib 1.1.x, but now this symbol
  has a slightly different effect. To avoid compatibility problems, do
  not define it here.


Gilles Vollant
info@winimage.com

Overview
========

This directory contains an update to the ZLib interface unit,
distributed by Borland as a Delphi supplemental component.

The original ZLib unit is Copyright (c) 1997,99 Borland Corp.,
and is based on zlib version 1.0.4.  There are a series of bugs
and security problems associated with that old zlib version, and
we recommend the users to update their ZLib unit.


Summary of modifications
========================

- Improved makefile, adapted to zlib version 1.2.1.

- Some field types from TZStreamRec are changed from Integer to
  Longint, for consistency with the zlib.h header, and for 64-bit
  readiness.

- The zlib_version constant is updated.

- The new Z_RLE strategy has its corresponding symbolic constant.

- The allocation and deallocation functions and function types
  (TAlloc, TFree, zlibAllocMem and zlibFreeMem) are now cdecl,
  and _malloc and _free are added as C RTL stubs.  As a result,
  the original C sources of zlib can be compiled out of the box,
  and linked to the ZLib unit.


Suggestions for improvements
============================

Currently, the ZLib unit provides only a limited wrapper around
the zlib library, and much of the original zlib functionality is
missing.  Handling compressed file formats like ZIP/GZIP or PNG
cannot be implemented without having this functionality.
Applications that handle these formats are either using their own,
duplicated code, or not using the ZLib unit at all.

Here are a few suggestions:

- Checksum class wrappers around adler32() and crc32(), similar
  to the Java classes that implement the java.util.zip.Checksum
  interface.

- The ability to read and write raw deflate streams, without the
  zlib stream header and trailer.  Raw deflate streams are used
  in the ZIP file format.

- The ability to read and write gzip streams, used in the GZIP
  file format, and normally produced by the gzip program.

- The ability to select a different compression strategy, useful
  to PNG and MNG image compression, and to multimedia compression
  in general.  Besides the compression level

    TCompressionLevel = (clNone, clFastest, clDefault, clMax);

  which, in fact, could have used the 'z' prefix and avoided
  TColor-like symbols

    TCompressionLevel = (zcNone, zcFastest, zcDefault, zcMax);

  there could be a compression strategy

    TCompressionStrategy = (zsDefault, zsFiltered, zsHuffmanOnly, zsRle);

- ZIP and GZIP stream handling via TStreams.


--
Cosmin Truta <cosmint@cs.ubbcluj.ro>
                        ZLib for Ada thick binding (ZLib.Ada)
                        Release 1.3

ZLib.Ada is a thick binding interface to the popular ZLib data
compression library, available at http://www.gzip.org/zlib/.
It provides Ada-style access to the ZLib C library.


        Here are the main changes since ZLib.Ada 1.2:

- Attension: ZLib.Read generic routine have a initialization requirement
  for Read_Last parameter now. It is a bit incompartible with previous version,
  but extends functionality, we could use new parameters Allow_Read_Some and
  Flush now.

- Added Is_Open routines to ZLib and ZLib.Streams packages.

- Add pragma Assert to check Stream_Element is 8 bit.

- Fix extraction to buffer with exact known decompressed size. Error reported by
  Steve Sangwine.

- Fix definition of ULong (changed to unsigned_long), fix regression on 64 bits
  computers. Patch provided by Pascal Obry.

- Add Status_Error exception definition.

- Add pragma Assertion that Ada.Streams.Stream_Element size is 8 bit.


        How to build ZLib.Ada under GNAT

You should have the ZLib library already build on your computer, before
building ZLib.Ada. Make the directory of ZLib.Ada sources current and
issue the command:

  gnatmake test -largs -L<directory where libz.a is> -lz

Or use the GNAT project file build for GNAT 3.15 or later:

  gnatmake -Pzlib.gpr -L<directory where libz.a is>


        How to build ZLib.Ada under Aonix ObjectAda for Win32 7.2.2

1. Make a project with all *.ads and *.adb files from the distribution.
2. Build the libz.a library from the ZLib C sources.
3. Rename libz.a to z.lib.
4. Add the library z.lib to the project.
5. Add the libc.lib library from the ObjectAda distribution to the project.
6. Build the executable using test.adb as a main procedure.


        How to use ZLib.Ada

The source files test.adb and read.adb are small demo programs that show
the main functionality of ZLib.Ada.

The routines from the package specifications are commented.


Homepage: http://zlib-ada.sourceforge.net/
Author: Dmitriy Anisimkov <anisimkov@yahoo.com>

Contributors: Pascal Obry <pascal@obry.org>, Steve Sangwine <sjs@essex.ac.uk>
This directory contains a .Net wrapper class library for the ZLib1.dll

The wrapper includes support for inflating/deflating memory buffers,
.Net streaming wrappers for the gz streams part of zlib, and wrappers
for the checksum parts of zlib. See DotZLib/UnitTests.cs for examples.

Directory structure:
--------------------

LICENSE_1_0.txt       - License file.
readme.txt            - This file.
DotZLib.chm           - Class library documentation
DotZLib.build         - NAnt build file
DotZLib.sln           - Microsoft Visual Studio 2003 solution file

DotZLib\*.cs          - Source files for the class library

Unit tests:
-----------
The file DotZLib/UnitTests.cs contains unit tests for use with NUnit 2.1 or higher.
To include unit tests in the build, define nunit before building.


Build instructions:
-------------------

1. Using Visual Studio.Net 2003:
   Open DotZLib.sln in VS.Net and build from there. Output file (DotZLib.dll)
   will be found ./DotZLib/bin/release or ./DotZLib/bin/debug, depending on
   you are building the release or debug version of the library. Check
   DotZLib/UnitTests.cs for instructions on how to include unit tests in the
   build.

2. Using NAnt:
   Open a command prompt with access to the build environment and run nant
   in the same directory as the DotZLib.build file.
   You can define 2 properties on the nant command-line to control the build:
   debug={true|false} to toggle between release/debug builds (default=true).
   nunit={true|false} to include or esclude unit tests (default=true).
   Also the target clean will remove binaries.
   Output file (DotZLib.dll) will be found in either ./DotZLib/bin/release
   or ./DotZLib/bin/debug, depending on whether you are building the release
   or debug version of the library.

   Examples:
     nant -D:debug=false -D:nunit=false
       will build a release mode version of the library without unit tests.
     nant
       will build a debug version of the library with unit tests
     nant clean
       will remove all previously built files.


---------------------------------
Copyright (c) Henrik Ravn 2004

Use, modification and distribution are subject to the Boost Software License, Version 1.0.
(See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)

Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast().


Use instructions
----------------
Assemble using MASM, and copy the object files into the zlib source
directory, then run the appropriate makefile, as suggested below.  You can
donwload MASM from here:

    http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=7a1c9da0-0510-44a2-b042-7ef370530c64

You can also get objects files here:

    http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

Build instructions
------------------
* With Microsoft C and MASM:
nmake -f win32/Makefile.msc LOC="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj"

* With Borland C and TASM:
make -f win32/Makefile.bor LOCAL_ZLIB="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj" OBJPA="+match686c.obj+match686.obj+inffas32.obj"

        ZLIB version 1.2.8 for AS400 installation instructions

I) From an AS400 *SAVF file:

1)      Unpacking archive to an AS400 save file

On the AS400:

_       Create the ZLIB AS400 library:

        CRTLIB LIB(ZLIB) TYPE(*PROD) TEXT('ZLIB compression API library')

_       Create a work save file, for example:

                CRTSAVF FILE(ZLIB/ZLIBSAVF)

On a PC connected to the target AS400:

_       Unpack the save file image to a PC file "ZLIBSAVF"
_       Upload this file into the save file on the AS400, for example
                using ftp in BINARY mode.


2)      Populating the ZLIB AS400 source library

On the AS400:

_       Extract the saved objects into the ZLIB AS400 library using:

RSTOBJ OBJ(*ALL) SAVLIB(ZLIB) DEV(*SAVF) SAVF(ZLIB/ZLIBSAVF) RSTLIB(ZLIB)


3)      Customize installation:

_       Edit CL member ZLIB/TOOLS(COMPILE) and change parameters if needed,
                according to the comments.

_       Compile this member with:

        CRTCLPGM PGM(ZLIB/COMPILE) SRCFILE(ZLIB/TOOLS) SRCMBR(COMPILE)


4)      Compile and generate the service program:

_       This can now be done by executing:

        CALL PGM(ZLIB/COMPILE)



II) From the original source distribution:

1)      On the AS400, create the source library:

        CRTLIB LIB(ZLIB) TYPE(*PROD) TEXT('ZLIB compression API library')

2)      Create the source files:

        CRTSRCPF FILE(ZLIB/SOURCES) RCDLEN(112) TEXT('ZLIB library modules')
        CRTSRCPF FILE(ZLIB/H)       RCDLEN(112) TEXT('ZLIB library includes')
        CRTSRCPF FILE(ZLIB/TOOLS)   RCDLEN(112) TEXT('ZLIB library control utilities')

3)      From the machine hosting the distribution files, upload them (with
                FTP in text mode, for example) according to the following table:

    Original    AS400   AS400    AS400 AS400
    file        file    member   type  description
                SOURCES                Original ZLIB C subprogram sources
    adler32.c           ADLER32  C     ZLIB - Compute the Adler-32 checksum of a dta strm
    compress.c          COMPRESS C     ZLIB - Compress a memory buffer
    crc32.c             CRC32    C     ZLIB - Compute the CRC-32 of a data stream
    deflate.c           DEFLATE  C     ZLIB - Compress data using the deflation algorithm
    gzclose.c           GZCLOSE  C     ZLIB - Close .gz files
    gzlib.c             GZLIB    C     ZLIB - Miscellaneous .gz files IO support
    gzread.c            GZREAD   C     ZLIB - Read .gz files
    gzwrite.c           GZWRITE  C     ZLIB - Write .gz files
    infback.c           INFBACK  C     ZLIB - Inflate using a callback interface
    inffast.c           INFFAST  C     ZLIB - Fast proc. literals & length/distance pairs
    inflate.c           INFLATE  C     ZLIB - Interface to inflate modules
    inftrees.c          INFTREES C     ZLIB - Generate Huffman trees for efficient decode
    trees.c             TREES    C     ZLIB - Output deflated data using Huffman coding
    uncompr.c           UNCOMPR  C     ZLIB - Decompress a memory buffer
    zutil.c             ZUTIL    C     ZLIB - Target dependent utility functions
                H                      Original ZLIB C and ILE/RPG include files
    crc32.h             CRC32    C     ZLIB - CRC32 tables
    deflate.h           DEFLATE  C     ZLIB - Internal compression state
    gzguts.h            GZGUTS   C     ZLIB - Definitions for the gzclose module
    inffast.h           INFFAST  C     ZLIB - Header to use inffast.c
    inffixed.h          INFFIXED C     ZLIB - Table for decoding fixed codes
    inflate.h           INFLATE  C     ZLIB - Internal inflate state definitions
    inftrees.h          INFTREES C     ZLIB - Header to use inftrees.c
    trees.h             TREES    C     ZLIB - Created automatically with -DGEN_TREES_H
    zconf.h             ZCONF    C     ZLIB - Compression library configuration
    zlib.h              ZLIB     C     ZLIB - Compression library C user interface
    as400/zlib.inc      ZLIB.INC RPGLE ZLIB - Compression library ILE RPG user interface
    zutil.h             ZUTIL    C     ZLIB - Internal interface and configuration
                TOOLS                  Building source software & AS/400 README
    as400/bndsrc        BNDSRC         Entry point exportation list
    as400/compile.clp   COMPILE  CLP   Compile sources & generate service program
    as400/readme.txt    README   TXT   Installation instructions

4)      Continue as in I)3).




Notes:  For AS400 ILE RPG programmers, a /copy member defining the ZLIB
                API prototypes for ILE RPG can be found in ZLIB/H(ZLIB.INC).
                Please read comments in this member for more information.

        Remember that most foreign textual data are ASCII coded: this
                implementation does not handle conversion from/to ASCII, so
                text data code conversions must be done explicitely.

        Mainly for the reason above, always open zipped files in binary mode.
Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast(), for 64 bits x86 (both AMD64 and Intel EM64t),
for use with Microsoft Macro Assembler (x64) for AMD64 and Microsoft C++ 64 bits.

gvmat64.asm is written by Gilles Vollant (2005), by using Brian Raiter 686/32 bits
   assembly optimized version from Jean-loup Gailly original longest_match function

inffasx64.asm and inffas8664.c were written by Chris Anderson, by optimizing
   original function from Mark Adler

Use instructions
----------------
Assemble the .asm files using MASM and put the object files into the zlib source
directory.  You can also get object files here:

     http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

define ASMV and ASMINF in your project. Include inffas8664.c in your source tree,
and inffasx64.obj and gvmat64.obj as object to link.


Build instructions
------------------
run bld_64.bat with Microsoft Macro Assembler (x64) for AMD64 (ml64.exe)

ml64.exe is given with Visual Studio 2005, Windows 2003 server DDK

You can get Windows 2003 server DDK with ml64 and cl for AMD64 from
  http://www.microsoft.com/whdc/devtools/ddk/default.mspx for low price)

This directory contains a Pascal (Delphi, Kylix) interface to the
zlib data compression library.


Directory listing
=================

zlibd32.mak     makefile for Borland C++
example.pas     usage example of zlib
zlibpas.pas     the Pascal interface to zlib
readme.txt      this file


Compatibility notes
===================

- Although the name "zlib" would have been more normal for the
  zlibpas unit, this name is already taken by Borland's ZLib unit.
  This is somehow unfortunate, because that unit is not a genuine
  interface to the full-fledged zlib functionality, but a suite of
  class wrappers around zlib streams.  Other essential features,
  such as checksums, are missing.
  It would have been more appropriate for that unit to have a name
  like "ZStreams", or something similar.

- The C and zlib-supplied types int, uInt, long, uLong, etc. are
  translated directly into Pascal types of similar sizes (Integer,
  LongInt, etc.), to avoid namespace pollution.  In particular,
  there is no conversion of unsigned int into a Pascal unsigned
  integer.  The Word type is non-portable and has the same size
  (16 bits) both in a 16-bit and in a 32-bit environment, unlike
  Integer.  Even if there is a 32-bit Cardinal type, there is no
  real need for unsigned int in zlib under a 32-bit environment.

- Except for the callbacks, the zlib function interfaces are
  assuming the calling convention normally used in Pascal
  (__pascal for DOS and Windows16, __fastcall for Windows32).
  Since the cdecl keyword is used, the old Turbo Pascal does
  not work with this interface.

- The gz* function interfaces are not translated, to avoid
  interfacing problems with the C runtime library.  Besides,
    gzprintf(gzFile file, const char *format, ...)
  cannot be translated into Pascal.


Legal issues
============

The zlibpas interface is:
  Copyright (C) 1995-2003 Jean-loup Gailly and Mark Adler.
  Copyright (C) 1998 by Bob Dellaca.
  Copyright (C) 2003 by Cosmin Truta.

The example program is:
  Copyright (C) 1995-2003 by Jean-loup Gailly.
  Copyright (C) 1998,1999,2000 by Jacques Nomssi Nzali.
  Copyright (C) 2003 by Cosmin Truta.

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the author be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

Building instructions for the DLL versions of Zlib 1.2.7
========================================================

This directory contains projects that build zlib and minizip using
Microsoft Visual C++ 9.0/10.0.

You don't need to build these projects yourself. You can download the
binaries from:
  http://www.winimage.com/zLibDll

More information can be found at this site.





Build instructions for Visual Studio 2008 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Compile assembly code (with Visual Studio Command Prompt) by running:
   bld_ml64.bat (in contrib\masmx64)
   bld_ml32.bat (in contrib\masmx86)
- Open contrib\vstudio\vc9\zlibvc.sln with Microsoft Visual C++ 2008
- Or run: vcbuild /rebuild contrib\vstudio\vc9\zlibvc.sln "Release|Win32"

Build instructions for Visual Studio 2010 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc10\zlibvc.sln with Microsoft Visual C++ 2010


Important
---------
- To use zlibwapi.dll in your application, you must define the
  macro ZLIB_WINAPI when compiling your application's source files.


Additional notes
----------------
- This DLL, named zlibwapi.dll, is compatible to the old zlib.dll built
  by Gilles Vollant from the zlib 1.1.x sources, and distributed at
    http://www.winimage.com/zLibDll
  It uses the WINAPI calling convention for the exported functions, and
  includes the minizip functionality. If your application needs that
  particular build of zlib.dll, you can rename zlibwapi.dll to zlib.dll.

- The new DLL was renamed because there exist several incompatible
  versions of zlib.dll on the Internet.

- There is also an official DLL build of zlib, named zlib1.dll. This one
  is exporting the functions using the CDECL convention. See the file
  win32\DLL_FAQ.txt found in this zlib distribution.

- There used to be a ZLIB_DLL macro in zlib 1.1.x, but now this symbol
  has a slightly different effect. To avoid compatibility problems, do
  not define it here.


Gilles Vollant
info@winimage.com

Overview
========

This directory contains an update to the ZLib interface unit,
distributed by Borland as a Delphi supplemental component.

The original ZLib unit is Copyright (c) 1997,99 Borland Corp.,
and is based on zlib version 1.0.4.  There are a series of bugs
and security problems associated with that old zlib version, and
we recommend the users to update their ZLib unit.


Summary of modifications
========================

- Improved makefile, adapted to zlib version 1.2.1.

- Some field types from TZStreamRec are changed from Integer to
  Longint, for consistency with the zlib.h header, and for 64-bit
  readiness.

- The zlib_version constant is updated.

- The new Z_RLE strategy has its corresponding symbolic constant.

- The allocation and deallocation functions and function types
  (TAlloc, TFree, zlibAllocMem and zlibFreeMem) are now cdecl,
  and _malloc and _free are added as C RTL stubs.  As a result,
  the original C sources of zlib can be compiled out of the box,
  and linked to the ZLib unit.


Suggestions for improvements
============================

Currently, the ZLib unit provides only a limited wrapper around
the zlib library, and much of the original zlib functionality is
missing.  Handling compressed file formats like ZIP/GZIP or PNG
cannot be implemented without having this functionality.
Applications that handle these formats are either using their own,
duplicated code, or not using the ZLib unit at all.

Here are a few suggestions:

- Checksum class wrappers around adler32() and crc32(), similar
  to the Java classes that implement the java.util.zip.Checksum
  interface.

- The ability to read and write raw deflate streams, without the
  zlib stream header and trailer.  Raw deflate streams are used
  in the ZIP file format.

- The ability to read and write gzip streams, used in the GZIP
  file format, and normally produced by the gzip program.

- The ability to select a different compression strategy, useful
  to PNG and MNG image compression, and to multimedia compression
  in general.  Besides the compression level

    TCompressionLevel = (clNone, clFastest, clDefault, clMax);

  which, in fact, could have used the 'z' prefix and avoided
  TColor-like symbols

    TCompressionLevel = (zcNone, zcFastest, zcDefault, zcMax);

  there could be a compression strategy

    TCompressionStrategy = (zsDefault, zsFiltered, zsHuffmanOnly, zsRle);

- ZIP and GZIP stream handling via TStreams.


--
Cosmin Truta <cosmint@cs.ubbcluj.ro>
                        ZLib for Ada thick binding (ZLib.Ada)
                        Release 1.3

ZLib.Ada is a thick binding interface to the popular ZLib data
compression library, available at http://www.gzip.org/zlib/.
It provides Ada-style access to the ZLib C library.


        Here are the main changes since ZLib.Ada 1.2:

- Attension: ZLib.Read generic routine have a initialization requirement
  for Read_Last parameter now. It is a bit incompartible with previous version,
  but extends functionality, we could use new parameters Allow_Read_Some and
  Flush now.

- Added Is_Open routines to ZLib and ZLib.Streams packages.

- Add pragma Assert to check Stream_Element is 8 bit.

- Fix extraction to buffer with exact known decompressed size. Error reported by
  Steve Sangwine.

- Fix definition of ULong (changed to unsigned_long), fix regression on 64 bits
  computers. Patch provided by Pascal Obry.

- Add Status_Error exception definition.

- Add pragma Assertion that Ada.Streams.Stream_Element size is 8 bit.


        How to build ZLib.Ada under GNAT

You should have the ZLib library already build on your computer, before
building ZLib.Ada. Make the directory of ZLib.Ada sources current and
issue the command:

  gnatmake test -largs -L<directory where libz.a is> -lz

Or use the GNAT project file build for GNAT 3.15 or later:

  gnatmake -Pzlib.gpr -L<directory where libz.a is>


        How to build ZLib.Ada under Aonix ObjectAda for Win32 7.2.2

1. Make a project with all *.ads and *.adb files from the distribution.
2. Build the libz.a library from the ZLib C sources.
3. Rename libz.a to z.lib.
4. Add the library z.lib to the project.
5. Add the libc.lib library from the ObjectAda distribution to the project.
6. Build the executable using test.adb as a main procedure.


        How to use ZLib.Ada

The source files test.adb and read.adb are small demo programs that show
the main functionality of ZLib.Ada.

The routines from the package specifications are commented.


Homepage: http://zlib-ada.sourceforge.net/
Author: Dmitriy Anisimkov <anisimkov@yahoo.com>

Contributors: Pascal Obry <pascal@obry.org>, Steve Sangwine <sjs@essex.ac.uk>
This directory contains a .Net wrapper class library for the ZLib1.dll

The wrapper includes support for inflating/deflating memory buffers,
.Net streaming wrappers for the gz streams part of zlib, and wrappers
for the checksum parts of zlib. See DotZLib/UnitTests.cs for examples.

Directory structure:
--------------------

LICENSE_1_0.txt       - License file.
readme.txt            - This file.
DotZLib.chm           - Class library documentation
DotZLib.build         - NAnt build file
DotZLib.sln           - Microsoft Visual Studio 2003 solution file

DotZLib\*.cs          - Source files for the class library

Unit tests:
-----------
The file DotZLib/UnitTests.cs contains unit tests for use with NUnit 2.1 or higher.
To include unit tests in the build, define nunit before building.


Build instructions:
-------------------

1. Using Visual Studio.Net 2003:
   Open DotZLib.sln in VS.Net and build from there. Output file (DotZLib.dll)
   will be found ./DotZLib/bin/release or ./DotZLib/bin/debug, depending on
   you are building the release or debug version of the library. Check
   DotZLib/UnitTests.cs for instructions on how to include unit tests in the
   build.

2. Using NAnt:
   Open a command prompt with access to the build environment and run nant
   in the same directory as the DotZLib.build file.
   You can define 2 properties on the nant command-line to control the build:
   debug={true|false} to toggle between release/debug builds (default=true).
   nunit={true|false} to include or esclude unit tests (default=true).
   Also the target clean will remove binaries.
   Output file (DotZLib.dll) will be found in either ./DotZLib/bin/release
   or ./DotZLib/bin/debug, depending on whether you are building the release
   or debug version of the library.

   Examples:
     nant -D:debug=false -D:nunit=false
       will build a release mode version of the library without unit tests.
     nant
       will build a debug version of the library with unit tests
     nant clean
       will remove all previously built files.


---------------------------------
Copyright (c) Henrik Ravn 2004

Use, modification and distribution are subject to the Boost Software License, Version 1.0.
(See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)

Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast().


Use instructions
----------------
Assemble using MASM, and copy the object files into the zlib source
directory, then run the appropriate makefile, as suggested below.  You can
donwload MASM from here:

    http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=7a1c9da0-0510-44a2-b042-7ef370530c64

You can also get objects files here:

    http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

Build instructions
------------------
* With Microsoft C and MASM:
nmake -f win32/Makefile.msc LOC="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj"

* With Borland C and TASM:
make -f win32/Makefile.bor LOCAL_ZLIB="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj" OBJPA="+match686c.obj+match686.obj+inffas32.obj"

        ZLIB version 1.2.7 for AS400 installation instructions

I) From an AS400 *SAVF file:

1)      Unpacking archive to an AS400 save file

On the AS400:

_       Create the ZLIB AS400 library:

        CRTLIB LIB(ZLIB) TYPE(*PROD) TEXT('ZLIB compression API library')

_       Create a work save file, for example:

                CRTSAVF FILE(ZLIB/ZLIBSAVF)

On a PC connected to the target AS400:

_       Unpack the save file image to a PC file "ZLIBSAVF"
_       Upload this file into the save file on the AS400, for example
                using ftp in BINARY mode.


2)      Populating the ZLIB AS400 source library

On the AS400:

_       Extract the saved objects into the ZLIB AS400 library using:

RSTOBJ OBJ(*ALL) SAVLIB(ZLIB) DEV(*SAVF) SAVF(ZLIB/ZLIBSAVF) RSTLIB(ZLIB)


3)      Customize installation:

_       Edit CL member ZLIB/TOOLS(COMPILE) and change parameters if needed,
                according to the comments.

_       Compile this member with:

        CRTCLPGM PGM(ZLIB/COMPILE) SRCFILE(ZLIB/TOOLS) SRCMBR(COMPILE)


4)      Compile and generate the service program:

_       This can now be done by executing:

        CALL PGM(ZLIB/COMPILE)



II) From the original source distribution:

1)      On the AS400, create the source library:

        CRTLIB LIB(ZLIB) TYPE(*PROD) TEXT('ZLIB compression API library')

2)      Create the source files:

        CRTSRCPF FILE(ZLIB/SOURCES) RCDLEN(112) TEXT('ZLIB library modules')
        CRTSRCPF FILE(ZLIB/H)       RCDLEN(112) TEXT('ZLIB library includes')
        CRTSRCPF FILE(ZLIB/TOOLS)   RCDLEN(112) TEXT('ZLIB library control utilities')

3)      From the machine hosting the distribution files, upload them (with
                FTP in text mode, for example) according to the following table:

    Original    AS400   AS400    AS400 AS400
    file        file    member   type  description
                SOURCES                Original ZLIB C subprogram sources
    adler32.c           ADLER32  C     ZLIB - Compute the Adler-32 checksum of a dta strm
    compress.c          COMPRESS C     ZLIB - Compress a memory buffer
    crc32.c             CRC32    C     ZLIB - Compute the CRC-32 of a data stream
    deflate.c           DEFLATE  C     ZLIB - Compress data using the deflation algorithm
    gzclose.c           GZCLOSE  C     ZLIB - Close .gz files
    gzlib.c             GZLIB    C     ZLIB - Miscellaneous .gz files IO support
    gzread.c            GZREAD   C     ZLIB - Read .gz files
    gzwrite.c           GZWRITE  C     ZLIB - Write .gz files
    infback.c           INFBACK  C     ZLIB - Inflate using a callback interface
    inffast.c           INFFAST  C     ZLIB - Fast proc. literals & length/distance pairs
    inflate.c           INFLATE  C     ZLIB - Interface to inflate modules
    inftrees.c          INFTREES C     ZLIB - Generate Huffman trees for efficient decode
    trees.c             TREES    C     ZLIB - Output deflated data using Huffman coding
    uncompr.c           UNCOMPR  C     ZLIB - Decompress a memory buffer
    zutil.c             ZUTIL    C     ZLIB - Target dependent utility functions
                H                      Original ZLIB C and ILE/RPG include files
    crc32.h             CRC32    C     ZLIB - CRC32 tables
    deflate.h           DEFLATE  C     ZLIB - Internal compression state
    gzguts.h            GZGUTS   C     ZLIB - Definitions for the gzclose module
    inffast.h           INFFAST  C     ZLIB - Header to use inffast.c
    inffixed.h          INFFIXED C     ZLIB - Table for decoding fixed codes
    inflate.h           INFLATE  C     ZLIB - Internal inflate state definitions
    inftrees.h          INFTREES C     ZLIB - Header to use inftrees.c
    trees.h             TREES    C     ZLIB - Created automatically with -DGEN_TREES_H
    zconf.h             ZCONF    C     ZLIB - Compression library configuration
    zlib.h              ZLIB     C     ZLIB - Compression library C user interface
    as400/zlib.inc      ZLIB.INC RPGLE ZLIB - Compression library ILE RPG user interface
    zutil.h             ZUTIL    C     ZLIB - Internal interface and configuration
                TOOLS                  Building source software & AS/400 README
    as400/bndsrc        BNDSRC         Entry point exportation list
    as400/compile.clp   COMPILE  CLP   Compile sources & generate service program
    as400/readme.txt    README   TXT   Installation instructions

4)      Continue as in I)3).




Notes:  For AS400 ILE RPG programmers, a /copy member defining the ZLIB
                API prototypes for ILE RPG can be found in ZLIB/H(ZLIB.INC).
                Please read comments in this member for more information.

        Remember that most foreign textual data are ASCII coded: this
                implementation does not handle conversion from/to ASCII, so
                text data code conversions must be done explicitely.

        Mainly for the reason above, always open zipped files in binary mode.
Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast(), for 64 bits x86 (both AMD64 and Intel EM64t),
for use with Microsoft Macro Assembler (x64) for AMD64 and Microsoft C++ 64 bits.

gvmat64.asm is written by Gilles Vollant (2005), by using Brian Raiter 686/32 bits
   assembly optimized version from Jean-loup Gailly original longest_match function

inffasx64.asm and inffas8664.c were written by Chris Anderson, by optimizing
   original function from Mark Adler

Use instructions
----------------
Assemble the .asm files using MASM and put the object files into the zlib source
directory.  You can also get object files here:

     http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

define ASMV and ASMINF in your project. Include inffas8664.c in your source tree,
and inffasx64.obj and gvmat64.obj as object to link.


Build instructions
------------------
run bld_64.bat with Microsoft Macro Assembler (x64) for AMD64 (ml64.exe)

ml64.exe is given with Visual Studio 2005, Windows 2003 server DDK

You can get Windows 2003 server DDK with ml64 and cl for AMD64 from
  http://www.microsoft.com/whdc/devtools/ddk/default.mspx for low price)

This directory contains a Pascal (Delphi, Kylix) interface to the
zlib data compression library.


Directory listing
=================

zlibd32.mak     makefile for Borland C++
example.pas     usage example of zlib
zlibpas.pas     the Pascal interface to zlib
readme.txt      this file


Compatibility notes
===================

- Although the name "zlib" would have been more normal for the
  zlibpas unit, this name is already taken by Borland's ZLib unit.
  This is somehow unfortunate, because that unit is not a genuine
  interface to the full-fledged zlib functionality, but a suite of
  class wrappers around zlib streams.  Other essential features,
  such as checksums, are missing.
  It would have been more appropriate for that unit to have a name
  like "ZStreams", or something similar.

- The C and zlib-supplied types int, uInt, long, uLong, etc. are
  translated directly into Pascal types of similar sizes (Integer,
  LongInt, etc.), to avoid namespace pollution.  In particular,
  there is no conversion of unsigned int into a Pascal unsigned
  integer.  The Word type is non-portable and has the same size
  (16 bits) both in a 16-bit and in a 32-bit environment, unlike
  Integer.  Even if there is a 32-bit Cardinal type, there is no
  real need for unsigned int in zlib under a 32-bit environment.

- Except for the callbacks, the zlib function interfaces are
  assuming the calling convention normally used in Pascal
  (__pascal for DOS and Windows16, __fastcall for Windows32).
  Since the cdecl keyword is used, the old Turbo Pascal does
  not work with this interface.

- The gz* function interfaces are not translated, to avoid
  interfacing problems with the C runtime library.  Besides,
    gzprintf(gzFile file, const char *format, ...)
  cannot be translated into Pascal.


Legal issues
============

The zlibpas interface is:
  Copyright (C) 1995-2003 Jean-loup Gailly and Mark Adler.
  Copyright (C) 1998 by Bob Dellaca.
  Copyright (C) 2003 by Cosmin Truta.

The example program is:
  Copyright (C) 1995-2003 by Jean-loup Gailly.
  Copyright (C) 1998,1999,2000 by Jacques Nomssi Nzali.
  Copyright (C) 2003 by Cosmin Truta.

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the author be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

Building instructions for the DLL versions of Zlib 1.2.8
========================================================

This directory contains projects that build zlib and minizip using
Microsoft Visual C++ 9.0/10.0.

You don't need to build these projects yourself. You can download the
binaries from:
  http://www.winimage.com/zLibDll

More information can be found at this site.





Build instructions for Visual Studio 2008 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Compile assembly code (with Visual Studio Command Prompt) by running:
   bld_ml64.bat (in contrib\masmx64)
   bld_ml32.bat (in contrib\masmx86)
- Open contrib\vstudio\vc9\zlibvc.sln with Microsoft Visual C++ 2008
- Or run: vcbuild /rebuild contrib\vstudio\vc9\zlibvc.sln "Release|Win32"

Build instructions for Visual Studio 2010 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc10\zlibvc.sln with Microsoft Visual C++ 2010

Build instructions for Visual Studio 2012 (32 bits or 64 bits)
--------------------------------------------------------------
- Uncompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc11\zlibvc.sln with Microsoft Visual C++ 2012


Important
---------
- To use zlibwapi.dll in your application, you must define the
  macro ZLIB_WINAPI when compiling your application's source files.


Additional notes
----------------
- This DLL, named zlibwapi.dll, is compatible to the old zlib.dll built
  by Gilles Vollant from the zlib 1.1.x sources, and distributed at
    http://www.winimage.com/zLibDll
  It uses the WINAPI calling convention for the exported functions, and
  includes the minizip functionality. If your application needs that
  particular build of zlib.dll, you can rename zlibwapi.dll to zlib.dll.

- The new DLL was renamed because there exist several incompatible
  versions of zlib.dll on the Internet.

- There is also an official DLL build of zlib, named zlib1.dll. This one
  is exporting the functions using the CDECL convention. See the file
  win32\DLL_FAQ.txt found in this zlib distribution.

- There used to be a ZLIB_DLL macro in zlib 1.1.x, but now this symbol
  has a slightly different effect. To avoid compatibility problems, do
  not define it here.


Gilles Vollant
info@winimage.com

Overview
========

This directory contains an update to the ZLib interface unit,
distributed by Borland as a Delphi supplemental component.

The original ZLib unit is Copyright (c) 1997,99 Borland Corp.,
and is based on zlib version 1.0.4.  There are a series of bugs
and security problems associated with that old zlib version, and
we recommend the users to update their ZLib unit.


Summary of modifications
========================

- Improved makefile, adapted to zlib version 1.2.1.

- Some field types from TZStreamRec are changed from Integer to
  Longint, for consistency with the zlib.h header, and for 64-bit
  readiness.

- The zlib_version constant is updated.

- The new Z_RLE strategy has its corresponding symbolic constant.

- The allocation and deallocation functions and function types
  (TAlloc, TFree, zlibAllocMem and zlibFreeMem) are now cdecl,
  and _malloc and _free are added as C RTL stubs.  As a result,
  the original C sources of zlib can be compiled out of the box,
  and linked to the ZLib unit.


Suggestions for improvements
============================

Currently, the ZLib unit provides only a limited wrapper around
the zlib library, and much of the original zlib functionality is
missing.  Handling compressed file formats like ZIP/GZIP or PNG
cannot be implemented without having this functionality.
Applications that handle these formats are either using their own,
duplicated code, or not using the ZLib unit at all.

Here are a few suggestions:

- Checksum class wrappers around adler32() and crc32(), similar
  to the Java classes that implement the java.util.zip.Checksum
  interface.

- The ability to read and write raw deflate streams, without the
  zlib stream header and trailer.  Raw deflate streams are used
  in the ZIP file format.

- The ability to read and write gzip streams, used in the GZIP
  file format, and normally produced by the gzip program.

- The ability to select a different compression strategy, useful
  to PNG and MNG image compression, and to multimedia compression
  in general.  Besides the compression level

    TCompressionLevel = (clNone, clFastest, clDefault, clMax);

  which, in fact, could have used the 'z' prefix and avoided
  TColor-like symbols

    TCompressionLevel = (zcNone, zcFastest, zcDefault, zcMax);

  there could be a compression strategy

    TCompressionStrategy = (zsDefault, zsFiltered, zsHuffmanOnly, zsRle);

- ZIP and GZIP stream handling via TStreams.


--
Cosmin Truta <cosmint@cs.ubbcluj.ro>
                        ZLib for Ada thick binding (ZLib.Ada)
                        Release 1.3

ZLib.Ada is a thick binding interface to the popular ZLib data
compression library, available at http://www.gzip.org/zlib/.
It provides Ada-style access to the ZLib C library.


        Here are the main changes since ZLib.Ada 1.2:

- Attension: ZLib.Read generic routine have a initialization requirement
  for Read_Last parameter now. It is a bit incompartible with previous version,
  but extends functionality, we could use new parameters Allow_Read_Some and
  Flush now.

- Added Is_Open routines to ZLib and ZLib.Streams packages.

- Add pragma Assert to check Stream_Element is 8 bit.

- Fix extraction to buffer with exact known decompressed size. Error reported by
  Steve Sangwine.

- Fix definition of ULong (changed to unsigned_long), fix regression on 64 bits
  computers. Patch provided by Pascal Obry.

- Add Status_Error exception definition.

- Add pragma Assertion that Ada.Streams.Stream_Element size is 8 bit.


        How to build ZLib.Ada under GNAT

You should have the ZLib library already build on your computer, before
building ZLib.Ada. Make the directory of ZLib.Ada sources current and
issue the command:

  gnatmake test -largs -L<directory where libz.a is> -lz

Or use the GNAT project file build for GNAT 3.15 or later:

  gnatmake -Pzlib.gpr -L<directory where libz.a is>


        How to build ZLib.Ada under Aonix ObjectAda for Win32 7.2.2

1. Make a project with all *.ads and *.adb files from the distribution.
2. Build the libz.a library from the ZLib C sources.
3. Rename libz.a to z.lib.
4. Add the library z.lib to the project.
5. Add the libc.lib library from the ObjectAda distribution to the project.
6. Build the executable using test.adb as a main procedure.


        How to use ZLib.Ada

The source files test.adb and read.adb are small demo programs that show
the main functionality of ZLib.Ada.

The routines from the package specifications are commented.


Homepage: http://zlib-ada.sourceforge.net/
Author: Dmitriy Anisimkov <anisimkov@yahoo.com>

Contributors: Pascal Obry <pascal@obry.org>, Steve Sangwine <sjs@essex.ac.uk>
This directory contains a .Net wrapper class library for the ZLib1.dll

The wrapper includes support for inflating/deflating memory buffers,
.Net streaming wrappers for the gz streams part of zlib, and wrappers
for the checksum parts of zlib. See DotZLib/UnitTests.cs for examples.

Directory structure:
--------------------

LICENSE_1_0.txt       - License file.
readme.txt            - This file.
DotZLib.chm           - Class library documentation
DotZLib.build         - NAnt build file
DotZLib.sln           - Microsoft Visual Studio 2003 solution file

DotZLib\*.cs          - Source files for the class library

Unit tests:
-----------
The file DotZLib/UnitTests.cs contains unit tests for use with NUnit 2.1 or higher.
To include unit tests in the build, define nunit before building.


Build instructions:
-------------------

1. Using Visual Studio.Net 2003:
   Open DotZLib.sln in VS.Net and build from there. Output file (DotZLib.dll)
   will be found ./DotZLib/bin/release or ./DotZLib/bin/debug, depending on
   you are building the release or debug version of the library. Check
   DotZLib/UnitTests.cs for instructions on how to include unit tests in the
   build.

2. Using NAnt:
   Open a command prompt with access to the build environment and run nant
   in the same directory as the DotZLib.build file.
   You can define 2 properties on the nant command-line to control the build:
   debug={true|false} to toggle between release/debug builds (default=true).
   nunit={true|false} to include or esclude unit tests (default=true).
   Also the target clean will remove binaries.
   Output file (DotZLib.dll) will be found in either ./DotZLib/bin/release
   or ./DotZLib/bin/debug, depending on whether you are building the release
   or debug version of the library.

   Examples:
     nant -D:debug=false -D:nunit=false
       will build a release mode version of the library without unit tests.
     nant
       will build a debug version of the library with unit tests
     nant clean
       will remove all previously built files.


---------------------------------
Copyright (c) Henrik Ravn 2004

Use, modification and distribution are subject to the Boost Software License, Version 1.0.
(See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)

Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast().


Use instructions
----------------
Assemble using MASM, and copy the object files into the zlib source
directory, then run the appropriate makefile, as suggested below.  You can
donwload MASM from here:

    http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=7a1c9da0-0510-44a2-b042-7ef370530c64

You can also get objects files here:

    http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

Build instructions
------------------
* With Microsoft C and MASM:
nmake -f win32/Makefile.msc LOC="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj"

* With Borland C and TASM:
make -f win32/Makefile.bor LOCAL_ZLIB="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj" OBJPA="+match686c.obj+match686.obj+inffas32.obj"

        ZLIB version 1.2.8 for AS400 installation instructions

I) From an AS400 *SAVF file:

1)      Unpacking archive to an AS400 save file

On the AS400:

_       Create the ZLIB AS400 library:

        CRTLIB LIB(ZLIB) TYPE(*PROD) TEXT('ZLIB compression API library')

_       Create a work save file, for example:

                CRTSAVF FILE(ZLIB/ZLIBSAVF)

On a PC connected to the target AS400:

_       Unpack the save file image to a PC file "ZLIBSAVF"
_       Upload this file into the save file on the AS400, for example
                using ftp in BINARY mode.


2)      Populating the ZLIB AS400 source library

On the AS400:

_       Extract the saved objects into the ZLIB AS400 library using:

RSTOBJ OBJ(*ALL) SAVLIB(ZLIB) DEV(*SAVF) SAVF(ZLIB/ZLIBSAVF) RSTLIB(ZLIB)


3)      Customize installation:

_       Edit CL member ZLIB/TOOLS(COMPILE) and change parameters if needed,
                according to the comments.

_       Compile this member with:

        CRTCLPGM PGM(ZLIB/COMPILE) SRCFILE(ZLIB/TOOLS) SRCMBR(COMPILE)


4)      Compile and generate the service program:

_       This can now be done by executing:

        CALL PGM(ZLIB/COMPILE)



II) From the original source distribution:

1)      On the AS400, create the source library:

        CRTLIB LIB(ZLIB) TYPE(*PROD) TEXT('ZLIB compression API library')

2)      Create the source files:

        CRTSRCPF FILE(ZLIB/SOURCES) RCDLEN(112) TEXT('ZLIB library modules')
        CRTSRCPF FILE(ZLIB/H)       RCDLEN(112) TEXT('ZLIB library includes')
        CRTSRCPF FILE(ZLIB/TOOLS)   RCDLEN(112) TEXT('ZLIB library control utilities')

3)      From the machine hosting the distribution files, upload them (with
                FTP in text mode, for example) according to the following table:

    Original    AS400   AS400    AS400 AS400
    file        file    member   type  description
                SOURCES                Original ZLIB C subprogram sources
    adler32.c           ADLER32  C     ZLIB - Compute the Adler-32 checksum of a dta strm
    compress.c          COMPRESS C     ZLIB - Compress a memory buffer
    crc32.c             CRC32    C     ZLIB - Compute the CRC-32 of a data stream
    deflate.c           DEFLATE  C     ZLIB - Compress data using the deflation algorithm
    gzclose.c           GZCLOSE  C     ZLIB - Close .gz files
    gzlib.c             GZLIB    C     ZLIB - Miscellaneous .gz files IO support
    gzread.c            GZREAD   C     ZLIB - Read .gz files
    gzwrite.c           GZWRITE  C     ZLIB - Write .gz files
    infback.c           INFBACK  C     ZLIB - Inflate using a callback interface
    inffast.c           INFFAST  C     ZLIB - Fast proc. literals & length/distance pairs
    inflate.c           INFLATE  C     ZLIB - Interface to inflate modules
    inftrees.c          INFTREES C     ZLIB - Generate Huffman trees for efficient decode
    trees.c             TREES    C     ZLIB - Output deflated data using Huffman coding
    uncompr.c           UNCOMPR  C     ZLIB - Decompress a memory buffer
    zutil.c             ZUTIL    C     ZLIB - Target dependent utility functions
                H                      Original ZLIB C and ILE/RPG include files
    crc32.h             CRC32    C     ZLIB - CRC32 tables
    deflate.h           DEFLATE  C     ZLIB - Internal compression state
    gzguts.h            GZGUTS   C     ZLIB - Definitions for the gzclose module
    inffast.h           INFFAST  C     ZLIB - Header to use inffast.c
    inffixed.h          INFFIXED C     ZLIB - Table for decoding fixed codes
    inflate.h           INFLATE  C     ZLIB - Internal inflate state definitions
    inftrees.h          INFTREES C     ZLIB - Header to use inftrees.c
    trees.h             TREES    C     ZLIB - Created automatically with -DGEN_TREES_H
    zconf.h             ZCONF    C     ZLIB - Compression library configuration
    zlib.h              ZLIB     C     ZLIB - Compression library C user interface
    as400/zlib.inc      ZLIB.INC RPGLE ZLIB - Compression library ILE RPG user interface
    zutil.h             ZUTIL    C     ZLIB - Internal interface and configuration
                TOOLS                  Building source software & AS/400 README
    as400/bndsrc        BNDSRC         Entry point exportation list
    as400/compile.clp   COMPILE  CLP   Compile sources & generate service program
    as400/readme.txt    README   TXT   Installation instructions

4)      Continue as in I)3).




Notes:  For AS400 ILE RPG programmers, a /copy member defining the ZLIB
                API prototypes for ILE RPG can be found in ZLIB/H(ZLIB.INC).
                Please read comments in this member for more information.

        Remember that most foreign textual data are ASCII coded: this
                implementation does not handle conversion from/to ASCII, so
                text data code conversions must be done explicitely.

        Mainly for the reason above, always open zipped files in binary mode.
Copyright (C) 2004, 2005, 2007-2009, 2012-2014, 2016  Internet Systems Consortium, Inc. ("ISC")
Copyright (C) 2001, 2003  Internet Software Consortium.
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

NOTES ON BIND 9.10 FOR WINDOWS:

BIND 9.10 is known to run on Windows XP, Vista, Windows 7,
and Windows Server 2003 and higher.
  
KIT INSTALLATION:

Unpack the kit into any convenient directory and run the BINDInstall
program.  This will install the named and associated programs into
the correct directories and set up the required registry keys.

Usually BINDInstall must be run by/as Administrator or it can fail
to operate on the filesystem or the registery or even return messages
like 'A referral was returned from the server". The best way to
avoid this kind of problems on Windows 7 or newer is:
 - open a "file explorer" aka finder windows
 - goes where the distribution was expanded
 - click right on the BINDInstall application
 - open "Properties" (last) menu
 - open "Compatibility" (second) tab
 - check on the (last) "Run this program as an administrator"
Unfortunately this is not saved by zip (or any archiver?) as
it is a property saved in the Registry.

BINDInstall requires that you install it under an account with
restricted privileges. The installer will prompt you for an account
name (the default is "named") and a password for that account. It
will also check for the existence of that account.  If it does not
exist is will create it with only the privileges required to run
BIND. If the account does exist it will check that it has only the
one privilege required: "Log on as a service".  If it has too many
privileges it will prompt you if you want to continue.

With BIND running under an account name, it is necessary for all
files and directories that BIND uses to have permissions set up for
the named account if the files are on an NTFS disk. BIND requires
that the account have read and write access to the directory for
the pid file, any files that are maintained either for slave zones
or for master zones supporting dynamic updates. The account will
also need read access to the named.conf and any other file that it
needs to read.

"NT AUTHORITY\LocalService" is also an acceptable account
(and the only acceptable on some recent versions of Windows).
This account is built into Windows and no password is required.
Appropriate file permissions will also need to be set for "NT
AUTHORITY\LocalService" similar to those that would have been
required for the "named" account.

It is important that on Windows the directory directive is used in
the options section to tell BIND where to find the files used in
named.conf (default "%ProgramFiles%\ISC BIND 9\etc\named.conf"). For
example:

	options {
		directory "C:\Program Files (x86)\ISC BIND 9\etc";
	};

for a 32 bit BIND on a 64 bit US Domestic Windows system.
Messages are logged to the Application log in the EventViewer.

CONTROLLING BIND:

Windows uses the same rndc program as is used on Unix systems.  The
rndc.conf file must be configured for your system in order to work.
You will need to generate a key for this. To do this use the
rndc-confgen program. The program will be installed in the same
directory as named: "%ProgramFiles%\ISC BIND 9\bin".  From the DOS
prompt, use the command this way:

rndc-confgen -a

which will create a rndc.key file in the "%ProgramFiles%\ISC BIND 9\etc"
directory. This will allow you to run rndc without an explicit
rndc.conf file or key and control entry in named.conf file. See
section 3.4.1.2 of the ARM for details of this. An rndc.conf can
also be generated by running:

rndc-confgen > rndc.conf

which will create the rndc.conf file in the current directory, but
not copy it to the "%ProgramFiles%\ISC BIND 9\etc" directory where
it needs to reside. If you create rndc.conf this way you will need
to copy the same key statement into named.conf.

The additions look like the following:

key "rndc-key" { algorithm hmac-sha256; secret "xxxxxxxxx=="; };

controls {
	inet 127.0.0.1 port 953 allow { localhost; } keys { "rndc-key"; };
};

Note that the value of the secret must come from the key generated
above for rndc and must be the same key value for both. Details of
this may be found in section 3.4.1.2 of the ARM. If you have rndc
on a Unix box you can use it to control BIND on the Windows box as
well as using the Windows version of rndc to control a BIND 9 daemon
on a Unix box. However you must have key statements valid for the
servers you wish to control, specifically the IP address and key
in both named.conf and rndc.conf. Again see section 3.4.1.2 of the
ARM for details.

In order to run rndc from a different system it is important to
ensure that the clocks are synchronized. The clocks must be kept
within 5 minutes of each other or the rndc commands will fail
authentication. Use NTP or other time synchronization software to
keep your clocks accurate. NTP can be found at http://www.ntp.org/.

In addition BIND is installed as a win32 system service, can be
started and stopped in the same way as any other service and
automatically starts whenever the system is booted. Signals are not
supported and are in fact ignored.

Note: Unlike most Windows applications, named does not, change its
working directory when started as a service.  If you wish to use
relative files in named.conf you will need to specify a working
directory using the directory directive options.

DOCUMENTATION:

This kit includes Documentation in HTML format.  The documentation
is not copied during the installation process so you should move
it to any convenient location for later reference. Of particular
importance is the BIND 9 Administrator's Reference Manual (Bv9ARM*.html)
which provides detailed information on BIND 9. In addition, there
are HTML pages for each of the BIND 9 applications.

INCLUDED TOOLS:

The following tools have been built for Windows: dig, nslookup,
host, nsupdate, ddns-confgen, rndc, rndc-confgen, named-checkconf,
named-checkzone, named-compilezone, named-journalprint,
named-rrchecker, dnssec-importkey, dnssec-keygen, dnssec-signzone,
dnssec-dsfromkey, dnssec-keyfromlabel, dnssec-revoke, dnssec-settime
and dnssec-verify.  The latter tools are for use with DNSSEC.  All tools
are installed in the "%ProgramFiles%\ISC BIND 9\bin" directory.

IMPORTANT NOTE ON USING THE TOOLS:

It is no longer necessary to create a resolv.conf file on Windows
as the tools will look in the registry for the required name server
information. However, if you do create a resolv.conf file as follows,
the tools will use it in preference to the registry name server
entries.

Place resolv.conf the "%ProgramFiles%\ISC BIND 9\etc" directory.
It must contain a list of recursive server addresses.  The format
of this file is:

nameserver 1.2.3.4
nameserver 5.6.7.8

Replace the above IP addresses with the real name server addresses.
127.0.0.1 is a valid address if you are running a recursive name
server on the localhost.

PROBLEMS:

Please report bugs to bind9-bugs@isc.org. Other questions can go
to the bind-users@isc.org mailing list.
Git does not record empty directories. Create a dummy file in each directory
here. Strictly speaking, putting dummy files in leaf directories should be
sufficient, but adding them everywhere reduces the risk of repeating the same
problem in case new directories are added.
Git does not record empty directories. Create a dummy file in each directory
here. Strictly speaking, putting dummy files in leaf directories should be
sufficient, but adding them everywhere reduces the risk of repeating the same
problem in case new directories are added.
Git does not record empty directories. Create a dummy file in each directory
here. Strictly speaking, putting dummy files in leaf directories should be
sufficient, but adding them everywhere reduces the risk of repeating the same
problem in case new directories are added.
Git does not record empty directories. Create a dummy file in each directory
here. Strictly speaking, putting dummy files in leaf directories should be
sufficient, but adding them everywhere reduces the risk of repeating the same
problem in case new directories are added.
Git does not record empty directories. Create a dummy file in each directory
here. Strictly speaking, putting dummy files in leaf directories should be
sufficient, but adding them everywhere reduces the risk of repeating the same
problem in case new directories are added.
Git does not record empty directories. Create a dummy file in each directory
here. Strictly speaking, putting dummy files in leaf directories should be
sufficient, but adding them everywhere reduces the risk of repeating the same
problem in case new directories are added.
libpcap for DOS
---------------

This file contains some notes on building and using libpcap for MS-DOS.
Look in `README' and `pcap.man' for usage and details. These targets are
supported:

 - Borland C 4.0+ small or large model.
 - Metaware HighC 3.1+ with PharLap DOS-extender
 - GNU C 2.7+ with djgpp 2.01+ DOS extender
 - Watcom C 11.x with DOS4GW extender

Note: the files in the libpcap.zip contains short truncated filenames.
  So for djgpp to work with these, disable the use of long file names by
  setting "LFN=n" in the environment. On the other hand, if you get libpcap
  from Github or the official libpcap.tar.gz, some filenames are beyond 8+3.
  In this case set "LFN=y".

Files specific to DOS are pcap-dos.[ch] and the assembly and C files in
the MSDOS sub-directory. Remember to built the libpcap library from the top
install directory. And not from the MSDOS sub-directory.

Note for djgpp users:
  If you got the libpcap from the official site www.tcpdump, then that
  distribution does NOT contain any sources for building 32-bit drivers.
  Instead get the full version at
     http://www.watt-32.net/pcap/libpcap.zip

  and set "USE_32BIT_DRIVERS = 1" in msdos\common.dj.



Requirements
------------

DOS-libpcap currently only works reliably with a real-mode Ethernet packet-
driver. This driver must be installed prior to using any program (e.g.
tcpdump) compiled with libpcap. Work is underway to implement protected-
mode drivers for 32-bit targets (djgpp only). The 3Com 3c509 driver is
working almost perfectly. Due to lack of LAN-cards, I've not had the
opportunity to test other drivers. These 32-bit drivers are modified
Linux drivers.


Required packages
-----------------

The following packages and tools must be present for all targets.

1. Watt-32 tcp/ip library. This library is *not* used to send or
   receive network data. It's mostly used to access the 'hosts'
   file and other <netdb.h> features. Get 'watt32s*.zip' at:

     http://www.watt-32.net

2. Exception handler and disassember library (libexc.a) is needed if
   "USE_EXCEPT = 1" in common.dj. Available at:

     http://www.watt-32.net/misc/exc_dx07.zip

3. Flex & Bison is used to generate parser for the filter handler
   pcap_compile:
     ftp://ftp.delorie.com/pub/djgpp/current/v2gnu/flx254b.zip
     ftp://ftp.delorie.com/pub/djgpp/current/v2gnu/bsn241b.zip

4. NASM assembler v 0.98 or later is required when building djgpp and
   Watcom targets:
     http://www.nasm.us/

5. sed (Stream Editor) is required for doing `make depend'.
   It's available at:
     ftp://ftp.delorie.com/pub/djgpp/current/v2gnu/sed422b.zip

   A touch tool to update the time-stamp of a file. E.g.:
     ftp://ftp.delorie.com/pub/djgpp/current/v2gnu/grep29b.zip

6. For djgpp rm.exe and cp.exe are required. These should already be
   part of your djgpp installation. Also required (experimental at the
   time) for djgpp is DLX 2.91 or later. This tool is for the generation
   of dynamically loadable modules.


Compiling libpcap
-----------------

Follow these steps in building libpcap:

1. Make sure you've installed Watt-32 properly (see it's `INSTALL' file).
   During that installation a environment variable `WATT_ROOT' is set.
   This variable is used for building libpcap also (`WATT_INC' is
   deducted from `WATT_ROOT'). djgpp users should also define environment
   variables `C_INCLUDE_PATH' and `LIBRARY_PATH' to point to the include
   directory and library directory respectively.  E.g. put this in your
   AUTOEXEC.BAT:
     set C_INCLUDE_PATH=c:/net/watt/inc
     set LIBRARY_PATH=c:/net/watt/lib

2. Revise the msdos/common.dj file for your djgpp/gcc installation;
   - change the value of `GCCLIB' to match location of libgcc.a.
   - set `USE_32BIT_DRIVERS = 1' to build 32-bit driver objects.


3. Build pcap by using appropriate makefile. For djgpp, use:
     `make -f msdos/makefile.dj'  (i.e. GNU `make')

   For a Watcom target say:
     `wmake -f msdos\makefile.wc'

   For a Borland target say:
     `maker -f msdos\Makefile pcap_bc.lib'  (Borland's `maker.exe')

   And for a HighC/Pharlap target say:
     `maker -f msdos\Makefile pcap_hc.lib'  (Borland's `maker.exe')

   You might like to change some `CFLAGS' -- only `DEBUG' define currently
   have any effect. It shows a rotating "fan" in upper right corner of
   screen.  Remove `DEBUG' if you don't like it. You could add
   `-fomit-frame-pointer' to `CFLAGS' to speed up the generated code.
   But note, this makes debugging and crash-traceback difficult. Only
   add it if you're fully confident your application is 100% stable.

   Note: Code in `USE_NDIS2' does not work at the moment.

4. The resulting library is put in current directory. There's some
   test-program for `libpcap': `filtertest.exe', `findalldevstest.exe',
     `nonblocktest.exe' and `opentest.exe'.

   But linking the library with `tcpdump' is the ultimate test. DOS/djgpp
   should now hopefully be a supported target. Get the sources at:
     http://www.tcpdump.org/
   or
     https://github.com/the-tcpdump-group/tcpdump/

   (click on the 'Download ZIP' on the right side of that page.)


Extensions to libpcap
---------------------

I've included some extra functions to DOS-libpcap:

  `pcap_config_hook (const char *keyword, const char *value)' :

    Allows an application to set values of internal libpcap variables.
    `keyword' and an associated `value' should be present in the `debug_tab[]'
    array in pcap-dos.c (currently only used to set debug-levels and parameters
    for the 32-bit network drivers.) Thus an application using DOS-libpcap can
    override the default value during it's configure process (see tcpdump's
    msdos/config.c file for an extended example).

  `pcap_set_wait (pcap_t *, void (*)(void), int)' :

    Only effective when reading offline traffic from dump-files.
    Function `pcap_offline_read()' will wait (and optionally yield)
    before printing next packet. This will simulate the pace the packets
    where actually recorded.



Happy sniffing !


Gisle Vanem <gvanem@yahoo.no>

October 1999, 2004, 2006, 2013

this directory exists to pacify sphinx-build.
this directory exists to pacify sphinx-build.
Copyright: please read the top of the source code.

Usage:
Objective: please read the help screen by executing the program without any 
           parameter.

Revision:
SMC: Shu-Min Chang

Who When   What
--- ------ --------------------------------------------------------------------
SMC 021107 Initial release Version 1.0 to ISC DHCP repository
SMC 030129 Fixed inclusion range calculation by sorting exclusion before
           passing to FindInclusionRanges
SMC 030228 release 1.0.1 to ISC DHCP repository

AT&T Freeware Year 2000 Certification
====================================

This is the "readme" file for the freeware application which has 
been certified by AT&T Labs as part of the "Freeware Y2K 
Certification Project".

DISCLAIMER
----------
    For its own internal business purposes AT&T Labs has
    assessed various programs obtained from the Internet for
    Year-2000 (Y2K) readiness that were not sufficiently certified
    for AT&T's needs.  As a service to the computing community
    AT&T Labs is freely releasing this information to the
    public as a series of "Y2K Application Updates", one update
    for what AT&T Labs considers an "application".

    For use outside of AT&T, AT&T Labs is not certifying this
    information is correct, that any software, including repairs
    and tests, will help others in any way, survive the year
    2000, nor work with current applications. Nor is AT&T
    taking any responsibility for repairing any Y2K problems
    that were overlooked nor repairing any bugs in any
    "Y2K Application Update". All risk of using this Y2K
    Application Update remains with the user who is expected
    to test that this update meets their needs.

    LICENSE TO USE
    AT&T's intent is to ensure these Y2K patches are freely
    available to the public but will not maintain a public web site
    for their distribution. Any copyright claims only only apply to 
    the specific changes made by Y2K to the code. Any original 
    copyright holders retain rights to unchanged code. Wherever 
    possible patches will be returned to the current owner(s) of the code.

    Owners and publishers are free to incorporate appropriate patches,
    upgrades, and tests within legal future distributions as long as
    they include the credit:

      Various Y2K updates and tests provided by AT&T Labs.
      Copyright 1999 AT&T.

    and any AT&T "comments" on the changed code remain intact.

    Any distributions of the updates must keep the entire update
    intact, without any change, including copyright and disclaimer
    information.  If integrated with the original application items
    not needed for an integrated release may be omitted. When
    distributed on the same media as the original application there
    must be no charge for this "Y2k Application Update".

    CONTACTS
    If you find any overlooked Y2K problems, or have other strictly Y2K
    repairs for the software, please E-mail:

            y2k@y2k.labs.att.com

    This address is strictly reserved for the topic at hand.
    AT&T makes no commitments to answer any E-mail
    to this address.  AT&T is free to use any submissions,
    including publishing in future Y2K related release notes,
    without payment or advance notice to the submitting person or
    persons... appropriate credit will be given in any future
    publications to the first person submitting something that
    AT&T uses.


======================================================================

Perl  ver - 4.036 
No. of Repairs: 2 Repairs
Compilation of Patches Required: No
OS Tested: Solaris 2.6

======================================================================

ORGANIZATION OF THE "Y2KFixes" DIRECTORY

The "Y2KFixes" directory has been included in the archive to give
you information about the Y2K testing that was conducted and their
results.

The Y2KFixes directory contains at least the following three files:
|----> NOTES.y2kfixes    -- Technical details about the Y2K Testing
|----> Readme.y2kfixes   -- this Readme file
|----> Results.y2kfixes  -- The results of Y2K Environment Tests

The directory may contain additional files and/or directories, as 
appropriate to the application, to provide the exact snapshots.


======================================================================

INSTALLING THE "PATCHES"

If you have downloaded a "patch", then you may install it as follows:

    At the same level as the source directory, invoke:

	patch -p < *.patches 

The patch file contains a header which has a manifest of changed files.

======================================================================

ADDITIONAL INSTRUCTIONS:


1) Extract the patches into perl-4.036 directory which is top level directory
   for the perl4 source.

2) cd to Y2KFixes.

3) It will have y2k directory which contains regression tests for Y2K testing.

4) now cd to ../t which contains TEST file for running this regression tests.

5) run TEST, see the results  & apply patches.

6) Once you apply the patch, you need to run a shell script in x2p/find2perl.SH
   which will generate find2perl.


======================================================================

SUPPORT

See http://y2k.labs.att.com/freeware.  There will be no ongoing 
support for the project. But if you have some very important issue, 
you may email us at: y2k@y2k.labs.att.com
Generic Unix ACPICA makefiles
-----------------------------

These makefiles are intended to generate the ACPICA utilities in
a Unix-like environment, with the original ACPICA code (not linuxized),
and in the original (git tree) ACPICA directory structure.

Windows binary versions of these tools are available at:

http://www.acpica.org/downloads/binary_tools.php

Documentation is available at acpica.org:

http://www.acpica.org/documentation/

The top level makefile will generate the following utilities:
Note: These utilities are tested and supported as 32-bit versions
only.

acpibin
acpiexec
acpihelp
acpinames
acpisrc
acpixtract
iasl

To generate all utilities:

cd acpica/generate/unix
make
make install   /* install all binaries to /usr/bin */


Requirements
------------

make
gcc compiler (4+)
bison or yacc
flex or lex


Configuration
-------------

The Makefile.config file contains the configuration information:

HOST =       _CYGWIN            /* Host system, must appear in acenv.h */
CC =         gcc                /* C compiler */
ACPICA_SRC = ../../../source    /* Location of acpica source tree */


Intermediate Files
------------------

The intermediate files for each utility (.o, etc.) are placed in the
subdirectory corresponding to each utility, not in the source code 
tree itself. This prevents collisions when different utilities compile
the same source modules with different options.


Output
------

The executable utilities are copied to the local bin directory.

"make install" will install the binaries to /usr/bin



1) acpibin, an AML file tool

acpibin compares AML files, dumps AML binary files to text files,
extracts binary AML from text files, and other AML file
manipulation.


2) acpiexec, a user-space AML interpreter

acpiexec allows the loading of ACPI tables and execution of control
methods from user space. Useful for debugging AML code and testing
the AML interpreter. Hardware access is simulated.


3) acpihelp, syntax help for ASL operators and reserved names

acpihelp displays the syntax for all of the ASL operators, as well
as information about the ASL/ACPI reserved names (4-char names that
start with underscore.)


4) acpinames, load and dump acpi namespace

acpinames loads an ACPI namespace from a binary ACPI table file.
This is a smaller version of acpiexec that loads an acpi table and
dumps the resulting namespace. It is primarily intended to demonstrate
the configurability of ACPICA.


5) acpisrc, a source code conversion tool

acpisrc converts the standard form of the acpica source release (included
here) into a version that meets Linux coding guidelines. This consists
mainly of performing a series of string replacements and transformations
to the code. It can also be used to clean the acpica source and generate
statistics.


6) acpixtract, extract binary ACPI tables from an acpidump

acpixtract is used to extract binary ACPI tables from the ASCII text
output of an acpidump utility (available on several different hosts.)


7) iasl, an optimizing ASL compiler/disassembler

iasl compiles ASL (ACPI Source Language) into AML (ACPI Machine
Language). This AML is suitable for inclusion as a DSDT in system
firmware. It also can disassemble AML, for debugging purposes.

Lint files for PC-Lint (FlexLint) by Gimpel Software, Inc.

These are the configuration and option files used to lint the 
ACPI-CA software.

lset.bat    - adds lint directory to the command line search path
lint.bat    - lint batch file for 32 and 64 bit lint
std16.lnt   - 16-bit options
std32.lnt   - 32-bit options
std64.lnt   - 64-bit options
options.lnt - common options
others      - windows/dos compiler option files

/*
 * Miscellaneous instructions for building and using the iASL compiler.
 */
Last update 9 December 2013.


1) Generating iASL from source
------------------------------

Generation of the ASL compiler from source code requires these items:

    1) The ACPICA source code tree.
    2) An ANSI C compiler.
    3) The Flex (or Lex) lexical analyzer generator.
    4) The Bison (or Yacc) parser generator.

There are three major ACPICA source code components that are required to
generate the compiler (Basically, the entire ACPICA source tree should
be installed):

    1) The ASL compiler source.
    2) The ACPICA Core Subsystem source. In particular, the Namespace
        Manager component is used to create an internal ACPI namespace
        and symbol table, and the AML Interpreter is used to evaluate
        constant expressions.
    3) The "common" source directory that is used for all ACPI components.


1a) Notes for Linux/Unix generation
-----------------------------------

iASL has been generated with these versions of Flex/Bison:

    flex:  Version 2.5.32
    bison: Version 2.6.2

Other required packages:

    make
    gcc C compiler
    m4 (macro processor required by bison)

On Linux/Unix systems, the following commands will build the compiler:

    cd acpica (or cd acpica/generate/unix)
    make clean
    make iasl


1b) Notes for Windows generation
--------------------------------

On Windows, the Visual Studio 2008 project file appears in this directory:

    generate/msvc9/AcpiComponents.sln

The Windows versions of GNU Flex/Bison must be installed, and they must
be installed in a directory that contains no embedded spaces in the
pathname. They cannot be installed in the default "c:\Program Files"
directory. This is a bug in Bison. The default Windows project file for
iASL assumes that these tools are installed at this location:

    c:\GnuWin32

Once the tools are installed, ensure that this path is added to the
default system $Path environment variable:

    c:\GnuWin32\bin

Goto: ControlPanel/System/AdvancedSystemSettings/EnvironmentVariables

Important: Now Windows must be rebooted to make the system aware of
the updated $Path. Otherwise, Bison will not be able to find the M4
interpreter library and will fail.

iASL has been generated with these versions of Flex/Bison for Windows:

    Flex for Windows:  V2.5.4a
    Bison for Windows: V2.4.1

Flex is available at:  http://gnuwin32.sourceforge.net/packages/flex.htm
Bison is available at: http://gnuwin32.sourceforge.net/packages/bison.htm



2) Integration as a custom tool for Visual Studio
-------------------------------------------------

This procedure adds the iASL compiler as a custom tool that can be used
to compile ASL source files. The output is sent to the VC output 
window.

a) Select Tools->Customize.

b) Select the "Tools" tab.

c) Scroll down to the bottom of the "Menu Contents" window. There you
   will see an empty rectangle. Click in the rectangle to enter a 
   name for this tool.

d) Type "iASL Compiler" in the box and hit enter. You can now edit
   the other fields for this new custom tool.

e) Enter the following into the fields:

   Command:             C:\Acpi\iasl.exe
   Arguments:           -vi "$(FilePath)"
   Initial Directory    "$(FileDir)"
   Use Output Window    <Check this option>

   "Command" must be the path to wherever you copied the compiler.
   "-vi" instructs the compiler to produce messages appropriate for VC.
   Quotes around FilePath and FileDir enable spaces in filenames.

f) Select "Close".

These steps will add the compiler to the tools menu as a custom tool.
By enabling "Use Output Window", you can click on error messages in
the output window and the source file and source line will be
automatically displayed by VC. Also, you can use F4 to step through
the messages and the corresponding source line(s).



3) Integrating iASL into a Visual Studio ASL project build
----------------------------------------------------------

This procedure creates a project that compiles ASL files to AML.

a) Create a new, empty project and add your .ASL files to the project

b) For all ASL files in the project, specify a custom build (under
Project/Settings/CustomBuild with the following settings (or similar):

Commands:
    c:\acpi\libraries\iasl.exe -vs -vi "$(InputPath)"

Output:
    $(InputDir)\$(InputPath).aml
The protocol directory contains non Architectural 
Protocols that span the FW, Platform, or application
space.#
# $NetBSD: readme,v 1.1 2000/04/14 20:24:40 is Exp $
#

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# MOTOROLA MICROPROCESSOR & MEMORY TECHNOLOGY GROUP
# M68000 Hi-Performance Microprocessor Division
# M68060 Software Package Production Release 
# 
# M68060 Software Package Copyright (C) 1993, 1994, 1995, 1996 Motorola Inc.
# All rights reserved.
# 
# THE SOFTWARE is provided on an "AS IS" basis and without warranty.
# To the maximum extent permitted by applicable law,
# MOTOROLA DISCLAIMS ALL WARRANTIES WHETHER EXPRESS OR IMPLIED,
# INCLUDING IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS
# FOR A PARTICULAR PURPOSE and any warranty against infringement with
# regard to the SOFTWARE (INCLUDING ANY MODIFIED VERSIONS THEREOF)
# and any accompanying written materials. 
# 
# To the maximum extent permitted by applicable law,
# IN NO EVENT SHALL MOTOROLA BE LIABLE FOR ANY DAMAGES WHATSOEVER
# (INCLUDING WITHOUT LIMITATION, DAMAGES FOR LOSS OF BUSINESS PROFITS,
# BUSINESS INTERRUPTION, LOSS OF BUSINESS INFORMATION, OR OTHER PECUNIARY LOSS)
# ARISING OF THE USE OR INABILITY TO USE THE SOFTWARE.
# 
# Motorola assumes no responsibility for the maintenance and support
# of the SOFTWARE.  
# 
# You are hereby granted a copyright license to use, modify, and distribute the
# SOFTWARE so long as this entire notice is retained without alteration
# in any modified and/or redistributed versions, and that such modified
# versions are clearly identified as such.
# No licenses are granted by implication, estoppel or otherwise under any
# patents or trademarks of Motorola, Inc.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Files in this directory:
-------------------------

fpsp.sa		Full FP Kernel Module - hex image
fpsp.s		Full FP Kernel Module - source code
fpsp.doc	Full FP Kernel Module - on-line documentation

pfpsp.sa	Partial FP Kernel Module - hex image
pfpsp.s		Partial FP Kernel Module - source code

fplsp.sa	FP Library Module - hex image
fplsp.s		FP Library Module - source code
fplsp.doc	FP Library Module - on-line documentation

isp.sa		Integer Unimplemented Kernel Module - hex image
isp.s		Integer Unimplemented Kernel Module - source code
isp.doc		Integer Unimplemented Kernel Module - on-line doc

ilsp.sa		Integer Unimplemented Library Module - hex image
ilsp.s		Integer Unimplemented Library Module - source code
ilsp.doc	Integer Unimplemented Library Module - on-line doc

fskeleton.s	Sample Call-outs needed by fpsp.sa and pfpsp.sa

iskeleton.s	Sample Call-outs needed by isp.sa

os.s		Sample Call-outs needed by fpsp.sa, pfpsp.sa, and isp.sa

ftest.sa	Simple test program to test that {p}fpsp.sa
		was connected properly; hex image
ftest.s		above test; source code

itest.sa	Simple test program to test that isp.sa was
		connected properly; hex image
itest.s		above test; source code

test.doc	on-line documentation for {i,f}test.sa

README		This file

ERRATA		Known errata for this release

MISC		Release file version numbers

Contents of the openssl\netware directory
==========================================

Regular files:

readme.txt     - this file
do_tests.pl    - perl script used to run the OpenSSL tests on NetWare
cpy_tests.bat  - batch to to copy test stuff to NetWare server
build.bat      - batch file to help with builds
set_env.bat    - batch file to help setup build environments
globals.txt    - results of initial code review to identify OpenSSL global variables


The following files are generated by the various scripts.  They are
recreated each time and it is okay to delete them.

*.def - command files used by Metrowerks linker
*.mak - make files generated by mk1mf.pl
There is a ppro flag in cast-586 which turns on/off
generation of pentium pro/II friendly code

This flag makes the inner loop one cycle longer, but generates 
code that runs %30 faster on the pentium pro/II, while only %7 slower
on the pentium.  By default, this flag is on.

The perl scripts in this directory are my 'hack' to generate
multiple different assembler formats via the one origional script.

The way to use this library is to start with adding the path to this directory
and then include it.

push(@INC,"perlasm","../../perlasm");
require "x86asm.pl";

The first thing we do is setup the file and type of assember

&asm_init($ARGV[0],$0);

The first argument is the 'type'.  Currently
'cpp', 'sol', 'a.out', 'elf' or 'win32'.
Argument 2 is the file name.

The reciprocal function is
&asm_finish() which should be called at the end.

There are 2 main 'packages'. x86ms.pl, which is the microsoft assembler,
and x86unix.pl which is the unix (gas) version.

Functions of interest are:
&external_label("des_SPtrans");	declare and external variable
&LB(reg);			Low byte for a register
&HB(reg);			High byte for a register
&BP(off,base,index,scale)	Byte pointer addressing
&DWP(off,base,index,scale)	Word pointer addressing
&stack_push(num)		Basically a 'sub esp, num*4' with extra
&stack_pop(num)			inverse of stack_push
&function_begin(name,extra)	Start a function with pushing of
				edi, esi, ebx and ebp.  extra is extra win32
				external info that may be required.
&function_begin_B(name,extra)	Same as norma function_begin but no pushing.
&function_end(name)		Call at end of function.
&function_end_A(name)		Standard pop and ret, for use inside functions
&function_end_B(name)		Call at end but with poping or 'ret'.
&swtmp(num)			Address on stack temp word.
&wparam(num)			Parameter number num, that was push
				in C convention.  This all works over pushes
				and pops.
&comment("hello there")		Put in a comment.
&label("loop")			Refer to a label, normally a jmp target.
&set_label("loop")		Set a label at this point.
&data_word(word)		Put in a word of data.

So how does this all hold together?  Given

int calc(int len, int *data)
	{
	int i,j=0;

	for (i=0; i<len; i++)
		{
		j+=other(data[i]);
		}
	}

So a very simple version of this function could be coded as

	push(@INC,"perlasm","../../perlasm");
	require "x86asm.pl";
	
	&asm_init($ARGV[0],"cacl.pl");

	&external_label("other");

	$tmp1=	"eax";
	$j=	"edi";
	$data=	"esi";
	$i=	"ebp";

	&comment("a simple function");
	&function_begin("calc");
	&mov(	$data,		&wparam(1)); # data
	&xor(	$j,		$j);
	&xor(	$i,		$i);

	&set_label("loop");
	&cmp(	$i,		&wparam(0));
	&jge(	&label("end"));

	&mov(	$tmp1,		&DWP(0,$data,$i,4));
	&push(	$tmp1);
	&call(	"other");
	&add(	$j,		"eax");
	&pop(	$tmp1);
	&inc(	$i);
	&jmp(	&label("loop"));

	&set_label("end");
	&mov(	"eax",		$j);

	&function_end("calc");

	&asm_finish();

The above example is very very unoptimised but gives an idea of how
things work.

There is also a cbc mode function generator in cbc.pl

&cbc(	$name,
	$encrypt_function_name,
	$decrypt_function_name,
	$true_if_byte_swap_needed,
	$parameter_number_for_iv,
	$parameter_number_for_encrypt_flag,
	$first_parameter_to_pass,
	$second_parameter_to_pass,
	$third_parameter_to_pass);

So for example, given
void BF_encrypt(BF_LONG *data,BF_KEY *key);
void BF_decrypt(BF_LONG *data,BF_KEY *key);
void BF_cbc_encrypt(unsigned char *in, unsigned char *out, long length,
        BF_KEY *ks, unsigned char *iv, int enc);

&cbc("BF_cbc_encrypt","BF_encrypt","BF_encrypt",1,4,5,3,-1,-1);

&cbc("des_ncbc_encrypt","des_encrypt","des_encrypt",0,4,5,3,5,-1);
&cbc("des_ede3_cbc_encrypt","des_encrypt3","des_decrypt3",0,6,7,3,4,5);

There are blowfish assembler generation scripts.
bf-586.pl version is for the pentium and
bf-686.pl is my original version, which is faster on the pentium pro.

When using a bf-586.pl, the pentium pro/II is %8 slower than using
bf-686.pl.  When using a bf-686.pl, the pentium is %16 slower
than bf-586.pl

So the default is bf-586.pl

First up, let me say I don't like writing in assembler.  It is not portable,
dependant on the particular CPU architecture release and is generally a pig
to debug and get right.  Having said that, the x86 architecture is probably
the most important for speed due to number of boxes and since
it appears to be the worst architecture to to get
good C compilers for.  So due to this, I have lowered myself to do
assembler for the inner DES routines in libdes :-).

The file to implement in assembler is des_enc.c.  Replace the following
4 functions
des_encrypt1(DES_LONG data[2],des_key_schedule ks, int encrypt);
des_encrypt2(DES_LONG data[2],des_key_schedule ks, int encrypt);
des_encrypt3(DES_LONG data[2],des_key_schedule ks1,ks2,ks3);
des_decrypt3(DES_LONG data[2],des_key_schedule ks1,ks2,ks3);

They encrypt/decrypt the 64 bits held in 'data' using
the 'ks' key schedules.   The only difference between the 4 functions is that
des_encrypt2() does not perform IP() or FP() on the data (this is an
optimization for when doing triple DES and des_encrypt3() and des_decrypt3()
perform triple des.  The triple DES routines are in here because it does
make a big difference to have them located near the des_encrypt2 function
at link time..

Now as we all know, there are lots of different operating systems running on
x86 boxes, and unfortunately they normally try to make sure their assembler
formating is not the same as the other peoples.
The 4 main formats I know of are
Microsoft	Windows 95/Windows NT
Elf		Includes Linux and FreeBSD(?).
a.out		The older Linux.
Solaris		Same as Elf but different comments :-(.

Now I was not overly keen to write 4 different copies of the same code,
so I wrote a few perl routines to output the correct assembler, given
a target assembler type.  This code is ugly and is just a hack.
The libraries are x86unix.pl and x86ms.pl.
des586.pl, des686.pl and des-som[23].pl are the programs to actually
generate the assembler.

So to generate elf assembler
perl des-som3.pl elf >dx86-elf.s
For Windows 95/NT
perl des-som2.pl win32 >win32.asm

[ update 4 Jan 1996 ]
I have added another way to do things.
perl des-som3.pl cpp >dx86-cpp.s
generates a file that will be included by dx86unix.cpp when it is compiled.
To build for elf, a.out, solaris, bsdi etc,
cc -E -DELF asm/dx86unix.cpp | as -o asm/dx86-elf.o
cc -E -DSOL asm/dx86unix.cpp | as -o asm/dx86-sol.o
cc -E -DOUT asm/dx86unix.cpp | as -o asm/dx86-out.o
cc -E -DBSDI asm/dx86unix.cpp | as -o asm/dx86bsdi.o
This was done to cut down the number of files in the distribution.

Now the ugly part.  I acquired my copy of Intels
"Optimization's For Intel's 32-Bit Processors" and found a few interesting
things.  First, the aim of the exersize is to 'extract' one byte at a time
from a word and do an array lookup.  This involves getting the byte from
the 4 locations in the word and moving it to a new word and doing the lookup.
The most obvious way to do this is
xor	eax,	eax				# clear word
movb	al,	cl				# get low byte
xor	edi	DWORD PTR 0x100+des_SP[eax] 	# xor in word
movb	al,	ch				# get next byte
xor	edi	DWORD PTR 0x300+des_SP[eax] 	# xor in word
shr	ecx	16
which seems ok.  For the pentium, this system appears to be the best.
One has to do instruction interleaving to keep both functional units
operating, but it is basically very efficient.

Now the crunch.  When a full register is used after a partial write, eg.
mov	al,	cl
xor	edi,	DWORD PTR 0x100+des_SP[eax]
386	- 1 cycle stall
486	- 1 cycle stall
586	- 0 cycle stall
686	- at least 7 cycle stall (page 22 of the above mentioned document).

So the technique that produces the best results on a pentium, according to
the documentation, will produce hideous results on a pentium pro.

To get around this, des686.pl will generate code that is not as fast on
a pentium, should be very good on a pentium pro.
mov	eax,	ecx				# copy word 
shr	ecx,	8				# line up next byte
and	eax,	0fch				# mask byte
xor	edi	DWORD PTR 0x100+des_SP[eax] 	# xor in array lookup
mov	eax,	ecx				# get word
shr	ecx	8				# line up next byte
and	eax,	0fch				# mask byte
xor	edi	DWORD PTR 0x300+des_SP[eax] 	# xor in array lookup

Due to the execution units in the pentium, this actually works quite well.
For a pentium pro it should be very good.  This is the type of output
Visual C++ generates.

There is a third option.  instead of using
mov	al,	ch
which is bad on the pentium pro, one may be able to use
movzx	eax,	ch
which may not incur the partial write penalty.  On the pentium,
this instruction takes 4 cycles so is not worth using but on the
pentium pro it appears it may be worth while.  I need access to one to
experiment :-).

eric (20 Oct 1996)

22 Nov 1996 - I have asked people to run the 2 different version on pentium
pros and it appears that the intel documentation is wrong.  The
mov al,bh is still faster on a pentium pro, so just use the des586.pl
install des686.pl

3 Dec 1996 - I added des_encrypt3/des_decrypt3 because I have moved these
functions into des_enc.c because it does make a massive performance
difference on some boxes to have the functions code located close to
the des_encrypt2() function.

9 Jan 1997 - des-som2.pl is now the correct perl script to use for
pentiums.  It contains an inner loop from
Svend Olaf Mikkelsen <svolaf@inet.uni-c.dk> which does raw ecb DES calls at
273,000 per second.  He had a previous version at 250,000 and the best
I was able to get was 203,000.  The content has not changed, this is all
due to instruction sequencing (and actual instructions choice) which is able
to keep both functional units of the pentium going.
We may have lost the ugly register usage restrictions when x86 went 32 bit
but for the pentium it has been replaced by evil instruction ordering tricks.

13 Jan 1997 - des-som3.pl, more optimizations from Svend Olaf.
raw DES at 281,000 per second on a pentium 100.

Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast(), for 64 bits x86 (both AMD64 and Intel EM64t),
for use with Microsoft Macro Assembler (x64) for AMD64 and Microsoft C++ 64 bits.

gvmat64.asm is written by Gilles Vollant (2005), by using Brian Raiter 686/32 bits
   assembly optimized version from Jean-loup Gailly original longest_match function

inffasx64.asm and inffas8664.c were written by Chris Anderson, by optimizing
   original function from Mark Adler

Use instructions
----------------
Assemble the .asm files using MASM and put the object files into the zlib source
directory.  You can also get object files here:

     http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

define ASMV and ASMINF in your project. Include inffas8664.c in your source tree,
and inffasx64.obj and gvmat64.obj as object to link.


Build instructions
------------------
run bld_64.bat with Microsoft Macro Assembler (x64) for AMD64 (ml64.exe)

ml64.exe is given with Visual Studio 2005, Windows 2003 server DDK

You can get Windows 2003 server DDK with ml64 and cl for AMD64 from
  http://www.microsoft.com/whdc/devtools/ddk/default.mspx for low price)

This directory contains a Pascal (Delphi, Kylix) interface to the
zlib data compression library.


Directory listing
=================

zlibd32.mak     makefile for Borland C++
example.pas     usage example of zlib
zlibpas.pas     the Pascal interface to zlib
readme.txt      this file


Compatibility notes
===================

- Although the name "zlib" would have been more normal for the
  zlibpas unit, this name is already taken by Borland's ZLib unit.
  This is somehow unfortunate, because that unit is not a genuine
  interface to the full-fledged zlib functionality, but a suite of
  class wrappers around zlib streams.  Other essential features,
  such as checksums, are missing.
  It would have been more appropriate for that unit to have a name
  like "ZStreams", or something similar.

- The C and zlib-supplied types int, uInt, long, uLong, etc. are
  translated directly into Pascal types of similar sizes (Integer,
  LongInt, etc.), to avoid namespace pollution.  In particular,
  there is no conversion of unsigned int into a Pascal unsigned
  integer.  The Word type is non-portable and has the same size
  (16 bits) both in a 16-bit and in a 32-bit environment, unlike
  Integer.  Even if there is a 32-bit Cardinal type, there is no
  real need for unsigned int in zlib under a 32-bit environment.

- Except for the callbacks, the zlib function interfaces are
  assuming the calling convention normally used in Pascal
  (__pascal for DOS and Windows16, __fastcall for Windows32).
  Since the cdecl keyword is used, the old Turbo Pascal does
  not work with this interface.

- The gz* function interfaces are not translated, to avoid
  interfacing problems with the C runtime library.  Besides,
    gzprintf(gzFile file, const char *format, ...)
  cannot be translated into Pascal.


Legal issues
============

The zlibpas interface is:
  Copyright (C) 1995-2003 Jean-loup Gailly and Mark Adler.
  Copyright (C) 1998 by Bob Dellaca.
  Copyright (C) 2003 by Cosmin Truta.

The example program is:
  Copyright (C) 1995-2003 by Jean-loup Gailly.
  Copyright (C) 1998,1999,2000 by Jacques Nomssi Nzali.
  Copyright (C) 2003 by Cosmin Truta.

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the author be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

Building instructions for the DLL versions of Zlib 1.2.10
========================================================

This directory contains projects that build zlib and minizip using
Microsoft Visual C++ 9.0/10.0.

You don't need to build these projects yourself. You can download the
binaries from:
  http://www.winimage.com/zLibDll

More information can be found at this site.





Build instructions for Visual Studio 2008 (32 bits or 64 bits)
--------------------------------------------------------------
- Decompress current zlib, including all contrib/* files
- Compile assembly code (with Visual Studio Command Prompt) by running:
   bld_ml64.bat (in contrib\masmx64)
   bld_ml32.bat (in contrib\masmx86)
- Open contrib\vstudio\vc9\zlibvc.sln with Microsoft Visual C++ 2008
- Or run: vcbuild /rebuild contrib\vstudio\vc9\zlibvc.sln "Release|Win32"

Build instructions for Visual Studio 2010 (32 bits or 64 bits)
--------------------------------------------------------------
- Decompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc10\zlibvc.sln with Microsoft Visual C++ 2010

Build instructions for Visual Studio 2012 (32 bits or 64 bits)
--------------------------------------------------------------
- Decompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc11\zlibvc.sln with Microsoft Visual C++ 2012

Build instructions for Visual Studio 2013 (32 bits or 64 bits)
--------------------------------------------------------------
- Decompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc12\zlibvc.sln with Microsoft Visual C++ 2013

Build instructions for Visual Studio 2015 (32 bits or 64 bits)
--------------------------------------------------------------
- Decompress current zlib, including all contrib/* files
- Open contrib\vstudio\vc14\zlibvc.sln with Microsoft Visual C++ 2015


Important
---------
- To use zlibwapi.dll in your application, you must define the
  macro ZLIB_WINAPI when compiling your application's source files.


Additional notes
----------------
- This DLL, named zlibwapi.dll, is compatible to the old zlib.dll built
  by Gilles Vollant from the zlib 1.1.x sources, and distributed at
    http://www.winimage.com/zLibDll
  It uses the WINAPI calling convention for the exported functions, and
  includes the minizip functionality. If your application needs that
  particular build of zlib.dll, you can rename zlibwapi.dll to zlib.dll.

- The new DLL was renamed because there exist several incompatible
  versions of zlib.dll on the Internet.

- There is also an official DLL build of zlib, named zlib1.dll. This one
  is exporting the functions using the CDECL convention. See the file
  win32\DLL_FAQ.txt found in this zlib distribution.

- There used to be a ZLIB_DLL macro in zlib 1.1.x, but now this symbol
  has a slightly different effect. To avoid compatibility problems, do
  not define it here.


Gilles Vollant
info@winimage.com

Visual Studio 2013 and 2015 Projects from Sean Hunt
seandhunt_7@yahoo.com

Overview
========

This directory contains an update to the ZLib interface unit,
distributed by Borland as a Delphi supplemental component.

The original ZLib unit is Copyright (c) 1997,99 Borland Corp.,
and is based on zlib version 1.0.4.  There are a series of bugs
and security problems associated with that old zlib version, and
we recommend the users to update their ZLib unit.


Summary of modifications
========================

- Improved makefile, adapted to zlib version 1.2.1.

- Some field types from TZStreamRec are changed from Integer to
  Longint, for consistency with the zlib.h header, and for 64-bit
  readiness.

- The zlib_version constant is updated.

- The new Z_RLE strategy has its corresponding symbolic constant.

- The allocation and deallocation functions and function types
  (TAlloc, TFree, zlibAllocMem and zlibFreeMem) are now cdecl,
  and _malloc and _free are added as C RTL stubs.  As a result,
  the original C sources of zlib can be compiled out of the box,
  and linked to the ZLib unit.


Suggestions for improvements
============================

Currently, the ZLib unit provides only a limited wrapper around
the zlib library, and much of the original zlib functionality is
missing.  Handling compressed file formats like ZIP/GZIP or PNG
cannot be implemented without having this functionality.
Applications that handle these formats are either using their own,
duplicated code, or not using the ZLib unit at all.

Here are a few suggestions:

- Checksum class wrappers around adler32() and crc32(), similar
  to the Java classes that implement the java.util.zip.Checksum
  interface.

- The ability to read and write raw deflate streams, without the
  zlib stream header and trailer.  Raw deflate streams are used
  in the ZIP file format.

- The ability to read and write gzip streams, used in the GZIP
  file format, and normally produced by the gzip program.

- The ability to select a different compression strategy, useful
  to PNG and MNG image compression, and to multimedia compression
  in general.  Besides the compression level

    TCompressionLevel = (clNone, clFastest, clDefault, clMax);

  which, in fact, could have used the 'z' prefix and avoided
  TColor-like symbols

    TCompressionLevel = (zcNone, zcFastest, zcDefault, zcMax);

  there could be a compression strategy

    TCompressionStrategy = (zsDefault, zsFiltered, zsHuffmanOnly, zsRle);

- ZIP and GZIP stream handling via TStreams.


--
Cosmin Truta <cosmint@cs.ubbcluj.ro>
                        ZLib for Ada thick binding (ZLib.Ada)
                        Release 1.3

ZLib.Ada is a thick binding interface to the popular ZLib data
compression library, available at http://www.gzip.org/zlib/.
It provides Ada-style access to the ZLib C library.


        Here are the main changes since ZLib.Ada 1.2:

- Attension: ZLib.Read generic routine have a initialization requirement
  for Read_Last parameter now. It is a bit incompartible with previous version,
  but extends functionality, we could use new parameters Allow_Read_Some and
  Flush now.

- Added Is_Open routines to ZLib and ZLib.Streams packages.

- Add pragma Assert to check Stream_Element is 8 bit.

- Fix extraction to buffer with exact known decompressed size. Error reported by
  Steve Sangwine.

- Fix definition of ULong (changed to unsigned_long), fix regression on 64 bits
  computers. Patch provided by Pascal Obry.

- Add Status_Error exception definition.

- Add pragma Assertion that Ada.Streams.Stream_Element size is 8 bit.


        How to build ZLib.Ada under GNAT

You should have the ZLib library already build on your computer, before
building ZLib.Ada. Make the directory of ZLib.Ada sources current and
issue the command:

  gnatmake test -largs -L<directory where libz.a is> -lz

Or use the GNAT project file build for GNAT 3.15 or later:

  gnatmake -Pzlib.gpr -L<directory where libz.a is>


        How to build ZLib.Ada under Aonix ObjectAda for Win32 7.2.2

1. Make a project with all *.ads and *.adb files from the distribution.
2. Build the libz.a library from the ZLib C sources.
3. Rename libz.a to z.lib.
4. Add the library z.lib to the project.
5. Add the libc.lib library from the ObjectAda distribution to the project.
6. Build the executable using test.adb as a main procedure.


        How to use ZLib.Ada

The source files test.adb and read.adb are small demo programs that show
the main functionality of ZLib.Ada.

The routines from the package specifications are commented.


Homepage: http://zlib-ada.sourceforge.net/
Author: Dmitriy Anisimkov <anisimkov@yahoo.com>

Contributors: Pascal Obry <pascal@obry.org>, Steve Sangwine <sjs@essex.ac.uk>
This directory contains a .Net wrapper class library for the ZLib1.dll

The wrapper includes support for inflating/deflating memory buffers,
.Net streaming wrappers for the gz streams part of zlib, and wrappers
for the checksum parts of zlib. See DotZLib/UnitTests.cs for examples.

Directory structure:
--------------------

LICENSE_1_0.txt       - License file.
readme.txt            - This file.
DotZLib.chm           - Class library documentation
DotZLib.build         - NAnt build file
DotZLib.sln           - Microsoft Visual Studio 2003 solution file

DotZLib\*.cs          - Source files for the class library

Unit tests:
-----------
The file DotZLib/UnitTests.cs contains unit tests for use with NUnit 2.1 or higher.
To include unit tests in the build, define nunit before building.


Build instructions:
-------------------

1. Using Visual Studio.Net 2003:
   Open DotZLib.sln in VS.Net and build from there. Output file (DotZLib.dll)
   will be found ./DotZLib/bin/release or ./DotZLib/bin/debug, depending on
   you are building the release or debug version of the library. Check
   DotZLib/UnitTests.cs for instructions on how to include unit tests in the
   build.

2. Using NAnt:
   Open a command prompt with access to the build environment and run nant
   in the same directory as the DotZLib.build file.
   You can define 2 properties on the nant command-line to control the build:
   debug={true|false} to toggle between release/debug builds (default=true).
   nunit={true|false} to include or esclude unit tests (default=true).
   Also the target clean will remove binaries.
   Output file (DotZLib.dll) will be found in either ./DotZLib/bin/release
   or ./DotZLib/bin/debug, depending on whether you are building the release
   or debug version of the library.

   Examples:
     nant -D:debug=false -D:nunit=false
       will build a release mode version of the library without unit tests.
     nant
       will build a debug version of the library with unit tests
     nant clean
       will remove all previously built files.


---------------------------------
Copyright (c) Henrik Ravn 2004

Use, modification and distribution are subject to the Boost Software License, Version 1.0.
(See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)

Summary
-------
This directory contains ASM implementations of the functions
longest_match() and inflate_fast().


Use instructions
----------------
Assemble using MASM, and copy the object files into the zlib source
directory, then run the appropriate makefile, as suggested below.  You can
donwload MASM from here:

    http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=7a1c9da0-0510-44a2-b042-7ef370530c64

You can also get objects files here:

    http://www.winimage.com/zLibDll/zlib124_masm_obj.zip

Build instructions
------------------
* With Microsoft C and MASM:
nmake -f win32/Makefile.msc LOC="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj"

* With Borland C and TASM:
make -f win32/Makefile.bor LOCAL_ZLIB="-DASMV -DASMINF" OBJA="match686.obj inffas32.obj" OBJPA="+match686c.obj+match686.obj+inffas32.obj"

Steps for adding TLS support for a new platform:

(1) Declare TLS variant in machine/types.h by defining either
__HAVE_TLS_VARIANT_I or __HAVE_TLS_VARIANT_II.

(2) _lwp_makecontext has to set the reserved register or kernel transfer
variable in uc_mcontext to the provided value of 'private'. See
src/lib/libc/arch/$PLATFORM/gen/_lwp.c.

This is not possible on the VAX as there is no free space in ucontext_t.
This requires either a special version of _lwp_create or versioning
everything using ucontext_t. Debug support depends on getting the data from
ucontext_t, so the second option is possibly required.

(3) _lwp_setprivate(2) has to update the same register as
_lwp_makecontext uses for the private area pointer. Normally
cpu_lwp_setprivate is provided by MD to reflect the kernel view and
enabled by defining __HAVE_CPU_LWP_SETPRIVATE in machine/types.h.
cpu_setmcontext is responsible for keeping the MI l_private field
synchronised by calling lwp_setprivate as needed.

cpu_switchto has to update the mapping.

_lwp_setprivate is used for the initial thread, all other threads
created by libpthread use _lwp_makecontext for this purpose.

(4) Provide __tls_get_addr and possible other MD functions for dynamic
TLS offset computation. If such alternative entry points exist (currently
only i386), also add a weak reference to 0 in src/lib/libc/tls/tls.c.

The generic implementation can be found in tls.c and is used with
__HAVE_COMMON___TLS_GET_ADDR. It depends on ___lwp_getprivate_fast
(see below).

(5) Implement the necessary relocation records in mdreloc.c.  There are
typically three relocation types found in dynamic binaries:

(a) R_TYPE(TLS_DTPOFF): Offset inside the module.  The common TLS code
ensures that the DTV vector points to offset 0 inside the module TLS block.
This is normally def->st_value + rela->r_addend.

(b) R_TYPE(TLS_DTPMOD): Module index.

(c) R_TYPE(TLS_TPOFF): Static TLS offset.  The code has to check whether
the static TLS offset for this module has been allocated
(defobj->tls_done) and otherwise call _rtld_tls_offset_allocate().  This
may fail if no static space is available and the object has been pulled
in via dlopen(3).

For TLS Variant I, this is typically:

def->st_value + rela->r_addend + defobj->tlsoffset + sizeof(struct tls_tcb)

e.g. the relocation doesn't include the fixed TCB.

For TLS Variant II, this is typically:

def->st_value - defobj->tlsoffset + rela->r_addend

e.g. starting offset is counting down from the TCB.

(6) Implement _lwp_getprivate_fast() in machine/mcontext.h and set
__HAVE___LWP_GETPRIVATE_FAST in machine/types.h.

(7) Test using src/tests/lib/libc/tls.  Make sure with "objdump -R" that
t_tls_dynamic has two TPOFF relocations and h_tls_dlopen.so.1 and
libh_tls_dynamic.so.1 have both two DTPMOD and DTPOFF relocations.
$NetBSD: README,v 1.4 2009/03/22 15:02:24 tsutsui Exp $

This distrib/cdrom directory was used to create bootable CD images
in the past releases with optional rsync(1) and mkisofs(8) tool.
All necessary binaries for ISOs must be prepared in ${DISTRIBDIR},
which is ${.OBJDIR}/download by default, and the binaries can be fetched
by rsync(1) with the "fetch" target defined in Makefile.

Now build.sh script has the "iso-image" target using our native makefs(8)
and this distrib/cdrom is being superseded, but our makefs(8) still
lacks support for a hybrid ISO9660/HFS file system which is required
to create bootable CD images for mac68k and macppc.

On the other hand, the recent version of mkisofs(8) seems to use
iconv(3) libraries on creating the hybrid ISO9660/HFS file system,
but the latest mkisofs(8) (at least cdrtools-2.01.01.36) from pkgsrc
dumps core on it. cdrtools-2.01 is required to build images. (PR pkg/37643)

Note you can also use compiled release binaries prepared in RELEASEDIR by
"build.sh release" if you need only single ISO image for a specific port.


Environment variables:

RELEASE
  specify target release version (no default, mandatory)

TARGET_CD_IMAGE
  specify target ${MACHINE}cd name
  (default: all targets defined in ${RELEASE}.conf)

MKISOFS
  specify mkisofs(8) path (default: /usr/pkg/bin/mkisofs)

RSYNC
  specify rsync(1) path (default: /usr/pkg/bin/rsync)

RSYNC_HOST
  specify rsync host (default: rsync.NetBSD.org)

RSYNC_PATH_PREFIX
  specify extra prefix path of NetBSD module names for rsync mirrors
   ex. "/pub", "/netbsd" etc. (default: empty)

DAILY_DIR
  specify directory name on using NetBSD-daily snapshot for -current
   ex. "200712060002Z" etc. (no default; mandatory for -current)

BRANCH
  spcify branch name on using NetBSD-daily snapshot (default: current)

RELEASE_SUFFIX
  specify suffix of directory names used for pre-releases
   ex. "_BETA2", "_RC5" etc. (default: empty)

RSYNC_PATH
  specify path used to rsync hosts (default: set from the above variables)


Example usage:

make RELEASE=5.0 TARGET_CD_IMAGE=mac68kcd fetch
 -> fetch 5.0 mac68k sets from
    rsync://rsync.NetBSD.org/NetBSD/NetBSD-5.0/

make RELEASE=4.0 TARGET_CD_IMAGE=mac68kcd RELEASE_SUFFIX=_RC5 fetch
 -> fetch 4.0_RC5 mac68k sets from
    rsync://rsync.NetBSD.org/NetBSD/NetBSD-4.0_RC5/

make RELEASE=current TARGET_CD_IMAGE=mac68kcd \
    DAILY_DIR=200903060000Z fetch
 -> fetch -current mac68k sets from
    rsync://rsync.NetBSD.org/NetBSD-daily/HEAD/200903060000Z/

make RELEASE=4.0 TARGET_CD_IMAGE=mac68kcd RELEASE_SUFFIX=_RC5 \
    RSYNC_HOST=rsync3.jp.NetBSD.org \
    RSYNC_PATH_PREFIX=/pub fetch
 -> fetch 4.0_RC5 mac68k sets from
    rsync://rsync3.jp.NetBSD.org/pub/NetBSD/NetBSD-4.0_RC5/

make RELEASE=5.0 TARGET_CD_IMAGE=mac68kcd \
    RSYNC_HOST=rsync.jp.NetBSD.org \
    RSYNC_PATH=/NetBSD-daily/netbsd-5/200903070000Z fetch
 -> fetch mac68k sets of netbsd-5 branch from
    rsync://rsync.jp.NetBSD.org/pub/NetBSD-daily/netbsd-5/200903070000Z

make RELEASE=5.0 TARGET_CD_IMAGE=mac68kcd all
 -> build an ISO image for mac68k from downloaded files as the above

make RELEASE=5.0 TARGET_CD_IMAGE=macppccd \
    DISTRIBDIR=/usr/obj.macppc/releasedir all
 -> build an ISO image for macppc from release binaries
    built by "build.sh release" in the RELEASEDIR 
    (use appropriate RELEASEDIR on your build environment)
$NetBSD: README,v 1.2 2003/07/26 17:06:38 salo Exp $

This "extra" directory exists to hold files that should be provided on any
TNF-created (and hopefully, third party created) CD-ROM images, such as
copyrights, boot instructions, etc.

- tv@NetBSD.org
$NetBSD: README,v 1.3 2006/07/23 13:33:53 bjh21 Exp $

Welcome to NetBSD for the Acorn RiscPC/A7000/NC !

For information about the boot loader, click Menu on !BtNetBSD and
choose "Help" from the "App. '!BtNetBSD'" submenu.  For information
about installing NetBSD, see the INSTALL document in the NetBSD
distribution.

Enjoy your installation !
begin 444 README.Booter
M("`@("`@("`@("`@("`@("`@("`@("`@(%)E8V]M;65N9&5D($)O;W1E<B!V
M97)S:6]N<PH*5F5R<VEO;CH@,2XY+C0L($]C=&]B97(@,7-T+"`Q.3DV"D%U
M=&AO<CH@0G)I86X@1V%E:V4*("`@*B!&:7)S="!";V]T97(@=&\@<W5P<&]R
M="!A;GD@-C@P-#`@*&DN92X@475A9')A+6-L87-S*2!M86-H:6YE<RX@2VYO
M=VX@=&\*("`@("!B;V]T(&ME<FYE;',@9G)O;2!.971"4T0O36%C-CAK('9E
M<G-I;VYS(#$N,2!A;F0@;&%T97(N($9O<B!K97)N96QS(&]L9&5R"B`@("`@
M=&AA;B`Q+C$L('EO=7(@;6EL96%G92!M87D@=F%R>2`M+2!I;B!T:&5O<GD@
M=&AE('-U<'!O<G0@:7,@<W1I;&P@=&AE<F4L"B`@("`@8G5T(&ET)W,@;F]T
M(&)E96X@=')I960@:6X@<75I=&4@82!W:&EL90H*5F5R<VEO;CH@,2XQ,2XQ
M+"!$96-E;6)E<B`S<F0L(#$Y.3<*075T:&]R<SH@4V-O='0@4F5Y;F]L9',L
M($YI9V5L(%!E87)S;VX*("`@*B!5<V5R(&EN=&5R9F%C92!I;7!R;W9E;65N
M=',L('!L=7,@=&AE(&9O;&QO=VEN9R!C:&%N9V5S(&9R;VT@96%R;&EE<B!V
M97)S:6]N<SH*("`@*R!!;&QO=W,@8F]O=&EN9R!W:71H(&QA<F=E<B!K97)N
M96QS("AM86YY('!O<W0@,2XR(&ME<FYE;',@<F5Q=6ER92!T:&ES($)O;W1E
M<BD*("`@*R!.97<@)T%U=&\@4V5T($=-5"!B:6%S)R!C:&5C:V)O>"!W:&EC
M:"!U<V5S('1H92`G36%P)R!-86,@3U,@0V]N=')O;"!P86YE;`H@("`@('1O
M('-E="!5;FEX)W,@=&EM92!Z;VYE(&EN9F]R;6%T:6]N"B`@("L@3F5W(&%B
M:6QI='D@=&\@8F]O="!F<F]M(&=Z:7`M960@:V5R;F5L(&9I;&5S"B`@("L@
M3F5W('-E<FEA;"!F:65L9',@=&\@<V5T(&1I9F9E<F5N="!D969A=6QT('-E
M<FEA;"!P;W)T('-P965D<PH@("`K($YE=R!-;VYI=&]R<R!O<'1I;VYS('=H
M:6-H(&%L;&]W(&-H86YG:6YG(&]F('1H92!M86EN('-C<F5E;B!D97!T:"!B
M969O<F4*("`@("!B;V]T:6YG"B`@("L@268@8F]O="!I<R!A8F]R=&5D+"!R
M97-E=',@;6]N:71O<B!D97!T:`H@("`K(%-A=F5S('!R969E<F5N8V5S(&EN
M(&$@<V5P87)A=&4@9FEL92P@<F%T:&5R('1H86X@:6X@=&AE('!R;V=R86TG
M<R!R97-O=7)C97,*("`@*R!!9&1E9"!V:61E;R!A9&1R97-S(&AA8VL@9F]R
M($Q#-#<U+TQ#-3<U($UA8W,G(&-O;G-O;&4@;VX@:6YT97)N86P@=FED96\*
M("`@*R!(86YD;&5S(&ME<FYE;',@:6X@36%C($]3(&9I;&5S(&UU8V@@8F5T
M=&5R"B`@("L@0F5T=&5R(&1E8G5G9VEN9R]E<G)O<B!O=71P=70@9F]R(&QO
M=RUL979E;"!30U-)('-T=69F"@I697)S:6]N.B`Q+C$Q+C,L($%P<FEL(#$X
M=&@L(#$Y.3@*075T:&]R<SH@3FEG96P@4&5A<G-O;BP@175G96YI;R!-86-I
M82!6:79O"B`@("H@57-E<B!I;G1E<F9A8V4@8VAA;F=E<R!A;F0@861D:71I
M;VX@;V8@)VIU;7`@:6YT;R!D96)U9V=E<B!A9G1E<B!B;V]T:6YG)PH@("`J
M($)U9R!F:7AE<R`H<V5R:6%L(&)O;W1I;F<L(&AA;F<@:68@82!R96UO=F%B
M;&4@4T-322!D979I8V4@=V%S(&]F9FQI;F4L"B`@("`@<')O8FQE;2!W:71H
M(&UO;FET;W(@9&5P=&@@)B!R97-O;'5T:6]N+6-H86YG:6YG(&QO9VEC*2P@
M<&QU<R!T:&4@9F]L;&]W:6YG"B`@("`@8VAA;F=E<R!F<F]M(&5A<FQI97(@
M=F5R<VEO;G,I.@H@("`K($)O;W1E<B!N;W<@8VAE8VMS(&9O<B!6:7)T=6%L
M($UE;6]R>2P@86YD('!O<',@=7`@86X@86QE<G0@86YD(&5X:71S(&EF('5S
M960*("`@*R!-;VYI=&]R<R!O<'1I;VYS(&YO=R!A;&QO=R!C:&%N9VEN9R!O
M9B!M86EN('-C<F5E;B=S(')E<V]L=71I;VX@86YD(&-O;&]U<B!M;V1E"B`@
M("`@*'1O(&=R97ES*2!B969O<F4@8F]O=&EN9RX@5&AE(&QA='1E<B!I<R!F
M;W(@<V]M92!O9B!T:&4@;F5W(%AS97)V97)S('=H:6-H"B`@("`@9&]N)W0@
M=V]R:R!W:71H(&-O;&]U<B!Y970*("`@*R!.;W<@8V]R<F5C=&QY(&MI;&QS
M(')U;FYI;F<@87!P;&EC871I;VYS"@H*("`@("`@("!);F9O<FUA=&EO;B!A
M8F]U="!O=&AE<B!V97)S:6]N<R!O9B!T:&4@0F]O=&5R(&-A;B!B92!F;W5N
M9"!I;B!T:&4@9FEL90I#:&%N9V5,;V<N='AT(&EN(&5I=&AE<B!T:&4@0F]O
M=&5R('-R8R!A<F-H:79E+"!O<B!A=#H*:'1T<#HO+W=W=RYM86-B<V0N8V]M
M+VUA8V)S9"]B;V]T97(M;6%N=6%L+T-H86YG94QO9RYT>'0*"B`@("`@("`@
M("`@("`@("`@("`@("`@("`@("`@("`@("`@("`@("!.:6=E;"!096%R<V]N
M(#QN:6=E;$!I;F0N=&%N<W4N8V]M+F%U/@H@("`@("`@("`@("`@("`@("`@
M("`@("`@("`@("`@("`@("`@("`@("`@("`@("`@("`@("`@("`@("`@(#,P
.=&@@2G5L>2P@,3DY.`H@
`
end
begin 444 README.Installer
M5&AE(&9E871U<F5S+V)U9R!F:7AE<R!F;W(@9&EF9F5R96YT('9E<G-I;VYS
M(&]F('1H92!I;G-T86QL97(*<VAO=6QD(&)E(&%S(&9O;&QO=W,Z"@II;G-T
M86QL97(@,2XP"2T@3W)I9VEN86P@<F5L96%S92!F<F]M($%L:6-E(&=R;W5P
M"@II;G-T86QL97(@,2XQ"2T@37D@9FER<W0@:6YV;VQV96UE;G0@=VET:"!I
M;G-T86QL97(*"49E871U<F4)+2!-=6QL=&EP;&4@87)C:&EV92!S96QE8W1I
M;VX@86YD(&EN<W1A;&QA=&EO;@H)1F5A='5R92`M(&%R8VAI=F5S('-E;&5C
M=&%B;&4@9F]R(&EN<W1A;&QI;F<@8GD@9')A9V=I;F<*"0D@(&]N('1O<"!O
M9B!T:&4@:6YS=&%L;&5R+@H)0G5X($9I>"`M($-$+5)/32!D979I8V4@;F\@
M;&]N9V5R(&UI<W)E8V]G;FEZ960@87,@82!H87)D(&1I<VL*"FEN<W1A;&QE
M<B`Q+C%A("`M($)U9R!F:7@@<F5L96%S90H)1F5A='5R92`M($9I<G-T(&%P
M<&5A<F%N8V4@;V8@<')O9W)E<W,@8F%R+@H)0G5G($9I>"`M(%!A=&-H('1O
M(&1E86P@=VET:"!L87)G97(@=&AA;B`Q1T(@9&ES:R!S:7IE+@H*:6YS=&%L
M;&5R(#$N,6(@("T@0G5G(&9I>"!R96QE87-E"@E&96%T=7)E("T@8F5T=&5R
M('!R;V=R97-S(&)A<@H)0G5G($9I>"`M(&)E='1E<B!V86QI9&%T:6]N(&]F
M(')O;W0@<&%R=&ET:6]N(&)E9F]R92!M;W5N=&EN9RX*"FEN<W1A;&QE<B`Q
M+C%C("`M($)U9R!F:7@@<F5L96%S90H)0G5G($9I>"`M($9I>&5D('!R;V)L
M96US('=I=&@@<')O9W)E<W,@8F%R(&]N($(F5R!S8W)E96XN"@II;G-T86QL
M97(@,2XQ9"`@+2!"=6<@9FEX(')E;&5A<V4*"4)U9R!&:7@@+2!F:7AE9"!E
M<G)O<B!W:71H(&-R96%T:6]N(&]F("]E=&,O9G-T86(N"@E"=6<@1FEX("T@
M<')I;W(@=F5R<VEO;G,@8V]M<&EL960@=VET:"!&4%4@;W!T:6UI>F%T:6]N
M(&]N+@H)"2`@06QL(&-O;7!I;&5S('-T87)T:6YG('=I=&@@=&AI<R!O;F4@
M87)E(&YO="X*"FEN<W1A;&QE<B`Q+C%E("`M($)U9R!F:7@@<F5L96%S90H)
M1F5A='5R92`M($EN8W)E87-E(&YU;6)E<B!O9B!T97)M:6YA;"!D979I8V4@
M9FEL97,@8W)E871E9`H)"2`@=&\@8F4@,38@:6YS=&5A9"!O9B`T+@H)0G5G
M($9I>"`M(%!R:6]R(&EN<W1A;&QE<G,@<F5A9"]W<F]T92!W:71H(&QO8V%L
M('1I;64@>F]N90H)"2`@=&EM97-T86UP+B`@1FEX960@=&AI<R!T;R!A9&0@
M8V]R<F5C="!'350@8FEA<R!O;@H)"2`@<F5A9',@86YD('=R:71E<RX*"FEN
M<W1A;&QE<B`Q+C%F"2T@0G5G(&9I>"!R96QE87-E"@E"=6<@1FEX("T@+V1E
M=B]T='D@86YD("]D978O<'1Y(&1E=FEC97,@861D960@:6X@,2XQ92!C:&%N
M9V5D"@D)("!T;R!U<V4@<')O<&5R(&AE>&%D96-I;6%L(&YA;6EN9R!S8VAE
M;64N"@D)("!&:6QE('-Y<W1E;2!T>7!E(')E<&]R=&5D(&-H86YG960@9G)O
M;2`B=69S(B!T;R`B9F9S(BX*"0D@(%!A9VEN9R!U;F1E<B`B;6]R92(@86YD
M(")D:7(B(')E+65N86)L960N"@II;G-T86QL97(@,2XQ9R`@+2!&96%T=7)E
M(')E;&5A<V4*"49E871U<F4@+2!-:7)R;W(@06QL96X@0G)I9V=S)W,@8VAA
M;F=E<R!T;R!.971"4T0@=&AA="!B87-I8V%L;'D*"0D@(')E8VQA:6T@=&AE
M(")D(B!P87)T:71I;VXN"@E&96%T=7)E("T@3F]W(&UA:V5S(&%L;"!D979I
M8V5S(&$@(DU!2T5$158@86QL(B!W;W5L9"!M86ME"@D)("!E>&-E<'0@(G9N
M9"HB(&%N9"`B8V-D*B(@9&5V:6-E<RX*"4)U9R!&:7@@+2!697)S:6]N(&YU
M;6)E<B!W87-N)W0@<VAO=VEN9R!U<"!I;B`B06)O=70B(&)O>`H)"2`@8F5C
M875S92!T97AT(&)O>"!S:7IE('=A<R!T;V\@<VUA;&PN("!&:7AE9"!N;W<N
M"@I!;GD@<75E<W1I;VYS(&-A;B!B92!F;W)W87)D960@=&\@;64@=FEA(&5M
B86EL+@H*4W1E=F4@0G)O=VX*<V)R;W=N0&)E<W0N8V]M"F\@
`
end
begin 444 README
M0F]O=&5R,BXP+C`N<V5A"0D)3&%T97-T('9E<G-I;VX@;V8@=&AE($)O;W1E
M<BX@4W5P<&]R=',*"0D)"45,1BP@241%+B!&<F]M($YI9V5L(%!E87)S;VX*
M"D)31%]-86,V.&M?0F]O=&5R+F)I;BP)"4QA=&5S="!V97)S:6]N(&]F('1H
M92!";V]T97(@86YD(&1O8W,L"D)O;W1E<DUA;G5A;"YS='AT+F)I;@D):6X@
M36%C0FEN87)Y(&9O<FTN($UO<W0@36%C($]3(&9T<"!C;&EE;G1S"@D)"0EC
M86X@8V]N=F5R="!T:&5S92!D:7)E8W1L>2!I;G1O($UA8R!/4PH)"0D)87!P
M;&EC871I;VYS(&]N(&1O=VYL;V%D+@H*0E-$7TUA8S8X:U]";V]T97(N8FEN
M+G5U92P)4V%M92!A<R!A8F]V92P@:6X@=74M96YC;V1E9"!F;W)M+@I";V]T
M97)-86YU86PN<W1X="YB:6XN=75E"@I-:V9S7S$N-#4N<V5A+FAQ>`D)3F5W
M('9E<G-I;VX@;V8@=&AE(&!M:V9S)R!U=&EL:71Y('1H870*"0D)"7-H;W5L
M9"!P<F]P97)L>2!S=7!P;W)T(")L87)G92(@9&ES:W,L"@D)"0EC;W)R96-T
M;'D@:&%N9&QE('!A<G1I=&EO;B!M87`@=7!D871E<RP*"0D)"6%N9"!C;VYV
M97)T(&%R8FET<F%R>2!P87)T:71I;VYS('1O('1H90H)"0D)='EP97,@3F5T
M0E-$('5N9&5R<W1A;F1S+@H)"0D)1G)O;2!";V(@3F5S=&]R"@I);G-T86QL
M97)?,2XQ9BYS96$N:'%X"0E.97<@=F5R<VEO;B!O9B!T:&4@26YS=&%L;&5R
M+"!W:71H('-O;64*"0D)"6%D9&5D(&9E871U<F5S(&%N9"!S979E<F%L(&)U
>9R!F:7AE<RX*"0D)"49R;VT@4W1E=F4@0G)O=VX*
`
end
This fails to link with a slew of "truncated relocation" errors... I assume
the same fate befell the pman miniroot and that's why it isn't being built
either (though that's only a guess).

I assume the problems are most likely due to the -Wa,-xgot trick we use
to be able to build crunched binaries on MIPS.

--rafal, May 21, 2002 20:39 GMT.

# $NetBSD: README,v 1.13 2013/08/06 22:33:59 soren Exp $

the scripts should be run from the directory where they reside.

makeflist:	output the list of files that should be in a
		distribution, according to the contents of the
		'lists' directory.

checkflist:	check the file list (as internally generated
		by makeflist) against the tree living in $DESTDIR.
		(that tree should be made with 'make distribution'.)

maketars:	make tarballs of the various sets in the distribution,
		based on the contents of the lists, the tree in
		$DESTDIR, and put the tarballs in $RELEASEDIR.
		Note that this script _doesn't_ create the 'secr'
		distribution, because (for now) it requires
		manual intervention to get the binaries right...
		(i'll add another script to create that dist, later.)

what's in 'lists':

lists describing file sets.  There are two sets of lists per file
set: machine dependent and machine-independent files. (there's
also another file in the 'man' dir, which is used by the 'man'
and 'misc' sets, but that's explained later.)

There is one machine-independent file, named "mi".  There are
N machine-dependent files (one per architecture), named "md.${ARCH}".

the sets are as follows:

	base:	the base binary set.  excludes everything described
		below.

	comp:	compiler tools.  All of the tools relating to C, C++,
		and FORTRAN (yes, there are two!) that are in the
		tree.  This includes includes, the linker, tool chain,
		and the .a versions of the libraries.  (obviously,
		base includes ldd, ld.so, and the shared versions.
		base also includes 'cpp', because that's used by X11.)
		includes the man pages for all the binaries contained
		within.  Also, includes all library and system call
		manual pages.

	debug:	Debugging libraries (_g.a/MKDEBUGLIB) and (.debug/MKDEBUG)
		binaries.

	etc:	/etc, and associated files (/var/cron/tabs, /root,
		etc.).  things that shouldn't be blindly reinstalled
		on an upgrade.

	games:	the games and their man pages.

	man:	all of the man pages for the system, except those
		listed elsewhere (e.g. in comp, games, misc, text).
		Includes machine-dependent man pages for this CPU.

	misc:	share/dict, share/doc, and the machine-dependent
		man pages for other CPUs which happen to always
		be installed.

	modules:	stand/${MACHINE}/${OSRELEASE}/modules kernel modules

	tests:	unit, regression, integration and stress tests for the
		whole system.

	text:	text processing tools.  groff and all of its friends.
		includes man pages for all bins contained within.

Each set must contain "./etc/mtree/set.<set name>" within the mi
list.  Failure to add this will break unprivileged builds.
$NetBSD: README.files,v 1.14 2009/04/23 01:56:48 snj Exp $

	   Tape, CD, Disk, and Netboot Images
	   ----- --- ----- --- ------- ------

This release or snapshot contains three installation image types.
The first, for floppies, is split into a multiple volume set.

	installation/floppy/disk1of3
	installation/floppy/disk2of3
	installation/floppy/disk3of3

	installation/diskimage/cdhdtape

	installation/instkernel/netbsd.gz

All three boot images load the same installation kernel into memory
and then make no further use of the source media. The general idea
is to load a kernel with a pre-initialized memory filesystem of
utilities and an installation program.

The floppy image set uses two floppies to load the install kernel.
The cdhdtape image can be written to a CD, hard drive, or tape and
then booted from the SRM console. The kernel image can be netbooted
or loaded off the root directory of an existing installation.

Note:	The netboot loader can load the netbsd.gz file directly; it
	is not necessary to ungzip this kernel first.

To copy the boot images to a magnetic disk under unix, the dd(1)
command can be used:

Floppy:
	dd if=disk1of3 of=/dev/rfd0a bs=18k
	(change floppies)
	dd if=disk2of3 of=/dev/rfd0a bs=18k
	(change floppies)
	dd if=disk3of3 of=/dev/rfd0a bs=18k


You can write the image to a hard drive too:

	dd bs=18k if=cdhdtape of=/dev/rsd1c
	dd bs=18k if=cdhdtape of=/dev/rsd1d (NetBSD/i386)

For a tape, it is important to use a block size of 512, so:

	dd bs=512 if=cdhdtape of=/dev/erst0	(NetBSD)
	dd bs=512 if=cdhdtape of=/dev/rmt0h	(Digital Unix)

Note that the bits on the installation media are only used when
initially loaded. They can be written to a hard drive, loaded, and
then overwritten during the installation with no conflict, or
alternatively, the boot CD or tape can be removed and replaced with
one containing the installation sets.

The install notes from this directory subtree are present on the
installation file system.
#	$NetBSD: README,v 1.4 2002/03/25 07:39:50 lukem Exp $

From: "Gordon W. Ross" <gwr>
Date: Tue, 3 Oct 95 16:37:39 EDT
Subject: New ramdisk, tiny shell, etc.
[ edited since the original mail ]

As part of my efforts to build a RAM-disk root kernel for the
sun3 port, I've developed some things that others may want:

New RAM-disk: [ see sys/dev/ramdisk.c ]

New "small/tiny" tools, to replace some of the larger
programs that one usually wants on a ramdisk root:

ssh: (small shell)
  * Consumes only about 8K of memory on an m68k!
    (saves about 100K in the ramdisk...)
  * Can run programs, possibly with I/O redirection
  * Just enough to let you run the programs needed
    while partitioning and copying miniroot to swap.

tls: (tiny ls)
  * Consumes only about 4K of memory on an m68k!
    (saves about 10K in the ramdisk...)
  * Long format only, nothing fancy.

Also, in the new "src/distrib/utils" area, I've made build
directories for some programs that build smaller versions,
usually by adding special CFLAGS

init_s:  (built with -DLETS_GET_SMALL)
  * Forces single-user mode
  * Eliminates unnecessary code

libhack:   small implementation of some libc functions
  * Needs only /etc/master.passwd (not pwd.db, spwd.db)
  * Reduces size of an m68k crunched binary by about 64K

x_dd:  built with -DNO_CONV (no conv=* options)

x_ifconfig:  supports inet only

The x_ prefix on the above is to make the names unique so
crunchgen will not confuse them with the real sources.

This directory contains configuration files for the Pluggable
Authentication Modules (PAM) library.

Each file details the module chain for a single service, and must be
named after that service.  If no configuration file is found for a
particular service, the /etc/pam.d/other is used instead.  If that
file does not exist, /etc/pam.conf is searched for entries matching
the specified service or, failing that, the "other" service.

See the pam(8) manual page for an explanation of the workings of the
PAM library and descriptions of the various files and modules.  Below
is a summary of the format for the pam.conf and /etc/pam.d/* files.

Configuration lines take the following form:

module-type	control-flag	module-path	arguments

Comments are introduced with a hash mark ('#').  Blank lines and lines
consisting entirely of comments are ignored.

The meanings of the different fields are as follows:

 module-type:
   auth:      prompt for a password to authenticate that the user is
              who they say they are, and set any credentials.
   account:   non-authentication based authorization, based on time,
              resources, etc.
   session:   housekeeping before and/or after login.
   password:  update authentication tokens.

 control-flag: How libpam handles success or failure of the module.
   required:   success is required; on failure all remaining
               modules are run, but the request will be denied.
   requisite:  success is required, and on failure no remaining
               modules are run.
   sufficient: success is sufficient, and if no previous required
               module failed, no remaining modules are run.
   binding:    success is sufficient; on failure all remaining
               modules are run, but the request will be denied.
   optional:   ignored unless the other modules return PAM_IGNORE.

 arguments: Module-specific options, plus some generic ones:
   debug:           syslog debug info.
   no_warn:         return no warning messages to the application.
                    Remove this to feed back to the user the
                    reason(s) they are being rejected.
   use_first_pass:  try authentication using password from the
                    preceding auth module.
   try_first_pass:  first try authentication using password from
                    the preceding auth module, and if that fails
                    prompt for a new password.
   use_mapped_pass: convert cleartext password to a crypto key.
   expose_account:  allow printing more info about the user when
                    prompting.

Note that having a "sufficient" module as the last entry for a
particular service and module type may result in surprising behaviour.
To get the intended semantics, add a "required" entry listing the
pam_deny module at the end of the chain.

$FreeBSD: src/etc/pam.d/README,v 1.5 2004/06/06 11:46:29 schweikh Exp $
$NetBSD: README,v 1.2 2004/12/12 08:54:34 christos Exp $
        $NetBSD: README.compileopts,v 1.1 2016/01/25 00:24:23 pooka Exp $

This file describes compile-time options (make variables) for
the rumpuser POSIX implementation.

Note: after changing an option, do a clean build.

Global options:

    RUMPUSER_THREADS

values: pthread/none/fiber or <undefined>
defval: <undefined>
effect: Define the way threading is implemented in the rumpuser hypercall
	implmentation.
	<undefined> - use default implementation (currently "pthread")
	pthread     - use pthreads to implement threading
	none        - do not support kernel threads at all
	fiber       - user a fiber interface, cooperatively scheduled contexts
#	$NetBSD: README,v 1.2 1998/01/09 04:12:19 perry Exp $

README

This is the remote mag tape library. It allows a program that uses
Unix system calls to transparently use a file (usually a tape drive) on
another system via /etc/rmt, simply by including <rmt.h>.  It is
particularly useful with tar and dd, and is supplied with GNU tar.

This package has evolved somewhat over the years.  My thanks to the
people who did most of the original work, and those who've contributed
bug fixes; appropriate credit is in the man page and source files.

Enjoy,

Arnold Robbins
Emory U. Computing Center
arnold@emoryu1.cc.emory.edu
gatech!emoryu1!arnold
+1 404 727 7636
Introduction

This document covers the native NetBSD compiler runtime.

Machine independent sources can be found in common. The crtbegin.c in
that directory is a useful template for deriving compact assembler
versions. That is preferable to decouple the result from changes in the
compiler logic.

A new platform should provide the following content in
arch/${MACHINE_ARCH} or arch/${MACHINE_CPU}:
- Makefile.inc: provides ELFSIZE corresponding to 32/64bit file format.
  If using the common C code instead of crtbegin.S also provide a -I option
  to find crtbegin.h in your arch subdir.
- crt0.S: provides setup code and the call to ___start.
- crtbegin.S or crtbegin.h: see below
- crtend.S: see below, most likely just a copy of an existing architecture
- crti.S: prefix part of .init/.fini sections, i.e. to ensure stack alignment
- crtn.S: suffix part of the .init/.fini sections, i.e. return to caller.


Overview of the common runtime support

The common runtime support contains two modules, crtbegin and crtend.
crtbegin is linked before all other object files of the program or
dynamic library, crtend after all other object files.  They frame the
lists of constructors, destructors, Java types and exception handling frames.

If done correctly, crtend contains no code and is therefore position
independent.  crtendS.o is therefore just a link to crtend.o.

crtbegin should be position-independent code.  crtbeginT.o doesn't have
to be PIC as it is statically linked.  The overhead is generally not
worth the trouble though.


Section types:
.ctor: writeable
.dtor: writeable
.eh_frame: read-only if platform allows mixing read-only and read-write
sections.  This is supported by GNU ld.
.jcr: writeable
.init: executable
.fini: executable


Non-local symbols:

Weak references:
- _Jv_RegisterClasses,
- __cxa_finalize (crtbeginS.o)
- __deregister_frame_info
- __register_frame_info

Hidden:
- __dso_handle: pointer to self for crtbeginS.o, NULL otherwise.
- __CTOR_LIST_END__


Initialisation (called from .init):

1.  Check that the init code hasn't started already, otherwise bail out.
2.  If __register_frame_info is NULL, skip to 4
3.  Call __register_frame_info with start of .eh_frame as first argument
    and a data object of at least 8 pointers as second argument.
4:  If _Jv_RegisterClasses is NULL, skip to 6
5:  Call _Jv_RegisterClasses with the first pointer of the .jcr section
    as argument.
6:  Iterate from the end of the .ctor section to the start.  Skip the
    terminating NULL and stop when reaching the starting (void *)-1 element.
    Call the pointers as void (*)(void) functions.


Deinitialisation (called from .fini):

1.  Check if the init code has already started, otherwise bail out.
2.  If this is not crtbeginS.o or __cxa_finalize is NULL, skip to 4.
3.  Call __cxa_finalize with a pointer into this Dynamic Shared Object (DSO)
    as first argument.
4.  Iterate from the start of the .dtor section to the send.  Skip the
    initial (void *)-1 and stop when reaching the terminating NULL element.
    Call the pointers as void (*)(void) functions.
5.  If __deregister_frame_info is NULL, return.
6.  Call __deregister_frame_info with the start of .eh_frame as the argument.


Since most of this can easily be done in C code, instead of providing a
crtbegin.S you can also chose to use the generic C implementation. Provide
a crtbegin.h file instead of crtbegin.S. In there put inline assembler
stubs (mostly copied from some other arch) and implement calls to the
helper functions __do_global_ctors_aux/__do_global_dtors_aux.
$NetBSD: README,v 1.7 2017/02/08 03:44:41 kamil Exp $

Due to limitations in the current pthread implementation, makecontext(3)
and sigaltstack(2) should not be used in programs which link against
libpthread (whether threads are used or not). This has been noted in the
makecontext(3), sigaltstack(2), and pthread(3) man pages.
	$NetBSD: README,v 1.7 2017/02/08 13:31:36 riastradh Exp $

libc: The C library.

* ELF symbols and source names

libc contains symbols for:

(a) standard library routines in C and POSIX,
(b) published NetBSD-specific nonstandard extensions,
(c) internal symbols, and
(d) old versions of any published library routines.

** Standard library routines

If a library routine is standard and its signature has never changed,
it is provided as an ELF global symbol.  Its name is declared normally
in the appropriate header file.

=> Example: The names `malloc' and `free' are declared normally in
   <stdlib.h> (src/include/stdlib.h):

	void	*malloc(size_t);
	void	 free(void *);

   libc provides the following ELF symbols:

	malloc		global
	free		global

   In the implementation of libc, malloc and free are defined normally
   in src/lib/libc/stdlib/jemalloc.c:

	void *
	malloc(size_t size)
	{
	...

	void
	free(void *ptr)
	{
	...

** NetBSD-specific nonstandard extensions

If a library routine is nonstandard but published and its signature has
never changed, it is provided as an ELF weak symbol aliasing an ELF
global symbol of the same name with an underscore prefix.

The name is declared normally in the appropriate header file, provided
that the relevant feature macro, such as _NETBSD_SOURCE, is defined.

Within libc, the name is defined in "namespace.h"
(src/lib/libc/include/namespace.h) as a macro expanding to the
underscored name, which is included before the relevant header file, so
that

(a) the definition in a .c file will define the underscored ELF global
symbol, and

(b) the declaration in the standard header file will match the
definition in the .c file.

Alongside the definition in the .c file is a __weak_alias directive to
create the ELF weak symbol alias.

=> Example: For the nonstandard extension consttime_memequal, the
   header file <string.h> (src/include/string.h) declares
   `consttime_memequal' normally, if the caller defines _NETBSD_SOURCE:

	#if defined(_NETBSD_SOURCE)
	...
	int	consttime_memequal(const void *, const void *, size_t);
	...
	#endif	/* _NETBSD_SOURCE */

   libc provides the following ELF symbols:

	_consttime_memequal	global
	consttime_memequal	weak alias for	_consttime_memequal

   In the implementation of libc, the header file "namespace.h"
   (src/lib/libc/include/namespace.h) defines `consttime_memequal' as a
   macro expanding to `_consttime_memequal':

	#define	consttime_memequal	_consttime_memequal

   The source file src/common/lib/libc/string/consttime_memequal.c
   includes "namespace.h" and <string.h>, and defines
   `consttime_memequal' normally:

	int
	consttime_memequal(const void *b1, const void *b2, size_t len)
	{
	...

   Macro expansion replaces `consttime_memequal' by
   `_consttime_memequal', which is the ELF global symbol this defines.
   Alongside the definition is

	__weak_alias(consttime_memequal,_consttime_memequal)

   to provide `consttime_memequal' as an ELF weak symbol aliasing
   `_consttime_memequal'.

** Internal symbols

If a library routine is internal to libc, it is defined as an ELF
global symbol with an underscore prefix.  Its name is declared in the
appropriate internal header file.

=> Example: The implementations of opendir and rewinddir use a common
   subroutine _initdir, which is not part of the libc API or ABI -- it
   is just an internal subroutine.

   libc provides the following ELF symbols:

	_initdir	global

   The name `_initdir' is declared normally in
   src/lib/libc/gen/dirent_private.h:

	int	_initdir(DIR *, int, const char *);

   The name `_initdir' is defined normally in
   src/lib/libc/gen/initdir.c:

	int
	_initdir(DIR *dirp, int fd, const char *name)
	{
	...

** Old versions of library routines

If the signature or semantics of a library routine foo changed in (for
example) NetBSD 6.0, then libc provides

(1) an ELF global symbol `_foo' implementing its old signature,
(2) an ELF weak symbol `foo' aliasing `_foo', and
(3) an ELF global symbol `__foo50' implementing its new signature (yes,
    `__foo50', not `__foo60').

The name foo is declared in the appropriate header file, under any
relevant feature macros, with a __RENAME directive so that for calls to
foo, the compiler will generate relocations for __foo50.  Old programs,
compiled with the old signature, will continue to use the old symbol.

=> Example: In NetBSD 5.0, time_t was int32_t on every machine.  In
   NetBSD 6.0 and onward, time_t is int64_t on every machine.
   Consequently, the signature of time(3), written as

	time_t	time(time_t *);

   was effectively

	int32_t	time(int32_t *);

   before NetBSD 6.0.  In NetBSD 6.0, it changed to be effectively

	int64_t time(int64_t *);

   Before NetBSD 6.0, libc provided the following libc symbols:

	_time		global (implementing the old signature)
	time		weak alias for _time

   In NetBSD 6.0 and later, libc provides the following ELF symbols:

	_time		global (implementing the old signature)
	time		weak alias for _time
	__time50	global (implementing the new signature)

   (Note that the only change is to add __time50, so that existing
   programs linked against old versions of libc will see the same
   semantics for the symbols that were already there.)

   The header file <time.h> (src/include/time.h) declares

	time_t	time(time_t *) __RENAME(__time50);

   so that compiling C programs that call time will yield objects that
   use the __time50 symbol from libc.  However, old programs that were
   compiled against the 32-bit declaration will continue to use the
   32-bit symbol from libc.

   The header file "namespace.h" (src/lib/libc/include/namespace.h)
   defines `time' as a macro expanding to `_time':

	#define	time	_time

   The source file src/lib/libc/gen/time.c includes "namespace.h" and
   <time.h> and defines `time' normally:

	time_t
	time(time_t *t)
	{
	...

   Macro expansion replaces `time' by `_time', but the
   `__RENAME(__time50)' directive on the declaration <time.h> (to which
   the "namespace.h" macro expansion also applies) means the ELF global
   symbol defined here is actually `__time50'.

   The header file <compat/include/time.h>
   (src/lib/libc/compat/include/time.h) declares

	int32_t	time(int32_t *);

   The source file src/lib/libc/compat/gen/compat_time.c includes
   "namespace.h", <compat/include/time.h>, and <time.h>, but suppresses
   the normal declaration of `time' in <time.h> by defining
   __LIBC12_SOURCE__ and thus gets it from <compat/include/time.h>
   instead.  Then compat_time.c defines `time' normally:

	int32_t
	time(int32_t *t)
	{
	...

   Again, macro expansion replaces `time' by `_time', but since there
   is no __RENAME directive in <compat/include/time.h>, the resulting
   ELF global symbol is `_time'.  (Actually, compat_time.c just has
   `#define time_t int32_t' and `#include "gen/time.c"' to get the same
   text of the definition of time.  The above definition is what we get
   effectively by substituting int32_t for the type time_t.)

   Finally, alongside the definition in compat_time.c is

	__weak_alias(time,_time)

   to define `time' as an ELF weak symbol aliasing `_time'.

   The net effect is that NetBSD 6's libc provides the same definitions
   as NetBSD 5's libc for the symbols `time' and `_time', so that old
   programs that were compiled in NetBSD 5 will continue to work with
   NetBSD 6's libc.  But programs compiled in NetBSD 6 will have 64-bit
   time_t.
#	$NetBSD: README,v 1.3 1996/05/03 21:17:07 cgd Exp $
#	@(#)README	8.27 (Berkeley) 9/1/94

This is version 1.85 of the Berkeley DB code.

For information on compiling and installing this software, see the file
PORT/README.

Newer versions of this software will periodically be made available by
anonymous ftp from ftp.cs.berkeley.edu.  An archive in compressed format
is in ucb/4bsd/db.tar.Z, or in gzip format in ucb/4bsd/db.tar.gz.  If
you'd like to receive announcements of future releases of this software,
send email to the contact address below.

Email questions may be addressed to Keith Bostic at bostic@cs.berkeley.edu.

============================================
Distribution contents:

Makefile.inc	Ignore this, it's the 4.4BSD subsystem Makefile.
PORT		The per OS/architecture directories to use to build
		libdb.a, if you're not running 4.4BSD.  See the file
		PORT/README for more information.
README		This file.
btree		The B+tree routines.
changelog	List of changes, per version.
db		The dbopen(3) interface routine.
docs		Various USENIX papers, and the formatted manual pages.
hash		The extended linear hashing routines.
man		The unformatted manual pages.
mpool		The memory pool routines.
recno		The fixed/variable length record routines.
test		Test package.

============================================
Debugging:

If you're running a memory checker (e.g. Purify) on DB, make sure that
you recompile it with "-DPURIFY" in the CFLAGS, first.  By default,
allocated pages are not initialized by the DB code, and they will show
up as reads of uninitialized memory in the buffer write routines.
#	$NetBSD: README,v 1.5 1999/02/16 17:59:18 kleink Exp $
#	@(#)README	8.1 (Berkeley) 6/4/93

This package implements a superset of the hsearch and dbm/ndbm libraries.

Test Programs:
	All test programs which need key/data pairs expect them entered
	with key and data on separate lines

	tcreat3.c	
		Takes 
			bucketsize (bsize), 
			fill factor (ffactor), and
			initial number of elements (nelem).  
		Creates a hash table named hashtest containing the 
		keys/data pairs entered from standard in.
	thash4.c
		Takes
			bucketsize (bsize), 
			fill factor (ffactor), 
			initial number of elements (nelem)
			bytes of cache (ncached), and
			file from which to read data  (fname)
		Creates a table from the key/data pairs on standard in and
		then does a read of each key/data in fname
	tdel.c
		Takes
			bucketsize (bsize), and
			fill factor (ffactor).
			file from which to read data (fname)
		Reads each key/data pair from fname and deletes the
		key from the hash table hashtest
	tseq.c
		Reads the key/data pairs in the file hashtest and writes them
		to standard out.
	tread2.c
		Takes
			butes of cache (ncached).
		Reads key/data pairs from standard in and looks them up
		in the file hashtest.
	tverify.c
		Reads key/data pairs from standard in, looks them up
		in the file hashtest, and verifies that the data is
		correct.

NOTES:

The man page ../man/db.3 explains the interface to the hashing system.
The file hash.ps is a postscript copy of a paper explaining
the history, implementation, and performance of the hash package.

"bugs" or idiosyncracies

If you have a lot of overflows, it is possible to run out of overflow
pages.  Currently, this will cause a message to be printed on stderr.
Eventually, this will be indicated by a return error code.

If you are using the ndbm interface and exit without flushing or closing the
file, you may lose updates since the package buffers all writes.  Also,
the db interface only creates a single database file.  To avoid overwriting
the user's original file, the suffix ".db" is appended to the file name
passed to dbm_open.  Additionally, if your code "knows" about the historic
.dir and .pag files, it will break.  

There is a fundamental difference between this package and the old hsearch.
Hsearch requires the user to maintain the keys and data in the application's
allocated memory while hash takes care of all storage management.  The down
side is that the byte strings passed in the ENTRY structure must be null
terminated (both the keys and the data).
#	$NetBSD: README,v 1.2 1995/02/27 13:24:00 cgd Exp $
#	@(#)README	8.1 (Berkeley) 6/4/93

These are the current memory pool routines.
They aren't ready for prime time, yet, and
the interface is expected to change.

--keith
$NetBSD: README.txt,v 1.1 2000/06/06 08:15:02 bjh21 Exp $

Package Overview for SoftFloat Release 2a

John R. Hauser
1998 December 13


SoftFloat is a software implementation of floating-point that conforms to
the IEC/IEEE Standard for Binary Floating-Point Arithmetic.  SoftFloat is
distributed in the form of C source code.  Compiling the SoftFloat sources
generates two things:

-- A SoftFloat object file (typically `softfloat.o') containing the complete
   set of IEC/IEEE floating-point routines.

-- A `timesoftfloat' program for evaluating the speed of the SoftFloat
   routines.  (The SoftFloat module is linked into this program.)

The SoftFloat package is documented in four text files:

   softfloat.txt          Documentation for using the SoftFloat functions.
   softfloat-source.txt   Documentation for compiling SoftFloat.
   softfloat-history.txt  History of major changes to SoftFloat.
   timesoftfloat.txt      Documentation for using `timesoftfloat'.

Other files in the package comprise the source code for SoftFloat.

Please be aware that some work is involved in porting this software to other
targets.  It is not just a matter of getting `make' to complete without
error messages.  I would have written the code that way if I could, but
there are fundamental differences between systems that I can't make go away.
You should not attempt to compile SoftFloat without first reading both
`softfloat.txt' and `softfloat-source.txt'.

At the time of this writing, the most up-to-date information about
SoftFloat and the latest release can be found at the Web page `http://
HTTP.CS.Berkeley.EDU/~jhauser/arithmetic/SoftFloat.html'.

$NetBSD: README.NetBSD,v 1.2 2002/05/21 23:51:05 bjh21 Exp $

This is a modified version of part of John Hauser's SoftFloat 2a package.
This version has been heavily modified to support its use with GCC to
implement built-in floating-point operations, but compiling
softfloat.c without SOFTFLOAT_FOR_GCC defined should get you the same
results as from the original.

This directory contains source for a library of binary -> decimal
and decimal -> binary conversion routines, for single-, double-,
and extended-precision IEEE binary floating-point arithmetic, and
other IEEE-like binary floating-point, including "double double",
as in

	T. J. Dekker, "A Floating-Point Technique for Extending the
	Available Precision", Numer. Math. 18 (1971), pp. 224-242

and

	"Inside Macintosh: PowerPC Numerics", Addison-Wesley, 1994

The conversion routines use double-precision floating-point arithmetic
and, where necessary, high precision integer arithmetic.  The routines
are generalizations of the strtod and dtoa routines described in

	David M. Gay, "Correctly Rounded Binary-Decimal and
	Decimal-Binary Conversions", Numerical Analysis Manuscript
	No. 90-10, Bell Labs, Murray Hill, 1990;
	http://cm.bell-labs.com/cm/cs/what/ampl/REFS/rounding.ps.gz

(based in part on papers by Clinger and Steele & White: see the
references in the above paper).

The present conversion routines should be able to use any of IEEE binary,
VAX, or IBM-mainframe double-precision arithmetic internally, but I (dmg)
have so far only had a chance to test them with IEEE double precision
arithmetic.

The core conversion routines are strtodg for decimal -> binary conversions
and gdtoa for binary -> decimal conversions.  These routines operate
on arrays of unsigned 32-bit integers of type ULong, a signed 32-bit
exponent of type Long, and arithmetic characteristics described in
struct FPI; FPI, Long, and ULong are defined in gdtoa.h.  File arith.h
is supposed to provide #defines that cause gdtoa.h to define its
types correctly.  File arithchk.c is source for a program that
generates a suitable arith.h on all systems where I've been able to
test it.

The core conversion routines are meant to be called by helper routines
that know details of the particular binary arithmetic of interest and
convert.  The present directory provides helper routines for 5 variants
of IEEE binary floating-point arithmetic, each indicated by one or
two letters:

	f	IEEE single precision
	d	IEEE double precision
	x	IEEE extended precision, as on Intel 80x87
		and software emulations of Motorola 68xxx chips
		that do not pad the way the 68xxx does, but
		only store 80 bits
	xL	IEEE extended precision, as on Motorola 68xxx chips
	Q	quad precision, as on Sun Sparc chips
	dd	double double, pairs of IEEE double numbers
		whose sum is the desired value

For decimal -> binary conversions, there are three families of
helper routines: one for round-nearest (or the current rounding
mode on IEEE-arithmetic systems that provide the C99 fegetround()
function, if compiled with -DHonor_FLT_ROUNDS):

	strtof
	strtod
	strtodd
	strtopd
	strtopf
	strtopx
	strtopxL
	strtopQ

one with rounding direction specified:

	strtorf
	strtord
	strtordd
	strtorx
	strtorxL
	strtorQ

and one for computing an interval (at most one bit wide) that contains
the decimal number:

	strtoIf
	strtoId
	strtoIdd
	strtoIx
	strtoIxL
	strtoIQ

The latter call strtoIg, which makes one call on strtodg and adjusts
the result to provide the desired interval.  On systems where native
arithmetic can easily make one-ulp adjustments on values in the
desired floating-point format, it might be more efficient to use the
native arithmetic.  Routine strtodI is a variant of strtoId that
illustrates one way to do this for IEEE binary double-precision
arithmetic -- but whether this is more efficient remains to be seen.

Functions strtod and strtof have "natural" return types, float and
double -- strtod is specified by the C standard, and strtof appears
in the stdlib.h of some systems, such as (at least some) Linux systems.
The other functions write their results to their final argument(s):
to the final two argument for the strtoI... (interval) functions,
and to the final argument for the others (strtop... and strtor...).
Where possible, these arguments have "natural" return types (double*
or float*), to permit at least some type checking.  In reality, they
are viewed as arrays of ULong (or, for the "x" functions, UShort)
values. On systems where long double is the appropriate type, one can
pass long double* final argument(s) to these routines.  The int value
that these routines return is the return value from the call they make
on strtodg; see the enum of possible return values in gdtoa.h.

Source files g_ddfmt.c, misc.c, smisc.c, strtod.c, strtodg.c, and ulp.c
should use true IEEE double arithmetic (not, e.g., double extended),
at least for storing (and viewing the bits of) the variables declared
"double" within them.

One detail indicated in struct FPI is whether the target binary
arithmetic departs from the IEEE standard by flushing denormalized
numbers to 0.  On systems that do this, the helper routines for
conversion to double-double format (when compiled with
Sudden_Underflow #defined) penalize the bottom of the exponent
range so that they return a nonzero result only when the least
significant bit of the less significant member of the pair of
double values returned can be expressed as a normalized double
value.  An alternative would be to drop to 53-bit precision near
the bottom of the exponent range.  To get correct rounding, this
would (in general) require two calls on strtodg (one specifying
126-bit arithmetic, then, if necessary, one specifying 53-bit
arithmetic).

By default, the core routine strtodg and strtod set errno to ERANGE
if the result overflows to +Infinity or underflows to 0.  Compile
these routines with NO_ERRNO #defined to inhibit errno assignments.

Routine strtod is based on netlib's "dtoa.c from fp", and
(f = strtod(s,se)) is more efficient for some conversions than, say,
strtord(s,se,1,&f).  Parts of strtod require true IEEE double
arithmetic with the default rounding mode (round-to-nearest) and, on
systems with IEEE extended-precision registers, double-precision
(53-bit) rounding precision.  If the machine uses (the equivalent of)
Intel 80x87 arithmetic, the call
	_control87(PC_53, MCW_PC);
does this with many compilers.  Whether this or another call is
appropriate depends on the compiler; for this to work, it may be
necessary to #include "float.h" or another system-dependent header
file.

Source file strtodnrp.c gives a strtod that does not require 53-bit
rounding precision on systems (such as Intel IA32 systems) that may
suffer double rounding due to use of extended-precision registers.
For some conversions this variant of strtod is less efficient than the
one in strtod.c when the latter is run with 53-bit rounding precision.

The values that the strto* routines return for NaNs are determined by
gd_qnan.h, which the makefile generates by running the program whose
source is qnan.c.  Note that the rules for distinguishing signaling
from quiet NaNs are system-dependent.  For cross-compilation, you need
to determine arith.h and gd_qnan.h suitably, e.g., using the
arithmetic of the target machine.

C99's hexadecimal floating-point constants are recognized by the
strto* routines (but this feature has not yet been heavily tested).
Compiling with NO_HEX_FP #defined disables this feature.

When compiled with -DINFNAN_CHECK, the strto* routines recognize C99's
NaN and Infinity syntax.  Moreover, unless No_Hex_NaN is #defined, the
strto* routines also recognize C99's NaN(...) syntax: they accept
(case insensitively) strings of the form NaN(x), where x is a string
of hexadecimal digits and spaces; if there is only one string of
hexadecimal digits, it is taken for the fraction bits of the resulting
NaN; if there are two or more strings of hexadecimal digits, each
string is assigned to the next available sequence of 32-bit words of
fractions bits (starting with the most significant), right-aligned in
each sequence.

For binary -> decimal conversions, I've provided just one family
of helper routines:

	g_ffmt
	g_dfmt
	g_ddfmt
	g_xfmt
	g_xLfmt
	g_Qfmt

which do a "%g" style conversion either to a specified number of decimal
places (if their ndig argument is positive), or to the shortest
decimal string that rounds to the given binary floating-point value
(if ndig <= 0).  They write into a buffer supplied as an argument
and return either a pointer to the end of the string (a null character)
in the buffer, if the buffer was long enough, or 0.  Other forms of
conversion are easily done with the help of gdtoa(), such as %e or %f
style and conversions with direction of rounding specified (so that, if
desired, the decimal value is either >= or <= the binary value).
On IEEE-arithmetic systems that provide the C99 fegetround() function,
if compiled with -DHonor_FLT_ROUNDS, these routines honor the current
rounding mode.

For an example of more general conversions based on dtoa(), see
netlib's "printf.c from ampl/solvers".

For double-double -> decimal, g_ddfmt() assumes IEEE-like arithmetic
of precision max(126, #bits(input)) bits, where #bits(input) is the
number of mantissa bits needed to represent the sum of the two double
values in the input.

The makefile creates a library, gdtoa.a.  To use the helper
routines, a program only needs to include gdtoa.h.  All the
source files for gdtoa.a include a more extensive gdtoaimp.h;
among other things, gdtoaimp.h has #defines that make "internal"
names end in _D2A.  To make a "system" library, one could modify
these #defines to make the names start with __.

Various comments about possible #defines appear in gdtoaimp.h,
but for most purposes, arith.h should set suitable #defines.

Systems with preemptive scheduling of multiple threads require some
manual intervention.  On such systems, it's necessary to compile
dmisc.c, dtoa.c gdota.c, and misc.c with MULTIPLE_THREADS #defined,
and to provide (or suitably #define) two locks, acquired by
ACQUIRE_DTOA_LOCK(n) and freed by FREE_DTOA_LOCK(n) for n = 0 or 1.
(The second lock, accessed in pow5mult, ensures lazy evaluation of
only one copy of high powers of 5; omitting this lock would introduce
a small probability of wasting memory, but would otherwise be harmless.)
Routines that call dtoa or gdtoa directly must also invoke freedtoa(s)
to free the value s returned by dtoa or gdtoa.  It's OK to do so whether
or not MULTIPLE_THREADS is #defined, and the helper g_*fmt routines
listed above all do this indirectly (in gfmt_D2A(), which they all call).

By default, there is a private pool of memory of length 2000 bytes
for intermediate quantities, and MALLOC (see gdtoaimp.h) is called only
if the private pool does not suffice.   2000 is large enough that MALLOC
is called only under very unusual circumstances (decimal -> binary
conversion of very long strings) for conversions to and from double
precision.  For systems with preemptively scheduled multiple threads
or for conversions to extended or quad, it may be appropriate to
#define PRIVATE_MEM nnnn, where nnnn is a suitable value > 2000.
For extended and quad precisions, -DPRIVATE_MEM=20000 is probably
plenty even for many digits at the ends of the exponent range.
Use of the private pool avoids some overhead.

Directory test provides some test routines.  See its README.
I've also tested this stuff (except double double conversions)
with Vern Paxson's testbase program: see

	V. Paxson and W. Kahan, "A Program for Testing IEEE Binary-Decimal
	Conversion", manuscript, May 1991,
	ftp://ftp.ee.lbl.gov/testbase-report.ps.Z .

(The same ftp directory has source for testbase.)

Some system-dependent additions to CFLAGS in the makefile:

	HU-UX: -Aa -Ae
	OSF (DEC Unix): -ieee_with_no_inexact
	SunOS 4.1x: -DKR_headers -DBad_float_h

If you want to put this stuff into a shared library and your
operating system requires export lists for shared libraries,
the following would be an appropriate export list:

	dtoa
	freedtoa
	g_Qfmt
	g_ddfmt
	g_dfmt
	g_ffmt
	g_xLfmt
	g_xfmt
	gdtoa
	strtoIQ
	strtoId
	strtoIdd
	strtoIf
	strtoIx
	strtoIxL
	strtod
	strtodI
	strtodg
	strtof
	strtopQ
	strtopd
	strtopdd
	strtopf
	strtopx
	strtopxL
	strtorQ
	strtord
	strtordd
	strtorf
	strtorx
	strtorxL

When time permits, I (dmg) hope to write in more detail about the
present conversion routines; for now, this README file must suffice.
Meanwhile, if you wish to write helper functions for other kinds of
IEEE-like arithmetic, some explanation of struct FPI and the bits
array may be helpful.  Both gdtoa and strtodg operate on a bits array
described by FPI *fpi.  The bits array is of type ULong, a 32-bit
unsigned integer type.  Floating-point numbers have fpi->nbits bits,
with the least significant 32 bits in bits[0], the next 32 bits in
bits[1], etc.  These numbers are regarded as integers multiplied by
2^e (i.e., 2 to the power of the exponent e), where e is the second
argument (be) to gdtoa and is stored in *exp by strtodg.  The minimum
and maximum exponent values fpi->emin and fpi->emax for normalized
floating-point numbers reflect this arrangement.  For example, the
P754 standard for binary IEEE arithmetic specifies doubles as having
53 bits, with normalized values of the form 1.xxxxx... times 2^(b-1023),
with 52 bits (the x's) and the biased exponent b represented explicitly;
b is an unsigned integer in the range 1 <= b <= 2046 for normalized
finite doubles, b = 0 for denormals, and b = 2047 for Infinities and NaNs.
To turn an IEEE double into the representation used by strtodg and gdtoa,
we multiply 1.xxxx... by 2^52 (to make it an integer) and reduce the
exponent e = (b-1023) by 52:

	fpi->emin = 1 - 1023 - 52
	fpi->emax = 1046 - 1023 - 52

In various wrappers for IEEE double, we actually write -53 + 1 rather
than -52, to emphasize that there are 53 bits including one implicit bit.
Field fpi->rounding indicates the desired rounding direction, with
possible values
	FPI_Round_zero = toward 0,
	FPI_Round_near = unbiased rounding -- the IEEE default,
	FPI_Round_up = toward +Infinity, and
	FPI_Round_down = toward -Infinity
given in gdtoa.h.

Field fpi->sudden_underflow indicates whether strtodg should return
denormals or flush them to zero.  Normal floating-point numbers have
bit fpi->nbits in the bits array on.  Denormals have it off, with
exponent = fpi->emin.  Strtodg provides distinct return values for normals
and denormals; see gdtoa.h.

Compiling g__fmt.c, strtod.c, and strtodg.c with -DUSE_LOCALE causes
the decimal-point character to be taken from the current locale; otherwise
it is '.'.

Source files dtoa.c and strtod.c in this directory are derived from
netlib's "dtoa.c from fp" and are meant to function equivalently.
When compiled with Honor_FLT_ROUNDS #defined (on systems that provide
FLT_ROUNDS and fegetround() as specified in the C99 standard), they
honor the current rounding mode.  Because FLT_ROUNDS is buggy on some
(Linux) systems -- not reflecting calls on fesetround(), as the C99
standard says it should -- when Honor_FLT_ROUNDS is #defined, the
current rounding mode is obtained from fegetround() rather than from
FLT_ROUNDS, unless Trust_FLT_ROUNDS is also #defined.

Compile with -DUSE_LOCALE to use the current locale; otherwise
decimal points are assumed to be '.'.  With -DUSE_LOCALE, unless
you also compile with -DNO_LOCALE_CACHE, the details about the
current "decimal point" character string are cached and assumed not
to change during the program's execution.

On machines with a 64-bit long double and perhaps a 113-bit "quad"
type, you can invoke "make Printf" to add Printf (and variants, such
as Fprintf) to gdtoa.a.  These are analogs, declared in stdio1.h, of
printf and fprintf, etc. in which %La, %Le, %Lf, and %Lg are for long
double and (if appropriate) %Lqa, %Lqe, %Lqf, and %Lqg are for quad
precision printing.

Please send comments to	David M. Gay (dmg at acm dot org, with " at "
changed at "@" and " dot " changed to ".").
This directory contains source for several test programs:

dt is for conversion to/from double; it permits input of pairs of
32-bit hex integers as #hhhhhhhh hhhhhhhh (i.e., the initial '#'
indicates hex input).  No initial # ==> decimal input.
After the input number is an optional : mode ndigits
(colon, and decimal integers for parameters "mode" and "ndigits"
to gdtoa).

Qtest, ddtest, dtest, ftest, xLtest and xtest are for conversion to/from

	f	IEEE single precision
	d	IEEE double precision
	xL	IEEE extended precision, as on Motorola 680x0 chips
	x	IEEE extended precision, as on Intel 80x87 chips or
			software emulation of Motorola 680x0 chips
	Q	quad precision, as on Sun Sparc chips
	dd	double double, pairs of IEEE double numbers
		whose sum is the desired value

They're all similar, except for the precision.  They test both
directed roundings and interval input (the strtoI* routines).
Lines that begin with "r" specify or interrogate the desired rounding
direction:

	0 = toward 0
	1 = nearest (default)
	2 = toward +Infinity
	3 = toward -Infinity

These are the FPI_Round_* values in gdota.h.  The "r" value is sticky:
it stays in effect til changed.  To change the value, give a line that
starts with r followed by 0, 1, 2, or 3.  To check the value, give "r"
by itself.

Lines that begin with n followed by a number specify the ndig
argument for subsequent calls to the relevant g_*fmt routine.

Lines that start with # followed by the appropriate number of
hexadecimal strings (see the comments) give the big-endian
internal representation of the desired number.

When routines Qtest, xLtest, and xtest are used on machines whose
long double is of type "quad" (for Qtest) or "extended" (for x*test),
they try to print with %Lg as another way to show binary values.

Program ddtest also accepts (white-space separated) pairs of decimal
input numbers; it converts both with strtod and feeds the result
to g_ddfmt.

Program dItest exercises strtodI and strtoId.

Programs dItestsi and ddtestsi are for testing the sudden-underflow
logic (on double and double-double conversions).

Program strtodt tests strtod on some hard cases (in file testnos3)
posted by Fred Tydeman to comp.arch.arithmetic on 26 Feb. 1996.
To get correct results on Intel (x86) systems, the rounding precision
must be set to 53 bits.  This can be done, e.g., by invoking
fpinit_ASL(), whose source appears in
http://www.netlib.org/ampl/solvers/fpinit.c .

The obad directory shows results expected on (at least some) Intel x86
Linux systems and may not be relevant to other systems.

You can optionally compile getround.c with -DHonor_FLT_ROUNDS
to manually test strtof, strtod, etc., using fegetround().
You can also or alternatively compile getround.c with
-DUSE_MY_LOCALE (when ../gdtoa.a is compiled with -DUSE_LOCALE)
to test multi-byte decimal points.

If in the parent directory, you have sucessfully invoked "make Printf"
to add a "printf" (called Printf and accessed via ../stdio1.h), then
here you can use "make pf_test" and (if you have both a 64-bit long
double and a 113-bit "quad" double type) "make pf_testLq" for a brief
test of %g and %a variants in Printf.

These are simple test programs, not meant for exhaustive testing,
but for manually testing "interesting" cases.  Paxson's testbase
is good for more exhaustive testing, in part with random inputs.
See ftp://ftp.ee.lbl.gov/testbase-report.ps.Z .
#	$NetBSD: README,v 1.2 1998/01/09 04:11:52 perry Exp $

RPCSRC 4.0 7/11/89

This distribution contains Sun Microsystem's implementation of the
RPC and XDR protocols and is compatible with 4.2BSD and 4.3BSD.  Also
included is complete documentation, utilities, RPC service
specification files, and demonstration services in the format used by
the RPC protocol compiler (rpcgen).  See WHAT'S NEW below for
details.

NOTE ABOUT SECURE RPC:

This release of RPCSRC contains most of the code needed to implement
Secure RPC (see "DES Authentication" in the RPC Protocol Specification,
doc/rpc.rfc.ms).  Due to legal considerations, we are unable to
distribute an implementation of DES, the Data Encryption Standard, which
Secure RPC requires.  For this reason, all of the files, documentation, and
programs associated with Secure RPC have been placed into a separate
directory, secure_rpc.  The RPC library contained in the main body of this
release *DOES NOT* support Secure RPC.  See secure_rpc/README for more
details.  (A DES library was posted in Volume 18 of comp.sources.unix.)

If you wish to report bugs found in this release, send mail to:

Portable ONC/NFS
Sun Microsystems, Inc
MS 12-33
2550 Garcia Avenue
Mountain View, CA  94043

or send Email to nfsnet@sun.com (the Internet) or sun!nfsnet (Usenet).

ROADMAP

The directory hierarchy is as follows:

    demo/       Various demonstration services
    demo/dir        Remote directory lister
    demo/msg        Remote console message delivery service
    demo/sort       Remote sort service

    doc/        Documentation for RPC, XDR and NFS in "-ms" format.

    etc/        Utilities (rpcinfo and portmap).  portmap must be
                started by root before any other RPC network services are
                used.  SEE BELOW FOR BUGFIX TO 4.3BSD COMPILER.

    man/        Manual pages for RPC library, rpcgen, and utilities.

    rpc/        The RPC and XDR library.  SEE BELOW
                FOR BUGFIX TO 4.2BSD COMPILER.

    rpcgen/     The RPC Language compiler (for .x files)

    rpcsvc/     Service definition files for various services and the
                server and client code for the Remote Status service.

    secure_rpc/ The files in this directory are used to build a version of
                the RPC library with DES Authentication.  See the README
                file in that directory for more details.

BUILD INSTRUCTIONS

Makefiles can be found in all directories except for man.  The
Makefile in the top directory will cause these others to be invoked
(except for in the doc, man and demo directories), in turn building the
entire release.

WARNING!  THE DEFAULT INSTALLATION PROCEDURES WILL INSTALL FILES
IN /usr/include, /usr/lib, /usr/bin and /etc.

The master RPC include file, rpc/rpc.h, is used by all programs and
routines that use RPC.  It includes other RPC and system include files
needed by the RPC system.  PLEASE NOTE: If your system has NFS, it
may have been based on Sun's NFS Source.  The include files installed
by this package may duplicate include files you will find on your NFS
system.  The RPCSRC 4.0 include files are upwardly compatible to all
NFS Source include files as of the date of this distribution (not
including any new definitions or declarations added by your system
vendor).  HOWEVER: Please read the comments towards the end of
rpc/rpc.h regarding rpc/netdb.h.  You may need to uncomment the
inclusion of that file if the structures it defines are already
defined by your system's include files.

After making any compiler fixes that are needed (see below), at
the top directory, type:

    make install

For all installations, the Makefile macro DESTDIR is prepended to the
installation path.  It is defined to be null in the Makefiles, so
installations are relative to root.  (You will probably need root
privileges for installing the files under the default path.)  To
install the files under some other tree (e.g., /usr/local), use the
command:

    make install DESTDIR=/usr/local

This will place the include files in /usr/local/usr/include, the RPC
library in /usr/local/usr/lib, rpcgen in /usr/local/usr/bin, and the
utilities in /usr/local/etc.  You'll have to edit the Makefiles or
install the files by hand if you want to do anything other than this
kind of relocation of the installation tree.

The RPC library will be built and installed first.  By default it is
installed in /usr/lib as "librpclib.a".  The directory
/usr/include/rpc will also be created, and several header files will
be installed there.  ALL RPC SERVICES INCLUDE THESE HEADER FILES.

The programs in etc/ link in routines from librpclib.a.  If you change
where it is installed, be sure to edit etc/'s Makefile to reflect this.
These programs are installed in /etc.  PORTMAP MUST BE RUNNING ON
YOUR SYSTEM BEFORE YOU START ANY OTHER RPC SERVICE.

rpcgen is installed in /usr/bin.  This program is required to build
the demonstration services in demo and the rstat client and server in
rpcsvc/.

The rpcsvc/ directory will install its files in the directory
/usr/include/rpcsvc.  The Remote Status service (rstat_svc) will be
compiled and installed in /etc.  If you wish to make this service
available, you should either start this service when needed or have
it started at boot time by invoking it in your /etc/rc.local script.
(Be sure that portmap is started first!)  Sun has modified its
version of inetd to automatically start RPC services.  (Use "make
LIB=" when building rstat on a Sun Workstation.)  The Remote Status
client (rstat) will be installed in /usr/bin.  This program queries
the rstat_svc on a remote host and prints a system status summary
similar to the one printed by "uptime".

The documentation is not built during the "make install" command.
Typing "make" in the doc directory will cause all of the manuals to
be formatted using nroff into a single file.  We have had a report
that certain "troff" equivalents have trouble processing the full
manual.  If you have trouble, try building the manuals individually
(see the Makefile).

The demonstration services in the demo directory are not built by the
top-level "make install" command.  To build these, cd to the demo
directory and enter "make".  The three services will be built.
RPCGEN MUST BE INSTALLED in a path that make can find.  To run the
services, start the portmap program as root and invoke the service
(you probably will want to put it in the background).  rpcinfo can be
used to check that the service succeeded in getting registered with
portmap, and to ping the service (see rpcinfo's man page).  You can
then use the corresponding client program to exercise the service.
To build these services on a Sun workstation, you must prevent the
Makefile from trying to link the RPC library (as these routines are
already a part of Sun's libc).  Use: "make LIB=".

BUGFIX FOR 4.3BSD COMPILER

The use of a 'void *' declaration for one of the arguments in
the reply_proc() procedure in etc/rpcinfo.c will trigger a bug
in the 4.3BSD compiler.  The bug is fixed by the following change to
the compiler file mip/manifest.h:

*** manifest.h.r1.1	Thu Apr 30 13:52:25 1987
--- manifest.h.r1.2	Mon Nov 23 18:58:17 1987
***************
*** 21,27 ****
  /*
   * Bogus type values
   */
! #define TNULL	PTR		/* pointer to UNDEF */
  #define TVOID	FTN		/* function returning UNDEF (for void) */
  
  /*
--- 21,27 ----
  /*
   * Bogus type values
   */
! #define TNULL	INCREF(MOETY)	/* pointer to MOETY -- impossible type */
  #define TVOID	FTN		/* function returning UNDEF (for void) */
  
  /*

If you cannot fix your compiler, change the declaration in reply_proc()
from 'void *' to 'char *'.

BUGFIX FOR 4.2BSD COMPILER

Unpatched 4.2BSD compilers complain about valid C.  You can make old
compilers happy by changing some voids to ints.  However, the fix to
the 4.2 VAX compiler is as follows (to mip/trees.c):

*** trees.c.r1.1	Mon May 11 13:47:58 1987
--- trees.c.r1.2	Wed Jul  2 18:28:52 1986
***************
*** 1247,1253 ****
  		if(o==CAST && mt1==0)return(TYPL+TYMATCH);
  		if( mt12 & MDBI ) return( TYPL+LVAL+TYMATCH );
  		else if( (mt1&MENU)||(mt2&MENU) ) return( LVAL+NCVT+TYPL+PTMATCH+PUN );
! 		else if( mt12 == 0 ) break;
  		else if( mt1 & MPTR ) return( LVAL+PTMATCH+PUN );
  		else if( mt12 & MPTI ) return( TYPL+LVAL+TYMATCH+PUN );
  		break;
--- 1261,1269 ----
  		if(o==CAST && mt1==0)return(TYPL+TYMATCH);
  		if( mt12 & MDBI ) return( TYPL+LVAL+TYMATCH );
  		else if( (mt1&MENU)||(mt2&MENU) ) return( LVAL+NCVT+TYPL+PTMATCH+PUN );
! 		/* if right is TVOID and looks like a CALL, is not ok */
! 		else if (mt2 == 0 && (p->in.right->in.op == CALL || p->in.right->in.op == UNARY CALL))
! 			break;
  		else if( mt1 & MPTR ) return( LVAL+PTMATCH+PUN );
  		else if( mt12 & MPTI ) return( TYPL+LVAL+TYMATCH+PUN );
  		break;

WHAT'S NEW IN THIS RELEASE: RPCSRC 4.0

The previous release was RPCSRC 3.9.  As with all previous releases,
this release is based directly on files from Sun Microsystem's
implementation.

Upgrade from RPCSRC 3.9

1)  RPCSRC 4.0 upgrades RPCSRC 3.9.  Improvements from SunOS 4.0 have
    been integrated into this release.

Secure RPC (in the secure_rpc/ directory)

2)  DES Authentication routines and programs are provided.
3)  A new manual, "Secure NFS" is provided, which describes Secure RPC
    and Secure NFS.
4)  Skeleton routines and manual pages are provided which describe the
    DES encryption procedures required by Secure RPC.  HOWEVER, NO DES
    ROUTINE IS PROVIDED.

New Functionality

5)  rpcinfo can now be used to de-register services from the portmapper
    which may have terminated abnormally.
6)  A new client, rstat, is provided which queries the rstat_svc and
    prints a status line similar to the one displayed by "uptime".
README for the tz distribution

"What time is it?" -- Richard Deacon as The King
"Any time you want it to be." -- Frank Baxter as The Scientist
					(from the Bell System film "About Time")

The Time Zone Database (often called tz or zoneinfo) contains code and
data that represent the history of local time for many representative
locations around the globe.  It is updated periodically to reflect
changes made by political bodies to time zone boundaries, UTC offsets,
and daylight-saving rules.

See <https://www.iana.org/time-zones/repository/tz-link.html> or the
file tz-link.htm for how to acquire the code and data.  Once acquired,
read the comments in the file 'Makefile' and make any changes needed
to make things right for your system, especially if you are using some
platform other than GNU/Linux.  Then run the following commands,
substituting your desired installation directory for "$HOME/tzdir":

	make TOPDIR=$HOME/tzdir install
	$HOME/tzdir/etc/zdump -v America/Los_Angeles

Historical local time information has been included here to:

*	provide a compendium of data about the history of civil time
	that is useful even if not 100% accurate;

*	give an idea of the variety of local time rules that have
	existed in the past and thus an idea of the variety that may be
	expected in the future;

*	provide a test of the generality of the local time rule description
	system.

The information in the time zone data files is by no means authoritative;
fixes and enhancements are welcome.  Please see the file CONTRIBUTING
for details.

Thanks to these Time Zone Caballeros who've made major contributions to the
time conversion package: Keith Bostic; Bob Devine; Paul Eggert; Robert Elz;
Guy Harris; Mark Horton; John Mackin; and Bradley White.  Thanks also to
Michael Bloom, Art Neilson, Stephen Prince, John Sovereign, and Frank Wales
for testing work, and to Gwillim Law for checking local mean time data.
Thanks in particular to Arthur David Olson, the project's founder and first
maintainer, to whom the time zone community owes the greatest debt of all.
None of them are responsible for remaining errors.

-----

This file is in the public domain, so clarified as of 2009-05-17 by
Arthur David Olson.  The other files in this distribution are either
public domain or BSD licensed; see the file LICENSE for details.
#	$NetBSD: README,v 1.2 1998/01/09 04:12:00 perry Exp $

This is a nearly-public-domain reimplementation of the V8 regexp(3) package.
It gives C programs the ability to use egrep-style regular expressions, and
does it in a much cleaner fashion than the analogous routines in SysV.

	Copyright (c) 1986 by University of Toronto.
	Written by Henry Spencer.  Not derived from licensed software.

	Permission is granted to anyone to use this software for any
	purpose on any computer system, and to redistribute it freely,
	subject to the following restrictions:

	1. The author is not responsible for the consequences of use of
		this software, no matter how awful, even if they arise
		from defects in it.

	2. The origin of this software must not be misrepresented, either
		by explicit claim or by omission.

	3. Altered versions must be plainly marked as such, and must not
		be misrepresented as being the original software.

Barring a couple of small items in the BUGS list, this implementation is
believed 100% compatible with V8.  It should even be binary-compatible,
sort of, since the only fields in a "struct regexp" that other people have
any business touching are declared in exactly the same way at the same
location in the struct (the beginning).

This implementation is *NOT* AT&T/Bell code, and is not derived from licensed
software.  Even though U of T is a V8 licensee.  This software is based on
a V8 manual page sent to me by Dennis Ritchie (the manual page enclosed
here is a complete rewrite and hence is not covered by AT&T copyright).
The software was nearly complete at the time of arrival of our V8 tape.
I haven't even looked at V8 yet, although a friend elsewhere at U of T has
been kind enough to run a few test programs using the V8 regexp(3) to resolve
a few fine points.  I admit to some familiarity with regular-expression
implementations of the past, but the only one that this code traces any
ancestry to is the one published in Kernighan & Plauger (from which this
one draws ideas but not code).

Simplistically:  put this stuff into a source directory, copy regexp.h into
/usr/include, inspect Makefile for compilation options that need changing
to suit your local environment, and then do "make r".  This compiles the
regexp(3) functions, compiles a test program, and runs a large set of
regression tests.  If there are no complaints, then put regexp.o, regsub.o,
and regerror.o into your C library, and regexp.3 into your manual-pages
directory.

Note that if you don't put regexp.h into /usr/include *before* compiling,
you'll have to add "-I." to CFLAGS before compiling.

The files are:

Makefile	instructions to make everything
regexp.3	manual page
regexp.h	header file, for /usr/include
regexp.c	source for regcomp() and regexec()
regsub.c	source for regsub()
regerror.c	source for default regerror()
regmagic.h	internal header file
try.c		source for test program
timer.c		source for timing program
tests		test list for try and timer

This implementation uses nondeterministic automata rather than the
deterministic ones found in some other implementations, which makes it
simpler, smaller, and faster at compiling regular expressions, but slower
at executing them.  In theory, anyway.  This implementation does employ
some special-case optimizations to make the simpler cases (which do make
up the bulk of regular expressions actually used) run quickly.  In general,
if you want blazing speed you're in the wrong place.  Replacing the insides
of egrep with this stuff is probably a mistake; if you want your own egrep
you're going to have to do a lot more work.  But if you want to use regular
expressions a little bit in something else, you're in luck.  Note that many
existing text editors use nondeterministic regular-expression implementations,
so you're in good company.

This stuff should be pretty portable, given appropriate option settings.
If your chars have less than 8 bits, you're going to have to change the
internal representation of the automaton, although knowledge of the details
of this is fairly localized.  There are no "reserved" char values except for
NUL, and no special significance is attached to the top bit of chars.
The string(3) functions are used a fair bit, on the grounds that they are
probably faster than coding the operations in line.  Some attempts at code
tuning have been made, but this is invariably a bit machine-specific.
$NetBSD: README,v 1.4 2012/05/18 15:36:21 jruoho Exp $

When adding new tests, please try to follow the following conventions.

1. For library routines, including system calls, the directory structure of
   the tests should follow the directory structure of the real source tree.
   For instance, interfaces available via the C library should follow:

	src/lib/libc/gen -> src/tests/lib/libc/gen
	src/lib/libc/sys -> src/tests/lib/libc/sys
	...

2. Equivalently, all tests for userland utilities should try to follow their
   location in the source tree. If this can not be satisfied, the tests for
   a utility should be located under the directory to which the utility is
   installed. Thus, a test for env(1) should go to src/tests/usr.bin/env.
   Likewise, a test for tcpdump(8) should be in src/tests/usr.sbin/tcpdump,
   even though the source code for the program is located under src/external.

3. Otherwise use your own discretion.
regular expression test set
Lines are at least three fields, separated by one or more tabs.  "" stands
for an empty field.  First field is an RE.  Second field is flags.  If
C flag given, regcomp() is expected to fail, and the third field is the
error name (minus the leading REG_).

Otherwise it is expected to succeed, and the third field is the string to
try matching it against.  If there is no fourth field, the match is
expected to fail.  If there is a fourth field, it is the substring that
the RE is expected to match.  If there is a fifth field, it is a comma-
separated list of what the subexpressions should match, with - indicating
no match for that one.  In both the fourth and fifth fields, a (sub)field
starting with @ indicates that the (sub)expression is expected to match
a null string followed by the stuff after the @; this provides a way to
test where null strings match.  The character `N' in REs and strings
is newline, `S' is space, `T' is tab, `Z' is NUL.

The full list of flags:
  -	placeholder, does nothing
  b	RE is a BRE, not an ERE
  &	try it as both an ERE and a BRE
  C	regcomp() error expected, third field is error name
  i	REG_ICASE
  m	("mundane") REG_NOSPEC
  s	REG_NOSUB (not really testable)
  n	REG_NEWLINE
  ^	REG_NOTBOL
  $	REG_NOTEOL
  #	REG_STARTEND (see below)
  p	REG_PEND

For REG_STARTEND, the start/end offsets are those of the substring
enclosed in ().
AT&T test data from: http://www2.research.att.com/~gsf/testregex/

Quoting from the page:
    testregex.c 2004-05-31 is the latest source for the AT&T Research
    regression test harness for the X/Open regex pattern match interface.
    See testregex(1) for option and test input details. The source and
    test data posted here are license free.

#	$NetBSD: README,v 1.1 2011/01/07 15:05:58 pgoyette Exp $
#	@(#)README	8.8 (Berkeley) 7/31/94

Fairly large files (the command files) are built in this directory during
the test runs, and even larger files (the database files) are created in
"/var/tmp".  If the latter directory doesn't exist, set the environmental
variable TMPDIR to a directory where the files can be built.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
The script file consists of lines with an initial character which is
the command for that line, or an initial character indicating a key
or data entry for a previous command.

Legal command characters are as follows:

c: compare a record
	+ must be followed by [kK][dD]; the data value in the database
	  associated with the specified key is compared to the specified
	  data value.
e: echo a string
	+ writes out the rest of the line into the output file; if the
	  last character is not a carriage-return, a newline is appended.
f: set the flags for the next command
	+ no value zero's the flags
g: do a get command
	+ must be followed by [kK]
	+ writes out the retrieved data DBT.
o [r]: dump [reverse]
	+ dump the database out, if 'r' is set, in reverse order.
p: do a put command
	+ must be followed by [kK][dD]
r: do a del command
	+ must be followed by [kK] unless R_CURSOR flag set.
S: sync the database
s: do a seq command
	+ must be followed by [kK] if R_CURSOR flag set.
	+ writes out the retrieved data DBT.

Legal key/data characters are as follows:

D [file]: data file
	+ set the current data value to the contents of the file
d [data]:
	+ set the current key value to the contents of the line.
K [file]: key file
	+ set the current key value to the contents of the file
k [data]:
	+ set the current key value to the contents of the line.

Blank lines, lines with leading white space, and lines with leading
hash marks (#) are ignored.

Options to dbtest are as follows:

	-d: Set the DB_LOCK flag.
	-f: Use the file argument as the database file.
	-i: Use the rest of the argument to set elements in the info
	    structure.  If the type is btree, then "-i cachesize=10240"
	    will set BTREEINFO.cachesize to 10240.
	-o: The rest of the argument is the output file instead of
	    using stdout.
	-s: Don't delete the database file before opening it, i.e.
	    use the database file from a previous run.

Dbtest requires two arguments, the type of access "hash", "recno"
or "btree", and the script name or "-" to indicate stdin.
This test may fail if

 - your /etc/services file is not in sync with what this test expects
 - your /etc/hosts file or DNS have unusual entries for "localhost"
   (a duplicate "localhost 127.0.0.1" line in /etc/hosts for example)

On kernels without IPv6 support some of the tests are skipped.
The tests in this directory where written at the same time tmpfs was
developed.  This is why, if you follow the order of tests in the Atffile,
you will notice that they start checking the most basic things and end
checking the less common ones.  Furthermore, tests try not to use features
tested by further tests in the lists.

However, the above is not the most appropriate testing procedure when you
have a working file system because some separation in test programs does
not make sense afterwards.

Many of the tests here are applicable to any file system.  They should be
refactored to be reusable on any mounted file system, which could also
remove the need to do the mount/unmount steps in each and every test case.

Possibly take a look at the file system tests in FreeBSD.  They seem to be
much more complete, even though they are written in Perl and therefore not
directly usable.
	$NetBSD: README,v 1.1 2010/07/26 15:53:00 pooka Exp $

This directory contains the necessary bits to get an NFS server
running in a rump kernel.  In essence, it's:

  * rpcbind
  * mountd
  * nfsd

Additionally, you need the libc rpc code which is in
tests/fs/common/nfsrpc.

TODO: make the standard nfs userspace services usable (the challenge
comes from rpc being in libc).

questions? ==> pooka@netbsd.org
Tip can be configured in a number of ways:

ACU's:
-----

ACU				Define in makefile
--------------------		---------------
BIZCOMP 1022, 1031		BIZ1022, BIZ1031
DEC DF02-AC, DF03-AC		DF02, DF03
DEC DN-11/Able Quadracall	DN11
Ventel				VENTEL
Vadic 831			V831

New ACU's may be added by editing the ACU description table
in acutab.c and writing a ``driver''.

Variables:
---------

Tip's internal workings revolve around a set of (possibly)
user defined variables.  These are statically initialized
in vars.c, and from the remote file.

Note that adding or deleting variables requires tip to be completedly
recompiled, as indexes into the variable table are used to avoid
expensive lookups.  These defines are set in tip.h.

Commands:
--------

The command dispatch table is defined in cmdtab.c.  Commands
may have attributes such as EXPerimental and PRIVileged (only
root may execute).



--------------------------------------------------------------------------

Recent changes about Jan 82

A new, improved version of tip is now available.  The most important
addition is the capacility to specify a phone number with tip.  The
default baud rate is 1200.  To use it do:

	tip phone-number
or
	tip -300 phone-number

for 300 baud.

A ~^Z command has been added to tip as well.

A new cu program is available that interfaces to the tip program.
It attempts to give the same user interface as cu but it is really
the tip program so you have all the advantages of tip.  This allows
cu (actually tip) to search for a free ACU instead of having the
user specify which one he wants.
#	@(#)README	8.1 (Berkeley) 6/6/93

col - filter out reverse line feeds.

Options are:
	-b	do not print any backspaces (last character written is printed)
	-f	allow half line feeds in output, by default characters between
		lines are pushed to the line below
	-x	do not compress spaces into tabs.
	-l num	keep (at least) num lines in memory, 128 are kept by default

In the 32V source code to col(1) the default behavior was to NOT compress
spaces into tabs.  There was a -h option which caused it to compress spaces
into tabs.  There was no -x flag.

The 32V documentation, however, was consistent with the SVID (actually, V7
at the time) and documented a -x flag (as defined above) while making no
mention of a -h flag.  Just before 4.3BSD went out, CSRG updated the manual
page to reflect the way the code worked.  Suspecting that this was probably
the wrong way to go, this version adopts the SVID defaults, and no longer
documents the -h option.

The S5 -p flag is not supported because it isn't clear what it does (looks
like a kludge introduced for a particular printer).

Known differences between AT&T's col and this one (# is delimiter):
	Input			AT&T col		this col
	#\nabc\E7def\n#		#   def\nabc\r#		#   def\nabc\n#
	#a#			##			#a\n#
		- last line always ends with at least one \n (or \E9)
	#1234567 8\n#		#1234567\t8\n#		#1234567 8\n#
		- single space not expanded to tab
     -f #a\E8b\n#		#ab\n#			# b\E9\ra\n#
		- can back up past first line (as far as you want) so you
		  *can* have a super script on the first line
	#\E9_\ba\E8\nb\n#	#\n_\bb\ba\n#		#\n_\ba\bb\n#
		- always print last character written to a position,
		  AT&T col claims to do this but doesn't.

If a character is to be placed on a line that has been flushed, a warning
is produced (the AT&T col is silent).   The -l flag (not in AT&T col) can
be used to increase the number of lines buffered to avoid the problem.

General algorithm: a limited number of lines are buffered in a linked
list.  When a printable character is read, it is put in the buffer of
the current line along with the column it's supposed to be in.  When
a line is flushed, the characters in the line are sorted according to
column and then printed.

	@(#)README	8.1 (Berkeley) 6/9/93

Compress version 4.0 improvements over 3.0:
	o compress() speedup (10-50%) by changing division hash to xor
	o decompress() speedup (5-10%)
	o Memory requirements reduced (3-30%)
	o Stack requirements reduced to less than 4kb
	o Removed 'Big+Fast' compress code (FBITS) because of compress speedup
    	o Portability mods for Z8000 and PC/XT (but not zeus 3.2)
	o Default to 'quiet' mode
	o Unification of 'force' flags
	o Manual page overhaul
	o Portability enhancement for M_XENIX
	o Removed text on #else and #endif
	o Added "-V" switch to print version and options
	o Added #defines for SIGNED_COMPARE_SLOW
	o Added Makefile and "usermem" program
	o Removed all floating point computations
	o New programs: [deleted]

The "usermem" script attempts to determine the maximum process size.  Some
editing of the script may be necessary (see the comments).  [It should work
fine on 4.3 bsd.] If you can't get it to work at all, just create file
"USERMEM" containing the maximum process size in decimal.

The following preprocessor symbols control the compilation of "compress.c":

	o USERMEM		Maximum process memory on the system
	o SACREDMEM		Amount to reserve for other proceses
	o SIGNED_COMPARE_SLOW	Unsigned compare instructions are faster
	o NO_UCHAR		Don't use "unsigned char" types
	o BITS			Overrules default set by USERMEM-SACREDMEM
	o vax			Generate inline assembler
	o interdata		Defines SIGNED_COMPARE_SLOW
	o M_XENIX		Makes arrays < 65536 bytes each
	o pdp11			BITS=12, NO_UCHAR
	o z8000			BITS=12
	o pcxt			BITS=12
	o BSD4_2		Allow long filenames ( > 14 characters) &
				Call setlinebuf(stderr)

The difference "usermem-sacredmem" determines the maximum BITS that can be
specified with the "-b" flag.

memory: at least		BITS
------  -- -----                ----
     433,484			 16
     229,600			 15
     127,536			 14
      73,464			 13
           0			 12

The default is BITS=16.

The maximum bits can be overrulled by specifying "-DBITS=bits" at
compilation time.

WARNING: files compressed on a large machine with more bits than allowed by 
a version of compress on a smaller machine cannot be decompressed!  Use the
"-b12" flag to generate a file on a large machine that can be uncompressed 
on a 16-bit machine.

The output of compress 4.0 is fully compatible with that of compress 3.0.
In other words, the output of compress 4.0 may be fed into uncompress 3.0 or
the output of compress 3.0 may be fed into uncompress 4.0.

The output of compress 4.0 not compatible with that of
compress 2.0.  However, compress 4.0 still accepts the output of
compress 2.0.  To generate output that is compatible with compress
2.0, use the undocumented "-C" flag.

	-from mod.sources, submitted by vax135!petsd!joe (Joe Orost), 8/1/85
--------------------------------

Enclosed is compress version 3.0 with the following changes:

1.	"Block" compression is performed.  After the BITS run out, the
	compression ratio is checked every so often.  If it is decreasing,
	the table is cleared and a new set of substrings are generated.

	This makes the output of compress 3.0 not compatible with that of
	compress 2.0.  However, compress 3.0 still accepts the output of
	compress 2.0.  To generate output that is compatible with compress
	2.0, use the undocumented "-C" flag.

2.	A quiet "-q" flag has been added for use by the news system.

3.	The character chaining has been deleted and the program now uses
	hashing.  This improves the speed of the program, especially
	during decompression.  Other speed improvements have been made,
	such as using putc() instead of fwrite().

4.	A large table is used on large machines when a relatively small
	number of bits is specified.  This saves much time when compressing
	for a 16-bit machine on a 32-bit virtual machine.  Note that the
	speed improvement only occurs when the input file is > 30000
	characters, and the -b BITS is less than or equal to the cutoff
	described below.

Most of these changes were made by James A. Woods (ames!jaw).  Thank you
James!

To compile compress:

	cc -O -DUSERMEM=usermem -o compress compress.c

Where "usermem" is the amount of physical user memory available (in bytes).  
If any physical memory is to be reserved for other processes, put in 
"-DSACREDMEM sacredmem", where "sacredmem" is the amount to be reserved.

The difference "usermem-sacredmem" determines the maximum BITS that can be
specified, and the cutoff bits where the large+fast table is used.

memory: at least		BITS		cutoff
------  -- -----                ----            ------
   4,718,592 			 16		  13
   2,621,440 			 16		  12
   1,572,864			 16		  11
   1,048,576			 16		  10
     631,808			 16               --
     329,728			 15               --
     178,176			 14		  --
      99,328			 13		  --
           0			 12		  --

The default memory size is 750,000 which gives a maximum BITS=16 and no
large+fast table.

The maximum bits can be overruled by specifying "-DBITS=bits" at
compilation time.

If your machine doesn't support unsigned characters, define "NO_UCHAR" 
when compiling.

If your machine has "int" as 16-bits, define "SHORT_INT" when compiling.

After compilation, move "compress" to a standard executable location, such 
as /usr/local.  Then:
	cd /usr/local
	ln compress uncompress
	ln compress zcat

On machines that have a fixed stack size (such as Perkin-Elmer), set the
stack to at least 12kb.  ("setstack compress 12" on Perkin-Elmer).

Next, install the manual (compress.l).
	cp compress.l /usr/man/manl
	cd /usr/man/manl
	ln compress.l uncompress.l
	ln compress.l zcat.l

		- or -

	cp compress.l /usr/man/man1/compress.1
	cd /usr/man/man1
	ln compress.1 uncompress.1
	ln compress.1 zcat.1

					regards,
					petsd!joe

Here is a note from the net:

>From hplabs!pesnta!amd!turtlevax!ken Sat Jan  5 03:35:20 1985
Path: ames!hplabs!pesnta!amd!turtlevax!ken
From: ken@turtlevax.UUCP (Ken Turkowski)
Newsgroups: net.sources
Subject: Re: Compress release 3.0 : sample Makefile
Organization: CADLINC, Inc. @ Menlo Park, CA

In the compress 3.0 source recently posted to mod.sources, there is a
#define variable which can be set for optimum performance on a machine
with a large amount of memory.  A program (usermem) to calculate the
useable amount of physical user memory is enclosed, as well as a sample
4.2bsd Vax Makefile for compress.

Here is the README file from the previous version of compress (2.0):

>Enclosed is compress.c version 2.0 with the following bugs fixed:
>
>1.	The packed files produced by compress are different on different
>	machines and dependent on the vax sysgen option.
>		The bug was in the different byte/bit ordering on the
>		various machines.  This has been fixed.
>
>		This version is NOT compatible with the original vax posting
>		unless the '-DCOMPATIBLE' option is specified to the C
>		compiler.  The original posting has a bug which I fixed, 
>		causing incompatible files.  I recommend you NOT to use this
>		option unless you already have a lot of packed files from
>		the original posting by thomas.
>2.	The exit status is not well defined (on some machines) causing the
>	scripts to fail.
>		The exit status is now 0,1 or 2 and is documented in
>		compress.l.
>3.	The function getopt() is not available in all C libraries.
>		The function getopt() is no longer referenced by the
>		program.
>4.	Error status is not being checked on the fwrite() and fflush() calls.
>		Fixed.
>
>The following enhancements have been made:
>
>1.	Added facilities of "compact" into the compress program.  "Pack",
>	"Unpack", and "Pcat" are no longer required (no longer supplied).
>2.	Installed work around for C compiler bug with "-O".
>3.	Added a magic number header (\037\235).  Put the bits specified
>	in the file.
>4.	Added "-f" flag to force overwrite of output file.
>5.	Added "-c" flag and "zcat" program.  'ln compress zcat' after you
>	compile.
>6.	The 'uncompress' script has been deleted; simply 
>	'ln compress uncompress' after you compile and it will work.
>7.	Removed extra bit masking for machines that support unsigned
>	characters.  If your machine doesn't support unsigned characters,
>	define "NO_UCHAR" when compiling.
>
>Compile "compress.c" with "-O -o compress" flags.  Move "compress" to a
>standard executable location, such as /usr/local.  Then:
>	cd /usr/local
>	ln compress uncompress
>	ln compress zcat
>
>On machines that have a fixed stack size (such as Perkin-Elmer), set the
>stack to at least 12kb.  ("setstack compress 12" on Perkin-Elmer).
>
>Next, install the manual (compress.l).
>	cp compress.l /usr/man/manl		- or -
>	cp compress.l /usr/man/man1/compress.1
>
>Here is the README that I sent with my first posting:
>
>>Enclosed is a modified version of compress.c, along with scripts to make it
>>run identically to pack(1), unpack(1), an pcat(1).  Here is what I
>>(petsd!joe) and a colleague (petsd!peora!srd) did:
>>
>>1. Removed VAX dependencies.
>>2. Changed the struct to separate arrays; saves mucho memory.
>>3. Did comparisons in unsigned, where possible.  (Faster on Perkin-Elmer.)
>>4. Sorted the character next chain and changed the search to stop
>>prematurely.  This saves a lot on the execution time when compressing.
>>
>>This version is totally compatible with the original version.  Even though
>>lint(1) -p has no complaints about compress.c, it won't run on a 16-bit
>>machine, due to the size of the arrays.
>>
>>Here is the README file from the original author:
>> 
>>>Well, with all this discussion about file compression (for news batching
>>>in particular) going around, I decided to implement the text compression
>>>algorithm described in the June Computer magazine.  The author claimed
>>>blinding speed and good compression ratios.  It's certainly faster than
>>>compact (but, then, what wouldn't be), but it's also the same speed as
>>>pack, and gets better compression than both of them.  On 350K bytes of
>>>unix-wizards, compact took about 8 minutes of CPU, pack took about 80
>>>seconds, and compress (herein) also took 80 seconds.  But, compact and
>>>pack got about 30% compression, whereas compress got over 50%.  So, I
>>>decided I had something, and that others might be interested, too.
>>>
>>>As is probably true of compact and pack (although I haven't checked),
>>>the byte order within a word is probably relevant here, but as long as
>>>you stay on a single machine type, you should be ok.  (Can anybody
>>>elucidate on this?)  There are a couple of asm's in the code (extv and
>>>insv instructions), so anyone porting it to another machine will have to
>>>deal with this anyway (and could probably make it compatible with Vax
>>>byte order at the same time).  Anyway, I've linted the code (both with
>>>and without -p), so it should run elsewhere.  Note the longs in the
>>>code, you can take these out if you reduce BITS to <= 15.
>>>
>>>Have fun, and as always, if you make good enhancements, or bug fixes,
>>>I'd like to see them.
>>>
>>>=Spencer (thomas@utah-20, {harpo,hplabs,arizona}!utah-cs!thomas)
>>
>>					regards,
>>					joe
>>
>>--
>>Full-Name:  Joseph M. Orost
>>UUCP:       ..!{decvax,ucbvax,ihnp4}!vax135!petsd!joe
>>US Mail:    MS 313; Perkin-Elmer; 106 Apple St; Tinton Falls, NJ 07724
>>Phone:      (201) 870-5844
This is the C indenter, it originally came from the University of Illinois
via some distribution tape for PDP-11 Unix.  It has subsequently been
hacked upon by James Gosling @ CMU.  It isn't very pretty, and really needs
to be completely redone, but it is probably the nicest C pretty printer
around.

Further additions to provide "Kernel Normal Form" were contributed
by the folks at Sun Microsystems.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> From mnetor!yunexus!oz@uunet.UU.NET Wed Mar  9 15:30:55 1988
> Date: Tue, 8 Mar 88 18:36:25 EST
> From: yunexus!oz@uunet.UU.NET (Ozan Yigit)
> To: bostic@okeeffe.berkeley.edu
> Cc: ccvaxa!willcox@uunet.UU.NET, jag@sun.com, rsalz@uunet.UU.NET
> In-Reply-To: Keith Bostic's message of Tue, 16 Feb 88 16:09:06 PST 
> Subject: Re: Indent...

Thank you for your response about indent. I was wrong in my original
observation (or mis-observation :-). UCB did keep the Illinois
copyright intact.

The issue still is whether we can distribute indent, and if we can, which
version. David Willcox (the author) states that:

| Several people have asked me on what basis I claim that indent is in
| the public domain.  I knew I would be sorry I made that posting.
| 
| Some history.  Way back in 1976, the project I worked on at the
| University of Illinois Center for Advanced Computation had a huge
| battle about how to format C code.  After about a week of fighting, I
| got disgusted and wrote a program, which I called indent, to reformat C
| code.  It had a bunch of different options that would let you format
| the output the way you liked.  In particular, all of the different
| formats being championed were supported.
| 
| It was my first big C program.  It was ugly.  It wasn't designed, it
| just sort of grew.  But it pretty much worked, and it stopped most of
| the fighting.
| 
| As a matter of form, I included a University of Illinois Copyright
| notice.  However, my understanding was that, since the work was done
| on an ARPA contract, it was in the public domain.
| 
| Time passed.  Some years later, indent showed up on one of the early
| emacs distributions.
| 
| Later still, someone from UC Berlekey called the UofI and asked if
| indent was in the public domain.  They wanted to include it in their
| UNIX distributions, along with the emacs stuff.  I was no longer at the
| UofI, but Rob Kolstad, who was, asked me about it.  I told him I didn't
| care if they used it, and since then it has been on the BSD distributions.
| 
| Somewhere along the way, several other unnamed people have had their
| hands in it.  It was converted to understand version 7 C.  (The
| original was version 6.)  It was converted from its original filter
| interface to its current "blow away the user's file" interface.
| The $HOME/.indent.pro file parsing was added.  Some more formatting
| options were added.
| 
| The source I have right now has two copyright notices.  One is the
| original from the UofI.  One is from Berkeley.
| 
| I am not a lawyer, and I certainly do not understand copyright law.  As
| far as I am concerned, the bulk of this program, everything covered by
| the UofI copyright, is in the public domain, and worth every penny.
| Berkeley's copyright probably should only cover their changes, and I
| don't know their feelings about sending it out.  

In any case, there appears to be noone at UofI to clarify/and change
that copyright, but I am confident (based on the statements of its
author) that the code, as it stands with its copyright, is
distributable, and will not cause any legal problems.

Hence, the issue reduces to *which* one to distribute through
comp.sources.unix. I would suggest that with the permission of you
folks (given that you have parts copyrighted), we distribute the 4.3
version of indent, which appears to be the most up-to-date version. I
happen to have just about every known version of indent, including the
very original submission from the author to a unix tape, later the
G-Emacs version, any 4.n version, sun version and the Unipress
version.  I still think we should not have to "go-back-in-time" and
re-do all the work you people have done.

I hope to hear from you as to what you think about this. You may of
course send 4.3 version to the moderator directly, or you can let me
know of your permission, and I will send the sources, or you can let
me know that 4.3 version is off-limits, in which case we would probably
have to revert to an older version. One way or another, I hope to get
a version of indent to comp.sources.unix.

regards..	oz

cc: ccvaxa!willcox
    sun.com!jar
    uunet!rsalz

$NetBSD: README,v 1.2 1997/08/02 21:30:08 perry Exp $

CRUNCH 0.3 README				7/23/94

Crunch is available via anonymous ftp to ftp.cs.umd.edu in
		pub/bsd/crunch-0.3.tar.gz

WHAT'S NEW IN 0.3

* The prototype awk script has been replaced by a more capable and
  hopefully more robust C program.
* No fragile template makefiles or dependencies on the details of the
  bsd build environment.
* You can build crunched binaries even with no sources on-line, you
  just need the .o files.  Crunchgen still will try to figure out as
  much as possible on its own, but you can override its guessing by
  specifying the list of .o files explicitly.
* Crunch itself has been bmake'd and some man pages written, so it
  should be ready to install.
* Added patch for FreeBSD from Jordan Hubbard, plus the .conf files used
  for the FreeBSD install floppies as examples.


INTRODUCTION

Crunch is a little package that helps create "crunched" binaries for use
on boot, install, and fixit floppies.  A crunched binary in this case is
one where many programs have been linked together into one a.out file.
The different programs are run depending on the value of argv[0], so
hard links to the crunched binary suffice to simulate a perfectly normal
system.

As an example, I have created an 980K crunched "fixit" binary containing
the following programs in their entirety:

	cat chmod cp date dd df echo ed expr hostname kill ln ls mkdir
	mt mv pwd rcp rm rmdir sh sleep stty sync test [ badsect chown
	clri disklabel dump rdump dmesg fdisk fsck halt ifconfig init
	mknod mount newfs ping reboot restore rrestore swapon umount
	ftp rsh sed telnet rlogin vi cpio gzip gunzip gzcat

Note carefully: vi, cpio, gzip, ed, sed, dump/restore, some networking
utilities, and the disk management utilities, all in a binary small
enough to fit on a 1.2 MB root filesystem floppy (albeit with the kernel
on its own boot floppy).  A more reasonable subset can be made to fit
easily with a kernel for a decent one-disk fixit filesystem.

The linking together of different programs by hand is an old
space-saving technique.  Crunch automates the process by building the
necessary stub files and makefile for you (via the crunchgen program),
and by doctoring the symbol tables of the component .o files to allow
them to link without "symbol multiply defined" conflicts (via the
crunchide program).


BUILDING CRUNCH

Just type make, then make install.

Crunch was written and tested under NetBSD/i386, but should work under
other PC BSD systems that use GNU ld.

The crunchgen(1) and crunchide(1) man pages have more details on using
crunch, and the examples subdirectory contains some working .conf files
and a sample Makefile.

CREDITS

Thanks to the NetBSD team for a consistently high quality effort in
bringing together a solid, state of the art development environment.

Thanks to the FreeBSD guys; Rod Grimes, Nate Williams and Jordan
Hubbard; and to Bruce Evans, for immediate and detailed feedback on
crunch 0.1, and for pressing me to make the prototype more useable.

Crunch was written for the Maruti Hard Real-Time Operating System
project at the University of Maryland, to help make for better install
and recovery procedures for our NetBSD-based development environment. It
is copyright (c) 1994 by the University of Maryland under a UCB-style
freely- redistributable notice.  See the file COPYRIGHT for details.

Please let me know of any problems or of enhancements you make to this
package.  I'm particularly interested in the details of what you found
was good to put on your fixit or install disks.  Thanks!

Share and Enjoy,
Jaime
............................................................................
: Stand on my shoulders, : jds@cs.umd.edu  :                  James da Silva
: not on my toes.        : uunet!mimsy!jds : http://www.cs.umd.edu/users/jds

#	$NetBSD: README,v 1.2 1996/04/06 06:00:59 thorpej Exp $

This is a program which I wrote as a clone of the UNIX 'units'
command.  I threw it together in a couple days, but it seems to work,
with some restrictions.  I have tested it under DOS with Borland C and
Ultrix 4.2, and SunOS 4.1.  

This program differs from the unix units program in the following
ways:
   it can gracefully handle exponents larger than 9 in output
   it uses 'e' to denote exponentiation in numbers
   prefixes are listed in the units file
   it tries both -s and -es plurals
   it allows use of * for multiply and ^ for exponentiation in the input
   the output format is somewhat different

Adrian Mariano (adrian@cam.cornell.edu or mariano@geom.umn.edu)


This is a distribution of both client and server telnet.  These programs
have been compiled on:
			telnet	telnetd
	4.4 BSD-Lite	  x	  x
	4.3 BSD Reno	  X	  X
	UNICOS 9.1	  X	  X
	UNICOS 9.0	  X	  X
	UNICOS 8.0	  X	  X
	BSDI 2.0	  X	  X
	Solaris 2.4       x       x (no linemode in server)
	SunOs 4.1.4	  X	  X (no linemode in server)
	Ultrix 4.3	  X	  X (no linemode in server)
	Ultrix 4.1	  X	  X (no linemode in server)

In addition, previous versions have been compiled on the following
machines, but were not available for testing this version.
			telnet	telnetd
	Next1.0		  X	  X
	UNICOS 8.3	  X	  X
	UNICOS 7.C	  X	  X
	UNICOS 7.0	  X	  X
	SunOs 4.0.3c	  X	  X (no linemode in server)
	4.3 BSD		  X  	  X (no linemode in server)
	DYNIX V3.0.12	  X	  X (no linemode in server)
	Ultrix 3.1	  X	  X (no linemode in server)
	Ultrix 4.0	  X	  X (no linemode in server)
	SunOs 3.5	  X	  X (no linemode in server)
	SunOs 4.1.3	  X	  X (no linemode in server)
	Solaris 2.2       x       x (no linemode in server)
	Solaris 2.3       x       x (no linemode in server)
	BSDI 1.0	  X	  X
	BSDI 1.1	  X	  X
	DYNIX V3.0.17.9	  X	  X (no linemode in server)
	HP-UX 8.0	  x       x (no linemode in server)

This code should work, but there are no guarantees.

Oct 23, 1995

This is a bugfix release.

	The change in the previous release from using makeutx() to
	pututxline() caused problems on SunOS/Solaris.  It has been
	changed back to using makeutx().  Symptoms include users
	getting error messages when logging in about not being able
	to open the tty.

	Using memmove() instead of memcpy() caused problems under
	SunOS 4.x, since it doesn't have memmove().  Config.generic
	has been modified to include mem.o for SunOS 4.x.

	Some new code was added to telnetd to do some environment
	variable cleanup before execing login.  Thanks to Sam Hartman
	at MIT for pointing this out.

	A couple of other minor bugfixes.

May 30, 1995

This release represents what is on the 4.4BSD-Lite2 release, which
should be the final BSD release.  I will continue to support of
telnet, The code (without encryption) is available via anonymous ftp
from ftp.cray.com, in src/telnet/telnet.YY.MM.DD.NE.tar.Z, where
YY.MM.DD is replaced with the year, month and day of the release.
If you can't find it at one of these places, at some point in the
near future information about the latest releases should be available
from ftp.borman.com.

In addition, the version with the encryption code is available via
ftp from net-dist.mit.edu, in the directory /pub/telnet.  There
is a README file there that gives further information on how
to get the distribution.

Questions, comments, bug reports and bug fixes can be sent to
one of these addresses:
		dab@borman.com
		dab@cray.com
		dab@bsdi.com

This release is mainly bug fixes and code cleanup.

	Replace all calls to bcopy()/bzero() with calls to
	memmove()/memset() and all calls to index()/rindex()
	with calls to strchr()/strrchr().

	Add some missing diagnostics for option tracing
	to telnetd.

	Add support for BSDI 2.0 and Solaris 2.4.

	Add support for UNICOS 8.0

	Get rid of expanded tabs and trailing white spaces.

	From Paul Vixie:
		Fix for telnet going into an endless spin
		when the session dies abnormally.

	From Jef Poskanzer:
		Changes to allow telnet to compile
		under SunOS 3.5.

	From Philip Guenther:
		makeutx() doesn't expand utmpx,
		use pututxline() instead.

	From Chris Torek:
		Add a sleep(1) before execing login
		to avoid race condition that can eat
		up the login prompt.
		Use terminal speed directly if it is
		not an encoded value.

	From Steve Parker:
		Fix to realloc() call.  Fix for execing
		login on solaris with no user name.

January 19, 1994

This is a list of some of the changes since the last tar release
of telnet/telnetd.  There are probably other changes that aren't
listed here, but this should hit a lot of the main ones.

   General:
	Changed #define for AUTHENTICATE to AUTHENTICATION
	Changed #define for ENCRYPT to ENCRYPTION
	Changed #define for DES_ENCRYPT to DES_ENCRYPTION

	Added support for SPX authentication: -DSPX

	Added support for Kerberos Version 5 authentication: -DKRB5

	Added support for ANSI C function prototypes

	Added support for the NEW-ENVIRON option (RFC-1572)
	including support for USERVAR.

	Made support for the old Environment Option (RFC-1408)
	conditional on -DOLD_ENVIRON

	Added #define ENV_HACK - support for RFC 1571

	The encryption code is removed from the public distributions.
	Domestic 4.4 BSD distributions contain the encryption code.

	ENV_HACK: Code to deal with systems that only implement
		the old ENVIRON option, and have reversed definitions
		of ENV_VAR and ENV_VAL.  Also fixes ENV processing in
		client to handle things besides just the default set...

	NO_BSD_SETJMP: UNICOS configuration for
		UNICOS 6.1/6.0/5.1/5.0 systems.

	STREAMSPTY: Use /dev/ptmx to get a clean pty.  This
		is for SVr4 derivatives (Like Solaris)

	UTMPX: For systems that have /etc/utmpx. This is for
		SVr4 derivatives (Like Solaris)

	Definitions for BSDI 1.0

	Definitions for 4.3 Reno and 4.4 BSD.

	Definitions for UNICOS 8.0 and UNICOS 7.C

	Definitions for Solaris 2.0

	Definitions for HP-UX 8.0

	Latest Copyright notices from Berkeley.

	FLOW-CONTROL: support for RFC-XXXx


   Client Specific:

	Fix the "send" command to not send garbage...

	Fix status message for "skiprc"

	Make sure to send NAWS after telnet has been suspended
	or an external command has been run, if the window size
	has changed.

	sysV88 support.

   Server Specific:

	Support flowcontrol option in non-linemode servers.

	-k Server supports Kludge Linemode, but will default to
	   either single character mode or real Linemode support.
	   The user will have to explicitly ask to switch into
	   kludge linemode. ("stty extproc", or escape back to
	   to telnet and say "mode line".)

	-u Specify the length of the hostname field in the utmp
	   file.  Hostname longer than this length will be put
	   into the utmp file in dotted decimal notation, rather
	   than putting in a truncated hostname.
	
	-U Registered hosts only.  If a reverse hostname lookup
	   fails, the connection will be refused.

	-f/-F
	   Allows forwarding of credentials for KRB5.

February 22, 1991:

    Features:

	This version of telnet/telnetd has support for both
	the AUTHENTICATION and ENCRYPTION options.  The
	AUTHENTICATION option is fairly well defined, and
	an option number has been assigned to it.  The
	ENCRYPTION option is still in a state of flux; an
	option number has been assigned to, but it is still
	subject to change.  The code is provided in this release
	for experimental and testing purposes.

	The telnet "send" command can now be used to send
	do/dont/will/wont commands, with any telnet option
	name.  The rules for when do/dont/will/wont are sent
	are still followed, so just because the user requests
	that one of these be sent doesn't mean that it will
	be sent...

	The telnet "getstatus" command no longer requires
	that option printing be enabled to see the response
	to the "DO STATUS" command.

	A -n flag has been added to telnetd to disable
	keepalives.

	A new telnet command, "auth" has been added (if
	AUTHENTICATE is defined).  It has four sub-commands,
	"status", "disable", "enable" and "help".

	A new telnet command, "encrypt" has been added (if
	ENCRYPT is defined).  It has many sub-commands:
	"enable", "type", "start", "stop", "input",
	"-input", "output", "-output", "status", and "help".

	The LOGOUT option is now supported by both telnet
	and telnetd, a new command, "logout", was added
	to support this.

	Several new toggle options were added:
	    "autoencrypt", "autodecrypt", "autologin", "authdebug",
	    "encdebug", "skiprc", "verbose_encrypt"

	An "rlogin" interface has been added.  If the program
	is named "rlogin", or the "-r" flag is given, then
	an rlogin type of interface will be used.
		~.	Terminates the session
		~<susp> Suspend the session
		~^]	Escape to telnet command mode
		~~	Pass through the ~.
	    BUG: If you type the rlogin escape character
		 in the middle of a line while in rlogin
		 mode, you cannot erase it or any characters
		 before it.  Hopefully this can be fixed
		 in a future release...

    General changes:

	A "libtelnet.a" has now been created.  This library
	contains code that is common to both telnet and
	telnetd.  This is also where library routines that
	are needed, but are not in the standard C library,
	are placed.

	The makefiles have been re-done.  All of the site
	specific configuration information has now been put
	into a single "Config.generic" file, in the top level
	directory.  Changing this one file will take care of
	all three subdirectories.  Also, to add a new/local
	definition, a "Config.local" file may be created
	at the top level; if that file exists, the subdirectories
	will use that file instead of "Config.generic".

	Many 1-2 line functions in commands.c have been
	removed, and just inserted in-line, or replaced
	with a macro.

    Bug Fixes:

	The non-termio code in both telnet and telnetd was
	setting/clearing CTLECH in the sg_flags word.  This
	was incorrect, and has been changed to set/clear the
	LCTLECH bit in the local mode word.

	The SRCRT #define has been removed.  If IP_OPTIONS
	and IPPROTO_IP are defined on the system, then the
	source route code is automatically enabled.

	The NO_GETTYTAB #define has been removed; there
	is a compatibility routine that can be built into
	libtelnet to achieve the same results.

	The server, telnetd, has been switched to use getopt()
	for parsing the argument list.

	The code for getting the input/output speeds via
	cfgetispeed()/cfgetospeed() was still not quite
	right in telnet.  Posix says if the ispeed is 0,
	then it is really equal to the ospeed.

	The suboption processing code in telnet now has
	explicit checks to make sure that we received
	the entire suboption (telnetd was already doing this).

	The telnet code for processing the terminal type
	could cause a core dump if an existing connection
	was closed, and a new connection opened without
	exiting telnet.

	Telnetd was doing a TCSADRAIN when setting the new
	terminal settings;  This is not good, because it means
	that the tcsetattr() will hang waiting for output to
	drain, and telnetd is the only one that will drain
	the output...  The fix is to use TCSANOW which does
	not wait.

	Telnetd was improperly setting/clearing the ISTRIP
	flag in the c_lflag field, it should be using the
	c_iflag field. 

	When the child process of telnetd was opening the
	slave side of the pty, it was re-setting the EXTPROC
	bit too early, and some of the other initialization
	code was wiping it out.  This would cause telnetd
	to go out of linemode and into single character mode.

	One instance of leaving linemode in telnetd forgot
	to send a WILL ECHO to the client, the net result
	would be that the user would see double character
	echo.

	If the MODE was being changed several times very
	quickly, telnetd could get out of sync with the
	state changes and the returning acks; and wind up
	being left in the wrong state.

September 14, 1990:

	Switch the client to use getopt() for parsing the
	argument list.  The 4.3Reno getopt.c is included for
	systems that don't have getopt().

	Use the posix _POSIX_VDISABLE value for what value
	to use when disabling special characters.  If this
	is undefined, it defaults to 0x3ff.

	For non-termio systems, TIOCSETP was being used to
	change the state of the terminal.  This causes the
	input queue to be flushed, which we don't want.  This
	is now changed to TIOCSETN.

	Take out the "#ifdef notdef" around the code in the
	server that generates a "sync" when the pty output
	is flushed.  The potential problem is that some older
	telnet clients may go into an infinite loop when they
	receive a "sync", if so, the server can be compiled
	with "NO_URGENT" defined.

	Fix the client where it was setting/clearing the OPOST
	bit in the c_lflag field, not the c_oflag field.

	Fix the client where it was setting/clearing the ISTRIP
	bit in the c_lflag field, not the c_iflag field.  (On
	4.3Reno, this is the ECHOPRT bit in the c_lflag field.)
	The client also had its interpretation of WILL BINARY
	and DO BINARY reversed.

	Fix a bug in client that would cause a core dump when
	attempting to remove the last environment variable.

	In the client, there were a few places were switch()
	was being passed a character, and if it was a negative
	value, it could get sign extended, and not match
	the 8 bit case statements.  The fix is to and the
	switch value with 0xff.

	Add a couple more printoption() calls in the client, I
	don't think there are any more places were a telnet
	command can be received and not printed out when
	"options" is on.

	A new flag has been added to the client, "-a".  Currently,
	this just causes the USER name to be sent across, in
	the future this may be used to signify that automatic
	authentication is requested.

	The USER variable is now only sent by the client if
	the "-a" or "-l user" options are explicitly used, or
	if the user explicitly asks for the "USER" environment
	variable to be exported.  In the server, if it receives
	the "USER" environment variable, it won't print out the
	banner message, so that only "Password:" will be printed.
	This makes the semantics more like rlogin, and should be
	more familiar to the user.  (People are not used to
	getting a banner message, and then getting just a
	"Password:" prompt.)

	Re-vamp the code for starting up the child login
	process.  The code was getting ugly, and it was
	hard to tell what was really going on.  What we
	do now is after the fork(), in the child:
		1) make sure we have no controlling tty
		2) open and initialize the tty
		3) do a setsid()/setpgrp()
		4) makes the tty our controlling tty.
	On some systems, #2 makes the tty our controlling
	tty, and #4 is a no-op.  The parent process does
	a gets rid of any controlling tty after the child
	is fork()ed.

	Use the strdup() library routine in telnet, instead
	of the local savestr() routine.  If you don't have
	strdup(), you need to define NO_STRDUP.

	Add support for ^T (SIGINFO/VSTATUS), found in the
	4.3Reno distribution.  This maps to the AYT character.
	You need a 4-line bugfix in the kernel to get this
	to work properly:

	> *** tty_pty.c.ORG	Tue Sep 11 09:41:53 1990
	> --- tty_pty.c	Tue Sep 11 17:48:03 1990
	> ***************
	> *** 609,613 ****
	> 			if ((tp->t_lflag&NOFLSH) == 0)
	> 				ttyflush(tp, FREAD|FWRITE);
	> ! 			pgsignal(tp->t_pgrp, *(unsigned int *)data);
	> 			return(0);
	> 		}
	> --- 609,616 ----
	> 			if ((tp->t_lflag&NOFLSH) == 0)
	> 				ttyflush(tp, FREAD|FWRITE);
	> ! 			pgsignal(tp->t_pgrp, *(unsigned int *)data, 1);
	> ! 			if ((*(unsigned int *)data == SIGINFO) &&
	> ! 			    ((tp->t_lflag&NOKERNINFO) == 0))
	> ! 				ttyinfo(tp);
	> 			return(0);
	> 		}

	The client is now smarter when setting the telnet escape
	character; it only sets it to one of VEOL and VEOL2 if
	one of them is undefined, and the other one is not already
	defined to the telnet escape character.

	Handle TERMIOS systems that have separate input and output
	line speed settings embedded in the flags.

	Many other minor bug fixes.

June 20, 1990:
	Re-organize makefiles and source tree.  The telnet/Source
	directory is now gone, and all the source that was in
	telnet/Source is now just in the telnet directory.

	Separate makefile for each system are now gone.  There
	are two makefiles, Makefile and Makefile.generic.
	The "Makefile" has the definitions for the various
	system, and "Makefile.generic" does all the work.
	There is a variable called "WHAT" that is used to
	specify what to make.  For example, in the telnet
	directory, you might say:
		make 4.4bsd WHAT=clean
	to clean out the directory.

	Add support for the ENVIRON and XDISPLOC options.
	In order for the server to work, login has to have
	the "-p" option to preserve environment variables.

	Add the SOFT_TAB and LIT_ECHO modes in the LINEMODE support.

	Add the "-l user" option to command line and open command
	(This is passed through the ENVIRON option).

	Add the "-e" command line option, for setting the escape
	character.

	Add the "-D", diagnostic, option to the server.  This allows
	the server to print out debug information, which is very
	useful when trying to debug a telnet that doesn't have any
	debugging ability.

	Turn off the literal next character when not in LINEMODE.

	Don't recognize ^Y locally, just pass it through.

	Make minor modifications for Sun4.0 and Sun4.1

	Add support for both FORW1 and FORW2 characters.  The
	telnet escape character is set to whichever of the
	two is not being used.  If both are in use, the escape
	character is not set, so when in linemode the user will
	have to follow the escape character with a <CR> or <EOF)
	to get it passed through.

	Commands can now be put in single and double quotes, and
	a backslash is now an escape character.  This is needed
	for allowing arbitrary strings to be assigned to environment
	variables.

	Switch telnetd to use macros like telnet for keeping
	track of the state of all the options.

	Fix telnetd's processing of options so that we always do
	the right processing of the LINEMODE option, regardless
	of who initiates the request to turn it on.  Also, make
	sure that if the other side went "WILL ECHO" in response
	to our "DO ECHO", that we send a "DONT ECHO" to get the
	option turned back off!

	Fix the TERMIOS setting of the terminal speed to handle both
	BSD's separate fields, and the SYSV method of CBAUD bits.

	Change how we deal with the other side refusing to enable
	an option.  The sequence used to be: send DO option; receive
	WONT option; send DONT option.  Now, the sequence is: send
	DO option; receive WONT option.  Both should be valid
	according to the spec, but there has been at least one
	client implementation of telnet identified that can get
	really confused by this.  (The exact sequence, from a trace
	on the server side, is (numbers are number of responses that
	we expect to get after that line...):

		send WILL ECHO	1 (initial request)
		send WONT ECHO	2 (server is changing state)
		recv DO ECHO	1 (first reply, ok.  expect DONT ECHO next)
		send WILL ECHO	2 (server changes state again)
		recv DONT ECHO	1 (second reply, ok.  expect DO ECHO next)
		recv DONT ECHO	0 (third reply, wrong answer. got DONT!!!)
	***	send WONT ECHO	  (send WONT to acknowledge the DONT)
		send WILL ECHO	1 (ask again to enable option)
		recv DO ECHO	0

		recv DONT ECHO	0
		send WONT ECHO	1
		recv DONT ECHO	0
		recv DO ECHO	1
		send WILL ECHO	0
		(and the last 5 lines loop forever)

	The line with the "***" is last of the WILL/DONT/WONT sequence.
	The change to the server to not generate that makes this same
	example become:

		send will ECHO	1
		send wont ECHO	2
		recv do ECHO	1
		send will ECHO	2
		recv dont ECHO	1
		recv dont ECHO	0
		recv do ECHO	1
		send will ECHO	0

	There is other option negotiation going on, and not sending
	the third part changes some of the timings, but this specific
	example no longer gets stuck in a loop.  The "telnet.state"
	file has been modified to reflect this change to the algorithm.

	A bunch of miscellaneous bug fixes and changes to make
	lint happier.

	This version of telnet also has some KERBEROS stuff in
	it. This has not been tested, it uses an un-authorized
	telnet option number, and uses an out-of-date version
	of the (still being defined) AUTHENTICATION option.
	There is no support for this code, do not enable it.


March 1, 1990:
CHANGES/BUGFIXES SINCE LAST RELEASE:
	Some support for IP TOS has been added.  Requires that the
	kernel support the IP_TOS socket option (currently this
	is only in UNICOS 6.0).

	Both telnet and telnetd now use the cc_t typedef.  typedefs are
	included for systems that don't have it (in termios.h).

	SLC_SUSP was not supported properly before.  It is now.

	IAC EOF was not translated  properly in telnetd for SYSV_TERMIO
	when not in linemode.  It now saves a copy of the VEOF character,
	so that when ICANON is turned off and we can't trust it anymore
	(because it is now the VMIN character) we use the saved value.

	There were two missing "break" commands in the linemode
	processing code in telnetd.

	Telnetd wasn't setting the kernel window size information
	properly.  It was using the rows for both rows and columns...

Questions/comments go to
		David Borman
		Cray Research, Inc.
		655F Lone Oak Drive
		Eagan, MN 55123
		dab@cray.com.

README:	You are reading it.

Config.generic:
	This file contains all the OS specific definitions.  It
	has pre-definitions for many common system types, and is
	in standard makefile format.  See the comments at the top
	of the file for more information.

Config.local:
	This is not part of the distribution, but if this file exists,
	it is used instead of "Config.generic".  This allows site
	specific configuration without having to modify the distributed
	"Config.generic" file.

kern.diff:
	This file contains the diffs for the changes needed for the
	kernel to support LINEMODE is the server.  These changes are
	for a 4.3BSD system.  You may need to make some changes for
	your particular system.

	There is a new bit in the terminal state word, TS_EXTPROC.
	When this bit is set, several aspects of the terminal driver
	are disabled.  Input line editing, character echo, and
	mapping of signals are all disabled.  This allows the telnetd
	to turn of these functions when in linemode, but still keep
	track of what state the user wants the terminal to be in.

	New ioctl()s:

		TIOCEXT		Turn on/off the TS_EXTPROC bit
		TIOCGSTATE	Get t_state of tty to look at TS_EXTPROC bit
		TIOCSIG		Generate a signal to processes in the
				current process group of the pty.

	There is a new mode for packet driver, the TIOCPKT_IOCTL bit.
	When packet mode is turned on in the pty, and the TS_EXTPROC
	bit is set, then whenever the state of the pty is changed, the
	next read on the master side of the pty will have the TIOCPKT_IOCTL
	bit set, and the data will contain the following:
		struct xx {
			struct sgttyb a;
			struct tchars b;
			struct ltchars c;
			int t_state;
			int t_flags;
		}
	This allows the process on the server side of the pty to know
	when the state of the terminal has changed, and what the new
	state is.

	However, if you define USE_TERMIO or SYSV_TERMIO, the code will
	expect that the structure returned in the TIOCPKT_IOCTL is
	the termio/termios structure.

stty.diff:
	This file contains the changes needed for the stty(1) program
	to report on the current status of the TS_EXTPROC bit.  It also
	allows the user to turn on/off the TS_EXTPROC bit.  This is useful
	because it allows the user to say "stty -extproc", and the
	LINEMODE option will be automatically disabled, and saying "stty
	extproc" will re-enable the LINEMODE option.

telnet.state:
	Both the client and server have code in them to deal
	with option negotiation loops.  The algorithm that is
	used is described in this file.

telnet:
	This directory contains the client code.  No kernel changes are
	needed to use this code.

telnetd:
	This directory contains the server code.  If LINEMODE or KLUDGELINEMODE
	are defined, then the kernel modifications listed above are needed.

libtelnet:
	This directory contains code that is common to both the client
	and the server.

arpa:
	This directory has a new <arpa/telnet.h>

libtelnet/Makefile.4.4:
telnet/Makefile.4.4:
telnetd/Makefile.4.4:
	These are the makefiles that can be used on a 4.3Reno
	system when this software is installed in /usr/src/lib/libtelnet,
	/usr/src/libexec/telnetd, and /usr/src/usr.bin/telnet.


The following TELNET options are supported:
	
	LINEMODE:
		The LINEMODE option is supported as per RFC1116.  The
		FORWARDMASK option is not currently supported.

	BINARY: The client has the ability to turn on/off the BINARY
		option in each direction.  Turning on BINARY from
		server to client causes the LITOUT bit to get set in
		the terminal driver on both ends,  turning on BINARY
		from the client to the server causes the PASS8 bit
		to get set in the terminal driver on both ends.

	TERMINAL-TYPE:
		This is supported as per RFC1091.  On the server side,
		when a terminal type is received, termcap/terminfo
		is consulted to determine if it is a known terminal
		type.  It keeps requesting terminal types until it
		gets one that it recognizes, or hits the end of the
		list.  The server side looks up the entry in the
		termcap/terminfo data base, and generates a list of
		names which it then passes one at a time to each
		request for a terminal type, duplicating the last
		entry in the list before cycling back to the beginning.

	NAWS:	The Negotiate about Window Size, as per RFC 1073.

	TERMINAL-SPEED:
		Implemented as per RFC 1079

	TOGGLE-FLOW-CONTROL:
		Implemented as per RFC 1080

	TIMING-MARK:
		As per RFC 860

	SGA:	As per RFC 858

	ECHO:	As per RFC 857

	LOGOUT: As per RFC 727

	STATUS:
		The server will send its current status upon
		request.  It does not ask for the clients status.
		The client will request the servers current status
		from the "send getstatus" command.

	ENVIRON:
		This option is currently being defined by the IETF
		Telnet Working Group, and an RFC has not yet been
		issued, but should be in the near future...

	X-DISPLAY-LOCATION:
		This functionality can be done through the ENVIRON
		option, it is added here for completeness.

	AUTHENTICATION:
		This option is currently being defined by the IETF
		Telnet Working Group, and an RFC has not yet been
		issued.  The basic framework is pretty much decided,
		but the definitions for the specific authentication
		schemes is still in a state of flux.

	ENCRYPTION:
		This option is currently being defined by the IETF
		Telnet Working Group, and an RFC has not yet been
		issued.  The draft RFC is still in a state of flux,
		so this code may change in the future.
The cal(1) date routines were written from scratch, basically from first
principles.  The algorithm for calculating the day of week from any
Gregorian date was "reverse engineered".  This was necessary as most of
the documented algorithms have to do with date calculations for other
calendars (e.g. julian) and are only accurate when converted to gregorian
within a narrow range of dates.

1 Jan 1 is a Saturday because that's what cal says and I couldn't change
that even if I was dumb enough to try.  From this we can easily calculate
the day of week for any date.  The algorithm for a zero based day of week:

	calculate the number of days in all prior years (year-1)*365
	add the number of leap years (days?) since year 1 
		(not including this year as that is covered later)
	add the day number within the year
		this compensates for the non-inclusive leap year
		calculation
	if the day in question occurs before the gregorian reformation
		(3 sep 1752 for our purposes), then simply return 
		(value so far - 1 + SATURDAY's value of 6) modulo 7.
	if the day in question occurs during the reformation (3 sep 1752
		to 13 sep 1752 inclusive) return THURSDAY. This is my
		idea of what happened then. It does not matter much as
		this program never tries to find day of week for any day
		that is not the first of a month.
	otherwise, after the reformation, use the same formula as the
		days before with the additional step of subtracting the
		number of days (11) that were adjusted out of the calendar
		just before taking the modulo.

It must be noted that the number of leap years calculation is sensitive
to the date for which the leap year is being calculated.  A year that occurs
before the reformation is determined to be a leap year if its modulo of
4 equals zero.  But after the reformation, a year is only a leap year if
its modulo of 4 equals zero and its modulo of 100 does not.  Of course,
there is an exception for these century years.  If the modulo of 400 equals
zero, then the year is a leap year anyway.  This is, in fact, what the
gregorian reformation was all about (a bit of error in the old algorithm
that caused the calendar to be inaccurate.)

Once we have the day in year for the first of the month in question, the
rest is trivial.
The sys_info script is a small script which will show the version
information for installed utilities.  It also works on the kernel, and
on most libraries.

Its use is as follow:

	[19:41:13] agc@netbsd-002 ...external/bsd/sys_info [4568] > ./sys_info -a
	awk-20121220
	bind-9.10.3pl3
	bzip2-1.0.6
	calendar-20160601
	ftpd-20110904
	g++-4.8.5
	gcc-4.8.5
	grep-2.5.1anb1
	gzip-20150113
	bozohttpd-20151231
	NetBSD-7.99.26
	netpgp-3.99.17
	netpgpverify-20160214
	ntp-4.2.8pl5
	openssl-1.0.1r
	sqlite3-3.12.2
	openssh-7.1
	opensshd-7.1
	tcsh-6.19.00
	xz-5.2.1
	[19:41:20] agc@netbsd-002 ...external/bsd/sys_info [4569] > ./sys_info ntp ssh netpgp
	ntp-4.2.8pl5
	openssh-7.1
	netpgp-3.99.17
	[19:41:31] agc@netbsd-002 ...external/bsd/sys_info [4570] > ./sys_info ntp ssh netbsd
	ntp-4.2.8pl5
	openssh-7.1
	NetBSD-7.99.26
	[19:41:38] agc@netbsd-002 ...external/bsd/sys_info [4571] >

The -a option can be given to the script to print out the information
on all known components.

The sys_info script also works on libraries, returning their
"versions" as given by the shared object version numbers.

	[19:45:06] agc@netbsd-002 ...external/bsd/sys_info [4572] > ./sys_info libevent libXfont libc netbsd
	libevent-4.0
	libXfont-3.0
	libc-12.200
	NetBSD-7.99.26
	[19:45:27] agc@netbsd-002 ...external/bsd/sys_info [4573] >

Alistair Crooks
Wed Jun  1 19:44:01 PDT 2016
$Id: README,v 1.1 2010/11/23 20:48:40 yamt Exp $

it's a C-version of tpfmt.sh.  see usr.sbin/tprof/README for the usage.
$NetBSD: README,v 1.15 2012/06/14 04:14:36 riz Exp $

Organization of Sources:

This directory hierarchy is using an organization that separates
source for programs that we have obtained from external third
parties (where NetBSD is not the primary maintainer) from the
system source.

The hierarchy is grouped by license, and then package per license,
and is organized as follows:

	external/

	    Makefile
			Descend into the license sub-directories.

	    <license>/
			Per-license sub-directories.

		Makefile
			Descend into the package sub-directories.

		<package>/
			Per-package sub-directories.

		    Makefile
			Build the package.
			
		    dist/
			The third-party source for a given package.

		    bin/
		    lib/
		    sbin/
			BSD makefiles "reach over" from these into
			"../dist/".

This arrangement allows for packages to be easily disabled or
excised as necessary, either on a per-license or per-package basis.

The licenses currently used are:

	apache2		Apache 2.0 license.
			http://www.opensource.org/licenses/apache2.0.php

	atheros		Atheros License.

	bsd		BSD (or equivalent) licensed software, possibly with
			the "advertising clause".
			http://www.opensource.org/licenses/bsd-license.php

	cddl		Common Development and Distribution License (the sun
			license which is based on the Mozilla Public License
			version 1.1).
			http://www.opensource.org/licenses/cddl1.php

	gpl2		GNU Public License, version 2 (or earlier).
			http://www.opensource.org/licenses/gpl-2.0.php

	gpl3		GNU Public License, version 3.
			http://www.opensource.org/licenses/gpl-3.0.html

	historical	Lucent's old license:
			http://www.opensource.org/licenses/historical.php
			
	ibm-public	IBM's public license:
			http://www.opensource.org/licenses/ibmpl.php

	intel-fw-eula	Intel firmware license with redistribution
			restricted to OEM.

	intel-fw-public	Intel firmware license permitting redistribution with
			terms similar to BSD licensed software.

	intel-public	Intel license permitting redistribution with
			terms similar to BSD licensed software.

	mit		MIT (X11) style license.
			http://www.opensource.org/licenses/mit-license.php

	public-domain	Non-license for code that has been explicitly put
			into the Public Domain.

	realtek		RealTek license.

	zlib		Zlib (BSD-like) license.
			http://www.zlib.net/zlib_license.html

If a package has components covered by different licenses
(for example, GPL2 and the LGPL), use the <license> subdirectory
for the more restrictive license.

If a package allows the choice of a license to use, we'll
generally use the less restrictive license.

If in doubt about where a package should be located, please
contact <core@NetBSD.org> for advice.


Migration Strategy:


Eventually src/dist (and associated framework in other base source
directories) and src/gnu will be migrated to this hierarchy.


Maintenance Strategy:

The sources under src/external/<license>/<package>/dist/ are
generally a combination of a published distribution plus changes
that we submit to the maintainers and that are not yet published
by them.

Make sure all changes made to the external sources are submitted
to the appropriate maintainer, but only after coordinating with
the NetBSD maintainers.
Guide - Guide to the DTraceToolkit

    How to get started, and a table of contents.

QuickStart

	1. The top most useful scripts are in this directory.
	2. Try running them with "-h". Eg, "./execsnoop -h".
	3. Read Docs/Contents for a full list of scripts.

QuickStart-by-Screenshot

	1. Look through the examples in the Examples directory until
	   you see an output you like
	2. Find the script and run it
	3. Look for its man page in Man

Not-so-QuickStart
	
	1. Welcome!
	2. Check the Table of Contents below to become famaliar with the
	   directory structure of the DTraceToolkit.
	3. See Docs/Faq for any initial questions.
	4. Read Docs/Contents for a list of scripts.
	5. Read Docs/Readme to see where scripts are documented.
	6. Check Docs/Links for further DTrace.
	7. Once famaliar with the toolkit, the following may be useful to
	   add to your shell initialisation file,
		PATH=$PATH:/opt/DTT/Bin
		MANPATH=$MANPATH:/opt/DTT/Man
	   in this case assuming the toolkit was installed in /opt/DTT.

Installation

	1. Run ./install
	   This will replace any existing version of the DTraceToolkit
	   with this one. It will prompt. Final install location is
	   printed by this install script.

Table of Contents

   DTraceToolkit-X.XX/
	Bin/			Symlinks to all the scripts
	Apps/			Application specific scripts
	Cpu/			Scripts for CPU analysis
	Code/			Example code to practise on
	Disk/			Scripts for disk I/O analysis
	Docs/			Documentation
	   Contents		Command list for the Toolkit
	   Faq			Frequently asked questions
	   Links		Further DTrace links
	   Readme		Readme for using the docs
	Examples/		Examples of command usage
	Guide			This file!
	Include/		DTrace include files
	Java/			Scripts for tracing Java
	JavaScript/		Scripts for tracing JavaScript
	Kernel/			Scripts for kernel analysis
	License			The CDDL license
	Locks/			Scripts for lock analysis
	Man/			Man pages
	   man1m/		Man pages for the Toolkit commands
	Mem/			Scripts for memory analysis
	Misc/			Misc scripts
	Net/			Scripts for network analysis
	Notes/			Notes on Toolkit commands
	Perl/			Scripts for tracing Perl
	Php/			Scripts for tracing Php
	Proc/			Scripts for process analysis
	Python/			Scripts for tracing Python
	Ruby/			Scripts for tracing Ruby
	Shell/			Scripts for tracing Shell languages
	Snippits/		Snippits of D scripting
	System/			Scripts for system analysis
	Tcl/			Scripts for tracing Tcl
	User/			Scripts for user based activity analysis
	Zones/			Scripts for analysis by zone
	Version			DTraceToolkit version
	install			Install script, use for installs only

When you type ls in the DTraceToolkit, you will be looking at the top ten 
or so most useful scripts plus the top level directories. Other scripts have
been placed in meaningful subdirectories, such as Disk, Kernel, Proc, etc.

An optional Bin directory has been provided that links to all the scripts.

The DTraceToolkit is released under the CDDL license. It's the same open
source license that OpenSolaris has been released under.

Thank you for using the DTraceToolkit!

Intel PRO/Wireless 3945ABG/BG Network Connection Adapter
Copyright (C) 2006 - 2007 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-3945-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-3945.ucode provided in this package is required to be 
present on your system in order for the Intel PRO/Wireless 3945ABG/BG Network
Connection Adapter driver for Linux (iwlwifi-3945) to be able to operate
on your system.

On adapter initialization, and at varying times during the uptime of 
the adapter, the microcode is loaded into the RAM on the network 
adapter.  The microcode provides the low level MAC features including 
radio control and high precision timing events (backoff, transmit, 
etc.) while also providing varying levels of packet filtering which can 
be used to keep the host from having to handle packets that are not of 
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi-3945 driver will look for the file iwlwifi-3945.ucode using the 
kernel's firmware_loader infrastructure.  In order to function 
correctly, you need to have this support enabled in your kernel.  When 
you configure the kernel, you can find this option in the following 
location:

        Device Drivers ->
                Generic Driver Options ->
                        Hotplug firmware loading support


You can determine if your kernel currently has firmware loader support 
by looking for the CONFIG_FW_LOADER definition on your kernel's 
.config.

In addition to having the firmware_loader support in your kernel, you 
must also have a working hotplug and udev infrastructure configured.  
The steps for installing and configuring hotplug and udev are very 
distribution specific. 

Once you have the firmware loader in place (or if you aren't sure and 
you just want to try things to see if it works), you need to install 
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system 
distribution.  You can typically find this location by looking in the 
hotplug configuration file for your distro:

	% grep \"^FIRMWARE_DIR\" /etc/hotplug/firmware.agent

This should give you output like:

	FIRMWARE_DIR=/lib/firmware

If it lists more than one directory, you only need to put the 
microcode in one of them.  In the above example, installation is 
simply:

	% cp iwlwifi-3945.ucode /lib/firmware

You can now load the driver (see the INSTALL and 
README.iwlwifi-3945 provided with the iwlwifi-3945 package for 
information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-3945.ucode) is provided under the 
terms of the following license (available in the file 
LICENSE.iwlwifi-3945-ucode):

Copyright (c) 2006 - 2007, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without 
modification, are permitted provided that the following conditions are 
met:

* Redistributions must reproduce the above copyright notice and the 
  following disclaimer in the documentation and/or other materials 
  provided with the distribution. 
* Neither the name of Intel Corporation nor the names of its suppliers 
  may be used to endorse or promote products derived from this software 
  without specific prior written permission. 
* No reverse engineering, decompilation, or disassembly of this software 
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide, 
royalty-free, non-exclusive license under patents it now or hereafter 
owns or controls to make, have made, use, import, offer to sell and 
sell ("Utilize") this software, but solely to the extent that any 
such patent is necessary to Utilize the software alone, or in 
combination with an operating system licensed under an approved Open 
Source license as listed by the Open Source Initiative at 
http://opensource.org/licenses.  The patent license shall not apply to 
any other combinations which include this software.  No hardware per 
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND 
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, 
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND 
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE 
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, 
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS 
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND 
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE 
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH 
DAMAGE.

------------------------------
Copyright (C) 2005 - 2007, Intel Corporation

INFORMATION IN THIS DOCUMENT IS PROVIDED IN CONNECTION WITH INTEL PRODUCTS.  
EXCEPT AS PROVIDED IN INTEL'S TERMS AND CONDITIONS OF SALE FOR SUCH PRODUCTS, 
INTEL ASSUMES NO LIABILITY WHATSOEVER, AND INTEL DISCLAIMS ANY EXPRESS OR 
IMPLIED WARRANTY RELATING TO SALE AND/OR USE OF INTEL PRODUCTS, INCLUDING 
LIABILITY OR WARRANTIES RELATING TO FITNESS FOR A PARTICULAR PURPOSE, 
MERCHANTABILITY, OR INFRINGEMENT OF ANY PATENT, COPYRIGHT, OR OTHER 
INTELLECTUAL PROPERTY RIGHT. 

This document is subject to change without notice. 

* Other names and brands may be claimed as the property of others.
Intel Wireless WiFi Link 105 BGN Adapter
Copyright (C) 2006-2012 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-105-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-105-6.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-6" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-105-6.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-105-6.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-105-6.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-105-ucode):

Copyright (c) 2006-2012, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 5150AGN Adapter
Copyright (C) 2006-2009 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-5150-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-5150-2.ucode provided in this package must be 
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-2" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of 
the adapter, the microcode is loaded into the memory on the network 
adapter.  The microcode provides the low level MAC features including 
radio control and high precision timing events (backoff, transmit, 
etc.) while also providing varying levels of packet filtering which can 
be used to keep the host from having to handle packets that are not of 
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-5150-2.ucode using the 
kernel's firmware_loader infrastructure.  In order to function 
correctly, you need to have this support enabled in your kernel.  When 
you configure the kernel, you can find this option in the following 
location:

        Device Drivers ->
                Generic Driver Options ->
                        Hotplug firmware loading support


You can determine if your kernel currently has firmware loader support 
by looking for the CONFIG_FW_LOADER definition on your kernel's 
.config.

In addition to having the firmware_loader support in your kernel, you 
must also have a working hotplug and udev infrastructure configured.  
The steps for installing and configuring hotplug and udev are very 
distribution specific. 

Once you have the firmware loader in place (or if you aren't sure and 
you just want to try things to see if it works), you need to install 
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system 
distribution.  You can typically find this location by looking in the 
hotplug configuration file for your distro:

	% grep \"^FIRMWARE_DIR\" /etc/hotplug/firmware.agent

This should give you output like:

	FIRMWARE_DIR=/lib/firmware

If it lists more than one directory, you only need to put the 
microcode in one of them.  In the above example, installation is 
simply:

	% cp iwlwifi-5150-2.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-5150-2.ucode) is provided under the 
terms of the following license (available in the file 
LICENSE.iwlwifi-5150-ucode):

Copyright (c) 2006-2009, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without 
modification, are permitted provided that the following conditions are 
met:

* Redistributions must reproduce the above copyright notice and the 
  following disclaimer in the documentation and/or other materials 
  provided with the distribution. 
* Neither the name of Intel Corporation nor the names of its suppliers 
  may be used to endorse or promote products derived from this software 
  without specific prior written permission. 
* No reverse engineering, decompilation, or disassembly of this software 
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide, 
royalty-free, non-exclusive license under patents it now or hereafter 
owns or controls to make, have made, use, import, offer to sell and 
sell ("Utilize") this software, but solely to the extent that any 
such patent is necessary to Utilize the software alone, or in 
combination with an operating system licensed under an approved Open 
Source license as listed by the Open Source Initiative at 
http://opensource.org/licenses.  The patent license shall not apply to 
any other combinations which include this software.  No hardware per 
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND 
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, 
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND 
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE 
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, 
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS 
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND 
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE 
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH 
DAMAGE.
Intel Wireless WiFi Link 6005 AGN Adapter
Copyright (C) 2006-2012 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-6000g2b-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-6000g2b-6.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-6" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-6000g2b-6.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-6000g2b-6.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-6000g2b-6.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-6000g2b-ucode):

Copyright (c) 2006-2012, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 6000G2A AGN Adapter
Copyright (C) 2006-2011 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-6000g2a-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-6000g2a-5.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-5" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-6000g2a-5.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-6000g2a-5.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-6000g2a-5.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-6000g2a-ucode):

Copyright (c) 2006-2011, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 8260 AC Adapter
Copyright (C) 2015 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-8265-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The files iwlwifi-8265-22.ucode provided in this package
must be present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlwifi) to operate on your system.

The "-22" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi driver will look for the file iwlwifi-8265-22.ucode
using the kernel's firmware_class infrastructure.
More information can be found under Documentation/firmware_class in kernel
source. In order to function correctly, you need to have this support enabled
in your kernel.  When you configure the kernel, you can find this option in
the following location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-8265-22.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-8265-22.ucode) is provided
under the terms of the following license (available in the file
LICENSE.iwlwifi-8000-ucode):

Copyright (c) 2015, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:
 reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 4965AGN Adapter
Copyright (C) 2006 - 2009 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-4965-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-4965-2.ucode provided in this package must be 
present on your system in order for the Intel Wireless WiFi Link
4965AGN driver for Linux (iwlwifi-4965) to operate on your system.

The "-2" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of 
the adapter, the microcode is loaded into the RAM on the network 
adapter.  The microcode provides the low level MAC features including 
radio control and high precision timing events (backoff, transmit, 
etc.) while also providing varying levels of packet filtering which can 
be used to keep the host from having to handle packets that are not of 
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi-4965 driver will look for the file iwlwifi-4965-2.ucode using the 
kernel's firmware_loader infrastructure.  In order to function 
correctly, you need to have this support enabled in your kernel.  When 
you configure the kernel, you can find this option in the following 
location:

        Device Drivers ->
                Generic Driver Options ->
                        Hotplug firmware loading support


You can determine if your kernel currently has firmware loader support 
by looking for the CONFIG_FW_LOADER definition on your kernel's 
.config.

In addition to having the firmware_loader support in your kernel, you 
must also have a working hotplug and udev infrastructure configured.  
The steps for installing and configuring hotplug and udev are very 
distribution specific. 

Once you have the firmware loader in place (or if you aren't sure and 
you just want to try things to see if it works), you need to install 
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system 
distribution.  You can typically find this location by looking in the 
hotplug configuration file for your distro:

	% grep \"^FIRMWARE_DIR\" /etc/hotplug/firmware.agent

This should give you output like:

	FIRMWARE_DIR=/lib/firmware

If it lists more than one directory, you only need to put the 
microcode in one of them.  In the above example, installation is 
simply:

	% cp iwlwifi-4965-2.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-4965.ucode) is provided under the 
terms of the following license (available in the file 
LICENSE.iwlwifi-4965-ucode):

Copyright (c) 2006, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without 
modification, are permitted provided that the following conditions are 
met:

* Redistributions must reproduce the above copyright notice and the 
  following disclaimer in the documentation and/or other materials 
  provided with the distribution. 
* Neither the name of Intel Corporation nor the names of its suppliers 
  may be used to endorse or promote products derived from this software 
  without specific prior written permission. 
* No reverse engineering, decompilation, or disassembly of this software 
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide, 
royalty-free, non-exclusive license under patents it now or hereafter 
owns or controls to make, have made, use, import, offer to sell and 
sell ("Utilize") this software, but solely to the extent that any 
such patent is necessary to Utilize the software alone, or in 
combination with an operating system licensed under an approved Open 
Source license as listed by the Open Source Initiative at 
http://opensource.org/licenses.  The patent license shall not apply to 
any other combinations which include this software.  No hardware per 
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND 
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, 
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND 
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE 
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, 
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS 
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND 
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE 
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH 
DAMAGE.
Intel Wireless WiFi Link 1000 Adapter
Copyright (C) 2006-2009 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-1000-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-1000-3.ucode provided in this package must be 
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-3" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of 
the adapter, the microcode is loaded into the memory on the network 
adapter.  The microcode provides the low level MAC features including 
radio control and high precision timing events (backoff, transmit, 
etc.) while also providing varying levels of packet filtering which can 
be used to keep the host from having to handle packets that are not of 
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-1000-3.ucode using the 
kernel's firmware_loader infrastructure.  In order to function 
correctly, you need to have this support enabled in your kernel.  When 
you configure the kernel, you can find this option in the following 
location:

        Device Drivers ->
                Generic Driver Options ->
                        Hotplug firmware loading support


You can determine if your kernel currently has firmware loader support 
by looking for the CONFIG_FW_LOADER definition on your kernel's 
.config.

In addition to having the firmware_loader support in your kernel, you 
must also have a working hotplug and udev infrastructure configured.  
The steps for installing and configuring hotplug and udev are very 
distribution specific. 

Once you have the firmware loader in place (or if you aren't sure and 
you just want to try things to see if it works), you need to install 
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system 
distribution.  You can typically find this location by looking in the 
hotplug configuration file for your distro:

	% grep \"^FIRMWARE_DIR\" /etc/hotplug/firmware.agent

This should give you output like:

	FIRMWARE_DIR=/lib/firmware

If it lists more than one directory, you only need to put the 
microcode in one of them.  In the above example, installation is 
simply:

	% cp iwlwifi-1000-3.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-1000-3.ucode) is provided under the 
terms of the following license (available in the file 
LICENSE.iwlwifi-1000-ucode):

Copyright (c) 2006-2009, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without 
modification, are permitted provided that the following conditions are 
met:

* Redistributions must reproduce the above copyright notice and the 
  following disclaimer in the documentation and/or other materials 
  provided with the distribution. 
* Neither the name of Intel Corporation nor the names of its suppliers 
  may be used to endorse or promote products derived from this software 
  without specific prior written permission. 
* No reverse engineering, decompilation, or disassembly of this software 
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide, 
royalty-free, non-exclusive license under patents it now or hereafter 
owns or controls to make, have made, use, import, offer to sell and 
sell ("Utilize") this software, but solely to the extent that any 
such patent is necessary to Utilize the software alone, or in 
combination with an operating system licensed under an approved Open 
Source license as listed by the Open Source Initiative at 
http://opensource.org/licenses.  The patent license shall not apply to 
any other combinations which include this software.  No hardware per 
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND 
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, 
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND 
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE 
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, 
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS 
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND 
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE 
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH 
DAMAGE.
Intel Wireless WiFi Link 100AGN Adapter
Copyright (C) 2006-2010 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-100-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-100-5.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-5" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-100-5.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-100-5.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-100-5.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-100-ucode):

Copyright (c) 2006-2010, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 3160 AC Adapter
Copyright (C) 2014 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-3160-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-3160-9.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlwifi) to operate on your system.

The "-9" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi driver will look for the file iwlwifi-3160-9.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-3160-9.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-3160-9.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-3160-ucode):

Copyright (c) 2014, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 6200AGN and 6300AGN Adapter
Copyright (C) 2006-2010 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-6000-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-6000-4.ucode provided in this package must be 
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-4" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of 
the adapter, the microcode is loaded into the memory on the network 
adapter.  The microcode provides the low level MAC features including 
radio control and high precision timing events (backoff, transmit, 
etc.) while also providing varying levels of packet filtering which can 
be used to keep the host from having to handle packets that are not of 
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-6000-4.ucode using the 
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When 
you configure the kernel, you can find this option in the following 
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support 
by looking for the CONFIG_FW_LOADER definition on your kernel's 
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific. 

Once you have the firmware loader in place (or if you aren't sure and 
you just want to try things to see if it works), you need to install 
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system 
distribution.  You can typically find this location by looking in the 
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-6000-4.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-6000-4.ucode) is provided under the 
terms of the following license (available in the file 
LICENSE.iwlwifi-6000-ucode):

Copyright (c) 2006-2010, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without 
modification, are permitted provided that the following conditions are 
met:

* Redistributions must reproduce the above copyright notice and the 
  following disclaimer in the documentation and/or other materials 
  provided with the distribution. 
* Neither the name of Intel Corporation nor the names of its suppliers 
  may be used to endorse or promote products derived from this software 
  without specific prior written permission. 
* No reverse engineering, decompilation, or disassembly of this software 
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide, 
royalty-free, non-exclusive license under patents it now or hereafter 
owns or controls to make, have made, use, import, offer to sell and 
sell ("Utilize") this software, but solely to the extent that any 
such patent is necessary to Utilize the software alone, or in 
combination with an operating system licensed under an approved Open 
Source license as listed by the Open Source Initiative at 
http://opensource.org/licenses.  The patent license shall not apply to 
any other combinations which include this software.  No hardware per 
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND 
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, 
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND 
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE 
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, 
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS 
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND 
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE 
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH 
DAMAGE.
Intel Wireless WiFi Link 6150BGN and 6250AGN Adapter
Copyright (C) 2006-2010 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-6050-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-6050-5.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-5" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-6050-5.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-6050-5.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-6050-5.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-6050-ucode):

Copyright (c) 2006-2010, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 7260 AC Adapter
Copyright (C) 2014 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-7260-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-7260-9.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlwifi) to operate on your system.

The "-9" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi driver will look for the file iwlwifi-7260-9.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-7260-9.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-7260-9.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-7260-ucode):

Copyright (c) 2014, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 2200 BGN Adapter
Copyright (C) 2006-2012 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-2000-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-2000-6.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-6" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-2000-6.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-2000-6.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-2000-6.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-2000-ucode):

Copyright (c) 2006-2012, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 8260 AC Adapter
Copyright (C) 2015 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-8000-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The files iwlwifi-8000C-16.ucode provided in this package
must be present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlwifi) to operate on your system.

The "-16" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi driver will look for the file iwlwifi-8000C-16.ucode
using the kernel's firmware_class infrastructure.
More information can be found under Documentation/firmware_class in kernel
source. In order to function correctly, you need to have this support enabled
in your kernel.  When you configure the kernel, you can find this option in
the following location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-8000C-16.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-8000C-16.ucode) is provided
under the terms of the following license (available in the file
LICENSE.iwlwifi-8000-ucode):

Copyright (c) 2015, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 2230 BGN Adapter
Copyright (C) 2006-2012 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-2030-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-2030-6.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-6" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-2030-6.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-2030-6.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-2030-6.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-2030-ucode):

Copyright (c) 2006-2012, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 7265 AC Adapter
Copyright (C) 2014 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-7265-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-7265-9.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlwifi) to operate on your system.

The "-9" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi driver will look for the file iwlwifi-7265-9.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-7265-9.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-7265-9.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-7265-ucode):

Copyright (c) 2014, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 5100AGN, 5300AGN, and 5350AGN Adapter
Copyright (C) 2006-2009 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-5000-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-5000-2.ucode provided in this package must be 
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-2" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of 
the adapter, the microcode is loaded into the memory on the network 
adapter.  The microcode provides the low level MAC features including 
radio control and high precision timing events (backoff, transmit, 
etc.) while also providing varying levels of packet filtering which can 
be used to keep the host from having to handle packets that are not of 
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-5000-2.ucode using the 
kernel's firmware_loader infrastructure.  In order to function 
correctly, you need to have this support enabled in your kernel.  When 
you configure the kernel, you can find this option in the following 
location:

        Device Drivers ->
                Generic Driver Options ->
                        Hotplug firmware loading support


You can determine if your kernel currently has firmware loader support 
by looking for the CONFIG_FW_LOADER definition on your kernel's 
.config.

In addition to having the firmware_loader support in your kernel, you 
must also have a working hotplug and udev infrastructure configured.  
The steps for installing and configuring hotplug and udev are very 
distribution specific. 

Once you have the firmware loader in place (or if you aren't sure and 
you just want to try things to see if it works), you need to install 
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system 
distribution.  You can typically find this location by looking in the 
hotplug configuration file for your distro:

	% grep \"^FIRMWARE_DIR\" /etc/hotplug/firmware.agent

This should give you output like:

	FIRMWARE_DIR=/lib/firmware

If it lists more than one directory, you only need to put the 
microcode in one of them.  In the above example, installation is 
simply:

	% cp iwlwifi-5000-2.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-5000-2.ucode) is provided under the 
terms of the following license (available in the file 
LICENSE.iwlwifi-5000-ucode):

Copyright (c) 2006-2009, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without 
modification, are permitted provided that the following conditions are 
met:

* Redistributions must reproduce the above copyright notice and the 
  following disclaimer in the documentation and/or other materials 
  provided with the distribution. 
* Neither the name of Intel Corporation nor the names of its suppliers 
  may be used to endorse or promote products derived from this software 
  without specific prior written permission. 
* No reverse engineering, decompilation, or disassembly of this software 
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide, 
royalty-free, non-exclusive license under patents it now or hereafter 
owns or controls to make, have made, use, import, offer to sell and 
sell ("Utilize") this software, but solely to the extent that any 
such patent is necessary to Utilize the software alone, or in 
combination with an operating system licensed under an approved Open 
Source license as listed by the Open Source Initiative at 
http://opensource.org/licenses.  The patent license shall not apply to 
any other combinations which include this software.  No hardware per 
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND 
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, 
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND 
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE 
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, 
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS 
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND 
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE 
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH 
DAMAGE.
Intel Wireless WiFi Link 135 BGN Adapter
Copyright (C) 2006-2012 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-135-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The file iwlwifi-135-6.ucode provided in this package must be
present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlagn) to operate on your system.

The "-6" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlagn driver will look for the file iwlwifi-135-6.ucode using the
kernel's firmware_class infrastructure. More information can be found under
Documentation/firmware_class in kernel source. In order to function
correctly, you need to have this support enabled in your kernel.  When
you configure the kernel, you can find this option in the following
location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-135-6.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-135-6.ucode) is provided under the
terms of the following license (available in the file
LICENSE.iwlwifi-135-ucode):

Copyright (c) 2006-2012, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:

* Redistributions must reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
Intel Wireless WiFi Link 3168 AC Adapter
Copyright (C) 2016 Intel Corporation.  All rights reserved.

Microcode Package README.iwlwifi-3168-ucode

INDEX

1. OVERVIEW
2. INSTALLATION
3. LICENSE


1. OVERVIEW

The files iwlwifi-3168-22.ucode provided in this package
must be present on your system in order for the Intel Wireless WiFi Link
AGN driver for Linux (iwlwifi) to operate on your system.

The "-22" in the filename reflects an interface/architecture version number.
It will change only when changes in new uCode releases make the new uCode
incompatible with earlier drivers.

On adapter initialization, and at varying times during the uptime of
the adapter, the microcode is loaded into the memory on the network
adapter.  The microcode provides the low level MAC features including
radio control and high precision timing events (backoff, transmit,
etc.) while also providing varying levels of packet filtering which can
be used to keep the host from having to handle packets that are not of
interest given the current operating mode of the device.

2. INSTALLATION

The iwlwifi driver will look for the file iwlwifi-3168-22.ucode
using the kernel's firmware_class infrastructure.
More information can be found under Documentation/firmware_class in kernel
source. In order to function correctly, you need to have this support enabled
in your kernel.  When you configure the kernel, you can find this option in
the following location:

        Device Drivers ->
                Generic Driver Options ->
                        Userspace firmware loading support


You can determine if your kernel currently has firmware loader support
by looking for the CONFIG_FW_LOADER definition on your kernel's
.config.

In addition to having the firmware_class support in your kernel, you
must also have a working udev and uevent infrastructure configured.
The steps for installing and configuring udev are very
distribution specific.

Once you have the firmware loader in place (or if you aren't sure and
you just want to try things to see if it works), you need to install
the microcode file into the appropriate location.

Where that appropriate location is depends (again) on your system
distribution.  You can typically find this location by looking in the
udev scripts of your distro, the default is /lib/firmware.

Installation of the firmware is simply:

        % cp iwlwifi-3168-22.ucode /lib/firmware

You can now load the driver (see the INSTALL and README.iwlwifi provided with
the iwlwifi package for information on building and using that driver.)

3. LICENSE

The microcode in this package (iwlwifi-3168-22.ucode) is provided
under the terms of the following license (available in the file
LICENSE.iwlwifi-8000-ucode):

Copyright (c) 2016, Intel Corporation.
All rights reserved.

Redistribution.  Redistribution and use in binary form, without
modification, are permitted provided that the following conditions are
met:
 reproduce the above copyright notice and the
  following disclaimer in the documentation and/or other materials
  provided with the distribution.
* Neither the name of Intel Corporation nor the names of its suppliers
  may be used to endorse or promote products derived from this software
  without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this software
  is permitted.

Limited patent license.  Intel Corporation grants a world-wide,
royalty-free, non-exclusive license under patents it now or hereafter
owns or controls to make, have made, use, import, offer to sell and
sell ("Utilize") this software, but solely to the extent that any
such patent is necessary to Utilize the software alone, or in
combination with an operating system licensed under an approved Open
Source license as listed by the Open Source Initiative at
http://opensource.org/licenses.  The patent license shall not apply to
any other combinations which include this software.  No hardware per
se is licensed hereunder.

DISCLAIMER.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.
                             CVS port to VMS

DISCLAIMER: This port must be considered experimental.  Although
previous versions have been in use at one large site since about
October, 1995, and the port is believed to be quite usable, various
VMS-specific quirks are known and the port cannot be considered as
mature as the ports to, say, Windows NT or unix.  As always, future
progress of this port will depend on volunteer and customer interest.

This port is of the CVS client only.  Or in other words, the port
implements the full set of CVS commands, but cannot access
repositories located on the local machine.  The repository must live
on another machine (a Unix box) which runs a complete port of CVS.

Most (all?) work to date has been done on OpenVMS/AXP 6.2.  Other VMS
variants might work too.

Provided that both your client and your server are recent (for
example, CVS 1.9.27 or later), you shouldn't need GNU patch or any
other executables other than CVS.EXE.

Please send bug reports to bug-cvs@nongnu.org.

As of CVS 1.5.something, this port passed most of the tests in
[.src]sanity.sh.  I say "most" because some tests to not apply to the
CVS client.  The tests were run by hand because the VMS POSIX shell
was incapable of running the script.  The tests that sanity.sh
provides are not conclusive but at least provides some assurance that
the client is usable.

To compile, you will need DEC C (CC), DEC UCX, and of course DCL
installed on your machine.  Just type "@build" in the top level
directory.  This will build the sources in each subdirectory, and link
the executable [.src]cvs.exe

Copy the executable to an appropriate directory, and define the symbol "CVS"
in a .COM file which everyone running CVS will need to run.  Here's an example
of what needs to be done.

$ CVS :== $YOUR_DEVICE:[YOUR.DIRECTORY.CVS]CVS.EXE

Accessing a remote repository can happen in several ways.

1. pserver
2. rsh - privileged (default)
3. rsh - unprivileged (on VMS side)

Here's how to do each of the above:

-------------------------------------------------------------------------------
1.  pserver.  This is the preferred way.  It works just as it is
documented in the CVS manual (see the README file in the CVS
distribution for more information on the manual).

-------------------------------------------------------------------------------
2. Using CVS internal rsh support (privileged)

VMS's RSH is unusable for CVS's purposes (that is, the one in UCX.
Don't know about Multinet).  However, there is code within CVS to
emulate RSH for purposes of contacting a CVS server "in the usual way"
via rshd.  Unfortunately, this requires the VMS CVS client to be
installed with OPER privilege, by your system administrator.

RSH uses privileged ports and trusted software/hosts to determine
which user on the client side is trying to connect.  Part of this
security is due to the fact that on VMS or UNIX, a non privileged
process is not permitted to bind a socket to a privileged port.

If rshd receives a connection on a non-privileged port, the connection is
immediately aborted.  Only connections arriving from a privileged port will
be authenticated and served.  The CVS client will therefore need privileges
under VMS to produce such a connection.

*** Please note that no careful examination has been done of the security
    implications of installing CVS with the OPER privilege.  If some hole
    exists, then by doing so, you will enable users who are already on
    your system to gain unauthorized privileges ***

-------------------------------------------------------------------------------
3. Using CVS internal rsh support (non-privileged)

There is a workaround, but this is one case where I think the cure is worse
than the disease.  If you patch an rshd to not care that the RSH originating
port is "non-privileged", the CVS VMS client will allow you to define the
logical CVS_RCMD_PORT to the port number where this patched rshd will be
listening.  I leave the talk of patching rshd to the gentle reader and his/her
friendly system administrator.

If I put an entry in my /etc/services file:

cvs_rcmd            4381/tcp        cvs_rcmd

And add a line to /etc/inetd.conf, then restart inetd via "kill -1"

cvs_rcmd  stream  tcp  nowait root /usr/sbin/tcpd  /usr/local/sbin/cvs_rcmd

On the VMS side, you will have to do this:

$ define CVS_RCMD_PORT 4381

Then run CVS in the "usual way".

Note that the patched rshd will need to be invoked via inetd as root, so it can
authenticate and _become_ the intended user, the same as the regular rshd.

***Please note that you will be installing a security hole by doing this.***

Please also note that this security hole is no larger than allowing a
Macintosh, PC (OS/2, NT, etc.) to have it's hostname in any .rhosts file,
as any user can create a privileged socket without authentication, under these
environments.  In fact, existing ports of CVS to these environment use this
to their advantage.

-------------------------------------------------------------------------------
Wildcard expansion is not yet implemented (i.e. CVS COMMIT *.c won't
work.)  I think that expand_wild should be calling lib$findfile
(util.c in gzip is said to provide an example), but noone has gotten
around to implementing this.

Log messages must be entered on the command line using -m or -F.  You
can use -e or define the logical EDITOR to cause CVS to try other
editors (TPU.EXE or any other editor which wants DCL command parsing
will not work) if you want to test what's available on your system.  I
haven't tested this, but if you install vi or emacs, chances are it
will probably work.  Just make sure the .EXE files are in a directory
listed in VAXC$PATH (is this a typo for DCL$PATH?  Also, will a
logical name work?).  If someone gets around to implementing it, we
should probably be using the callable editors (e.g. TPU$TPU), although
of course we also need interface(s) which are not locked into any
particular editors.

----------------------------------------

Notes regarding compiling on VAX/VMS 6.2 (not Alpha) (These are items
which hopefully will have cleaner solutions in the future, but here is
how to get around them for now):

* Need to compile lib/getdate.c with vaxc instead of decc to avoid a
compiler bugcheck.  Therefore one must add SYS$LIBRARY:VAXCRTL/LIBRARY
to the link.

* In src/ignore.c, change lstat to stat.  In vms/filesubr.c, change
"#ifdef S_ISLNK" to "#if 0".

* Ignore the warnings in vms/vmsmunch.c; the system include file
declares something as an int when it should be void *.  Not *our*
fault!

* Remove the #define's of mode_t in vms/vms.h and pid_t in vms/pwd.h.
Add "#include <sys/types.h>" in vms/pwd.h.

Credits:

Initial VMS port by Benjamin J. Lee <benjamin@cyclic.com>, Cyclic
Software, October 1, 1995 (Update March 1, 1996).
				CVS Kit

	   Copyright (C) 1986-2005 Free Software Foundation, Inc.

	   Portions Copyright (C) 1998-2005 Derek Price,
	                                    & Ximbiot <http://ximbiot.com>.
	   Portions Copyright (C) 1993-1994 Brian Berliner.
	   Portions Copyright (C) 1992 Brian Berliner and Jeff Polk.
	   Portions Copyright (C) 1989-1992 Brian Berliner.
	   All Rights Reserved

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 1, or (at your option)
    any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

-------------------------------------------------------------------------------

Welcome to CVS!

If you have problems or think you have found a bug in CVS, see the
section BUGS in the CVS manual (also known as Version Management with
CVS by Per Cederqvist et al, or cvs.texinfo--see below for details).

If you are thinking of submitting changes to CVS, see the
file HACKING.

Please consult the INSTALL-CVS file for information on tested
configurations.  If you have a comment about an already tested
configuration, or have tried CVS on a new configuration, please let us
know as described in INSTALL-CVS.  Free software only works if we all help
out.

Finally, we cannot guarantee that this release will not completely wipe out
all of your work from your system.  We do some simple testing before each
release, but you are completely on your own.  We recommend testing this
release on a source repository that is not critical to your work.  THIS
SOFTWARE IS SUPPLIED COMPLETELY "AS IS".  NO WARRANTY....

Thanks for your support!

	-The CVS Team

-------------------------------------------------------------------------------

What Is CVS?

CVS is a version control system, which allows you to keep old versions
of files (usually source code), keep a log of who, when, and why
changes occurred, etc., like RCS or SCCS.  It handles multiple
developers, multiple directories, triggers to enable/log/control
various operations, and can work over a wide area network.  The
following tasks are not included; they can be done in conjunction with
CVS but will tend to require some script-writing and software other
than CVS: bug-tracking, build management (that is, make and make-like
tools), and automated testing.

And a whole lot more.  See the manual for more information.

-------------------------------------------------------------------------------

Notes to people upgrading from a previous release of CVS:

See the NEWS file for a description of features new in this version.

See the Compatibility section of the manual for information on
compatibility between CVS versions.  The quick summary is that as long
as you not using the optional watch features, there are no
compatibility problems with CVS 1.5 or later.

-------------------------------------------------------------------------------

Installation:

Please read the INSTALL-CVS file for installation instructions.  The brief
summary is:

	$ ./configure
	$ make
	(run the regression tests, if desired, via `make check')
	$ make install
	(create a repository if you don't already have one)

The documentation is in the doc subdirectory.  cvs.texinfo is the main
manual; cvs.info* and cvs.ps are the info and postscript versions,
respectively, generated from cvs.texinfo.  The postscript version is
for US letter size paper; we do this not because we consider this size
"better" than A4, but because we believe that the US letter version
will print better on A4 paper than the other way around. If you want a
version formatted for A4, add the line @afourpaper near the start of
cvs.texinfo and re-generate cvs.ps using TeX.

-------------------------------------------------------------------------------

* How do I get up-to-date information and information about other
versions of CVS?

See also 
	http://cvs.nongnu.org
	http://www.cvsnt.org

Anyone can add themselves to the following mailing lists:

    bug-cvs:  This is the list which users are requested to send bug reports
      to.  General CVS development and design discussions also tend to take
      place on this list.
    info-cvs:  This list is intended for user questions, including general
      help requests.
    cvs-announce:  CVS release announcements and other major
      announcements about the project are sent to this list.
    cvs-announce-binaries:  Announcements are made to this list
      when binaries for various platforms are built and initially
      posted for download.

To subscribe to any of these lists, send mail to <list>-request@nongnu.org
or visit http://savannah.nongnu.org/mail/?group=cvs and follow the instructions
for the list you wish to subscribe to.

The newsgroup for CVS (and other configuration management systems) is
comp.software.config-mgmt.  The gnu.cvs.help newsgroup is a 2-way mirror
of the info-cvs@nongnu.org mailing list and gnu.cvs.bug is similarly a 2-way
mirror of bug-cvs@nongnu.org.

-------------------------------------------------------------------------------

Credits:  See the AUTHORS file.
Many of the source files in this directory come from the GNULIB project
<http://savannah.gnu.org/projects/gnulib/>/<mailto:bug-gnulib@gnu.org>, and,
if they don't, they should.

What this means is that there is often at least one corresponding Autoconf
macro in the CVS/m4 directory and that bug fixes and enhancements to this code
should be sent to the GNULIB project and then reimported here after the GNULIB
developers approve and adopt the change.  Changes should not be made locally
without good reason!
This "contrib" directory is a place holder for code/scripts sent to me
by contributors around the world.  This README file will be kept
up-to-date from release to release.  BUT, we must point out that these
contributions are really, REALLY UNSUPPORTED.  In fact, we probably
don't even know what some of them really do.  We certainly do not
guarantee to have tried them, or ported them to work with this CVS
distribution.  If you have questions, your best bet is to contact the
original author, but you should not necessarily expect a reply, since
the author may not be available at the address given.

USE AT YOUR OWN RISK -- and all that stuff.

"Unsupported" also means that no one has volunteered to accept and check
in changes to this directory.  So submissions for new scripts to add
here are unlikely to be accepted.  Suggested changes to the existing
scripts here conceivably might, but that isn't clear either, unless of
course they come from the original author of the script.

If you have some software that works with CVS that you wish to offer it
is suggested that you make it available by FTP or HTTP and then announce
it on the info-cvs mailing list.

An attempt at a table of Contents for this directory:

	README		This file.

 	clmerge		A perl script to handle merge conflicts in GNU
			style ChangeLog files .
			Contributed by Tom Tromey <tromey@busco.lanl.gov>.

	cln_hist	A perl script to compress your
			$CVSROOT/CVSROOT/history file, as it can grow quite
			large after extended use.
			Contributed by David G. Grubbs <dgg@ksr.com>

	commit_prep	A perl script, to be combined with log_accum.pl, to
	log_accum	provide for a way to combine the individual log
			messages of a multi-directory "commit" into a
			single log message, and mail the result somewhere.
			Can also do other checks for $Id and that you are
			committing the correct revision of the file.
			Read the comments carefully.
			Contributed by David Hampton <hampton@cisco.com>.

	cvs2vendor	A shell script to move changes from a repository
			that was started without a vendor branch to one
			that has a vendor branch.
			Contributed by Greg A. Woods <woods@planix.com>.

	cvs_acls	A perl script that implements Access Control Lists
	cvs_acls.html	by using the "commitinfo" hook provided with the
			"cvs commit" command.
			Contributed by David G. Grubbs <dgg@ksr.com>.
			Enhanced by Peter Connolly <Peter.Connolly@cnet.com>.

	cvshelp.man	An introductory manual page written by Lowell Skoog
			<fluke!lowell@uunet.uu.net>.  It is most likely
			out-of-date relative to CVS 1.3, but still may be
			useful.

	debug_check_log	A shell script to help analyze sanity check failures.
			Contributed by Derek R. Price <derek@ximbiot.com>.

	descend		A shell script that can be used to recursively
	descend.man	descend through a directory.  In CVS 1.2, this was
			very useful, since many of the commands were not
			recursive.  In CVS 1.3 (and later), however, most of
			the commands are recursive.  However, this may still
			come in handy.
			Contributed by Lowell Skoog <fluke!lowell@uunet.uu.net>

	dirfns		A shar file which contains some code that might
			help your system support opendir/readdir/closedir,
			if it does not already.
			Copied from the C-News distribution.

	intro.doc	A user's view of what you need to know to get
			started with CVS.
			Contributed by <Steven.Pemberton@cwi.nl>.

	log		A perl script suitable for including in your
			$CVSROOT/CVSROOT/loginfo file for logging commit
			changes.  Includes the RCS revision of the change
			as part of the log.
			Contributed by Kevin Samborn <samborn@sunrise.com>.

	log_accum	See commit_prep.

	mfpipe		Another perl script for logging.  Allows you to
			pipe the log message to a file and/or send mail
			to some alias.
			Contributed by John Clyne <clyne@niwot.scd.ucar.edu>.

	pvcs2rcs	A perl script to convert a PVCS tree to an RCS tree.

	rcs-5.7-commitid.patch
			Patch to RCS 5.7 sources of 1995-06-16 to support
			the newphrase 'commitid' used by CVS and CVSNT.

	rcs-5.7-sameuserlocks.patch
			Patch to RCS 5.7 sources from 2003-10-24
			submitted to both help-rcs@gnu.org and Redhat.
			Included in many Redhat GNU/Linux distribution
			RPM files. A new -S switch to the co command
			will disallow the user to check out the same
			file twice.

	rcs-5.7-security.patch
			Patch to RCS 5.7 sources from 2001-01-05
			included in many GNU/Linux distributions as applied
			by Preston Brown <pbrown@redhat.com>
			tmpfile security patch from Olaf Kirch <okir@lst.de>.

	rcs-to-cvs	Script to import sources that may have been under
			RCS control already.
			Contributed by Per Cederqvist <ceder@lysator.liu.se>.

	rcs2log		A shell script to create a ChangeLog-format file
			given only a set of RCS files.
			Contributed by Paul Eggert <eggert@twinsun.com>.

	rcs2sccs	A shell script to convert simple RCS files into
			SCCS files, originally gleaned off the network
			somewhere (originally by "kenc") and modified by
			Jerry Jelinek <jerry@rmtc.Central.Sun.COM> and
			Brian Berliner <berliner@sun.com> to increase
			robustness and add support for one-level of branches.

	rcslock		A perl script that can be added to your commitinfo
			file that tries to determine if your RCS file is
			currently locked by someone else, as might be the
			case for a binary file.
			Contributed by John Rouillard <rouilj@cs.umb.edu>.

	sandbox_status
	sandbox_status.man
			Identifies files added, changed, or removed in a
			checked out CVS tree; also notices unknown files.
			Contributed by Lowell Skoog <fluke!lowell@uunet.uu.net>

	sccs2rcs	A C-shell script that can convert (some) SCCS files
			into RCS files, retaining the info contained in the
			SCCS file (like dates, author, and log message).
			Contributed by Ken Cox <kenstir@viewlogic.com>.

	verify_repo	A perl script to check an entire repository for
			corruption.
			Contributed by Donald Sharp <sharpd@cisco.com>.
Many of the macro files in this directory come from the GNULIB project
<http://savannah.gnu.org/projects/gnulib/>/<mailto:bug-gnulib@gnu.org>, and,
if they don't, they should.

What this means is that there is often at least one corresponding source file
in the CVS/lib directory and that bug fixes and enhancements to this code
should be sent to the GNULIB project and then reimported here after the GNULIB
developers approve and adopt the change.  Changes should not be made locally
without good reason!

CVS Access Control List Extension Patch

http://cvsacl.sourceforge.net/
sbaris@users.sourceforge.net

CVSACL is a patch for CVS. It adds two new subcommands
(acl & racl) to cvs for access control list management. It
provides advanced ACL definitions per modules, directories,
and files on branch/tag for remote cvs repository connections.
Execution of all CVS subcommands can be controlled with eight
different permissions.
ACL definitions works for only remote connections, local users
can access and modify repository, if unix file system permissions
allow. If you want all users to make remote connections to
repository, and not allow local users to access repository, you
have to set CVSServerRunAsUser keyword in aclconfig file
(explained below).
Still local users can use acl and racl subcommands to set
permissions on directories or files if they have acl admin rights (p)
on related directories or files.
So, in order to control all access to repository with this ACL
extension, you should use CVSServerRunAsUser keyword and force all
users to make remote connections. CVS repository administrator or
project managers have to use acl and racl subcommands to manage
permissions. But there is no gui client supporting these subcommands,
so you have to use cvs client itself either locally or remotely. 




Permission Types

- no access
  Command line character: n
  If a user given n permission, it is not allowed for any action on repository. 
- read
  Command line character: r
  r permission gives only read access on repository.
  With r permission you are allowed to run cvs subcommands: annotate,
  checkout, diff, export, log, rannotate, rdiff, rlog, status. 
- write
  Command line character: w
  w permission allows only cvs commit/checkin action.
  With w permission, you are not allowed to add/remove any file to/from
  repository, other permissions should be defines for that. 
- tag
  Command line character: t
  t permission allows cvs tag and rtag subcommands to run, so you may
  control tagging and untagging operations. t permission includes r
  permission, since without reading you can not tag/untag a file.
  However t permission does not include write permission, you can not
  commit a file with only t permission. 
- create
  Command line character: c
  c permission allows cvs add and import subcommands to run. To add or
  import a file/directory to repository, you have to given a c
  permission. Again, c permission does not include write permission,
  thus you may only add or import files, but you can not modify any
  existing file. After issuing add subcommand, you have to commit the file
  to complete adding. This commit subcommand is allowed because you are
  adding file and not modifying existing one. 
- delete
  Command line character: d
  d permission allows cvs remove command to run. To remove a file/directory
  from repository, d permission have to set. It does not include write
  permission, so you can not modify contents of an existing file on repository. 
- full access except admin rights
  Command line character: a
  a permission gives all access (above permissions) to repository, but it
  can not modify permissions. Only acl admins may modify the acl definitions. 
- acl admin
  Command line character: p
  p permission means that user is an acl admin, so it is allowed to make anything on repository. 


ACL Config Keywords
The administrative file aclconfig contains miscellaneous settings which
affect the behaviour of ACL extension. Currently defined keywords are:

UseCVSACL=value 
Use ACL definitions if set to yes. If you do not want to use ACLs for
some repositories in a patched CVS server, set this keyword to no. The default is no.

UseCVSACLDefaultPermissions=value 
Value can be any combination of valid permission types (w,r,t,c,d,t,a,p).
if there is no defined ACL and default permission in access file, or no
access file at all, this permissions are used. The default is p (admin rights),
if aclconfig file is created with cvs init. 

UseCVSGroups=value 
CVS does not have a CVSROOT/passwd file. However it can be created manually
(format should be same as /etc/group). If value set to yes, CVS checks for
groups in file $CVSROOT/CVSROOT/group The default value is no.

UseSystemGroups=value 
Group memberships for users are checked in file /etc/group, if value is set
to yes. The default value is no.

CVSACLFileLocation=value 
Originally access file is put under CVSROOT/CVSROOT, if you want a different
location, set value to a valid path. The default value is $CVSROOT/CVSROOT/access.

CVSGroupsFileLocation=value 
IF UseCVSGroups is set to yes, CVS looks for a group file under $CVSROOT/CVSROOT.
To use a different location for group file set value to a valid path to group.
The default value is $CVSROOT/CVSROOT/group.

UseSeparateACLFileForEachDir=value 
If value is set to yes, a separate ACL file (access) is created for each
directory in repository. If you have a really big repository
(directories>10,000 and files>100,000), performance may drop due to a big 
acl file, access. Setting the value to yes, may increase performance. Normally,
you will not need this. The default value is no.

StopAtFirstPermissionDenied=value
If StopAtFirstPermissionDenied is set to yes
operation will stop at first permission denied message.
e.g. when you send commit command for a directory, if you dont
have write permission for just one file under the directory, 
by default you will have a warning and commit will continue
on the other files. If you set this keyword to 'no', then 
commit operation will be stopped when inaccassable file found.
Default is no.

CVSServerRunAsUser=value 
Set CVSServerRunAsUser keyword to a valid system user.
When a user make a remote connection to CVS, after successfull authentication
cvs process switch to run as that user, or defined system user in
$CVSROOT/CVSROOT/passwd. So, you also have to set unix file permissions accordingly.
A better solution:
Add a user and group such as both cvsadm.
Set CVSServerRunAsUser keyword to cvsadm.
Change unix file system permissions for your repository,
make cvsadm user and group owner, and read,write,execute permissions and setgid.
(chown cvsadm -R /path/to/your/repository)
(chgrp cvsadm -R /path/to/your/repository)
(chmod 2770 -R /path/to/your/repository)
Add yourself to cvsadm group (since you are ACL administrator).
Therefore, only users making remote connections will have access to repository
if you give rights. Local users can not access to repository via a cvs client or directly.


Command Line Usage Information
acl command is used on checked out files or directories. racl command is
used on repository without a working copy. Usage information can be obtained
with standard cvs --help command.
Output of cvs --help acl and cvs --help racl: 

Usage: cvs racl [user||group:permissions] [-Rl] [-r tag]
        -R      Process directories recursively.
        -r rev  Existing revision/tag.
        -l      List defined ACLs.

Usage: cvs acl [user||group:permissions] [-Rl] [-r tag]
        -R      Process directories recursively.
        -r rev  Existing revision/tag.
        -l      List defined ACLs.

NOTICE: there is no more -d -f options for directory and file, acl/racl 
subcommands works just like other cvs subcommands.

You may directly set permissions for a user or group or add/remove
permissions with + and - signs to/from existing permissions.
If you do not give the branch/tag information, default value of HEAD
(main branch) will be used. You have to give branch/tag name with -r option.
You may type ALL for branch/tag field.

While checking for permissions, it goes thorough the list below. So the highest
significant permission is the first item in list.

- permissions assigned to username for specific directory or file. 
- permissions assigned to group name for specific directory or file. 
- permissions as defaults for specific directory or file. 
- permissions assigned to parent folders (inherits from the first parent
  which permissions are assigned).
- permissions as repository defaults. 
- permissions in aclconfig file. 




Examples
     /cvs/
      |
      |
      +--projectA/
      |	       |
      |        +---CVSROOT/
      |        |
      |        +---lib/
      |        |     |
      |        |     +---gnulib/
      |        |     |
      |        |     +---zlib/
      |        |
      |        +---src/
      |        |     |
      |        |     +---main.c
      |        |     |
      |        |     +---server.c
      |        |     |
      |        |     +---client.c
      |        |
      |        +---gui/
      |
      +--projectB/
We have above directory structure for a cvs repository, and no defined permissions.

Setting main default permissions:

$ cvs -d /cvs/projectA racl cvsadmin:p -r ALL ALL
$ cvs -d /cvs/projectA racl ALL:r -r ALL ALL
User cvsadmin will be an acl admin, and all other users will have only read
rights on all branches/tags in projectA repository. This is the default acl
definition and it overwrites default permissions in $CVSROOT/CVSROOT/aclconfig file.

$ cvs -d /cvs/projectA racl ALL:r -r ALL ALL
$ cvs -d /cvs/projectA racl ALL:n -r ALL gui
After executing these two commands, all users will have read access on all
directories and files except gui directory. Everyone will be denied to access to gui
directory becase no access, n, permissions is set.

Setting permissions directly on a file or directory:

$ cvs -d /cvs/projectA racl userX:wcd lib
$ cvs -d /cvs/projectA racl group1:w lib
First command will set write, create, and delete permissions for userX on directory
lib with branch HEAD (since no branch/tag information given, branch defaults to HEAD).
Second command will set only write permission for group1 on directory lib with branch HEAD.
Members of group1 will have only commit rights on lib directory, branch HEAD, they can
not add or remove any file, just modify existing files.
If userX is also a member of group1, userX will have write, create, and delete permissions
because it is specifically given these permissions.

$ cvs -d /cvs/projectA racl userY:wcd -r develStream lib
$ cvs -d /cvs/projectA racl userY:r -r integStream lib
These commands will give wcd permissions to userY on lib directory with tag develstream,
and r permissions on lib directory with tag integStream.

$ cvs -d /cvs/projectA racl userZ:wcd src
$ cvs -d /cvs/projectA racl userZ:r src/main.c
First command will give wcd permissions to userZ on src directory, but only read
permission on file main.c in src directory.

Using + and - signs to set permissions on a file or directory:

$ cvs -d /cvs/projectA racl userZ:+t src
$ cvs -d /cvs/projectA racl userZ:-cd src
$ cvs -d /cvs/projectA racl userZ:-wt src
Before the first command, userZ has wcd permissions on src directory, after issuing
command it will have wcdt permissions. Tag permission will be added. UserZ has wcdt
permissions, and we execute the second command to remove create and delete permissions.
So userZ has wt permissions. In the last command we also remove wt permissions, finally
userZ has no defined permissions left, and it will use the default permissions if set.

Listing permissions on a file or directory:

$ cvs -d /cvs/projectA racl -l src
$ cvs -d /cvs/projectA racl -l src
$ cvs -d /cvs/projectA racl -l src/main.c

First command will list the permissions for src directory.
Example output:
d src HEAD | userX:wcd group1:r | defaults:r
userX and group1 has assigned permissions, all other users will have default
permissions, which is only read.

Second command will list the permissions for files in src directory.
Example output:
f src/main.c HEAD | userX:wcd group1:r | defaults:r
f src/server.c HEAD | userX:wcd group1:r | defaults:r
f src/client.c HEAD | userX:wcd group1:r | defaults:r

Third command will list the permissions for main.c file in src directory.
Example output:
f src/main.c HEAD | userX:wcd group1:r | defaults:r


The subdirectories of this directory contain translations of the Cederqvist
(CVS manual).  For now, you can build these files by copying them into place
of the cvs.texinfo manual in the `doc' directory and running `make doc',
`make pdf', `make info', or whatever would have built the regular manual in the
form you were looking for.  If anyone would like to contribute `Makefile.am's
for the new directories, we would love to see them.

P.S.  I'm not sure about the organizational scheme I am using for these files.
If anyone would like to suggest another and point me to a few example projects
which contain translations of documentation, I would appreciate it.  I've
never dealt with translations before.  (Derek Price: <derek@ximbiot.com>.)
Hi,

To just generate the manual in portuguese, download the file
cvs.texinfo from the actual directory and compile the manual with:

makeinfo --html cvs.texinfo

If you what to help with the translation, send a message to
bug-cvs@nongnu.org and told us what peace you want to translate.

Some policy:

* commit just a compileable version

* don't erase original text. comment original text with '@c <en>'.

* don't translate comments in english (starting with '@c '. leave them
  untouched

* don't translate examples. let's concentrate in finish the work
  first.
The files in this directory come from the GNU diffutils project
<http://savannah.gnu.org/projects/diffutils/>/<mailto:bug-gnu-utils@gnu.org>,
and, if they don't, they should.

What this means is that bug fixes and enhancements to this code should be sent
to the diffutils project and then reimported here after the diffutils
developers approve and adopt the change.  Changes should not be made locally
without good reason!
This is the standalone distribution of GNU malloc.
GNU malloc is part of the GNU C Library, but is also distributed separately.

If you find bugs in GNU malloc, send reports to bug-glibc@prep.ai.mit.edu.

GNU malloc is free software.  See the file COPYING.LIB for copying conditions.

The makefile builds libmalloc.a and gmalloc.o.  If you are using GNU malloc
to replace your system's existing malloc package, it is important to make
sure you get all GNU functions, not some of the GNU functions and some from
the system library.  gmalloc.o has all the functions in one file, so using
that will make sure you don't accidentally mix the two malloc packages.
This directory contains sources and documentation for RCS.

RCS, the Revision Control System, manages multiple revisions of files.
RCS can store, retrieve, log, identify, and merge revisions.
It is useful for files that are revised frequently,
e.g. programs, documentation, graphics, and papers.

See the following files and directories for more information.

	COPYING - copying conditions
	CREDITS - authorship information
	INSTALL - generic installation instructions
	INSTALL.RCS - installation instructions specific to RCS
	NEWS - recent changes, and possible future changes
	REFS - references to RCS and related free software and documentation
	man - sources for manual page entries
	rcs.ms - troff source for the paper `RCS--A System for Version Control'
	rcs_func.ms - brief overview
	src - sources for programs
	src/version.c - the version number of this release

If you lack troff, you can get GNU groff to format the documentation.

Report problems and direct all questions to:
 
	 rcs-bugs@cs.purdue.edu

Id: README,v 5.24 1995/06/16 06:19:24 eggert Exp 
README for GNU DIFF

	Copyright (C) 1992, 1998, 2001, 2002 Free Software Foundation, Inc.

	This file is part of GNU Diffutils.

	This program is free software; you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation; either version 2, or (at your option)
	any later version.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY; without even the implied warranty of
	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
	GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program; see the file COPYING.  If not, write to
	the Free Software Foundation, Inc., 59 Temple Place - Suite 330,
	Boston, MA 02111-1307, USA.


This directory contains the GNU diff, diff3, sdiff, and cmp utilities.
Their features are a superset of the Unix features and they are
significantly faster.

Please see the file COPYING for copying conditions.

Please see the file doc/diff.texi (or doc/diff.info) for documentation
that can be printed with TeX, or read with the `info' program or with
Emacs's `M-x info'.  There are no man pages.

See the file INSTALL for generic compilation and installation instructions,
and the file INSTALLME for instructions specific to GNU diff.

See the file ABOUT-NLS for notes about translations.

Please report bugs to <bug-gnu-utils@gnu.org>.
Many of the files in this directory are taken from the fileutils,
shellutils, and textutils packages.

These files are used by a program called aclocal (part of the GNU automake
package).  aclocal uses these files to create aclocal.m4 which is in turn
used by autoconf to create the configure script at the top level in
this distribution.

The Makefile.am file in this directory is automatically generated
from the template file, Makefile.am.in.
This tree contains the LVM2 and device-mapper tools and libraries.

For more information about LVM2 read the changelog in the WHATS_NEW file.
Installation instructions are in INSTALL.

There is no warranty - see COPYING and COPYING.LIB.

Tarballs are available from:
  ftp://sources.redhat.com/pub/lvm2/

To access the CVS tree use:
  cvs -d :pserver:cvs@sources.redhat.com:/cvs/lvm2 login
  CVS password: cvs
  cvs -d :pserver:cvs@sources.redhat.com:/cvs/lvm2 co LVM2

Mailing list for general discussion related to LVM2:
  linux-lvm@redhat.com
  Subscribe from https://www.redhat.com/mailman/listinfo/linux-lvm

Mailing list for LVM2 development, patches and commits:
  lvm-devel@redhat.com
  Subscribe from https://www.redhat.com/mailman/listinfo/linux-lvm

Mailing list for device-mapper development, including kernel patches
and multipath-tools:
  dm-devel@redhat.com
  Subscribe from https://www.redhat.com/mailman/listinfo/dm-devel
http://poochiereds.net/svn/lvm2/

This is the lvm2create_initrd script written by Miguel Cabeca, with some small
modifications by myself.

Here are some other requirements and tips for using it:

1) this script uses busybox on the initrd image, hence busybox needs to be
installed when you create your initrd.

2) Make sure /etc/lvm/lvm.conf is set up correctly before running this. In
particular, if you're using LVM on RAID, make sure that you have a filter that
excludes the RAID component devices (this may not be necessary with the latest
patch by Luca Berra, but it doesn't hurt).

3) This initrd image does not support modules. If you need to plug in any
kernel modules during the initrd phase, then you'll need to hand-modify the
image.

4) The generated initrd image supports an 'lvm2rescue' mode as well. If you add
the parameter 'lvmrescue' on the kernel command line, it will run a shell at
the end of the initrd 'init' script. This can be helpful when trying to fix a
corrupt root volume or root LVM2 volume group.

5) No userspace md tools are installed, so if you're using LVM on RAID, then
you'll probably want to mark your RAID partitions as type 'fd' so that the
kernel will start them automagically (or hand-modify the image).

6) I'm not sure if devfs will work with this or not. udev, however does work,
and is recommended. Because the dm-* devices use dynamically allocated major
and minor numbers, kernel upgrades and the like can renumber your devices. To
fix this, you need to run a 'vgscan --mknodes' prior to fscking and mounting
your rootfs. Doing this with a static /dev creates a problem though -- you
will be modifying the root filesystem before it has been fsck'ed. udev gets
around this by mounting a ramdisk over /dev, but you'll probably need to add
a startup script that creates devices in /dev. The lvm2udev script in this
directory is an example of such a beast.

--
Jeffrey Layton <jtlayton@poochiereds.net>
                                                            -*-indented-text-*-

GNU make can utilize the Customs library, distributed with Pmake, to
provide builds distributed across multiple hosts.

In order to utilize this capability, you must first download and build
the Customs library.  It is contained in the Pmake distribution, which
can be obtained at:

  ftp://ftp.icsi.berkeley.edu/pub/ai/stolcke/software/

This integration was tested (superficially) with Pmake 2.1.33.


BUILDING CUSTOMS
----------------

First, build pmake and Customs.  You need to build pmake first, because
Customs require pmake to build.  Unfortunately, this is not trivial;
please see the pmake and Customs documentation for details.  The best
place to look for instructions is in the pmake-2.1.33/INSTALL file.

Note that the 2.1.33 Pmake distribution comes with a set of patches to
GNU make, distributed in the pmake-2.1.33/etc/gnumake/ directory.  These
patches are based on GNU make 3.75 (there are patches for earlier
versions of GNU make, also).  The parts of this patchfile which relate
directly to Customs support have already been incorporated into this
version of GNU make, so you should _NOT_ apply the patch file.

However, there are a few non-Customs specific (as far as I could tell)
changes here which are not incorporated (for example, the modification
to try expanding -lfoo to libfoo.so).  If you rely on these changes
you'll need to re-apply them by hand.

Install the Customs library and header files according to the
documentation.  You should also install the man pages (contrary to
comments in the documentation, they weren't installed automatically for
me; I had to cd to the ``pmake-2.1.33/doc'' directory and run ``pmake
install'' there directly).


BUILDING GNU MAKE
-----------------

Once you've installed Customs, you can build GNU make to use it.  When
configuring GNU make, merely use the ``--with-customs=DIR'' option.
Provide the directory containing the ``lib'' and ``include/customs''
subdirectories as DIR.  For example, if you installed the customs
library in /usr/local/lib and the headers in /usr/local/include/customs,
then you'd pass ``--with-customs=/usr/local'' as an option to configure.

Run make (or use build.sh) normally to build GNU make as described in
the INSTALL file.

See the documentation for Customs for information on starting and
configuring Customs.


INVOKING CUSTOMS-IZED GNU MAKE
-----------------------------

One thing you should be aware of is that the default build environment
for Customs requires root permissions.  Practically, this means that GNU
make must be installed setuid root to use Customs.

If you don't want to do this, you can build Customs such that root
permissions are not necessary.  Andreas Stolcke <stolcke@speech.sri.com>
writes:

 > pmake, gnumake or any other customs client program is not required to
 > be suid root if customs was compiled WITHOUT the USE_RESERVED_PORTS
 > option in customs/config.h.  Make sure the "customs" service in
 > /etc/services is defined accordingly (port 8231 instead of 1001).

 > Not using USE_RESERVED_PORTS means that a user with programming
 > skills could impersonate another user by writing a fake customs
 > client that pretends to be someone other than himself.  See the
 > discussion in etc/SECURITY.


PROBLEMS
--------

SunOS 4.1.x:
  The customs/sprite.h header file #includes the <malloc.h> header
  files; this conflicts with GNU make's configuration so you'll get a
  compile error if you use GCC (or any other ANSI-capable C compiler).

  I commented out the #include in sprite.h:107:

    #if defined(sun) || defined(ultrix) || defined(hpux) || defined(sgi)
    /* #include <malloc.h> */
    #else

  YMMV.


-------------------------------------------------------------------------------
Copyright (C) 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006
Free Software Foundation, Inc.
This file is part of GNU Make.

GNU Make is free software; you can redistribute it and/or modify it under the
terms of the GNU General Public License as published by the Free Software
Foundation; either version 2, or (at your option) any later version.

GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
GNU Make; see the file COPYING.  If not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Port of GNU Make to 32-bit protected mode on MSDOS and MS-Windows.

Builds with DJGPP v2 port of GNU C/C++ compiler and utilities.


New (since 3.74) DOS-specific features:

   1. Supports long filenames when run from DOS box on Windows 9x.

   2. Supports both stock DOS COMMAND.COM and Unix-style shells
      (details in ``Notes'' below).

   3. Supports DOS drive letters in dependencies and pattern rules.

   4. Better support for DOS-style backslashes in pathnames (but see
      ``Notes'' below).

   5. The $(shell) built-in can run arbitrary complex commands,
      including pipes and redirection, even when COMMAND.COM is your
      shell.

   6. Can be built without floating-point code (see below).

   7. Supports signals in child programs and restores the original
      directory if the child was interrupted.

   8. Can be built without (a previous version of) Make.

   9. The build process requires only standard tools.  (Optional
      targets like "install:" and "clean:" still need additional
      programs, though, see below.)

  10. Beginning with v3.78, the test suite works in the DJGPP
      environment (requires Perl and auxiliary tools; see below).


To install a binary distribution:

   Simply unzip the makNNNb.zip file (where NNN is the version number)
   preserving the directory structure (-d switch if you use PKUNZIP).
   If you are installing Make on Windows 9X or Windows 2000, use an
   unzip program that supports long filenames in zip files.  After
   unzipping, make sure the directory with make.exe is on your PATH,
   and that's all you need to use Make.


To build from sources:

   1. Unzip the archive, preserving the directory structure (-d switch
      if you use PKUNZIP).  If you build Make on Windows 9X or Windows
      2000, use an unzip program that supports long filenames in zip
      files.

      If you are unpacking an official GNU source distribution, use
      either DJTAR (which is part of the DJGPP development
      environment), or the DJGPP port of GNU Tar.

   2. Invoke the `configure.bat' batch file.

      If you are building Make in-place, i.e. in the same directory
      where its sources are kept, just type "configure.bat" and press
      [Enter].  Otherwise, you need to supply the path to the source
      directory as an argument to the batch file, like this:

	c:\djgpp\gnu\make-3.81\configure.bat c:/djgpp/gnu/make-3.81

      Note the forward slashes in the source path argument: you MUST
      use them here.

   3. If configure.bat doesn't find a working Make, it will suggest to
      use the `dosbuild.bat' batch file to build Make.  Either do as it
      suggests or install another Make program (a pre-compiled binary
      should be available from the usual DJGPP sites) and rerun
      configure.bat.

   4. If you will need to run Make on machines without an FPU, you
      might consider building a version of Make which doesn't issue
      floating-point instructions (they don't help much on MSDOS
      anyway).  To this end, edit the Makefile created by
      configure.bat and add -DNO_FLOAT to the value of CPPFLAGS.

   5. Invoke Make.

      If you are building from outside of the source directory, you
      need to tell Make where the sources are, like this:

	        make srcdir=c:/djgpp/gnu/make-3.81

      (configure.bat will tell you this when it finishes).  You MUST
      use a full, not relative, name of the source directory here, or
      else Make might fail.

   6. After Make finishes, if you have a Unix-style shell installed,
      you can use the `install' target to install the package.  You
      will also need GNU Fileutils and GNU Sed for this (they should
      be available from the DJGPP sites).

      By default, GNU make will install into your DJGPP installation
      area.  If you wish to use a different directory, override the
      DESTDIR variable when invoking "make install", like this:

		make install DESTDIR=c:/other/dir

      This causes the make executable to be placed in c:/other/dir/bin,
      the man pages in c:/other/dir/man, etc.

      Without a Unix-style shell, you will have to install programs
      and the docs manually.  Copy make.exe to a directory on your
      PATH, make.i* info files to your Info directory, and update the
      file `dir' in your Info directory by adding the following item
      to the main menu:

	* Make: (make.info).           The GNU make utility.

      If you have the `install-info' program (from the GNU Texinfo
      package), it will do that for you if you invoke it like this:

	install-info --info-dir=c:/djgpp/info c:/djgpp/info/make.info

      (If your Info directory is other than C:\DJGPP\INFO, change this
      command accordingly.)

   7. The `clean' targets also require Unix-style shell, and GNU Sed
      and `rm' programs (the latter from Fileutils).

   8. To run the test suite, type "make check".  This requires a Unix
      shell (I used the DJGPP port of Bash 2.03), Perl, Sed, Fileutils
      and Sh-utils.


Notes:
-----

   1. The shell issue.

      This is probably the most significant improvement, first
      introduced in the port of GNU Make 3.75.

      The original behavior of GNU Make is to invoke commands
      directly, as long as they don't include characters special to
      the shell or internal shell commands, because that is faster.
      When shell features like redirection or filename wildcards are
      involved, Make calls the shell.

      This port supports both DOS shells (the stock COMMAND.COM and its
      4DOS/NDOS replacements), and Unix-style shells (tested with the
      venerable Stewartson's `ms_sh' 2.3 and the DJGPP port of `bash' by
      Daisuke Aoyama <jack@st.rim.or.jp>).

      When the $SHELL variable points to a Unix-style shell, Make
      works just like you'd expect on Unix, calling the shell for any
      command that involves characters special to the shell or
      internal shell commands.  The only difference is that, since
      there is no standard way to pass command lines longer than the
      infamous DOS 126-character limit, this port of Make writes the
      command line to a temporary disk file and then invokes the shell
      on that file.

      If $SHELL points to a DOS-style shell, however, Make will not
      call it automatically, as it does with Unix shells.  Stock
      COMMAND.COM is too dumb and would unnecessarily limit the
      functionality of Make.  For example, you would not be able to
      use long command lines in commands that use redirection or
      pipes.  Therefore, when presented with a DOS shell, this port of
      Make will emulate most of the shell functionality, like
      redirection and pipes, and shall only call the shell when a
      batch file or a command internal to the shell is invoked.  (Even
      when a command is an internal shell command, Make will first
      search the $PATH for it, so that if a Makefile calls `mkdir',
      you can install, say, a port of GNU `mkdir' and have it called
      in that case.)

      The key to all this is the extended functionality of `spawn' and
      `system' functions from the DJGPP library; this port just calls
      `system' where it would invoke the shell on Unix.  The most
      important aspect of these functions is that they use a special
      mechanism to pass long (up to 16KB) command lines to DJGPP
      programs.  In addition, `system' emulates some internal
      commands, like `cd' (so that you can now use forward slashes
      with it, and can also change the drive if the directory is on
      another drive).  Another aspect worth mentioning is that you can
      call Unix shell scripts directly, provided that the shell whose
      name is mentioned on the first line of the script is installed
      anywhere along the $PATH.  It is impossible to tell here
      everything about these functions; refer to the DJGPP library
      reference for more details.

      The $(shell) built-in is implemented in this port by calling
      `popen'.  Since `popen' calls `system', the above considerations
      are valid for $(shell) as well.  In particular, you can put
      arbitrary complex commands, including pipes and redirection,
      inside $(shell), which is in many cases a valid substitute for
      the Unix-style command substitution (`command`) feature.


   2. "SHELL=/bin/sh" -- or is it?

      Many Unix Makefiles include a line which sets the SHELL, for
      those versions of Make which don't have this as the default.
      Since many DOS systems don't have `sh' installed (in fact, most
      of them don't even have a `/bin' directory), this port takes
      such directives with a grain of salt.  It will only honor such a
      directive if the basename of the shell name (like `sh' in the
      above example) can indeed be found in the directory that is
      mentioned in the SHELL= line (`/bin' in the above example), or
      in the current working directory, or anywhere on the $PATH (in
      that order).  If the basename doesn't include a filename
      extension, Make will look for any known extension that indicates
      an executable file (.exe, .com, .bat, .btm, .sh, and even .sed
      and .pl).  If any such file is found, then $SHELL will be
      defined to the exact pathname of that file, and that shell will
      hence be used for the rest of processing.  But if the named
      shell is *not* found, the line which sets it will be effectively
      ignored, leaving the value of $SHELL as it was before.  Since a
      lot of decisions that this port makes depend on the gender of
      the shell, I feel it doesn't make any sense to tailor Make's
      behavior to a shell which is nowhere to be found.

      Note that the above special handling of "SHELL=" only happens
      for Makefiles; if you set $SHELL in the environment or on the
      Make command line, you are expected to give the complete
      pathname of the shell, including the filename extension.

      The default value of $SHELL is computed as on Unix (see the Make
      manual for details), except that if $SHELL is not defined in the
      environment, $COMSPEC is used.  Also, if an environment variable
      named $MAKESHELL is defined, it takes precedence over both
      $COMSPEC and $SHELL.  Note that, unlike Unix, $SHELL in the
      environment *is* used to set the shell (since on MSDOS, it's
      unlikely that the interactive shell will not be suitable for
      Makefile processing).

      The bottom line is that you can now write Makefiles where some
      of the targets require a real (i.e. Unix-like) shell, which will
      nevertheless work when such shell is not available (provided, of
      course, that the commands which should always work, don't
      require such a shell).  More important, you can convert Unix
      Makefiles to MSDOS and leave the line which sets the shell
      intact, so that people who do have Unixy shell could use it for
      targets which aren't converted to DOS (like `install' and
      `uninstall', for example).


   3. Default directories.

      GNU Make knows about standard directories where it searches for
      library and include files mentioned in the Makefile.  Since
      MSDOS machines don't have standard places for these, this port
      will search ${DJDIR}/lib and ${DJDIR}/include respectively.
      $DJDIR is defined automatically by the DJGPP startup code as the
      root of the DJGPP installation tree (unless you've tampered with
      the DJGPP.ENV file).  This should provide reasonable default
      values, unless you moved parts of DJGPP to other directories.


   4. Letter-case in filenames.

      If you run Make on Windows 9x, you should be aware of the
      letter-case issue.  Make is internally case-sensitive, but all
      file operations are case-insensitive on Windows 9x, so
      e.g. files `FAQ', `faq' and `Faq' all refer to the same file, as
      far as Windows is concerned.  The underlying DJGPP C library
      functions honor the letter-case of the filenames they get from
      the OS, except that by default, they down-case 8+3 DOS filenames
      which are stored in upper case in the directory and would break
      many Makefiles otherwise.  (The details of which filenames are
      converted to lower case are explained in the DJGPP libc docs,
      under the `_preserve_fncase' and `_lfn_gen_short_fname'
      functions, but as a thumb rule, any filename that is stored in
      upper case in the directory, is a valid DOS 8+3 filename and
      doesn't include characters invalid on MSDOS FAT filesystems,
      will be automatically down-cased.)  User reports that I have
      indicate that this default behavior is generally what you'd
      expect; however, your input is most welcome.

      In any case, if you hit a situation where you must force Make to
      get the 8+3 DOS filenames in upper case, set FNCASE=y in the
      environment or in the Makefile.


   5. DOS-style pathnames.

      There are a lot of places throughout the program sources which
      make implicit assumptions about the pathname syntax.  In
      particular, the directories are assumed to be separated by `/',
      and any pathname which doesn't begin with a `/' is assumed to be
      relative to the current directory.  This port attempts to
      support DOS-style pathnames which might include the drive letter
      and use backslashes instead of forward slashes.  However, this
      support is not complete; I feel that pursuing this support too
      far might break some more important features, particularly if
      you use a Unix-style shell (where a backslash is a quote
      character).  I only consider support of backslashes desirable
      because some Makefiles invoke non-DJGPP programs which don't
      understand forward slashes.  A notable example of such programs
      is the standard programs which come with MSDOS.  Otherwise, you
      are advised to stay away from backslashes whenever possible.  In
      particular, filename globbing won't work on pathnames with
      backslashes, because the GNU `glob' library doesn't support them
      (backslash is special in filename wildcards, and I didn't want
      to break that).

      One feature which *does* work with backslashes is the filename-
      related built-in functions such as $(dir), $(notdir), etc.
      Drive letters in pathnames are also fully supported.



Bug reports:
-----------

   Bugs that are clearly related to the MSDOS/DJGPP port should be
   reported first on the comp.os.msdos.djgpp news group (if you cannot
   post to Usenet groups, write to the DJGPP mailing list,
   <djgpp@delorie.com>, which is an email gateway into the above news
   group).  For other bugs, please follow the procedure explained in
   the "Bugs" chapter of the Info docs.  If you don't have an Info
   reader, look up that chapter in the `make.i1' file with any text
   browser/editor.


   Enjoy,
			Eli Zaretskii <eliz@is.elta.co.il>


-------------------------------------------------------------------------------
Copyright (C) 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005,
2006 Free Software Foundation, Inc.
This file is part of GNU Make.

GNU Make is free software; you can redistribute it and/or modify it under the
terms of the GNU General Public License as published by the Free Software
Foundation; either version 2, or (at your option) any later version.

GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
GNU Make; see the file COPYING.  If not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Short: Port of GNU make with SAS/C (no ixemul.library required)
Author: GNU, Amiga port by Aaron "Optimizer" Digulla
Uploader: Aaron "Optimizer" Digulla (digulla@fh-konstanz.de)
Type: dev/c

This is a pure Amiga port of GNU make. It needs no extra libraries or
anything. It has the following features (in addition to any features of
GNU make):

- Runs Amiga-Commands with SystemTags() (Execute)
- Can run multi-line statements
- Allows to use Device-Names in targets:

	c:make : make.o

    is ok. To distinguish between device-names and target : or ::, MAKE
    looks for spaces. If there are any around :, it's taken as a target
    delimiter, if there are none, it's taken as the name of a device. Note
    that "make:make.o" tries to create "make.o" on the device "make:".
- Replaces @@ by a newline in any command line:

	if exists make @@\
	    delete make.bak quiet @@\
	    rename make make.bak @@\
	endif @@\
	$(CC) Link Make.o To make

    works. Note that the @@ must stand alone (ie. "make@@\" is illegal).
    Also be carefull that there is a space after the "\" (ie, at the
    beginning of the next line).
- Can be made resident to save space and time
- Amiga specific wildcards can be used in $(wildcard ...)

BUGS:
- The line

    dummy.h : src/*.c

tries to make dummy.h from "src/*.c" (ie. no wildcard-expansion takes
place). You have to use "$(wildcard src/*.c)" instead.

COMPILING FROM SCRATCH
----------------------

To recompile, you need SAS/C 6.51. make itself is not neccessary, there
is an smakefile.

1. Copy config.ami to config.h
2. If you use make to compie, copy Makefile.ami to Makefile and
    glob/Makefile.ami to glob/Makefile. Copy make into the current
    directory.

3. Run smake/make

INSTALLATION

Copy make somewhere in your search path (eg. sc:c or sc:bin).
If you plan to use recursive makes, install make resident:

    Resident make Add


-------------------------------------------------------------------------------
Copyright (C) 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004,
2005, 2006 Free Software Foundation, Inc.
This file is part of GNU Make.

GNU Make is free software; you can redistribute it and/or modify it under the
terms of the GNU General Public License as published by the Free Software
Foundation; either version 2, or (at your option) any later version.

GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
GNU Make; see the file COPYING.  If not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Port of GNU make to OS/2.

Features of GNU make that do not work under OS/2:
  - remote job execution
  - dynamic load balancing


Special features of the OS/2 version:

Due to the fact that some people might want to use sh syntax in
Makefiles while others might want to use OS/2's native shell cmd.exe,
GNU make supports both shell types. The following list defines the order
that is used to determine the shell:

 1. The shell specified by the environment variable MAKESHELL.
 2. The shell specified by the SHELL variable within a Makefile. As on
    Unix, SHELL is NOT taken from the environment.
 3. The shell specified by the COMSPEC environment variable.
 4. The shell specified by the OS2_SHELL environment variable.
 5. If none of the above is defined /bin/sh is used as default.  This
    happens e.g. in the make testsuite.

Note: - Points 3 and 4 can be turned off at compile time by adding
        -DNO_CMD_DEFAULT to the CPPFLAGS.
      - DOS support is not tested for EMX and therefore might not work.
      - The UNIXROOT environment variable is supported to find /bin/sh
        if it is not on the current drive.


COMPILATION OF GNU MAKE FOR OS/2:

I. ***** SPECIAL OPTIONS *****

 - At compile time you can turn off that cmd is used as default shell
   (but only /bin/sh). Simply set CPPFLAGS="-DNO_CMD_DEFAULT" and make
   will not use cmd unless you cause it to do so by setting MAKESHELL to
   cmd or by specifying SHELL=cmd in your Makefile.

 - At compile time you can set CPPFLAGS="-DNO_CHDIR2" to turn off that
   GNU make prints drive letters. This is necessary if you want to run
   the testsuite.


II. ***** REQUIREMENTS FOR THE COMPILATION *****

A standard Unix like build environment:

 - sh compatible shell (ksh, bash, ash, but tested only with pdksh 5.2.14
   release 2)
   If you use pdksh it is recommended to update to 5.2.14 release 2. Older
   versions may not work! You can get this version at
   http://www.math.ohio-state.edu/~ilya/software/os2/pdksh-5.2.14-bin-2.zip
 - GNU file utilities (make sure that install.exe from the file utilities
   is in front of your PATH before X:\OS2\INSTALL\INSTALL.EXE. I recommend
   also to change the filename to ginstall.exe instead of install.exe
   to avoid confusion with X:\OS2\INSTALL\INSTALL.EXE)
 - GNU shell utilities
 - GNU text utilities
 - gawk
 - grep
 - sed
 - GNU make 3.79.1 (special OS/2 patched version) or higher
 - perl 5.005 or higher
 - GNU texinfo (you can use 3.1 (gnuinfo.zip), but I recommend 4.0)

If you want to recreate the configuration files (developers only!)
you need also: GNU m4 1.4, autoconf 2.59, automake 1.8.2 (or compatible)


III. ***** COMPILATION AND INSTALLATION *****

 a) ** Developers only - Everyone else should skip this section **
    To recreate the configuration files use:

      export EMXSHELL=ksh
      aclocal -I config
      automake
      autoconf
      autoheader


b) Installation into x:/usr

   Note: Although it is possible to compile make using "./configure",
         "make", "make install" this is not recommended.  In particular,
         you must ALWAYS use LDFLAGS="-Zstack 0x8000" because the default
         stack size is far to small and make will not work properly!

Recommended environment variables and installation options:

    export ac_executable_extensions=".exe"
    export CPPFLAGS="-D__ST_MT_ERRNO__"
    export CFLAGS="-O2 -Zomf -Zmt"
    export LDFLAGS="-Zcrtdll -Zlinker /exepack:2 -Zlinker /pm:vio -Zstack 0x8000"
    export RANLIB="echo"
    ./configure --prefix=x:/usr --infodir=x:/usr/share/info --mandir=x:/usr/share/man --without-included-gettext
    make AR=emxomfar
    make install

Note: If you use gcc 2.9.x or higher I recommend to set also LIBS="-lgcc"

Note: You can add -DNO_CMD_DEFAULT and -DNO_CHDIR2 to CPPFLAGS.
      See section I. for details.


IV. ***** NLS support *****

GNU make has NLS (National Language Support), with the following
caveats:

 a) It will only work with GNU gettext, and
 b) GNU gettext support is not included in the GNU make package.

Therefore, if you wish to enable the internationalization features of
GNU make you must install GNU gettext on your system before configuring
GNU make.

You can choose the languages to be installed. To install support for
English, German and French only enter:

  export LINGUAS="en de fr"

If you don't specify LINGUAS all languages are installed.

If you don't want NLS support (English only) use the option
--disable-nls for the configure script.  Note if GNU gettext is not
installed then NLS will not be enabled regardless of this flag.


V. ***** Running the make test suite *****

To run the included make test suite you have to set

  CPPFLAGS="-D__ST_MT_ERRNO__ -DNO_CMD_DEFAULT -DNO_CHDIR2"

before you compile make. This is due to some restrictions of the
testsuite itself. -DNO_CMD_DEFAULT causes make to use /bin/sh as default
shell in every case. Normally you could simply set MAKESHELL="/bin/sh"
to do this but the testsuite ignores the environment. -DNO_CHDIR2 causes
make not to use drive letters for directory names (i.e. _chdir2() and
_getcwd2() are NOT used).  The testsuite interpretes the whole output of
make, especially statements like make[1]: Entering directory
`C:/somewhere/make-3.79.1/tests' where the testsuite does not expect the
drive letter. This would be interpreted as an error even if there is
none.

To run the testsuite do the following:

  export CPPFLAGS="-D__ST_MT_ERRNO__ -DNO_CMD_DEFAULT -DNO_CHDIR2"
  export CFLAGS="-Zomf -O2 -Zmt"
  export LDFLAGS="-Zcrtdll -s -Zlinker /exepack:2 -Zlinker /pm:vio -Zstack 0x8000"
  export RANLIB="echo"
  ./configure --prefix=x:/usr --disable-nls
  make AR=emxomfar
  make check

All tests should work fine with the exception of "default_names" which
is because OS/2 file systems are not case sensitive ("makefile" and
"Makefile" specify the same file).


-------------------------------------------------------------------------------
Copyright (C) 2003, 2004, 2005, 2006 Free Software Foundation, Inc.
This file is part of GNU Make.

GNU Make is free software; you can redistribute it and/or modify it under the
terms of the GNU General Public License as published by the Free Software
Foundation; either version 2, or (at your option) any later version.

GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
GNU Make; see the file COPYING.  If not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
This directory contains the 3.81 release of GNU Make.

See the file NEWS for the user-visible changes from previous releases.
In addition, there have been bugs fixed.

Please check the system-specific notes below for any caveats related to
your operating system.

For general building and installation instructions, see the file INSTALL.

If you need to build GNU Make and have no other `make' program to use,
you can use the shell script `build.sh' instead.  To do this, first run
`configure' as described in INSTALL.  Then, instead of typing `make' to
build the program, type `sh build.sh'.  This should compile the program
in the current directory.  Then you will have a Make program that you can
use for `./make install', or whatever else.

Some systems' Make programs are broken and cannot process the Makefile for
GNU Make.  If you get errors from your system's Make when building GNU
Make, try using `build.sh' instead.


GNU Make is free software.  See the file COPYING for copying conditions.


Downloading
-----------

GNU Make can be obtained in many different ways.  See a description here:

  http://www.gnu.org/software/software.html


Documentation
-------------

GNU make is fully documented in the GNU Make manual, which is contained
in this distribution as the file make.texinfo.  You can also find
on-line and preformatted (PostScript and DVI) versions at the FSF's web
site.  There is information there about ordering hardcopy documentation.

  http://www.gnu.org/
  http://www.gnu.org/doc/doc.html
  http://www.gnu.org/manual/manual.html


Development
-----------

GNU Make development is hosted by Savannah, the FSF's online development
management tool.  Savannah is here:

  http://savannah.gnu.org

And the GNU Make development page is here:

  http://savannah.gnu.org/projects/make/

You can find most information concerning the development of GNU Make at
this site.


Bug Reporting
-------------

You can send GNU make bug reports to <bug-make@gnu.org>.  Please see the
section of the GNU make manual entitled `Problems and Bugs' for
information on submitting useful and complete bug reports.

You can also use the online bug tracking system in the Savannah GNU Make
project to submit new problem reports or search for existing ones:

  http://savannah.gnu.org/bugs/?group=make

If you need help using GNU make, try these forums:

  help-make@gnu.org
  help-utils@gnu.org
  news:gnu.utils.help
  news:gnu.utils.bug

  http://savannah.gnu.org/support/?group=make

You may also find interesting patches to GNU Make available here:

  http://savannah.gnu.org/patch/?group=make

Note these patches are provided by our users as a service and we make no
statements regarding their correctness.  Please contact the authors
directly if you have a problem or suggestion for a patch available on
this page.


CVS Access
----------

The GNU make source repository is available via anonymous CVS from the
GNU Subversions CVS server; look here for details:

  http://savannah.gnu.org/cvs/?group=make

Please note: you won't be able to build GNU make from CVS without
installing appropriate maintainer's tools, such as GNU m4, automake,
autoconf, Perl, GNU make, and GCC.  See the README.cvs file for hints on
how to build GNU make once these tools are available.  We make no
guarantees about the contents or quality of the latest code in the CVS
repository: it is not unheard of for code that is known to be broken to
be checked in.  Use at your own risk.


System-specific Notes
---------------------

It has been reported that the XLC 1.2 compiler on AIX 3.2 is buggy such
that if you compile make with `cc -O' on AIX 3.2, it will not work
correctly.  It is said that using `cc' without `-O' does work.

The standard /bin/sh on SunOS 4.1.3_U1 and 4.1.4 is broken and cannot be
used to configure GNU make.  Please install a different shell such as
bash or pdksh in order to run "configure".  See this message for more
information:
  http://mail.gnu.org/archive/html/bug-autoconf/2003-10/msg00190.html

One area that is often a problem in configuration and porting is the code
to check the system's current load average.  To make it easier to test and
debug this code, you can do `make check-loadavg' to see if it works
properly on your system.  (You must run `configure' beforehand, but you
need not build Make itself to run this test.)

Another potential source of porting problems is the support for large
files (LFS) in configure for those operating systems that provide it.
Please report any bugs that you find in this area.  If you run into
difficulties, then as a workaround you should be able to disable LFS by
adding the `--disable-largefile' option to the `configure' script.

On systems that support micro- and nano-second timestamp values and
where stat(2) provides this information, GNU make will use it when
comparing timestamps to get the most accurate possible result.  However,
note that many current implementations of tools that *set* timestamps do
not preserve micro- or nano-second granularity.  This means that "cp -p"
and other similar tools (tar, etc.) may not exactly duplicate timestamps
with micro- and nano-second granularity on some systems.  If your build
system contains rules that depend on proper behavior of tools like "cp
-p", you should consider using the .LOW_RESOLUTION_TIME pseudo-target to
force make to treat them properly.  See the manual for details.


Ports
-----

  - See README.customs for details on integrating GNU make with the
    Customs distributed build environment from the Pmake distribution.

  - See readme.vms for details about GNU Make on OpenVMS.

  - See README.Amiga for details about GNU Make on AmigaDOS.

  - See README.W32 for details about GNU Make on Windows NT, 95, or 98.

  - See README.DOS for compilation instructions on MS-DOS and MS-Windows
    using DJGPP tools.

    A precompiled binary of the MSDOS port of GNU Make is available as part
    of DJGPP; see the WWW page http://www.delorie.com/djgpp/ for more
    information.

Please note there are two _separate_ ports of GNU make for Microsoft
systems: a native Windows tool built with (for example) MSVC or Cygwin,
and a DOS-based tool built with DJGPP.  Please be sure you are looking
at the right README!


-------------------------------------------------------------------------------
Copyright (C) 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997,
1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006 Free Software Foundation,
Inc.
This file is part of GNU Make.

GNU Make is free software; you can redistribute it and/or modify it under the
terms of the GNU General Public License as published by the Free Software
Foundation; either version 2, or (at your option) any later version.

GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
GNU Make; see the file COPYING.  If not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
This version of GNU make has been tested on Microsoft Windows 2000/XP/2003.
It has also been used on Windows 95/98/NT, and on OS/2.

It builds natively with MSVC 2.x, 4.x, 5.x, 6.x, and 2003 as well as
.NET 7.x and .NET 2003.

It builds with the MinGW port of GCC 3.x (tested with GCC 3.4.2).

The Windows 32-bit port of GNU make is maintained jointly by various
people.  It was originally made by Rob Tulloh.


Do this first, regardless of the build method you choose:
---------------------------------------------------------

 1. At the Windows command prompt run:

      if not exist NMakefile copy NMakefile.template NMakefile
      if not exist config.h copy config.h.W32 config.h

    Then edit config.h to your liking (especially the few shell-related
    defines near the end, or HAVE_CASE_INSENSITIVE_FS which corresponds
    to './configure --enable-case-insensitive-file-system').


Using make_msvc_net2003.vcproj
------------------------------

 2. Open make_msvc_net2003.vcproj in MSVS71 or MSVC71 or any compatible IDE,
    then build this project as usual.


Building with (MinGW-)GCC using build_w32.bat
---------------------------------------------

 2. Open a W32 command prompt for your installed (MinGW-)GCC, setup a
    correct PATH and other environment variables for it, then execute ...

	build_w32.bat gcc

    This produces gnumake.exe in the current directory.


Building with (MSVC++-)cl using build_w32.bat or NMakefile
----------------------------------------------------------

 2. Open a W32 command prompt for your installed (MSVC++-)cl, setup a
    correct PATH and other environment variables for it (usually via
    executing vcvars32.bat or vsvars32.bat from the cl-installation,
    e.g. "%VS71COMNTOOLS%vsvars32.bat"; or using a corresponding start
    menue entry from the cl-installation), then execute EITHER ...

	build_w32.bat

    (this produces WinDebug/gnumake.exe and WinRel/gnumake.exe)

    ... OR ...

	nmake /f NMakefile

    (this produces WinDebug/make.exe and WinRel/make.exe).


-------------------
-- Notes/Caveats --
-------------------

GNU make on Windows 32-bit platforms:

	This version of make is ported natively to Windows32 platforms
	(Windows NT 3.51, Windows NT 4.0, Windows 95, and Windows 98). It
	does not rely on any 3rd party software or add-on packages for
	building. The only thing needed is a version of Visual C++,
	which is the predominant compiler used on Windows32 platforms.

	Do not confuse this port of GNU make with other Windows32 projects
	which provide a GNU make binary. These are separate projects
	and are not connected to this port effort.

GNU make and sh.exe:

	This port prefers you have a working sh.exe somewhere on your
	system. If you don't have sh.exe, the port falls back to
	MSDOS mode for launching programs (via a batch file).
	The MSDOS mode style execution has not been tested that
	carefully though (The author uses GNU bash as sh.exe).

	There are very few true ports of Bourne shell for NT right now.
	There is a version of GNU bash available from Cygnus "Cygwin"
	porting effort (http://www.cygwin.com/).
	Other possibilities are the MKS version of sh.exe, or building
        your own with a package like NutCracker (DataFocus) or Portage
        (Consensys).  Also MinGW includes sh (http://mingw.org/).

GNU make and brain-dead shells (BATCH_MODE_ONLY_SHELL):

	Some versions of Bourne shell do not behave well when invoked
	as 'sh -c' from CreateProcess().  The main problem is they seem
	to have a hard time handling quoted strings correctly. This can
	be circumvented by writing commands to be executed to a batch
	file and then executing the command by calling 'sh file'.

	To work around this difficulty, this version of make supports
	a batch mode.  When BATCH_MODE_ONLY_SHELL is defined at compile
	time, make forces all command lines to be executed via script
	files instead of by command line.  In this mode you must have a
	working sh.exe in order to use parallel builds (-j).

	A native Windows32 system with no Bourne shell will also run
	in batch mode.  All command lines will be put into batch files
	and executed via $(COMSPEC) (%COMSPEC%).  Note that parallel
        builds (-j) require a working Bourne shell; they will not work
        with COM.

GNU make and Cygnus GNU Windows32 tools:

	Good news! Make now has native support for Cygwin sh. To enable,
	define the HAVE_CYGWIN_SHELL in config.h and rebuild make
	from scratch. This version of make tested with B20.1 of Cygwin.
	Do not define BATCH_MODE_ONLY_SHELL if you use HAVE_CYGWIN_SHELL.

GNU make and the MKS shell:

	There is now semi-official support for the MKS shell. To turn this
	support on, define HAVE_MKS_SHELL in the config.h.W32 before you
	build make.  Do not define BATCH_MODE_ONLY_SHELL if you turn
	on HAVE_MKS_SHELL.

GNU make handling of drive letters in pathnames (PATH, vpath, VPATH):

	There is a caveat that should be noted with respect to handling
	single character pathnames on Windows systems.	When colon is
	used in PATH variables, make tries to be smart about knowing when
	you are using colon as a separator versus colon as a drive
	letter.	 Unfortunately, something as simple as the string 'x:/'
	could be interpreted 2 ways: (x and /) or (x:/).

	Make chooses to interpret a letter plus colon (e.g. x:/) as a
	drive letter pathname.	If it is necessary to use single
	character directories in paths (VPATH, vpath, Path, PATH), the
	user must do one of two things:

	 a. Use semicolon as the separator to disambiguate colon. For
	    example use 'x;/' if you want to say 'x' and '/' are
	    separate components.

	 b. Qualify the directory name so that there is more than
	    one character in the path(s) used. For example, none
	    of these settings are ambiguous:

	      ./x:./y
	      /some/path/x:/some/path/y
	      x:/some/path/x:x:/some/path/y

	Please note that you are free to mix colon and semi-colon in the
	specification of paths.	 Make is able to figure out the intended
	result and convert the paths internally to the format needed
	when interacting with the operating system, providing the path
	is not within quotes, e.g. "x:/test/test.c".

	You are encouraged to use colon as the separator character.
	This should ease the pain of deciding how to handle various path
	problems which exist between platforms.	 If colon is used on
	both Unix and Windows systems, then no ifdef'ing will be
	necessary in the makefile source.

GNU make test suite:

	I verified all functionality with a slightly modified version
	of make-test-3.81 (modifications to get test suite to run
	on Windows NT). All tests pass in an environment that includes
	sh.exe.  Tests were performed on both Windows NT and Windows 95.

Building GNU make on Windows NT and Windows 95/98 with Microsoft Visual C:

	I did not provide a Visual C project file with this port as
	the project file would not be considered freely distributable
	(or so I think). It is easy enough to create one, though, if
	you know how to use Visual C.

	I build the program statically to avoid problems locating DLL's
	on machines that may not have MSVC runtime installed. If you
	prefer, you can change make to build with shared libraries by
	changing /MT to /MD in the NMakefile (or in build_w32.bat).

	The program has not been built for non-Intel architectures (yet).

	I have not tried to build with any other compilers than MSVC. I
	have heard that this is possible though so don't be afraid to
	notify me of your successes!

Pathnames and white space:

	Unlike Unix, Windows 95/NT systems encourage pathnames which
	contain white space (e.g. C:\Program Files\). These sorts of
	pathnames are legal under Unix too, but are never encouraged.
	There is at least one place in make (VPATH/vpath handling) where
	paths containing white space will simply not work. There may be
	others too. I chose to not try and port make in such a way so
	that these sorts of paths could be handled. I offer these
	suggestions as workarounds:

		1. Use 8.3 notation. i.e. "x:/long~1/", which is actually
		   "x:\longpathtest".  Type "dir /x" to view these filenames
		   within the cmd.exe shell.
		2. Rename the directory so it does not contain white space.

	If you are unhappy with this choice, this is free software
	and you are free to take a crack at making this work. The code
	in w32/pathstuff.c and vpath.c would be the places to start.

Pathnames and Case insensitivity:

	Unlike Unix, Windows 95/NT systems are case insensitive but case
	preserving.  For example if you tell the file system to create a
	file named "Target", it will preserve the case.  Subsequent access to
	the file with other case permutations will succeed (i.e. opening a
	file named "target" or "TARGET" will open the file "Target").

	By default, GNU make retains its case sensitivity when comparing
	target names and existing files or directories.  It can be
	configured, however, into a case preserving and case insensitive
	mode by adding a define for HAVE_CASE_INSENSITIVE_FS to
	config.h.W32.

	For example, the following makefile will create a file named
	Target in the directory subdir which will subsequently be used
	to satisfy the dependency of SUBDIR/DepTarget on SubDir/TARGET.
	Without HAVE_CASE_INSENSITIVE_FS configured, the dependency link
	will not be made:

	subdir/Target:
		touch $@

	SUBDIR/DepTarget: SubDir/TARGET
		cp $^ $@

	Reliance on this behavior also eliminates the ability of GNU make
	to use case in comparison of matching rules.  For example, it is
	not possible to set up a C++ rule using %.C that is different
	than a C rule using %.c.  GNU make will consider these to be the
	same rule and will issue a warning.

SAMBA/NTFS/VFAT:

	I have not had any success building the debug version of this
	package using SAMBA as my file server. The reason seems to be
	related to the way VC++ 4.0 changes the case name of the pdb
	filename it is passed on the command line. It seems to change
	the name always to to lower case. I contend that the VC++
	compiler should not change the casename of files that are passed
	as arguments on the command line. I don't think this was a
	problem in MSVC 2.x, but I know it is a problem in MSVC 4.x.

	The package builds fine on VFAT and NTFS filesystems.

	Most all of the development I have done to date has been using
	NTFS and long file names. I have not done any considerable work
	under VFAT. VFAT users may wish to be aware that this port of
	make does respect case sensitivity.

FAT:

	Version 3.76 added support for FAT filesystems. Make works
	around some difficulties with stat'ing of files and caching of
	filenames and directories internally.

Bug reports:

	Please submit bugs via the normal bug reporting mechanism which
	is described in the GNU make manual and the base README.

-------------------------------------------------------------------------------
Copyright (C) 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005,
2006 Free Software Foundation, Inc.
This file is part of GNU Make.

GNU Make is free software; you can redistribute it and/or modify it under the
terms of the GNU General Public License as published by the Free Software
Foundation; either version 2, or (at your option) any later version.

GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
GNU Make; see the file COPYING.  If not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
The test suite was originally written by Steve McGee and Chris Arthur.
It is covered by the GNU General Public License (Version 2), described
in the file COPYING.  It has been maintained as part of GNU make proper
since GNU make 3.78.

This entire test suite, including all test files, are copyright and
distributed under the following terms:

 -----------------------------------------------------------------------------
 Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,
 2002, 2003, 2004, 2005, 2006 Free Software Foundation, Inc.
 This file is part of GNU Make.

 GNU Make is free software; you can redistribute it and/or modify it under the
 terms of the GNU General Public License as published by the Free Software
 Foundation; either version 2, or (at your option) any later version.

 GNU Make is distributed in the hope that it will be useful, but WITHOUT ANY
 WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
 A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

 You should have received a copy of the GNU General Public License along with
 GNU Make; see the file COPYING.  If not, write to the Free Software
 Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 -----------------------------------------------------------------------------

The test suite requires Perl.  These days, you should have at least Perl
5.004 (available from ftp.gnu.org, and portable to many machines).  It
used to work with Perl 4.036 but official support for Perl 4.x was
abandoned a long time ago, due to lack of testbeds, as well as interest.

The test suite assumes that the first "diff" it finds on your PATH is
GNU diff, but that only matters if a test fails.

To run the test suite on a UNIX system, use "perl ./run_make_tests"
(or just "./run_make_tests" if you have a perl on your PATH).

To run the test suite on Windows NT or DOS systems, use
"perl.exe ./run_make-tests.pl".

By default, the test engine picks up the first executable called "make"
that it finds in your path.  You may use the -make_path option (ie,
"perl run_make_tests -make_path /usr/local/src/make-3.78/make") if
you want to run a particular copy.  This now works correctly with
relative paths and when make is called something other than "make" (like
"gmake").

Tests cannot end with a "~" character, as the test suite will ignore any
that do (I was tired of having it run my Emacs backup files as tests :))

Also, sometimes the tests may behave strangely on networked
filesystems.  You can use mkshadow to create a copy of the test suite in
/tmp or similar, and try again.  If the error disappears, it's an issue
with your network or file server, not GNU make (I believe).  This
shouldn't happen very often anymore: I've done a lot of work on the
tests to reduce the impacts of this situation.

The options/dash-l test will not really test anything if the copy of
make you are using can't obtain the system load.  Some systems require
make to be setgid sys or kmem for this; if you don't want to install
make just to test it, make it setgid to kmem or whatever group /dev/kmem
is (ie, "chgrp kmem make;chmod g+s make" as root).  In any case, the
options/dash-l test should no longer *fail* because make can't read
/dev/kmem.

A directory named "work" will be created when the tests are run which
will contain any makefiles and "diff" files of tests that fail so that
you may look at them afterward to see the output of make and the
expected result.

There is a -help option which will give you more information about the
other possible options for the test suite.


Open Issues
-----------

The test suite has a number of problems which should be addressed.  One
VERY serious one is that there is no real documentation.  You just have
to see the existing tests.  Use the newer tests: many of the tests
haven't been updated to use the latest/greatest test methods.  See the
ChangeLog in the tests directory for pointers.

The second serious problem is that it's not parallelizable: it scribbles
all over its installation directory and so can only test one make at a
time.  The third serious problem is that it's not relocatable: the only
way it works when you build out of the source tree is to create
symlinks, which doesn't work on every system and is bogus to boot.  The
fourth serious problem is that it doesn't create its own sandbox when
running tests, so that if a test forgets to clean up after itself that
can impact future tests.


Bugs
----

Any complaints/suggestions/bugs/etc. for the test suite itself (as
opposed to problems in make that the suite finds) should be handled the
same way as normal GNU make bugs/problems (see the README for GNU make).


                                                Paul D. Smith
						Chris Arthur
Id: README.dev,v 1.7 2003/11/24 15:11:06 karl Exp 
README.dev - Texinfo developer information.

  Copyright (C) 2002, 2003 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

The development sources for Texinfo is available through anonymous cvs
at Savannah, see
  http://savannah.gnu.org/cvs/?group=texinfo

This distribution uses whatever versions of automake, autoconf, and
gettext are listed in NEWS; usually the latest ones released.  If you
are getting the Texinfo sources from cvs, or change the Texinfo
configure.ac, you'll need to have these tools installed to (re)build.
You'll also need help2man.  (All of these are available from
ftp://ftp.gnu.org/gnu.)

Here's the order in which to run the tools for a fresh build:

  autoheader    # creates config.in, not necessarily needed every time
  aclocal -I m4 # for a new version of automake
  automake
  autoconf
  configure CFLAGS=-g --enable-maintainer-mode
  make

(with arguments to taste, of course.)  Or you can run

  ./bootstrap

instead of the various auto* tools.


One final note: If you would like to contribute to the GNU project by
implementing additional documentation output formats for Texinfo, that
would be great.  But please do not write a separate translator texi2foo
for your favorite format foo!  That is the hard way to do the job, and
makes extra work in subsequent maintenance, since the Texinfo language
is continually being enhanced and updated.  Instead, the best approach
is modify Makeinfo to generate the new format, as it does now for Info,
HTML, XML, and DocBook.
Id: README,v 1.16 2004/12/13 13:36:32 karl Exp 
This is the README file for the GNU Texinfo distribution.  Texinfo is
the preferred documentation format for GNU software.

  Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000,
  2001, 2002, 2003, 2004 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

See ./INSTALL* for installation instructions.

Primary distribution point: ftp://ftp.gnu.org/gnu/texinfo/
  (list of mirrors at: http://www.gnu.org/prep/ftp.html)

Home page: http://www.gnu.org/software/texinfo/
  (list of mirrors at: http://www.gnu.org/server/list-mirrors.html)
  This page includes links to other Texinfo-related programs.

Mailing lists and archives:
- bug-texinfo@gnu.org for bug reports or enhancement suggestions,
  archive: http://mail.gnu.org/pipermail/bug-texinfo
- help-texinfo@gnu.org for authoring questions and general discussion, 
  archive: http://mail.gnu.org/pipermail/help-texinfo
- texinfo-pretest@texinfo.org for pretests of new releases,
  archive: http://texinfo.org/ftp/texinfo-pretest-archive
There are no corresponding newsgroups.

Bug reports:
 please include enough information for the maintainers to reproduce the
 problem.  Generally speaking, that means:
- the contents of any input files necessary to reproduce the bug (crucial!).
- a description of the problem and any samples of the erroneous output.
- the version number of Texinfo and the program(s) involved (use --version).
- hardware, operating system, and compiler versions (uname -a).
- unusual options you gave to configure, if any (see config.status).
- anything else that you think would be helpful.

Patches are most welcome; if possible, please make them with diff -c and
include ChangeLog entries.

When sending email, please do not encode or split the messages in any
way if at all possible; it's easier to deal with one large message than
many small ones.  GNU shar (http://www.gnu.org/software/sharutils/) is a
convenient way of packaging multiple and/or binary files for email.

See README.dev for information on the Texinfo development environment --
any interested parties are welcome.  If you're a programmer and wish to
contribute, this should get you started.  And if you're not a
programmer, you can still make significant contributions by writing test
cases, checking the documentation against the implementation, etc.

This distribution includes the following files, among others:
    README                      This file.
    README.dev                  Texinfo developer information.

    INSTALL                     Texinfo-specific installation notes.
    NEWS                        Summary of new features by release.
    INTRODUCTION                Brief introduction to the system, and
                                how to create readable files from the
                                Texinfo source files in this distribution.

Texinfo documentation files (in ./doc):
    texinfo.txi                 Describes the Texinfo language and many
                                of the associated tools.  It tells how
                                to use Texinfo to write documentation,
                                how to use Texinfo mode in GNU Emacs,
                                TeX, makeinfo, and the Emacs Lisp
                                Texinfo formatting commands.

    info.texi                   This manual tells you how to use 
                                Info.  This document also comes as part of
                                GNU Emacs.  If you do not have Emacs,
                                you can format this Texinfo source
                                file with makeinfo or TeX and then
                                read the resulting Info file with the
                                standalone Info reader that is part of
                                this distribution.

    info-stnd.texi              This manual tells you how to use
                                the standalone GNU Info reader that is
                                included in this distribution as C
                                source (./info).

Printing related files:
    doc/texinfo.tex             This TeX definitions file tells
                                the TeX program how to typeset a
                                Texinfo file into a DVI file ready for
                                printing.

    util/texindex.c             This file contains the source for
                                the `texindex' program that generates
                                sorted indices used by TeX when
                                typesetting a file for printing.

    util/texi2dvi               This is a shell script for
                                producing an indexed DVI file using
                                TeX and texindex. 

Source files for standalone C programs: 
  ./lib
  ./makeinfo
  ./info

Installation files:
    Makefile.am                 What Automake uses to make a Makefile.in.
    Makefile.in                 What `configure' uses to make a Makefile,
                                created by Automake.
    configure.ac		What Autoconf uses to create `configure'.
    configure                   Configuration script for local conditions,
                                created by Autoconf.
Id: README,v 1.3 2004/04/11 17:56:46 karl Exp 
texinfo/lib/README

  Copyright (C) 2002 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

Common routines for the Texinfo programs.

Many are common to other GNU packages as well.
See the gnulib project at savannah: http://savannah.gnu.org/projects/gnulib
Id: README,v 1.3 2004/04/11 17:56:45 karl Exp 
texinfo/info/README

  Copyright (C) 2002 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

Info 2.0 is a complete rewrite of the original standalone Info I wrote in
1987, the first program I wrote for rms.  That program was something like
my second Unix program ever, and my die-hard machine language coding habits
tended to show through.  I found the original Info hard to read and
maintain, and thus decided to write this one.

The rewrite consists of about 12,000 lines of code written in about 12
days.  I believe this version of Info to be in much better shape than the
original Info.

Info 2.0 is substantially different from its original standalone
predecessor.  It appears almost identical to the GNU Emacs version, but has
the advantages of smaller size, ease of portability, and a built in library
which can be used in other programs (to get or display documentation from
Info files, for example).

A full listing of the commands available in Info can be gotten by typing
`?' while within an Info window.  This produces a node in a window which
can be viewed just like any Info node.

--Brian Fox <bfox@gnu.org>
Id: README,v 1.3 2004/04/11 17:56:46 karl Exp 
texinfo/makeinfo/README

  Copyright (C) 2002 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

makeinfo is a standalone program to convert Texinfo source into Info
files readable with standalone info or M-x info in Emacs.

makeinfo can also output other formats: plain ASCII (with --no-headers)
or HTML (with --html) or XML (with --xml).

The Emacs function M-x texinfo-format-buffer does more or less the same
job, but makeinfo is faster and gives better error messages.
Id: README,v 1.5 2004/04/11 17:56:47 karl Exp 
texinfo/util/README

  Copyright (C) 2002 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

Assorted Texinfo-related programs and scripts.

texindex, texi2dvi, and install-info get installed.
The other items here are for your amusement and/or hacking pleasure.

Id: README,v 1.4 2004/04/11 17:56:45 karl Exp 
texinfo/doc/README

  Copyright (C) 2002 Free Software Foundation, Inc.

  Copying and distribution of this file, with or without modification,
  are permitted in any medium without royalty provided the copyright
  notice and this notice are preserved.

This directory contains documentation on the Texinfo system and the TeX
sources needed to process Texinfo sources.  We recommend using the
texi2dvi included in this distribution to run a Texinfo manual through
TeX to produce a DVI file.

The .tex files are not installed automatically because TeX installations
vary so widely.  Installing them in the wrong place would give a false
sense of security.  So, you should simply cp *.tex to the appropriate
place.  If your installation follows the TeX Directory Structure
standard (http://tug.org/tds/), this will be the directory
TEXMF/tex/texinfo/ for texinfo.tex, TEXMF/tex/generic/dvips/ for epsf.tex,
and TEXMF/pdftex/plain/misc for pdfcolor.tex.  If you use the default
installation paths, TEXMF will be /usr/local/share/texmf.  On systems
with TeX preinstalled, as most GNU/Linux distributions offer, TEXMF
will often be something like /usr/share/texmf.

It is also possible to put these .tex files in a `local' place instead
of overwriting existing ones, but this is more complicated.  See your TeX
documentation in general and the texmf.cnf file in particular for information.

If you add files to your TeX installations, not just replace existing
ones, you very likely have to update your ls-R file; do this with the
mktexlsr command.  In older versions, this was named MakeTeXls-R.

You can get the latest texinfo.tex from
ftp://ftp.gnu.org/gnu/texinfo/texinfo.tex (and all GNU mirrors)
ftp://tug.org/tex/texinfo.tex (and all CTAN mirrors)
or on the FSF machines in /home/gd/gnu/doc/texinfo.tex.
If you have problems with the version in this distribution, please check
for a newer version.

epsf.tex comes with dvips distributions, and you may already have it
installed.  The version here is functionally identical but slightly
nicer than the one in dvips574.  The changes have been sent to the
epsf.tex maintainer.
Installation on Woe32 (WinNT/2000/XP, Win95/98/ME):

This file explains how to create binaries for the mingw execution environment.
For how to create binaries for the cygwin environment, please see the normal
INSTALL file.  MS Visual C/C++ with "nmake" is no longer supported.

I recommend to use the cygwin environment as the development environment
and mingw only as the target (runtime, deployment) environment.
For this, you need to install
  - cygwin,
  - the mingw runtime package, also from the cygwin site.

You must not install cygwin programs directly under /usr/local -
because the mingw compiler and linker would pick up the include files
and libraries from there, thus introducing an undesired dependency to
cygwin. You can for example achieve this by using the
configure option --prefix=/usr/local/cygwin each time you build a
program for cygwin.

Building for mingw is then achieved through the following configure
command:

   CPPFLAGS="-mno-cygwin -Wall -I/usr/local/mingw/include" \
   CFLAGS="-mno-cygwin -O2 -g" \
   CXXFLAGS="-mno-cygwin -O2 -g" \
   LDFLAGS="-mno-cygwin -L/usr/local/mingw/lib" \
   ./configure --host=i586-pc-mingw32 --prefix=/usr/local/mingw

The -mno-cygwin tells the cygwin compiler and linker to build for mingw.
The -I and -L option are so that packages previously built for the
same environment are found. The --host option tells the various
tools that you are building for mingw, not cygwin.
This is the GNU gettext package.  It is interesting for authors or
maintainers of other packages or programs which they want to see
internationalized.  As one step the handling of messages in different
languages should be implemented.  For this task GNU gettext provides
the needed tools and library functions.

Users of GNU packages should also install GNU gettext because some
other GNU packages will use the gettext program included in this
package to internationalize the messages given by shell scripts.

Another good reason to install GNU gettext is to make sure the
here included functions compile ok.  This helps to prevent errors
when installing other packages which use this library.  The message
handling functions are not yet part of POSIX and ISO/IEC standards
and therefore it is not possible to rely on facts about their
implementation in the local C library.  For this reason, GNU gettext
tries using the system's functionality only if it is a GNU gettext
implementation (possibly a different version); otherwise, compatibility
problems would occur.

We felt that the Uniforum proposals has the much more flexible interface
and, what is more important, does not burden the programmers as much as
the other possibility does.


Please share your results with us.  If this package compiles ok for
you future GNU release will likely also not fail, at least for reasons
found in message handling.  Send comments and bug reports to
		bug-gnu-gettext@gnu.org


The goal of this library was to give a unique interface to message
handling functions.  At least the same level of importance was to give
the programmer/maintainer the needed tools to maintain the message
catalogs.  The interface is designed after the proposals of the
Uniforum group.


The homepage of this package is at

           http://www.gnu.org/software/gettext/

The primary FTP site for its distribution is

           ftp://ftp.gnu.org/pub/gnu/gettext/


The configure script provides two non-standard options.  These will
also be available in other packages if they use the functionality of
GNU gettext.  Use

	--disable-nls

if you absolutely don't want to have messages handling code.  You will
always get the original messages (mostly English).  You could consider
using NLS support even when you do not need other tongues.  If you do
not install any messages catalogs or do not specify to use another but
the C locale you will not get translations.

The set of languages for which catalogs should be installed can also be
specified while configuring.  Of course they must be available but the
intersection of these two sets are computed automatically.  You could
once and for all define in your profile/cshrc the variable LINGUAS:

(Bourne Shell)		LINGUAS="de fr nl"; export LINGUAS

(C Shell)		setenv LINGUAS "de fr nl"

or specify it directly while configuring

	env LINGUAS="de fr nl" ./configure

Consult the manual for more information on language names.

The second configure option is

	--with-included-gettext

This forces to use the GNU implementation of the message handling library
regardless what the local C library provides.  This possibility is
useful if the local C library is a glibc 2.1.x or older, which didn't
have all the features the included libintl has.


Other files you might look into:

`ABOUT-NLS' -	current state of the GNU internationalization effort
`COPYING' -	copying conditions
`INSTALL' -	general compilation and installation rules
`NEWS' -	major changes in the current version
`THANKS' -	list of contributors


Some points you might be interested in before installing the package:

1.  If your system's C library already provides the gettext interface
    and its associated tools don't come from this package, it might be
    a good idea to configure the package with
    --program-transform-name='s/^gettext$/g&/;s/^msgfmt$/g&/;s/^xgettext$/g&/'

    Systems affected by this are:
        Solaris 2.x

2.  Some system have a very dumb^H^H^H^Hstrange version of msgfmt, the
    one which comes with xview.  This one is *not* usable.  It's best
    you delete^H^H^H^H^H^Hrename it or install this package as in the
    point above with
    --program-transform-name='s/^gettext$/g&/;s/^msgfmt$/g&/;s/^xgettext$/g&/'

3.  The locale name alias scheme implemented here is in a similar form
    implemented in the X Window System.  Especially the alias data base
    file can be shared.  Normally this file is found at something like

	/usr/lib/X11/locale/locale.alias

    If you have the X Window System installed try to find this file and
    specify the path at the make run:

    make aliaspath='/usr/lib/X11/locale:/usr/local/lib/locale'

    (or whatever is appropriate for you).  The file name is always
    locale.alias.
    In the misc/ subdirectory you find an example for an alias database file.

4.  The msgmerge program performs fuzzy search in the message sets.  It
    might run a long time on slow systems.  I saw this problem when running
    it on my old i386DX25.  The time can really be several minutes,
    especially if you have long messages and/or a great number of
    them.
       If you have a faster implementation of the fstrcmp() function and
    want to share it with the rest of us, please contact me.
Installation on Woe32 (WinNT/2000/XP, Win95/98/ME):

Building requires the mingw development environment (includes gcc).
MS Visual C/C++ with "nmake" is no longer supported.
This directory contains the runtime parts of GNU gettext, namely the libraries
for the end user, but neither documentation nor programs for the translator
or the maintainer.
Installation on Woe32 (WinNT/2000/XP, Win95/98/ME):

Building requires the mingw development environment (includes gcc).
MS Visual C/C++ with "nmake" is no longer supported.
These files are used by a program called aclocal (part of the GNU automake
package).  aclocal uses these files to create aclocal.m4 which is in turn
used by autoconf to create the configure script at the the top level in
this distribution.
Installation on Woe32 (WinNT/2000/XP, Win95/98/ME):

Building requires the mingw development environment (includes gcc).
MS Visual C/C++ with "nmake" is no longer supported.
            GNU libasprintf - automatic formatted output to strings

This package makes the C formatted output routines (fprintf et al.) usable
in C++ programs.


Sample use
----------

  char *pathname = autosprintf ("%s/%s", directory, filename);
  cerr << autosprintf ("syntax error in %s:%d: %s", filename, line, errstring);


Benefits
--------

The benefits of autosprintf over the usual "piecewise meal" idiom

  cerr << "syntax error in " << filename << ":" << line << ": " << errstring;

are:

  - Reuses of the standard POSIX printf facility. Easy migration from C to C++.

  - English sentences are kept together.

  - Internationalization requires format strings, because
    1. Internationalization requires the ability for the translator to change
       the order of parts of a sentence. The POSIX printf formatted output
       functions (and thus also autosprintf) support this through the %m$ and
       *m$ syntax.
    2. Translators are used to translate one string per sentence, not
       multiple strings per sentence, and not C++ code statements.

  - Reduces the risk of programming errors due to forgotten state in the
    output stream (e.g.  'cout << hex;'  not followed by  'cout << dec;').

The benefits of autosprintf over C sprintf are:

  - Autosprintf avoids buffer overruns and truncated results.
    The C sprintf() function often leads to buffer overruns. On the other
    hand, the C snprintf() function requires extra coding for an a priori
    estimate of the result's size and truncates the result if the estimate
    was too low.

  - Autosprintf avoids memory leaks.
    Temporarily allocated memory is cleaned up automatically.


Installation
------------

See INSTALL. Usually "./configure; make; make install" should work.

The installed files are:
  - An include file "autosprintf.h" which defines the class 'autosprintf',
    in the namespace 'gnu'.
  - A library libasprintf containing this class.


Use
---

To use the class autosprintf, use

  #include "autosprintf.h"
  using gnu::autosprintf;

and link with the linker option

  -lasprintf


Misc notes
----------

An instance of class 'autosprintf' contains the formatted output result;
this string is freed when the instance's destructor is run.

The class name 'autosprintf' is meant to remind the C function sprintf(),
the GNU C library function asprintf(), and the C++ autoptr programming idiom.


Distribution
------------
    http://www.haible.de/bruno/gnu/libasprintf-1.0.tar.gz

Homepage
--------
    http://www.haible.de/bruno/packages-libasprintf.html

Bug reports to:
---------------
    <bug-gnu-gettext@gnu.org>


Bruno Haible <brunoe@clisp.org>
A copy of djgpp can be obtained at http://www.delorie.com/djgpp/.
This is a port of GNU Gettext @VER@ to MSDOS/DJGPP.


        TO USE THE GNU GETTEXT LIBRARY YOU **MUST** MODIFY YOUR C-LIBRARY.
        PLEASE, READ SECTION #2 (Installing the binary package) CAREFULLY
        TO LEARN HOW TO INSTALL THE GNU GETTEXT LIBRARY AND HOW TO CHANGE
        YOUR C-LIBRARY AND SYSTEM HEADER FILE.
        TO USE THE GNU GETTEXT LIBRARY YOU **MUST** DOWNLOAD AND INSTALL
        LICV17B.ZIP TOO. THIS IS **NOT** OPTIONAL.
        IT IS NOT RECOMMED TO DOWNLOAD THE GNU DISTRIBUTION OF GETTEXT
        BECAUSE ONLY THE DJGPP PORT WILL CONTAIN THE REQUIRED HEADER AND
        OBJECT FILE TO PATCH THE C LIBRARY.


1.:     DJGPP specific changes.
        =======================

        The DJGPP specific changes are the followings:
     1) The conflict existing between the BORLAND-compatibility gettext function
        from DJGPP's libc.a defined in conio.h and the GNU gettext function from
        libintl.a defined in libintl.h has been removed. But this conflict can not
        be removed **WITHOUT** changing a system header file and libc.a.
     1.1)  libc.a and system header changes.
        In conio.c, the BORLAND-compatibility gettext function has been renamed
        into _conio_gettext. In conio.h some code has been added to check if
        libintl.h is included or not by the same source file. If libintl.h is NOT
        included, the BORLAND-compatibility gettext function will be available as
        gettext. If libintl.h has been included then the BORLAND-compatibility
        gettext function will **ONLY** be available as _conio_gettext.
        The BORLAND-compatibility gettext function is now available as gettext
        and _conio_gettext.
     1.2)  GNU gettext library changes.
        If both headers, libintl.h and conio.h, are included in the same source
        file the gettext keyword makes **ALWAYS** reference to the GNU gettext
        function and **NEVER** to the BORLAND-compatibility gettext function.

     2) The binary package gtxt@packageversion@b.zip contains all needed files to get NLS
        support for the following DJGPP ports:
          bison-1.32 (bsn132s.zip)
          enscript-1.6.2 (ens162s.zip)
          fileutils-4.0 (fil40s.zip)
          grep-2.4 (grep24s.zip)
          id-utils-3.2 (idu32s.zip)
          make-3.79.1 (mak3791s.zip)
          recode-3.6 (rcode36s.zip)
          sed-3.02.80 (sed3028s.zip)
          sharutils-4.2c (shar42cs.zip)
          sh-utils-2.0j (shl20js.zip)
          tar-1.12a (tar112as.zip)
          texinfo-4.0 (txi40s.zip)
          textutils-2.0 (txt20s.zip)

        See section #4 for further information about this issue.
        To implement NLS support for one of those packages you will also need
        to download the following packages:
          gtxt@packageversion@b.zip (binaries of GNU Gettext @VER@)
          licv17b.zip (binaries of GNU libiconv 1.7)
          fil40b.zip  (binaries of GNU Fileutils 4.0)
          shl20jb.zip (binaries of GNU Sh-utils 2.0j)


2.:     Installing the binary package.
        ==============================

2.1.:   To use this binary package you **MUST** install licv17b.zip or later
        first. licv17b.zip provides the required functionality to recode the
        .mo files at run time from the unix charsets used to create them to the
        dos codepages used to display them. Copy the binary distribution into
        the top DJGPP installation directory. If you are installing Gettext on
        a dual DOS/WINDOWS 9X systems, you *MUST* first turn off the generation
        of numeric tails for 8.3 aliases Windows creats for long file names.
        For information about how to do this, please read the DJGPP FAQ List
        V 2.30, chapter 22.19: "How to Set Up a Dual DOS/Windows Installation".
        It should be noticed that neither the libintl.a library nor the
        binaries (xgettext.exe, gettext.exe, etc.) contain any code to handle
        nuneric tails of short file names. This implies that if you install
        the binary packages in a DOS box of Win9X (LFN) **WITHOUT** turning
        off the numeric tail generation you will **NOT** be able to use NLS
        on plain DOS. Once again: if you want NLS support on both Win9X **AND**
        on plain DOS you **MUST** turn off the numeric tail generation **BEFORE**
        installing the binary package. After having installed the package
        you can turn on numerical tail generation again if you wish.
        All this also applies to any other package that has been compiled with
        NLS support. You **MUST** turn off numeric tail generation every time
        you install a package that has been compiled with NLS or the binaries
        will **NOT** be able to find their .mo files (translations) when you
        switch to plain DOS.

2.2.:   Copy the binary distribution into the top DJGPP installation directory,
        just unzip it preserving the directory structure running *ONE* of the
        following commands:
          unzip32 gtxt@packageversion@b.zip      or
          djtarx gtxt@packageversion@b.zip       or
          pkunzip -d gtxt@packageversion@b.zip

2.3.:   Changing libc.a and conio.h.
        Apart from the ussual directories, the binary package will create the
        following directory:
          %DJDIR%/gnu/gtxt-@treeversion@/djgpp/djdev-2.03
        where %DJDIR% stands for the root of your DJGPP installation tree.
        Cd into the djdev-2.03 directory. You will find the following files:
          conio.diffs
          conio.h
          conio.o
        conio.diffs is a patch file that documents the changes I have done against
        the files of the original djdev203.zip and djlsr203.zip distributions.
        This file is not needed by the average user. conio.h is the modified header
        and conio.o is the recompiled new conio.c file that will replace the old
        conio.o contained in libc.a.

        For all commands that will follow now I will assume that you have
        cd'ed into the %DJDIR%/gnu/gtxt-@treeversion@/djgpp/djdev-2.03 directory,
        where %DJDIR% represents the path to your DJGPP installation. First,
        you should backup your old header and library. For this task, run the
        following command sequence (cp is the copy program from fil40b.zip):
          cp /dev/env/DJDIR/include/conio.h /dev/env/DJDIR/include/conio.bak
          cp /dev/env/DJDIR/lib/libc.a /dev/env/DJDIR/lib/libc.bak

        Now you can copy the new header into your include directory
        running the command:
          cp conio.h /dev/env/DJDIR/include

        Now you can substitute the old conio.o file in libc.a with the new one.
        For this task you will need the ar program from binutils.
        Run the command:
          ar -rv /dev/env/DJDIR/lib/libc.a conio.o
        You are done.

2.3.:   The NLS controling environment variables, LANG and LANGUAGE, must be
        set to their appropiate values. The exact way how these variables
        should be set depends on your operating system:

        * For Windows 98 systems:
          - Click START;
          - Choose Programs->Accessories->System Tools->System Information;
          - Click Tools in the menu-bar, then choose "System Configuration";
          - Use the tab provided there for editing your AUTOEXEC.BAT as
            explained below.

        * For Windows NT systems:
          - Right-click "My Computer", then select "Properties";
          - Click the "Environment" tab;
          - Add a new variables LANG and LANGUAGE and set their values to the
            wanted language codes file as explained below.

        * For all other systems (DOS, Windows 3.X and Windows 95): use any
          text editor, e.g. the standard EDIT, to edit the file AUTOEXEC.BAT
          in the root directory of the boot drive (usually, C:).

        The values of the two environment variables LANG and LANGUAGE should be
        set like this:

          set LANG=xx
          set LANGUAGE=yy:zz

        xx, yy and zz are place holders for the wanted language codes. For
        posible values, please read below.
        The LANG entry is obligatory, the LANGUAGE entry may be omited. The
        LANG variable selects the locale charsets (dos codepage) to be used to
        display the program's output and the catalog (.mo file) that contains
        the translated strings to be used. The LANGUAGE variable allows you to
        select an alternate catalog than the one stipulated by LANG. Replace
        xx, yy and zz by the language code of the catalogs you want to use. It
        should be noticed that LANGUAGE has *ALWAYS* higher priority than LANG.
        The LANG variable not only selects a catalog, it also specifies the dos
        codepage that will be used as locale charset. All this means that the
        translation strings contained in the catalogs (.mo files) will be
        recoded at runtime to the dos codepage stipulated by the value of LANG.
        This runtime recoding is needed because the .mo files may have been
        written using a charset that is not compatible with the charset that
        will be used on the machine and OS where the .mo files contents will be
        displayed. The .po files of the GNU packages, from which the .mo files
        are generated, are typical examples of this. Usualy, they have been
        written using some ISO-8859-nn charset (an unix charset) and shall be
        displayed on a DOS/WIN95 machine that uses some dos codepage.

        Some examples:
        If you only want to use the catalog containing the translations for
        your mother tongue (in my case the spanish translations) the above
        lines will only use the LANG variable and will look like this:

          set LANG=es

        In this case, LANG defines the locale charset (CP850 in this case) to
        be used for the on-the-fly recoding of the catalog (.mo file) contents
        **AND AT THE SAME TIME** the translation/language (.mo file) to be used.

        If you want to use the spanish (es) and german (de) catalogs the above
        lines will look like this:

          set LANG=es
          set LANGUAGE=es:de

        In this case a DJGPP binary that has been compiled with NLS support
        will first search for the spanish translation of a string. If a
        translation for that particular string can not be found in the spanish
        .mo file then it will search for a german translation of that string in
        the german .mo file and if a german translation of that string can also
        not been found it will default to display the build-in english string.
        No mather if a spanish, a german or an english build-in string is
        selected, the string is always recoded to the dos codepage stipulated
        by LANG. In this case: CP850. In the above example, LANGUAGE defines
        the set of languages to be used and their priority (from left to right).
        At the same time, LANG defines the locale charset (dos codepage) to be
        used to recode **ALL** translated string, no matter which language
        (.mo file) is used.
        If you want to reverse this search order the above lines would look
        like this one:

          set LANG=es
          set LANGUAGE=de:es

        Now let us assume that an user wants to use the swedish catalogs on
        a machine that loads codepage CP437 when it is booted. It should be
        noticed that the locale charset for Sweden is CP850 and not CP437.
        In this case, the lines must look like this:

          set LANG=en_US
          set LANGUAGE=sv

        LANG reflects the available codepage/charset and LANGUAGE selects the
        wanted translation catalog. en_US means CP437. Now, the contents of the
        catalog are recoded to CP437 instead to CP850 because CP437 is the
        codepage used to display messages on screen. Of course, not every
        combination of catalogs and locale charset (dos codepages) makes sense.
        E.G.: selecting as locale charset chinese (LANG=zh_TW) and the french
        translations (LANGUAGE=fr) will certainly not generate an usefull
        screen output.

        The content of LANG is a language code. Examples are fr for french,
        en_US for US english, etc. This language code is an alias for the
        locale charset to be used for runtime recoding. The complete list of
        all available aliases can be found in %DJDIR%/lib/charset.alias. This
        file is a table with two entries: left entry is the alias (en_US,
        de_AT, etc.), right entry is the corresponding dos codepage that will
        be used for that language code (alias). It should be noticed that it is
        also possible to select a codepage directely. E.G.: Instead of setting:

          set LANG=en_US

        you may directely set:

          set LANG=CP437

        cp437 or 437 are also valid settings for CP437. This overwrites any
        settings in charset.alias. The settings in the environment always
        overwrite the settings in charset.alias. Please note that if you omit
        LANG, LANGUAGE will not be honored at all. Because the information
        about which locale charset shall be used for recoding is needed,
        if LANG is omitted by the user this information will not be available
        and consequently LANGUAGE will be ignored and no translation at all
        will be done.
        If for some reason you want to disable NLS, then you should comment
        out the LANG variable or remove them from your AUTOEXEC.BAT file or
        select 'C' as your catalog:

          set LANG=C

        or clear it by setting:

          set LANG=

        You can also change during a DOS session in Win9X or on plain DOS the
        values of the LANG and LANGUAGE variables by setting or clearing them
        from the DOS prompt.

2.5.:   To create an entry for the gettext info docs in your dir file
        run from the top DJGPP installation directory the command:
          install-info --info-dir=./info ./info/gettext.info

2.6.:   The binaries distributed in this package have NLS support.
        E.G. run the command:
          xgettext
        and the binary should talk to you in your mother tonge, if
        supported.
        For futher information about GNU gettext please read the info docs.


3.:     Building the binaries from sources.
        ===================================

3.1.:   To build the binaries you will need the following binary packages:
          djdev203.zip (or a later but NOT a prior version)
          bsh203b.zip  (or a later but NOT a prior version)
          gcc303b.zip, bnu2112b.zip, mak3791b.zip,
          fil40b.zip, shl20jb.zip, txt20b.zip,
          txi40b.zip, grep24b.zip, sed3028b.zip,
          licv17b.zip

        If you want to run the check you will need also:
          dif272b.zip

        If you want to recreate the html docs you will also need:
          gro116b.zip  (or a later but NOT a prior version)
          perl561b.zip (or a later but NOT a prior version)

        All this packages can be found in the v2gnu directory of any
        Simtel.NET mirror.
        You must have licv17b.zip or a later version installed before
        configuring or compiling the package or the configuration and build
        process will fail due to unresolved references to libiconv.a
        You will need bsh203b.zip or later and *NOT* a prior version or the
        build will fail. The same applies to djdev203.zip.
        This updated versions have been recompiled with djdev203.zip and know
        about the "/dev/env" functionality introduced with djdev203.zip. All the
        other packages are the ones I have used to build the binaries from this
        sources. Previuos versions of this packages may do the job as well but
        I have not tested this.

3.2.:   Create a temporary directory and copy the source package into the
        directory. If you download the source distribution from one of the
        DJGPP archives, just unzip it preserving the directory structure
        running *ONE* of the following commands:
          unzip32 gtxt@packageversion@s.zip      or
          djtarx gtxt@packageversion@s.zip       or
          pkunzip -d gtxt@packageversion@s.zip

        Source distributions downloaded from one of the GNU FTP sites need
        some more work to unpack. First, you *MUST* use the `djtar' program
        to unzip the package.  That is because some file names in the official
        distributions need to be changed to avoid problems on the various
        platforms supported by DJGPP. `djtar' can rename files on the fly given
        a file with name mappings. The distribution includes a file
        `djgpp/fnchange.lst' with the necessary mappings. So you need first
        to retrieve that file, and then invoke `djtar' to unpack the
        distribution. Here is how:

          djtar -x -p -o @V@/djgpp/fnchange.lst @V@.tar.gz > lst
          djtar -x -n lst @V@.tar.gz

        (The name of the distribution archive and the top-level directory will
        be different for versions other than @VER@.)

        It is always recommended to download the DJGPP packages from some
        Simtel.NET mirror and *NOT* the original GNU distribution because
        only the binary distribution of the DJGPP port will contain the
        files needed to patch libc.a. This are: conio.h and conio.o.

3.3.:   This package is preconfigured for NLS support and for run time recoding
        due to the functionality provided by libiconv.a from licv17b.zip.
        This implies that licv17b.zip *MUST* be installed *before* you try to
        compile the package or the build process will fail.
        It should be noticed that when you compile your own binaries with NLS
        you must also *always* link with libiconv.a
        If you compile this package with a later version of libc.a or if you
        prefer no NLS support at all you will have to reconfigure this package.
        The configuration batch file of this package, located in the djgpp
        directory, allows you to enable or disable NLS support and to compile
        from a different partition than from where the sources are located.
        config.bat always configures the package for NLS support enabled and
        for in-place compilation if no options are given.
        The available NLS options are:
          NLS
          no-NLS

        If for some reason you want no NLS support you will have to reconfigure
        the package. For this purpose cd into the top srcdir (gtxt-@treeversion@)
        and run the following commands:
          make distclean
          djgpp\config no-NLS

        This step is **NOT** optional and the "distclean" option must be used.
        If you do not use the "distclean" option the config.cache file will not
        be deleted. In this case you are **NOT** reconfiguring because the
        configuration informations are read from the cache file instead of being
        newly computed.
        You **MUST** specify "no-NLS" or config.bat will default to "NLS".
        To build the programs in a directory other than where the sources are,
        you must add a parameter that specifies the source directory,
        e.g:
          x:\src\gnu\gtxt-@treeversion@\djgpp\config x:/src/gnu/gtxt-@treeversion@ no-NLS

        Lets assume you want to build the binaries in a directory placed on a 
        different drive (z:\build in this case) from where the sources are,
        then you will run the following commands:
          z:
          cd \build
          x:\src\gnu\gtxt-@treeversion@\djgpp\config x:/src/gnu/gtxt-@treeversion@ no-NLS

        If you want NLS support you will omit "no-NLS" or replace it by
        "NLS" in the above examples.
        The order of the "NLS" option and the srcdir option does *NOT* matter.
        You *MUST* use forward slashes to specify the source directory.

        This batch file will set same environment variables, make MSDOS
        specific modifications to the Makefile.ins and supply all other
        needed options to the configure script.

3.4.:   To compile the package run from the top srcdir the command:
          make

3.5.:   Now you can run the tests if you like.
        From the top srcdir run the command:
          make check

        Non test should fail.

3.6.:   To install the binaries, header, library, catalogs, and info docs
        run the following command from the top srcdir:
          make install CATALOGS="xx.gmo yy.gmo zz.gmo"
        or
          make install CATALOGS="xx.gmo yy.gmo zz.gmo" prefix=z:/some/other/place

        This will install the products into your DJGPP installation tree given
        by the default prefix "/dev/env/DJDIR". If you prefer to install them
        into some other directory you will have to set prefix to the appropiate
        value. Replace xx, yy and zz by the language codes of the catalogs you
        want to install.

3.7.:   Now you have to set the LANG environment variable.
        Please refer to section 2.3 for further information.


4.:     NLS support for other DJGPP ports.
        ==================================

        This package contains all needed files to get NLS support for the
        following DJGPP ports:
          bison-1.32 (bsn132s.zip)
          enscript-1.6.2 (ens162s.zip)
          fileutils-4.0 (fil40s.zip)
          grep-2.4 (grep24s.zip)
          id-utils-3.2 (idu32s.zip)
          make-3.79.1 (mak3791s.zip)
          recode-3.6 (rcode36s.zip)
          sed-3.02.80 (sed3028s.zip)
          sharutils-4.2c (shar42cs.zip)
          sh-utils-2.0j (shl20js.zip)
          tar-1.12a (tar112as.zip)
          texinfo-4.0 (txi40s.zip)
          textutils-2.0 (txt20s.zip)

        The files needed are placed in the NLS_for_djgpp_packages tree located
        in djgpp directory. I will explane this using grep-2.4 as example.
        This means that file names or command names may change from port to port.
        The configuration batch files and the sed scripts of every package have
        the same name as the original ones that this ones will replace. If you
        are familiar with the original package you shall have no difficulties
        in reconfigure the package for NLS support.
        Please inspect the tree NLS_for_djgpp_packages to see what files will
        be replaced.

4.1.:   To reconfigure and recompile a source package with NLS support you
        *MUST* install the gtxt@packageversion@b.zip and licv17b.zip packages
        first. NLS support will **NOT** work with any prior version of the above
        mentioned packages. Before installing gtxt@packageversion@b.zip and licv17b.zip
        you *MUST* deinstall the old packages if you ever have installed them.
        For this purpose use the provided manifest files from the old packages.
        Old packages means previous beta releases of gtxt@packageversion@b.zip and licv17b.zip
        *AND* also previous versions of gettext like gettext 0.10.32, etc.

4.2.:   We will assume that the required sources will be unzipped into
        a directory called src.
        Copy grep24s.zip into /src and decompress them preserving the directory
        structure running the command:
          unzip32 *.zip
        This will create the directory:
          /src/gnu/grep-2.4

        The binary package gtxt@packageversion@b.zip will create the directory:
          %DJDIR%/gnu/gtxt-@treeversion@/djgpp/NLS_for_djgpp_packages/grep-2.4
        This directory contains all needed files.
        The files are:
          grep-2.4/djgpp/config.bat  (new .bat file that replaces the original one.)
          grep-2.4/djgpp/config.sed  (sed script needed to modify configure.)
          grep-2.4/djgpp/config.site (defaults for configure.)

        Now we will xcopy the needed files into the original grep-2.4 directory.
        First we will cd into the grep-2.4 directory and then we will run the
        following command:
          xcopy %DJDIR%\gnu\gtxt-@treeversion@\djgpp\NLS_for_djgpp_packages\grep-2.4 /v/s/e

4.3.:   Before the package can be reconfigured, the old configuration must be
        cleared. Run the command:
          make distclean

        This will remove all Makefiles, config.h and config.cache file with old
        configuration information. This step is *NOT* optional and it must be
        used the "distclean" target.

4.4.:   Now the package can be configured running the command:
          djgpp\config
        if you want to build the products in the /src/grep-2.4 directory, or:
          c:\src\grep-2.4\djgpp\config c:/src/grep-2.4
        if you want to build the products on a different drive or directory.
        You can still configure without NLS support if you want. In this case
        simply add the option "no-NLS" to the above commands.

4.5.:   Now the package can be compiled and checked by running the commands:
          make
          make check
        The first command will create also all the available translation
        catalogs (.gmo files). Before running the tests you should clear
        the LANGUAGE and/or LANG variable or the tests will probably fail.

4.6.:   Now the products can be installed by running the command:
          make install CATALOGS="xx.gmo yy.gmo"

        Replace xx and yy by the appropiate language codeof the catalogs you
        want to install. If you omit CATALOGS then all catalogs will be installed.
        You can install into a temp directory if you want by specifying a prefix:
          make install prefix=z:/tmp CATALOGS="xx.gmo yy.gmo zz.gmo"

4.7.:   Now you have to set the LANG and LANGUAGE environment variable.
        Please refer to 2.4.


        Send GNU gettext specific bug reports to <bug-gnu-gettext@gnu.org>.
        Send suggestions and bug reports concerning the DJGPP port to
        comp.os.msdos.djgpp or <djgpp@delorie.com>.


Enjoy.

        Guerrero, Juan Manuel <st001906@hrz1.hrz.tu-darmstadt.de>
This is a port of GNU Gettext 0.11.5 to MSDOS/DJGPP.


        TO USE THE GNU GETTEXT LIBRARY YOU **MUST** MODIFY YOUR C-LIBRARY.
        PLEASE, READ SECTION #2 (Installing the binary package) CAREFULLY
        TO LEARN HOW TO INSTALL THE GNU GETTEXT LIBRARY AND HOW TO CHANGE
        YOUR C-LIBRARY AND SYSTEM HEADER FILE.
        TO USE THE GNU GETTEXT LIBRARY YOU **MUST** DOWNLOAD AND INSTALL
        LICV17B.ZIP TOO. THIS IS **NOT** OPTIONAL.
        IT IS NOT RECOMMED TO DOWNLOAD THE GNU DISTRIBUTION OF GETTEXT
        BECAUSE ONLY THE DJGPP PORT WILL CONTAIN THE REQUIRED HEADER AND
        OBJECT FILE TO PATCH THE C LIBRARY.


1.:     DJGPP specific changes.
        =======================

        The DJGPP specific changes are the followings:
     1) The conflict existing between the BORLAND-compatibility gettext function
        from DJGPP's libc.a defined in conio.h and the GNU gettext function from
        libintl.a defined in libintl.h has been removed. But this conflict can not
        be removed **WITHOUT** changing a system header file and libc.a.
     1.1)  libc.a and system header changes.
        In conio.c, the BORLAND-compatibility gettext function has been renamed
        into _conio_gettext. In conio.h some code has been added to check if
        libintl.h is included or not by the same source file. If libintl.h is NOT
        included, the BORLAND-compatibility gettext function will be available as
        gettext. If libintl.h has been included then the BORLAND-compatibility
        gettext function will **ONLY** be available as _conio_gettext.
        The BORLAND-compatibility gettext function is now available as gettext
        and _conio_gettext.
     1.2)  GNU gettext library changes.
        If both headers, libintl.h and conio.h, are included in the same source
        file the gettext keyword makes **ALWAYS** reference to the GNU gettext
        function and **NEVER** to the BORLAND-compatibility gettext function.

     2) The binary package gtxt05b.zip contains all needed files to get NLS
        support for the following DJGPP ports:
          bison-1.32 (bsn132s.zip)
          enscript-1.6.2 (ens162s.zip)
          fileutils-4.0 (fil40s.zip)
          grep-2.4 (grep24s.zip)
          id-utils-3.2 (idu32s.zip)
          make-3.79.1 (mak3791s.zip)
          recode-3.6 (rcode36s.zip)
          sed-3.02.80 (sed3028s.zip)
          sharutils-4.2c (shar42cs.zip)
          sh-utils-2.0j (shl20js.zip)
          tar-1.12a (tar112as.zip)
          texinfo-4.0 (txi40s.zip)
          textutils-2.0 (txt20s.zip)

        See section #4 for further information about this issue.
        To implement NLS support for one of those packages you will also need
        to download the following packages:
          gtxt05b.zip (binaries of GNU Gettext 0.11.5)
          licv17b.zip (binaries of GNU libiconv 1.7)
          fil40b.zip  (binaries of GNU Fileutils 4.0)
          shl20jb.zip (binaries of GNU Sh-utils 2.0j)


2.:     Installing the binary package.
        ==============================

2.1.:   To use this binary package you **MUST** install licv17b.zip or later
        first. licv17b.zip provides the required functionality to recode the
        .mo files at run time from the unix charsets used to create them to the
        dos codepages used to display them. Copy the binary distribution into
        the top DJGPP installation directory. If you are installing Gettext on
        a dual DOS/WINDOWS 9X systems, you *MUST* first turn off the generation
        of numeric tails for 8.3 aliases Windows creats for long file names.
        For information about how to do this, please read the DJGPP FAQ List
        V 2.30, chapter 22.19: "How to Set Up a Dual DOS/Windows Installation".
        It should be noticed that neither the libintl.a library nor the
        binaries (xgettext.exe, gettext.exe, etc.) contain any code to handle
        nuneric tails of short file names. This implies that if you install
        the binary packages in a DOS box of Win9X (LFN) **WITHOUT** turning
        off the numeric tail generation you will **NOT** be able to use NLS
        on plain DOS. Once again: if you want NLS support on both Win9X **AND**
        on plain DOS you **MUST** turn off the numeric tail generation **BEFORE**
        installing the binary package. After having installed the package
        you can turn on numerical tail generation again if you wish.
        All this also applies to any other package that has been compiled with
        NLS support. You **MUST** turn off numeric tail generation every time
        you install a package that has been compiled with NLS or the binaries
        will **NOT** be able to find their .mo files (translations) when you
        switch to plain DOS.

2.2.:   Copy the binary distribution into the top DJGPP installation directory,
        just unzip it preserving the directory structure running *ONE* of the
        following commands:
          unzip32 gtxt05b.zip      or
          djtarx gtxt05b.zip       or
          pkunzip -d gtxt05b.zip

2.3.:   Changing libc.a and conio.h.
        Apart from the ussual directories, the binary package will create the
        following directory:
          %DJDIR%/gnu/gtxt-011.5/djgpp/djdev-2.03
        where %DJDIR% stands for the root of your DJGPP installation tree.
        Cd into the djdev-2.03 directory. You will find the following files:
          conio.diffs
          conio.h
          conio.o
        conio.diffs is a patch file that documents the changes I have done against
        the files of the original djdev203.zip and djlsr203.zip distributions.
        This file is not needed by the average user. conio.h is the modified header
        and conio.o is the recompiled new conio.c file that will replace the old
        conio.o contained in libc.a.

        For all commands that will follow now I will assume that you have
        cd'ed into the %DJDIR%/gnu/gtxt-011.5/djgpp/djdev-2.03 directory,
        where %DJDIR% represents the path to your DJGPP installation. First,
        you should backup your old header and library. For this task, run the
        following command sequence (cp is the copy program from fil40b.zip):
          cp /dev/env/DJDIR/include/conio.h /dev/env/DJDIR/include/conio.bak
          cp /dev/env/DJDIR/lib/libc.a /dev/env/DJDIR/lib/libc.bak

        Now you can copy the new header into your include directory
        running the command:
          cp conio.h /dev/env/DJDIR/include

        Now you can substitute the old conio.o file in libc.a with the new one.
        For this task you will need the ar program from binutils.
        Run the command:
          ar -rv /dev/env/DJDIR/lib/libc.a conio.o
        You are done.

2.3.:   The NLS controling environment variables, LANG and LANGUAGE, must be
        set to their appropiate values. The exact way how these variables
        should be set depends on your operating system:

        * For Windows 98 systems:
          - Click START;
          - Choose Programs->Accessories->System Tools->System Information;
          - Click Tools in the menu-bar, then choose "System Configuration";
          - Use the tab provided there for editing your AUTOEXEC.BAT as
            explained below.

        * For Windows NT systems:
          - Right-click "My Computer", then select "Properties";
          - Click the "Environment" tab;
          - Add a new variables LANG and LANGUAGE and set their values to the
            wanted language codes file as explained below.

        * For all other systems (DOS, Windows 3.X and Windows 95): use any
          text editor, e.g. the standard EDIT, to edit the file AUTOEXEC.BAT
          in the root directory of the boot drive (usually, C:).

        The values of the two environment variables LANG and LANGUAGE should be
        set like this:

          set LANG=xx
          set LANGUAGE=yy:zz

        xx, yy and zz are place holders for the wanted language codes. For
        posible values, please read below.
        The LANG entry is obligatory, the LANGUAGE entry may be omited. The
        LANG variable selects the locale charsets (dos codepage) to be used to
        display the program's output and the catalog (.mo file) that contains
        the translated strings to be used. The LANGUAGE variable allows you to
        select an alternate catalog than the one stipulated by LANG. Replace
        xx, yy and zz by the language code of the catalogs you want to use. It
        should be noticed that LANGUAGE has *ALWAYS* higher priority than LANG.
        The LANG variable not only selects a catalog, it also specifies the dos
        codepage that will be used as locale charset. All this means that the
        translation strings contained in the catalogs (.mo files) will be
        recoded at runtime to the dos codepage stipulated by the value of LANG.
        This runtime recoding is needed because the .mo files may have been
        written using a charset that is not compatible with the charset that
        will be used on the machine and OS where the .mo files contents will be
        displayed. The .po files of the GNU packages, from which the .mo files
        are generated, are typical examples of this. Usualy, they have been
        written using some ISO-8859-nn charset (an unix charset) and shall be
        displayed on a DOS/WIN95 machine that uses some dos codepage.

        Some examples:
        If you only want to use the catalog containing the translations for
        your mother tongue (in my case the spanish translations) the above
        lines will only use the LANG variable and will look like this:

          set LANG=es

        In this case, LANG defines the locale charset (CP850 in this case) to
        be used for the on-the-fly recoding of the catalog (.mo file) contents
        **AND AT THE SAME TIME** the translation/language (.mo file) to be used.

        If you want to use the spanish (es) and german (de) catalogs the above
        lines will look like this:

          set LANG=es
          set LANGUAGE=es:de

        In this case a DJGPP binary that has been compiled with NLS support
        will first search for the spanish translation of a string. If a
        translation for that particular string can not be found in the spanish
        .mo file then it will search for a german translation of that string in
        the german .mo file and if a german translation of that string can also
        not been found it will default to display the build-in english string.
        No mather if a spanish, a german or an english build-in string is
        selected, the string is always recoded to the dos codepage stipulated
        by LANG. In this case: CP850. In the above example, LANGUAGE defines
        the set of languages to be used and their priority (from left to right).
        At the same time, LANG defines the locale charset (dos codepage) to be
        used to recode **ALL** translated string, no matter which language
        (.mo file) is used.
        If you want to reverse this search order the above lines would look
        like this one:

          set LANG=es
          set LANGUAGE=de:es

        Now let us assume that an user wants to use the swedish catalogs on
        a machine that loads codepage CP437 when it is booted. It should be
        noticed that the locale charset for Sweden is CP850 and not CP437.
        In this case, the lines must look like this:

          set LANG=en_US
          set LANGUAGE=sv

        LANG reflects the available codepage/charset and LANGUAGE selects the
        wanted translation catalog. en_US means CP437. Now, the contents of the
        catalog are recoded to CP437 instead to CP850 because CP437 is the
        codepage used to display messages on screen. Of course, not every
        combination of catalogs and locale charset (dos codepages) makes sense.
        E.G.: selecting as locale charset chinese (LANG=zh_TW) and the french
        translations (LANGUAGE=fr) will certainly not generate an usefull
        screen output.

        The content of LANG is a language code. Examples are fr for french,
        en_US for US english, etc. This language code is an alias for the
        locale charset to be used for runtime recoding. The complete list of
        all available aliases can be found in %DJDIR%/lib/charset.alias. This
        file is a table with two entries: left entry is the alias (en_US,
        de_AT, etc.), right entry is the corresponding dos codepage that will
        be used for that language code (alias). It should be noticed that it is
        also possible to select a codepage directely. E.G.: Instead of setting:

          set LANG=en_US

        you may directely set:

          set LANG=CP437

        cp437 or 437 are also valid settings for CP437. This overwrites any
        settings in charset.alias. The settings in the environment always
        overwrite the settings in charset.alias. Please note that if you omit
        LANG, LANGUAGE will not be honored at all. Because the information
        about which locale charset shall be used for recoding is needed,
        if LANG is omitted by the user this information will not be available
        and consequently LANGUAGE will be ignored and no translation at all
        will be done.
        If for some reason you want to disable NLS, then you should comment
        out the LANG variable or remove them from your AUTOEXEC.BAT file or
        select 'C' as your catalog:

          set LANG=C

        or clear it by setting:

          set LANG=

        You can also change during a DOS session in Win9X or on plain DOS the
        values of the LANG and LANGUAGE variables by setting or clearing them
        from the DOS prompt.

2.5.:   To create an entry for the gettext info docs in your dir file
        run from the top DJGPP installation directory the command:
          install-info --info-dir=./info ./info/gettext.info

2.6.:   The binaries distributed in this package have NLS support.
        E.G. run the command:
          xgettext
        and the binary should talk to you in your mother tonge, if
        supported.
        For futher information about GNU gettext please read the info docs.


3.:     Building the binaries from sources.
        ===================================

3.1.:   To build the binaries you will need the following binary packages:
          djdev203.zip (or a later but NOT a prior version)
          bsh203b.zip  (or a later but NOT a prior version)
          gcc303b.zip, bnu2112b.zip, mak3791b.zip,
          fil40b.zip, shl20jb.zip, txt20b.zip,
          txi40b.zip, grep24b.zip, sed3028b.zip,
          licv17b.zip

        If you want to run the check you will need also:
          dif272b.zip

        If you want to recreate the html docs you will also need:
          gro116b.zip  (or a later but NOT a prior version)
          perl561b.zip (or a later but NOT a prior version)

        All this packages can be found in the v2gnu directory of any
        Simtel.NET mirror.
        You must have licv17b.zip or a later version installed before
        configuring or compiling the package or the configuration and build
        process will fail due to unresolved references to libiconv.a
        You will need bsh203b.zip or later and *NOT* a prior version or the
        build will fail. The same applies to djdev203.zip.
        This updated versions have been recompiled with djdev203.zip and know
        about the "/dev/env" functionality introduced with djdev203.zip. All the
        other packages are the ones I have used to build the binaries from this
        sources. Previuos versions of this packages may do the job as well but
        I have not tested this.

3.2.:   Create a temporary directory and copy the source package into the
        directory. If you download the source distribution from one of the
        DJGPP archives, just unzip it preserving the directory structure
        running *ONE* of the following commands:
          unzip32 gtxt05s.zip      or
          djtarx gtxt05s.zip       or
          pkunzip -d gtxt05s.zip

        Source distributions downloaded from one of the GNU FTP sites need
        some more work to unpack. First, you *MUST* use the `djtar' program
        to unzip the package.  That is because some file names in the official
        distributions need to be changed to avoid problems on the various
        platforms supported by DJGPP. `djtar' can rename files on the fly given
        a file with name mappings. The distribution includes a file
        `djgpp/fnchange.lst' with the necessary mappings. So you need first
        to retrieve that file, and then invoke `djtar' to unpack the
        distribution. Here is how:

          djtar -x -p -o gettext-0.11.5/djgpp/fnchange.lst gettext-0.11.5.tar.gz > lst
          djtar -x -n lst gettext-0.11.5.tar.gz

        (The name of the distribution archive and the top-level directory will
        be different for versions other than 0.11.5.)

        It is always recommended to download the DJGPP packages from some
        Simtel.NET mirror and *NOT* the original GNU distribution because
        only the binary distribution of the DJGPP port will contain the
        files needed to patch libc.a. This are: conio.h and conio.o.

3.3.:   This package is preconfigured for NLS support and for run time recoding
        due to the functionality provided by libiconv.a from licv17b.zip.
        This implies that licv17b.zip *MUST* be installed *before* you try to
        compile the package or the build process will fail.
        It should be noticed that when you compile your own binaries with NLS
        you must also *always* link with libiconv.a
        If you compile this package with a later version of libc.a or if you
        prefer no NLS support at all you will have to reconfigure this package.
        The configuration batch file of this package, located in the djgpp
        directory, allows you to enable or disable NLS support and to compile
        from a different partition than from where the sources are located.
        config.bat always configures the package for NLS support enabled and
        for in-place compilation if no options are given.
        The available NLS options are:
          NLS
          no-NLS

        If for some reason you want no NLS support you will have to reconfigure
        the package. For this purpose cd into the top srcdir (gtxt-011.5)
        and run the following commands:
          make distclean
          djgpp\config no-NLS

        This step is **NOT** optional and the "distclean" option must be used.
        If you do not use the "distclean" option the config.cache file will not
        be deleted. In this case you are **NOT** reconfiguring because the
        configuration informations are read from the cache file instead of being
        newly computed.
        You **MUST** specify "no-NLS" or config.bat will default to "NLS".
        To build the programs in a directory other than where the sources are,
        you must add a parameter that specifies the source directory,
        e.g:
          x:\src\gnu\gtxt-011.5\djgpp\config x:/src/gnu/gtxt-011.5 no-NLS

        Lets assume you want to build the binaries in a directory placed on a 
        different drive (z:\build in this case) from where the sources are,
        then you will run the following commands:
          z:
          cd \build
          x:\src\gnu\gtxt-011.5\djgpp\config x:/src/gnu/gtxt-011.5 no-NLS

        If you want NLS support you will omit "no-NLS" or replace it by
        "NLS" in the above examples.
        The order of the "NLS" option and the srcdir option does *NOT* matter.
        You *MUST* use forward slashes to specify the source directory.

        This batch file will set same environment variables, make MSDOS
        specific modifications to the Makefile.ins and supply all other
        needed options to the configure script.

3.4.:   To compile the package run from the top srcdir the command:
          make

3.5.:   Now you can run the tests if you like.
        From the top srcdir run the command:
          make check

        Non test should fail.

3.6.:   To install the binaries, header, library, catalogs, and info docs
        run the following command from the top srcdir:
          make install CATALOGS="xx.gmo yy.gmo zz.gmo"
        or
          make install CATALOGS="xx.gmo yy.gmo zz.gmo" prefix=z:/some/other/place

        This will install the products into your DJGPP installation tree given
        by the default prefix "/dev/env/DJDIR". If you prefer to install them
        into some other directory you will have to set prefix to the appropiate
        value. Replace xx, yy and zz by the language codes of the catalogs you
        want to install.

3.7.:   Now you have to set the LANG environment variable.
        Please refer to section 2.3 for further information.


4.:     NLS support for other DJGPP ports.
        ==================================

        This package contains all needed files to get NLS support for the
        following DJGPP ports:
          bison-1.32 (bsn132s.zip)
          enscript-1.6.2 (ens162s.zip)
          fileutils-4.0 (fil40s.zip)
          grep-2.4 (grep24s.zip)
          id-utils-3.2 (idu32s.zip)
          make-3.79.1 (mak3791s.zip)
          recode-3.6 (rcode36s.zip)
          sed-3.02.80 (sed3028s.zip)
          sharutils-4.2c (shar42cs.zip)
          sh-utils-2.0j (shl20js.zip)
          tar-1.12a (tar112as.zip)
          texinfo-4.0 (txi40s.zip)
          textutils-2.0 (txt20s.zip)

        The files needed are placed in the NLS_for_djgpp_packages tree located
        in djgpp directory. I will explane this using grep-2.4 as example.
        This means that file names or command names may change from port to port.
        The configuration batch files and the sed scripts of every package have
        the same name as the original ones that this ones will replace. If you
        are familiar with the original package you shall have no difficulties
        in reconfigure the package for NLS support.
        Please inspect the tree NLS_for_djgpp_packages to see what files will
        be replaced.

4.1.:   To reconfigure and recompile a source package with NLS support you
        *MUST* install the gtxt05b.zip and licv17b.zip packages
        first. NLS support will **NOT** work with any prior version of the above
        mentioned packages. Before installing gtxt05b.zip and licv17b.zip
        you *MUST* deinstall the old packages if you ever have installed them.
        For this purpose use the provided manifest files from the old packages.
        Old packages means previous beta releases of gtxt05b.zip and licv17b.zip
        *AND* also previous versions of gettext like gettext 0.10.32, etc.

4.2.:   We will assume that the required sources will be unzipped into
        a directory called src.
        Copy grep24s.zip into /src and decompress them preserving the directory
        structure running the command:
          unzip32 *.zip
        This will create the directory:
          /src/gnu/grep-2.4

        The binary package gtxt05b.zip will create the directory:
          %DJDIR%/gnu/gtxt-011.5/djgpp/NLS_for_djgpp_packages/grep-2.4
        This directory contains all needed files.
        The files are:
          grep-2.4/djgpp/config.bat  (new .bat file that replaces the original one.)
          grep-2.4/djgpp/config.sed  (sed script needed to modify configure.)
          grep-2.4/djgpp/config.site (defaults for configure.)

        Now we will xcopy the needed files into the original grep-2.4 directory.
        First we will cd into the grep-2.4 directory and then we will run the
        following command:
          xcopy %DJDIR%\gnu\gtxt-011.5\djgpp\NLS_for_djgpp_packages\grep-2.4 /v/s/e

4.3.:   Before the package can be reconfigured, the old configuration must be
        cleared. Run the command:
          make distclean

        This will remove all Makefiles, config.h and config.cache file with old
        configuration information. This step is *NOT* optional and it must be
        used the "distclean" target.

4.4.:   Now the package can be configured running the command:
          djgpp\config
        if you want to build the products in the /src/grep-2.4 directory, or:
          c:\src\grep-2.4\djgpp\config c:/src/grep-2.4
        if you want to build the products on a different drive or directory.
        You can still configure without NLS support if you want. In this case
        simply add the option "no-NLS" to the above commands.

4.5.:   Now the package can be compiled and checked by running the commands:
          make
          make check
        The first command will create also all the available translation
        catalogs (.gmo files). Before running the tests you should clear
        the LANGUAGE and/or LANG variable or the tests will probably fail.

4.6.:   Now the products can be installed by running the command:
          make install CATALOGS="xx.gmo yy.gmo"

        Replace xx and yy by the appropiate language codeof the catalogs you
        want to install. If you omit CATALOGS then all catalogs will be installed.
        You can install into a temp directory if you want by specifying a prefix:
          make install prefix=z:/tmp CATALOGS="xx.gmo yy.gmo zz.gmo"

4.7.:   Now you have to set the LANG and LANGUAGE environment variable.
        Please refer to 2.4.


        Send GNU gettext specific bug reports to <bug-gnu-gettext@gnu.org>.
        Send suggestions and bug reports concerning the DJGPP port to
        comp.os.msdos.djgpp or <djgpp@delorie.com>.


Enjoy.

        Guerrero, Juan Manuel <st001906@hrz1.hrz.tu-darmstadt.de>

Welcome!
========

This is the OS/2 port of GNU gettext internationalization library.


Compatibility
=============

The library has been compiled with -Zmt flag, but it doesn't matter as soon
as you use the EMX single-threaded runtime fix (emx-strt-fix-0.0.2.zip).

The library is fully compatible with the previous port of gettext library
(0.10.35) which is largely used especialy by XFree86/2 programs. All the
old programs that I have with gettext support run fine with the new version
of the DLL.


Installation
============

If you set the GNULOCALEDIR environment variable to point to your
x:/xxx/share/locale directory, it will override any other setting. That is,
unpack the binary distribution over /emx, set GNULOCALEDIR=x:/emx/share/locale
(where x: is the drive letter of your EMX installation) and that's all.

If you use the UNIXROOT environment variable, the default catalogue search
paths will be like on Unices, e.g. $(UNIXROOT)/usr/lib and
$(UNIXROOT)/usr/share/locale. GNULOCALEDIR always overrides this.

Now if you haven't did it earlier, set the language identifier that you use.
This is done by adding a "SET LANG=xxx" environment setting to your CONFIG.SYS,
where xxx is the identifier of your language (example: en_UK for English in UK,
ru_RU for Russian in Russia. Also you can use names like "russian", "italian"
and so on - see the share/locale/locale.alias file).

This port of gettext supports character set conversions. This means that if
your .mo files were written using new gettext guidelines, e.g. they contain a
message like this:

msgid ""
msgstr "Content-Type: text/plain; charset=koi8-r\n"

the messages will be properly converted to your active codepage using OS/2
Unicode API. For example, russian message catalog gettext.mo is in the
KOI8-R (codepage 878) encoding while OS/2 uses codepage 866. Now when you
run any of these tools it detects that the active OS/2 codepage is 866 and
performs the translation from CP878 -> CP866 for every message.

If you want to override the character set used to output messages (for example
in XFree86 for Russian the KOI8-R encoding (codepage 878) is used) you can
set the output character set by adding a postfix to the LANG environment
variable, this way:

set LANG=ru_RU.KOI8-R

or (equivalent):

set LANG=ru_RU.CP878

or (same effect):

set LANG=ru_RU.IBM-878

If the output character set is ommited from the LANG variable, the default
codepage is ALWAYS taken from the operating system (e.g. the codepage setting
from locale.alias is always ignored, so "russian" stays just for "ru_RU" and
not for "ru_RU.ISO-8859-5"); you may want to set it just if you want to
override the active OS/2 codepage.


XFree86 setup
=============

If you use XFree86 and the OS/2 default character set is different from the
XFree86 default character set (e.g. for Russain CP866 vs KOI8-R), you can add
the following (or similar) statement to your startx.cmd file (after the
commands dealing with HOME and X11SHELL):

call VALUE 'LANG', 'ru_RU.KOI8-R', env

Otherwise you can get incorrect (wrong codepage) output from programs that
previously worked (e.g. GIMP 1.22). This is because earlier versions of gettext
didn't support character set translations.


Implementation remarks
======================

The codepage conversion code uses OS/2 Unicode API, thus it falls under the
limits that OS/2 Unicode API has. For example, OS/2 Unicode API does not
support the BIG5 East Asian character set nor ISO-8859-X where X > 9 (at
least with Warp4 with fixpack 14 that I have). If someone knows the
OS/2 API identifiers for BIG5 or ISO8859-10,... encodings, please tell me!

Since gettext 0.11 iconv emulation layer supports correctly UTF-8. Also
I have added theoretical support for the following East Asian encodings:
EUC-JP, EUC-KR, EUC-TW, EUC-CN. However, these encodings are (I believe)
supported only on East Asian editions of OS/2. The code pages for them are
listed in the \language\codepage\ucstbl.lst file but the codepage files
themselves are missing; I believe they are ommited from European OS/2's
due to their large size.

Also I have added "support" for the BIG5 codeset as an alias for IBM-950
codepage. However, I'm not very sure about this; in any case OS/2 does not
support (as far as I know) anything closer to BIG5.


Additional API
==============

This package provides additionaly the iconv() API that can be used by
developers for doing more feature-full Unix ports. The iconv() API is used
to convert text between various codepages. The intl.h header file contains
the prototypes and definitions needed for iconv(); if you configure software
with autoconf it possibly will find intl.h and set up the software accordingly.

All these functions are exported from INTL.DLL. The iconv.a import library
imports all the iconv* functions from INTL.DLL. So, like on Unix, now you can
#include <iconv.h>, then link with -liconv and you will get a fully functional
iconv implementation.


Rebuilding the library
======================

The library is quite easy to rebuild. Since the OS/2 support is provided now
out-of-the-box in gettext, you just have to download and unpack the source
archive. Now there are two ways to rebuild the gettext library:

1. If you're a masochist you can go the clumsy configure/make Unix way. This
is not recommended however as I found no way to tell libtool to generate a
slightly non-standard DLL which will be backward compatible with gettext
0.10.35. The compatibility is achieved by prepending backward.def to the
export definition file generated with emximp or somehow else. Thus it is
highly recommended you build using the second way, if it is possible.

2. Go to os2 and just run `make'. If you have all the required tools,
it should painlessly compile. Finally, if you want a binary distribution
archive, do `make distr'. The weak side of building this way is that makefile
is somewhat fragile. This means that if the makefile is left unmodified and
a new version of gettext is rolled out, it *may* not work. But every possible
attempt was made to ensure that the makefile takes most important build
parameters from their autoconf counterparts.

WARNING: Due to bugs in GNU Make 3.76.1 (at least in its OS/2 port) you can
get sometimes (depending on make version and makefile modification :) funny
messages like these:

zip warning: name not matched: emx/src/gettext-0.10.40/support/os2/iconv.h

or even:

*** No rule to make target `out/release/intl.a', needed by `all'.  Stop.

Such messages are a bad joke. Ignore it, and re-run make. This is a
long-standing bug in GNU make, alas.

If you want a debug version of library, you can do `make DEBUG=1'.

If you don't have the LxLite tool installed, do `make LXLITE=0'

NB: For best results, it is highly recommended that you use at least emxbind.exe
and ld.exe from gcc 3.0.2 or later, since they contain a number of fixes that
will help you generate a more optimal DLL.


Contributors
============

Hung-Chi Chu <hcchu@r350.ee.ntu.edu.tw>
	the original port of gettext (0.10.35)

Jun SAWATAISHI <jsawa@attglobal.net>
	some more work on it and submitted the patches to GNU team, although
	they were not completely integrated.

Andrew Zabolotny <zap@cobra.ru>
	Succeeded to remove almost all OS/2-specific #ifdef's from mainstream
	source code, wrote the dedicated OS/2 makefile, wrote the iconv wrapper
	around OS/2 Unicode API, added support for locale translations.
This subpackage defines autoconf macros for linking with shared libraries.
These files are used by a program called aclocal (part of the GNU automake
package).  aclocal uses these files to create aclocal.m4 which is in turn
used by autoconf to create the configure script at the the top level in
this distribution.
Installation on Woe32 (WinNT/2000/XP, Win95/98/ME):

Building requires the mingw development environment (includes gcc).
MS Visual C/C++ with "nmake" is no longer supported.
This directory contains the tools of GNU gettext, for the translator and the
maintainer, and the associated documentation.
This directory contains simple examples of the use of GNU gettext.
Each example is a simple "hello world" program with a very small message
catalog, written in a particular programming language for a particular
environment.

    Example                    Language          GUI Environment

    hello-c                    C
    hello-c-gnome              C                 GNOME
    hello-c++                  C++
    hello-c++-qt               C++               Qt
    hello-c++-kde              C++               KDE
    hello-c++-gnome            C++               GNOME
    hello-c++-wxwidgets        C++               wxWidgets
    hello-objc                 ObjectiveC
    hello-objc-gnustep         ObjectiveC        GNUstep
    hello-objc-gnome           ObjectiveC        GNOME
    hello-sh                   Shell
    hello-python               Python
    hello-clisp                Lisp
    hello-librep               librep
    hello-guile                Scheme
    hello-smalltalk            Smalltalk
    hello-java                 Java
    hello-java-awt             Java              AWT
    hello-java-swing           Java              Swing
    hello-csharp               C#
    hello-csharp-forms         C#                Forms
    hello-gawk                 awk
    hello-pascal               Pascal
    hello-ycp                  YCP               libyui
    hello-tcl                  Tcl
    hello-tcl-tk               Tcl               Tk
    hello-perl                 Perl
    hello-php                  PHP

Before building an example, you need to
  1. Build and install the GNU gettext package, as described in the INSTALL
     file.
  2. cd to the example and do
        ./autogen.sh
  3. Then you can build the example as usual:
        ./configure --prefix=/some/prefix
        make
        make install
     and see it work by executing
        /some/prefix/bin/hello

The gettext/PHP binding has a limitation: While it works fine for standalone
PHP programs, it cannot be used inside a web server, to translate parts of web
pages into the preferred encoding of user that makes a HTTP connection. The
reason is that a web server usually is multithreaded, and the gettext() API
relies on the process' global locale.
Before you read the hello.cs source code:

Preface about GUI Programming Methodologies
===========================================

The traditional GUI programming methodology for Windows GUI programmers
is to assemble controls using a GUI builder. These GUI builders
don't have good techniques for determining the size and position of the
controls depending on their contents. Instead, they *hardcode* the
size and positions of the controls in each panel, as fixed numbers,
measured in pixels.

What are the consequences?

1) Consequences for all users:
   Such panels would not look nice when the user resizes them. So the
   programmer simply makes the dialogs non-resizable. When such a
   panel then contains a scrollable list of items, with 100 items and
   a scroll window of 5 items, a user's normal reaction is to enlarge
   the dialog, to see more items. But the dialog is not resizable!
   Frustration.

2) Consequences for disabled users:
   Some users need bigger fonts for working comfortably. Guess what
   happens when the user changes the size of the default system font?
   Many labels in dialogs are truncated.

3) Consequences for internationalization:
   The translation of a term or label in another language often needs
   more screen space. For example, Japanese translations often are 30%
   longer than the original English label. Therefore, if only the strings
   of a dialog are localized, many labels are truncated.

Problems 1 and 2 are usually accepted in the Windows programmers
community. (Problem 1 is not fatal, only frustrating. And problem 2
affects only a small proportion of the users; they are simply ignored.)
Problem 3 is "solved" by letting the localization team not only translate
the strings, but also redo the layout of each dialog.

In contrast, the methodology of programmers of the Qt/KDE, Gtk/GNOME,
wxWidgets, AWT, Swing, Tk toolkits is to have the positions and sizes
of controls determined at runtime, according to
  - the needs of the control itself,
  - the needs of the other controls in the panel,
  - the available panel size, given by the user through resizing.
The common technology for this approach is to group related controls
together in containers, and perform size and position propagations
between the controls of the container, the container, the container's
container etc. These computations are performed by so-called
"layout manager" objects.
Other technologies such as global constraint systems (as in Garnet) or
spring-like attachments are not so much in use anymore nowadays.

This programmed-resizing methodology solves the problems 1), 2) and 3).

What are the associated costs and efforts? Taking the programmed-resizing
methodology as baseline, the hardcoded sizes and positions approach has
  - the advantage that the programmer saves about 1/3 of the GUI
    programming work (namely choosing the layout managers and setting
    alignment hints),
  - the drawback that each localization team has much more work, namely
    to rearrange the controls in the panel.
In most free software projects, there are at least ca. 5 localizations;
successful projects even have 30 or 50 localizations.
In other words, a program built with hardcoded sizes and positions
cannot afford many localizations, or the effort for localization will
be prohibitively high.

For this reason, we strongly recommend to use the programmed-resizing
methodology. In this example, since the Windows.Forms package lacks
layout manager classes, we compute the layout by hand, through an
override of the OnResize method. For larger programs, we would recommend
to build a few simple layout managers, to get on par with the layout
abilities found in Qt, Swing, etc.
(The layout system of Gtk/GNOME is somewhat particular: It does not
provide the ability to set a preferred alignment on controls like labels.
Instead one uses intermediate containers for the purpose of alignment.)

Acknowledgement: This preface borrows ideas from an article of Luke Plant.

Copyright (C) 2006 Free Software Foundation, Inc.
These files are used by a program called aclocal (part of the GNU automake
package).  aclocal uses these files to create aclocal.m4 which is in turn
used by autoconf to create the configure script at the the top level in
this distribution.
GREP DJGPP README
=================

To compile the Grep package with DJGPP tools, you will need the
following tools (the names of the archives on DJGPP ftp sites where
these tools are available are in parentheses):

	  -  The basic DJGPP development environment, including the
             GCC compiler and the libraries (v2gnu/gccNNNb.zip,
             v2gnu/bnuNNNb.zip, v2/djdevNNN.zip).

	  -  GNU Make revision 3.75 or later (v2gnu/makNNNb.zip).

	  -  GNU Bash (v2gnu/bshNNNb.zip).

	  -  GNU Sed (v2gnu/sedNNNb.zip).

	  -  GNU M4 (v2gnu/m4-NNNb.zip).

	  -  GNU Fileutils (v2gnu/filNNNb.zip), Textutils
             (v2gnu/txtNNNb.zip) and Diffutils (v2gnu/difNNNb.zip).

	  -  A (previous version of) GNU Grep (v2gnu/grepNNb.zip).

Running the tests ("make check" in the top-level directory)
additionally requires Gawk (v2gnu/gwkNNNb.zip).  TAGS and ID targets
require `etags' (from the Emacs distribution) and `mkid' (from
ID-utils, v2gnu/iduNNb.zip), respectively.

All of these tools are available from the DJGPP archive sites.

To build Grep:
	  sh autogen.sh
      sh configure
	  make


Source distributions on DJGPP sites usually come pre-configured, so
all you need to do in order to build the programs is to say "make".
However, source distributions on GNU ftp sites, like ftp.gnu.org,
need to be configured by running sh configure.  You will also need
to run it if you need to configure Grep differently than for the
default configuration, for example if you want to install the programs
in a directory other than the bin subdirectory of your DJGPP
installation.

To test that the package works, say "make check".  If you don't have a
file named sh.exe somewhere on your PATH, "make check" will refuse to
run, as it needs a Unix-like shell.

To install, either copy the executables and man pages to the
appropriate directories, or say "make install".  To clean up, say
"make clean" or "make distclean".

Please note the -u and -U options that specifically target MS-DOS and
MS-Windows environments.  They are described in the Grep man page in
this distribution.

National Language Support doesn't work in this port, so don't expect
the programs to talk to you in any language but English.

Please post any problems in the DOS version to the comp.os.msdos.djgpp
news group first, especially if they have something to do with the
DOS-specific aspects.

To create the files required for the documentation package
perform the following in the docs directory:
        make grep.dvi
        make grep.ps
        makeinfo --html grep.texi -o grep.html


2.5f ported by Andrew Cottrell <anddjgpp@ihug.com.au>

Enjoy,
				Eli Zaretskii <eliz@is.elta.co.il>
Newer ALPHA versions of this code are available through
anonymous CVS:

$ cvs -d :pserver:anoncvs@subversions.gnu.org:/cvs checkout grep

If you are using an old version of cvs, you might have to run
$ cvs -d :pserver:anoncvs@subversions.gnu.org:/cvs login
first (and just press enter at the password prompt).

Please send comments and problem reports to <bug-grep@gnu.org>.
This is GNU grep, the "fastest grep in the west" (we hope).  All
bugs reported in previous releases have been fixed.  Many exciting new
bugs have probably been introduced in this revision.

GNU grep is provided "as is" with no warranty.  The exact terms
under which you may use and (re)distribute this program are detailed
in the GNU General Public License, in the file COPYING.

GNU grep is based on a fast lazy-state deterministic matcher (about
twice as fast as stock Unix egrep) hybridized with a Boyer-Moore-Gosper
search for a fixed string that eliminates impossible text from being
considered by the full regexp matcher without necessarily having to
look at every character.  The result is typically many times faster
than Unix grep or egrep.  (Regular expressions containing backreferencing
will run more slowly, however.)

See the files AUTHORS and THANKS for a list of authors and other contributors.

See the file INSTALL for compilation and installation instructions.

See the file NEWS for a description of major changes in this release.

See the file TODO for ideas on how you could help us improve grep.

Send bug reports to bug-grep@gnu.org.
$NetBSD: README,v 1.2 2012/06/12 21:01:10 jdf Exp $

The awk.texi in this directory comes from GNU awk 3.1.3 but we build
it as part of NetBSD with nawk.  We keep it here so that it is in
the right license subdirectory.
  README.MinGW
  ============

  Contributed by Keith Marshall (keith.d.marshall@ntlworld.com)


  INTRODUCTION
  ------------

  This file provides recommendations for building a Win32 implementation of
  GNU Groff, using the MinGW port of GCC for Microsoft (TM) Windows-32
  platforms.  It is intended to supplement the standard installation
  instructions (see file INSTALL); it does not replace them.

  You require both the MinGW implementation of GCC and its supporting MSYS
  toolkit, which provides a Win-32 implementation of the GNU bash shell, and a
  few other essential utilities; these may be obtained from

    http://sourceforge.net/projects/mingw

  by following the appropriate download links, where they are available as
  self-extracting executable installation packages.  If installing both from
  scratch, it is recommended that MinGW is installed first, as the MSYS
  installer can then automatically set up the proper environment for running
  MinGW.

  Additionally, if you wish to compile groff with support for its HTML output
  capability, some additional tools are required as decribed in the section
  PREREQUISITES FOR HTML OUTPUT later in this file.


  BUILDING GROFF WITH MINGW
  -------------------------

  Assuming that you have obtained the appropriate groff distribution, and that
  you are already running an MSYS shell, then the configuration, compilation,
  and installation of groff, using MinGW, is performed in much the same way as
  it is described in the INSTALL file, which is provided with the groff
  distribution.  The installation steps are summarised below:

  1. Change working directory to any suitable location where you may unpack
     the groff distribution; you must be authorized for write access.
     Approximately 30MB of free disk space are needed.

  2. Unpack the groff distribution:

       tar xzf <download-path>/groff-<version>.tar.gz

     This creates a new sub-directory, groff-<version>, containing an image of
     the groff source tree.  You should now change directory, to make this
     ./groff-<version> your working directory.

  3. If you are intending to build groff with support for HTML output, then
     you must now ensure that the prerequisites described in the later section
     PREREQUISITES FOR HTML OUTPUT are satisfied, before proceeding to build
     groff; in particular, please ensure that all required support programs
     are installed in the current PATH.

  4. You are now ready to configure, build, and install groff.  This is
     accomplished using the conventional procedure, as described in the file
     INSTALL, i.e.

       ./configure --prefix=<win32-install-path> ...
       make
       make install

     Please observe the syntax for the configure command, indicated above; the
     default value for --prefix is not suitable for use with MinGW, so the
     --prefix=<win32-install-path> option must be specified, where
     <win32-install-path> is the chosen MS-Windows directory in which the
     groff application files are to be installed (see the later section
     entitled CHOOSING AN INSTALLATION PATH).  Any other desired configuration
     options may also be specified, as described in the standard groff
     installation instructions.

  5. After completing the above, groff should be successfully installed; the
     build directory is no longer required; it may be simply deleted in its
     entirety.  Alternatively, you may choose to keep it, but to remove all
     files which can be reproduced later, by repeating the configure, make and
     make install steps; this is readily accomplished by the command

       make distclean


  This completes the installation of groff; please read the final sections of
  this file, GROFF RUNTIME ENVIRONMENT and CAVEATS AND BUGS, for advice on
  setting up the runtime environment, and avoiding known runtime problems,
  before running groff.


  CHOOSING AN INSTALLATION PATH
  -----------------------------

  It may be noted that the above instructions indicate that the ./configure
  command must be invoked with an argument specifying a preference for
  --prefix=<win32-install-path>, whereas the standard groff installation
  instructions indicate that this may be omitted, in which case it defaults to
  --prefix=/usr/local.

  In the case of building with MinGW, the default behaviour of configure is
  not appropriate for the following reasons.

  o The MSYS environment creates a virtual UNIX-like file system, with its
    root mapped to the actual MS-Windows directory where MSYS itself is
    installed; /usr is also mapped to this MSYS installation directory.

  o All of the MSYS tools, and the MinGW implementation of GCC, refer to files
    via this virtual file system representation; thus, if the
    --prefix=<win32-install-path> is not specified when groff is configured,
    `make install' causes groff to be installed in <MSYS-install-path>/local.

  o groff needs to know its own installation path, so that it can locate its
    own installed components.  This information is compiled in, using the
    exact form specified with the --prefix=<win32-install-path> option to
    configure.

  o Knowledge of the MSYS virtual file system is not imparted to groff; it
    expects the compiled-in path to its components to be a fully qualified
    MS-Windows path name (although UNIX-style slashes are permitted, and
    preferred to the MS-Windows style backslashes, to demarcate the directory
    hierarchy).  Thus, when configuring groff, if
    --prefix=<win32-install-path> is not correctly specified, then the
    installed groff application looks for its components in /usr/local, and
    most likely doesn't find them, because they are actually installed in
    <MSYS-install-path>/local.

  It is actually convenient, but by no means a requirement, to have groff
  installed in the /usr/local directory of the MSYS virtual file system; this
  makes it easy to invoke groff from the MSYS shell, since the virtual
  /usr/local/bin is normally added automatically to the PATH (the default
  PATH, as set in MSYS's /etc/profile), when MSYS is started.

  In order to install groff into MSYS's /usr/local directory, it is necessary
  to specify the fully qualified absolute MS-Windows path to this directory,
  when configuring groff, i.e.

    ./configure --prefix=<MSYS-install-path>/local ...

  For example, on a system where MSYS is installed in the MS-Windows directory
  D:\MSYS\1.0, the MSYS virtual path /usr/local resolves to the absolute
  MS-Windows native path D:\MSYS\1.0\local (the /usr component of the MSYS
  virtual path does not appear in the resolved absolute native path name since
  MSYS maps this directly to the root of the MSYS virtual file system).  Thus,
  the --prefix option should be specified to configure as

    ./configure --prefix=D:/MSYS/1.0/local ...

  Note that the backslash characters, which appear in the native MS-Windows
  form of the path name, are replaced by UNIX-style slashes in the argument to
  configure; this is the preferred syntax.

  Also note that the MS-Windows device designator (D: in this instance) is
  prepended to the specified path, in the normal MS-Windows format, and that,
  since upper and lower case distinctions are ignored in MS-Windows path
  names, any combination of upper and lower case is acceptable.


  PREREQUISITES FOR HTML OUTPUT
  -----------------------------

  If you intend to use groff for production of HTML output, then there are a
  few dependencies which must be satisfied.  Ideally, these should be resolved
  before attempting to configure and build groff, since the configuration
  script does check them.

  In order to produce HTML output, you first require a working implementation
  of Ghostscript; either the AFPL Ghostscript or the GNU Ghostscript
  implementation for MS-Windows should be suitable, depending on your
  licensing preference.  It is highly recommended to use version 8.11 or
  higher due to bugs in older versions.  These may be obtained, in the form of
  self-installing binary packages, by following the download links for the
  chosen licensing option, from http://sourceforge.net/projects/ghostscript.

  Please note that these packages install the Ghostscript interpreter required
  by groff in the ./bin subdirectory of the Ghostscript installation
  directory, with the name gswin32c.exe.  However, groff expects this
  interpreter to be located in the system PATH, with the name gs.exe.  Thus,
  to ensure that groff can correctly locate the Ghostscript interpreter, it is
  recommended that the file gswin32c.exe should be copied from the Ghostscript
  installation directory to the MSYS /usr/local/bin directory, where it should
  be renamed to gs.exe.

  In addition to a working Ghostscript interpreter, you also require several
  image manipulation utilities, all of which may be scavenged from various
  packages available from http://sourceforge.net/projects/gnuwin32, and which
  should be installed in the MSYS /usr/local/bin directory, or any other
  suitable directory which is specified in the PATH.  These additional
  prerequisites are

    1. from the netpbm-<version>-bin.zip package:

         netpbm.dll
         pnmcrop.exe
         pnmcut.exe
         pnmtopng.exe
         pnmtops.exe

    2. from the libpng-<version>-bin.zip package:

         libpng.dll

    3. from the zlib-<version>-bin.zip package:

         zlib-1.dll, which must be renamed to zlib.dll

    4. from the psutils-<version>-bin.zip package:

         psselect.exe

  Note that it is not necessary to install the above four packages in their
  entirety; of course, you may do so if you wish.


  GROFF RUNTIME ENVIRONMENT
  -------------------------

  The runtime environment, provided to groff by MSYS, is essentially the same
  as would be provided under a UNIX or GNU/Linux operating system; thus, any
  environment variables which may be used to customize the groff runtime
  environment have similar effects under MSYS, as they would in UNIX or
  GNU/Linux, with the exception that any variable specifying a path should
  adopt the same syntax as a native MS-Windows PATH specification.

  There is, however, one known problem which is associated with the
  implementation of the MS-Windows file system, and the manner in which the
  Microsoft runtime library (which is used by the MinGW implementation of GCC)
  generates names for temporary files.  This known problem arises when groff
  is invoked with a current working directory which refers to a network share,
  for which the user does not have write access in the root directory, and
  there is no environment variable set to define a writeable location for
  creating temporary files.  When these conditions arise, groff fails with a
  `permission denied' error, as soon as it tries to create any temporary file.

  To specify the location for creating temporary files, the standard UNIX or
  GNU/Linux implementation of groff provides the GROFF_TMPDIR or TMPDIR
  environment variables, whereas MS-Windows applications generally use TMP or
  TEMP; furthermore, the MS-Windows implementations of Ghostscript apparently
  support the use of only TEMP or TMPDIR.

  To avoid problems with creation of temporary files, it is recommended that
  you ensure that both TMP and TEMP are defined, with identical values, to
  point to a suitable location for creating temporary files; many MS-Windows
  boxes have them set already, and groff has been adapted to honour them, when
  built in accordance with the preceding instructions, using MinGW.


  CAVEATS AND BUGS
  ----------------

  There are two known issues, observed when running groff in the MinGW/MSYS
  environment, which would not affect groff in its native UNIX environment:

  o Running groff with the working directory set to a subdirectory of a
    network share, where the user does not have write permission in the root
    directory of the share, causes groff to fail with a `permission denied'
    exception, if the TMP environment variable is not appropriately defined;
    it may also be necessary to define the TEMP environment variable, to avoid
    a similar failure mode, when using the -Thtml output mode of groff.  This
    problem is more fully discussed in the preceding section, GROFF RUNTIME
    ENVIRONMENT.

  o When running groff (or nroff) to process standard input, where the
    standard input stream is obtained directly from the RXVT console provided
    with MSYS, groff cannot detect the end-of-file condition for the standard
    input stream, and hangs.  This appears to be caused by a fault in the MSYS
    implementation of RXVT; it may be worked around by either starting MSYS
    without RXVT (see the comments in the MSYS.BAT startup script); in this
    case standard input is terminated by typing <Ctrl-Z> followed by <RETURN>,
    on a new input line.  Alternatively, if you prefer to use MSYS with RXVT,
    you can enter the interactive groff command in the form

      cat | groff ...

    in which case <Ctrl-D> terminates the standard input stream, in just the
    same way it does on a UNIX system; the cat executable provided with MSYS
    does seem to trap the end-of-file condition, and properly signals groff
    that the input stream has terminated.
This is the GNU `groff' document formatting system.  The version
number is given in the file VERSION.

Included in this release are implementations of `troff', `pic', `eqn',
`tbl', `grn', `refer', `-man', `-mdoc', `-mom', and `-ms' macros, and
drivers for `PostScript', `TeX dvi' format, `HP LaserJet 4' printers,
`Canon CAPSL' printers, `HTML' format (beta status), and
typewriter-like devices.  Also included is a modified version of the
Berkeley `-me' macros, the enhanced version `gxditview' of the X11
`xditview' previewer, and an implementation of the `-mm' macros
contributed by Joergen Haegg (jh@axis.se).

See the file `INSTALL' for installation instructions.  You will
require a C++ compiler.

The file `NEWS' describes recent user-visible changes to `groff'.

`groff' is free software.  See the file `COPYING' for copying
permission.

The file `PROBLEMS' describes various problems that have been
encountered in compiling, installing, and running `groff'.

The most recent released version of `groff' is always available by
anonymous ftp from `ftp.gnu.org' in the directory `gnu/groff'.

The current development version of `groff' is available from a `CVS'
repository.  You can access it by first selecting a parent directory
in which to create a working copy (call it, say, `~/cvswork'), and
then executing the commands

  cd ~/cvswork
  CVS_RSH=ssh; export CVS_RSH
  cvs -d:ext:anoncvs@savannah.gnu.org/cvsroot/groff -z5 co groff

(Note that you need an `ssh' client for security reasons.)

This will create a subdirectory, `~/cvswork/groff', with a "checked
out" copy of the `CVS' repository.  An update of this working copy may
be achieved, at any later time by invoking the commands

  cd ~/cvswork/groff
  CVS_RSH=ssh cvs -z5 update -dP

Please read the `CVS' info pages for further details.

Finally, it is possible to access the `CVS' with a web browser by
pointing it to

  http://savannah.gnu.org/cvs/?group=groff

Alternatively, you can download snapshots (which are updated twice a day).
The complete `groff' source as a single file is available at

  http://groff.ffii.org/groff/devel/groff-current.tar.gz

A diff file relative to `groff-<version>', the latest official `groff'
release is available at

  http://groff.ffii.org/groff/devel/groff-<version>-current.diff.gz

Assuming that `groff-<version>.tar.gz' and
`groff-<version>-current.diff.gz' are in the same directory, do the
following to apply the diff file:

  tar xzvf groff-<version>.tar.gz
  cd groff-<version>
  gunzip -c ../groff-<version>-current.diff.gz | patch -p1

Depending on your requirements, you may need at least some of the
following tools to build `groff' directly from its source:

  ghostscript
  the psutils package
  the netpbm package
  texinfo 4.8
  bison >= 1.875b or byacc

Note that `texinfo' and `bison' or `byacc' are required only for
building from `CVS' sources (either a checked out working copy, or a
daily snapshot).  They are not required for building from a stable
release tarball.  Also note that the version numbers stated are the
minimum supported.  No version of `texinfo' < 4.8 will work, and the
original release of `bison' 1.875 is known not to work; you *may* find
that `bison' releases < 1.875 will work, but in case of difficulty,
please update to a later version *before* posting a bug report.

For *all* sources, you need ghostscript for creation of either `PDF' or
`HTML' output; the `netpbm' and `psutils' packages are required only for
`HTML' output.  If you don't intend to produce output in either of these
formats, then these packages are unnecessary.

In Linux Debian, the installation of `texinfo' is dangerous.  For it
creates a file `install-info' that will block the system installation.
So the created `/usr/local/bin/install-info' must be renamed.

The `groff' configure script searches for the X11 headers and
libraries `Xaw' and `Xmu'.  So the corresponding developer packages of
your system must be installed, otherwise `groff' does not install
`gxditview' and the `-TX*' devices.  In Debian, the developer packages
are `libxaw7-dev' and `libxmu-dev'.

Please report bugs using the form in the file `BUG-REPORT'; the idea of
this is to make sure that FSF has all the information it needs to fix
the bug.  At the very least, read the `BUG-REPORT' form and make sure
that you supply all the information that it asks for.  Even if you are
not sure that something is a bug, report it using `BUG-REPORT': this will
enable us to determine whether it really is a bug or not.

Three mailing lists are available:

  bug-groff@gnu.org          for reporting bugs
  groff@gnu.org              for general discussion of groff
  groff-commit@gnu.org       a read-only list showing commitments
                             to the CVS repository

You can post mails directly to the `bug-groff' list, without subscribing;
to post mails to the `groff' list you must subscribe to it.

To subscribe, send a mail to <list>-request@<domain> (example:
groff-request@gnu.org for the `groff' list) with the word `subscribe'
in either the subject or body of the email (don't include the quotes).
Alternatively, you may subscribe by visiting the web pages at

  http://lists.gnu.org/mailman/listinfo/bug-groff
  http://lists.gnu.org/mailman/listinfo/groff
  http://lists.gnu.org/mailman/listinfo/groff-commit

Each of these web pages also provides a link to a browseable archive of
postings to the corresponding mailing list.

GNU `groff' was written by James Clark <jjc@jclark.com>.  It is now
maintained by Ted Harding <ted.harding@nessie.mcc.ac.uk> and Werner
Lemberg <wl@gnu.org>.
Additional description for the shell version of `groffer'


Scripts

The shell version of `groffer' contains two files, `groffer.sh' and
`groffer2.sh'.

`groffer.sh' is a short introductory script without any functions.  I
can be run with a very poor Bourne shell.  It just contains some basic
variables, the reading of the configuration files, and the
determination of the shell for `groffer2.sh'.  This script is
transformed by `make' into `groffer' which will be installed into
@bindir@, which is usually /usr/local/bin.

`groffer2.sh' is a long main script with all functions; it is called
by `groffer.sh' (`groffer' after installation).  It is installed
unchanged into @libdir@/groff/groffer, which is usually
/usr/local/lib/groff/groffer.  This script can be called with a
different shell, using the `groffer' option `--shell'.


Options

The `groffer' script provides its own option parser.  It is compatible
to the usual GNU style command line This includes long option names
with two signs such as `--option', clusters of short options, the
mixing of options and non-option file names, the option `--' to close
the option handling, and it is possible to abbreviate the long option
names.

The flexible mixing of options and file names in GNU style is always
possible, even if the environment variable `$POSIXLY_CORRECT' is set
to a non-empty value.  This disables the rather wicked POSIX behavior
to terminate option parsing when the first non-option command line
argument is found.


Error Handling

Error handling and exit behavior is complicated by the fact that
`exit' can only escape from the current shell; trouble occurs in
subshells.  This was solved by sending kill signals, see $_PROCESS_ID
and error().


Function Definitions in `groffer2.sh'

Each funtion in groffer2.sh has a description that starts with the
function name and symbols for its arguments in paranthesis `()'.  Each
`<>' construction gives an argument name that just gives a hint on
what the argument is meant to be; these argument names are otherwise
irrelevant.  The `>' sign can be followed by another character that
shows how many of these arguments are possible.

<arg>      exactly 1 of this argument
<arg>?     0 or 1 of these arguments
<arg>*     arbitrarily many such arguments, incl. none
<arg>+     one or more such arguments
<arg>...   one or more such arguments
[...]      optional arguments

A function that starts with an underscore `_' is an internal function
for some other function.  The internal functions are defined just
after their corresponding function.


External Environment Variables

The groffer.sh script uses the following external system variables.
It is supposed that these variables are already exported outside of
groffer.sh; otherwise they do not have a value within the script.

external system environment variables that are explicitly used
$DISPLAY:		Presets the X display.
$LANG:			For language specific man pages.
$LC_ALL:		For language specific man pages.
$LC_MESSAGES:		For language specific man pages.
$PAGER:			Paging program for tty mode.
$PATH:			Path for the programs called (`:' separated list).

groffer native environment variables
$GROFFER_OPT		preset options for groffer.

all groff environment variables are used, see groff(1)
$GROFF_BIN_PATH:	Path for all groff programs.
$GROFF_COMMAND_PREFIX:	'' (normally) or 'g' (several troffs).
$GROFF_FONT_PATH:	Path to non-default groff fonts.
$GROFF_TMAC_PATH:	Path to non-default groff macro files.
$GROFF_TMPDIR:		Directory for groff temporary files.
$GROFF_TYPESETTER:	Preset default device.

all GNU man environment variables are used, see man(1).
$MANOPT:		Preset options for man pages.
$MANPATH:		Search path for man pages (: list).
$MANROFFSEQ:		Ignored because of grog guessing.
$MANSECT:		Search man pages only in sections (:).
$SYSTEM:		Man pages for different OS's (, list).


Object-oriented Functions

The groffer script provides an object-oriented construction (OOP).  In
object-oriented terminology, a type of object is called a `class'; a
function that handles objects from a class is named `method'.

In the groffer script, the object is a variable name whose content is
the object's data.  Methods are functions that have an object as first
argument.

The basic functions for object handling are obj_*().

The class `list' represents an array structure, see list_*().


Shell Compatibility

The `groffer' shell scripts are compatible to both the GNU and the
POSIX shell and utilities.  Care was taken to restrict the programming
technics used here in order to achieve POSIX compatibility as far back
as POSIX P1003.2 Draft 11.2 of September 1991.  This draft is
available at http://www.funet.fi/pub/doc/posix/p1003.2/d11.2 in the
internet.

The POSIX draft does not include `local' variables for functions.  So
this concept was replaced by global variables with a prefix that
differs for each function.  The prefix is chosen from the function
name.  These quasi-local variables are unset before each return of the
function.

The `groffer' scripts were tested under the shells `ash', `bash',
`bash-minimal', `dash', 'ksh', `mksh', `pdksh', 'posh', and `zsh'
without problems in Linux Debian.  A shell can be tested by the
`groffer' option `--shell', but that will run only with groffer2.sh.
To start it directly from the beginning under this shell the following
command can be used.

  <shell-name> groffer.sh --shell=<shell-name> <argument>...


Some shells are not fully POSIX compatible.  For them the following
restrictions were done.  For more information look at the
documentation `Portable shells' in the `info' page of `autoconf'
(look-up in Emacs-Help-Manuals_Info).

- The command parts `then', `else', and `do' must be written each on a
  line of their own.

- Replace `for i in "$@"' by `for i' and remove internal `;' (kah).

- Replace `set -- ...' by `set x ...; shift'.  After the first
  non-option argument, all arguments including those starting with `-'
  are accepted as non-option.  For variables or `$()' constructs with
  line-breaks, use `eval set' without quotes.  That transforms a
  line-break within a variable to a space.

- The name of the variable in `for' is chosen as a single character
  (old ash).  The content of such variables is not safe because it can
  also occur in other functions.  So it is often stored in an
  additional quasi-local variable.

- `echo' is not portable on options; some `echo' commands have many
  options, others have none.  So `echo -n' cannot be used, such that
  the output of each function has complete lines.  There are two
  methods to avoid having `-' as the first character of any argument.
  Either a character such as `x' can be prepended to the argument;
  this must later on be removed by `sed'.  Otherwise, `echo' can be
  replaced by `cat <<EOF'.

- `ls' has problems.  Old UNIX systems echoed the error message to
  standard output.  So handle the output with `sed'.  If the output
  contains `not found' map it to an empty string.

- As `test -e' is not available in Solaris 2.5 replace it by
  `test -f || test -d'.

- As `unset' is not supported by all shells replace it by `eval
  ${_UNSET}' where this variable is `unset' if it exists and `:'
  otherwise.

- Some shells have problems with options in `eval'.  So quoting must
  be done right to hide the options from `eval'.

- In backquote calls `` avoid the backquote ` in comments.

- Replace `true' by `:', `false' isn't used.

- Do not redefine builtins as functions (ash).

- Avoid `[^...]' in `case' patterns (ash).

- `trap' does not allow error code 127.

The scripts call the following commands with all options used:
.
:
apropos
break
bzip2 -c -d -t
cat
catz
cd
continue
echo
eval
expr
grep
groff -v
grog -T -X -Z
gs -c -d -f -q -s
gzip -c -d -f
less -r -R
ls
man -k --apropos
mkdir
mv
pwd
return
rm -f -r
rmdir
sed -e -n
set -e
sh -c
shift
test -c -d -f -r -s -w -x
trap
umask
unset


Bugs

If the `groffer' run is interrupted by Crtl-C the clean up is not done
by all shells.  The `trap' commands work for the shells `bash',
`bash-minimal', and 'ksh'; they do not work for `ash', `dash',
`pdksh', `posh', and `zsh'.


####### License

Last update: 19 August 2005

Copyright (C) 2003,2004,2005 Free Software Foundation, Inc.
Written by Bernd Warken

This file is part of `groffer', which is part of `groff'.

`groff' is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2, or (at your option)
any later version.

`groff' is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
License for more details.

You should have received a copy of the GNU General Public License
along with `groff'; see the files COPYING and LICENSE in the top
directory of the `groff' source.  If not, write to the Free Software
Foundation, 51 Franklin St - Fifth Floor, Boston, MA 02110-1301, USA.


####### Emacs settings

Local Variables:
mode: text
End:
README

The `groffer' program is the easiest way to read documents written in
some `roff' language, such as the `man pages', the manual pages in
many operating systems.


Input

Input comes from either standard input or command line parameters that
represent names of exisiting roff files or standardized specifications
for searching man pages.  All of these can be compressed in a format
that is decompressible by `gzip', including `.gz', `bz2', and `.Z'.

`groffer' has many built-in `man' functionalities to find and read the
manual pages on UNIX and similar operating systems.  It accepts the
information from an installed `man' program, but tries to find a man
path by itself.

`groffer' bundles all filespec parameters into a single output file in
the same way as `groff'.  The disadvantage of this is that all file
name arguments must use the same groff language.  To change this, the
option parsing must be revised for large parts.  It seems that this
would create incompatibilities, so the actual option strategy is kept.


Output

All input is first sent to `grog' to determine the necessary `groff'
options and then to `groff'.  So no special `groff' arguments must be
given.  But all `groff' options can be specified when this seems to be
appropriate.

The following displaying modes for the output are available:
- Display formatted input with
-- the X `roff' viewer `gxditview',
-- a Postcript viewer,
-- a PDF viewer,
-- a DVI viewer,
-- a web browser,
-- a pager in a text terminal (tty).
- Generate `groff' output on stdout without a viewer.
- Generate the `groff intermediate output' on standard output without
  postprocessing.
- Output the source code without any `groff' processing.
- There are some information outputs without `groff' processing, such
  as by option `-V' and the `man' like `whatis' and `apropos'
  outputs.

By default, the program tries to display with `gxditview' as graphical
device in X; on non-X text terminals, the `tty' text mode with a pager
is tried by default.


Compatibility

`groffer' consists of two shell scripts.  It should run on any POSIX
or Bourne style shell that supports shell functions.  See file
`README_SH' for more information.


Mailing lists

For reporting bugs of `groffer', groff's free mailing list
<bug-groff@gnu.org> can be used.

For a general discussion, the mailing list <groff@gnu.org> is more
useful, but one has to subscribe to this list at
http://lists.gnu.org/mailman/listinfo/groff.

See the `README' file in the top directory of the `groff' source
package for more details on these mailing lists.


####### License

Last update: 2 August 2005

Copyright (C) 2003,2004,2005 Free Software Foundation, Inc.
Written by Bernd Warken

This file is part of `groffer', which is part of `groff'.

`groff' is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2, or (at your option)
any later version.

`groff' is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received a copy of the GNU General Public License
along with `groff'; see the files COPYING and LICENSE in the top
directory of the `groff' source.  If not, write to the Free Software
Foundation, 51 Franklin St - Fifth Floor, Boston, MA 02110-1301, USA.


####### Emacs settings

Local Variables:
mode: text
End:

This directory contains examples of my enhancements to MM.

APP		The appendix macro
B1B2		Box macro with text
COVER		My general cover macro, this example is using
		ms.cov.
IND		A general indexing method, see manual for INITI
LT		The letter macro
LT.se		A swedish example with the extra
		swedish macros for getting a letter conforming
		to swedish standard letter, both left and right adjusted.
ML		Marked list, an extended list type
MOVE		The MOVE macro, how to begin to print on an exact position.
MUL		Enhanced multicolumn mode.
NCOL		Start on next column. (Not for MUL*)
ND		New date, with iso date example
References	How to use references
SETR		General reference system, see manual for INITR


Examples that I should have:

PIC		How to include postscript pictures, see manual for PIC
VERBON		Begin verbatim output


And remember, check the manual for all string and number registers,
I've made shure that mgm will be useful in several languages
and all english output can be redefined.
Check the manual for groff_mse (swedish format) and the
macro file, tmac.mse.
gdiffmk is approximately a recreation of the original Bell Labs/AT&T diffmk
command for troff/nroff documents, with enhancements.

It should not be confused with `diffmk' commands that operate on XML.

The inspiration for this code was a Perl 2 version written in 1989 by Randal
L. Schwartz.  See
  landfield.com/software/comp.sources.misc/archive-name/volume06/diffmk.p.gz

The command also attempts to reproduce some of the functionality of the old
`nrchbar' command.  See
  open-systems.ufl.edu/mirrors/ftp.isc.org/usenet/comp.sources.unix/volume10/nrchbar.Z

Thanks to Werner Lemberg for help in making the package more portable and
fit into the GNU groff source structure.

Gnu diff(1) with the -Dname option does all of the work and sed(1)
translates the output into something groff/troff/nroff can handle.

Note the BUGS on the man page.

The `tests' directory contains simple tests.  `runtests run' runs them and
compares the output against baseline files.  Calling `runtests' without
argument gives the usage.

----------------------------------------------------------------------------

Copyright (C) 2004, 2005 Free Software Foundation, Inc.
Written by Mike Bianchi <MBianchi@Foveal.com <mailto:MBianchi@Foveal.com>>

This file is part of the gdiffmk utility, which is part of groff.

groff is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2, or (at your option)
any later version.

groff is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
License for more details.

You should have received a copy of the GNU General Public License
along with groff; see the files COPYING and LICENSE in the top
directory of the groff source.  If not, write to the Free Software
Foundation, 51 Franklin St - Fifth Floor, Boston, MA 02110-1301, USA.
README for pdfmark.tmac
=======================

Copyright (C) 2004, Free Software Foundation Inc.
Contributed by Keith Marshall (keith.d.marshall@ntlworld.com)

This is free software.  See file COPYING, for copying permissions,
and warranty disclaimer.

This is a preview release of a proposed pdfmark.tmac macro package,
for use with GNU troff (groff).  It is not yet complete, and should
be considered as an alpha release;  there are a few problems to be
resolved (see file PROBLEMS).

Partial documentation is provided, in groff-ms format.  To convert
this to PDF format, you will require a working groff installation,
a working ghostscript installation, with the gs command in your PATH,
and a GNU-compatible make.  The tarball should be unpacked in the
top directory of your groff source tree, then:

  cd <groff-current>/contrib/pdfmark
  make pdfmark

where <groff-current> is the top directory of your current groff
source tree.

Included in this package, are:

  pdfmark.tmac -- the core pdfmark macro set
  spdf.tmac    -- a rudimentary set of bindings for ms macros
  pdfmark.ms   -- preliminary documentation
  cover.ms     -- a template for the documentation cover sheet
  gnu.eps      -- the groff logo, copied from the groff distribution
  Makefile     -- makefile, for formatting the documentation
  README       -- this file
  PROBLEMS     -- a list of known problems
  TODO         -- a list of planned features, not yet implemented

To make the pdfmark macros generally usable, copy pdfmark.tmac to the
'site-tmac' directory appropriate to your groff installation; (ms users
may also wish to copy spdf.tmac).  The macros may then be accessed, by
including the '-mpdfmark' option on the groff command line; (for ms
users, '-mspdf' is equivalent to '-ms -mpdfmark', with some extra
macros 'thrown in').

Comments, and bug reports are welcomed.  Please post to the groff
mailing list, groff@gnu.org; (you must be subscribed to this list to
post mails).  To subscribe, visit
  
  http://lists.gnu.org/mailman/listinfo/groff
This is gxditview, an X11 previewer for groff based on MIT's xditview.
This version can be used with the output of gtroff -Tps as well as
with -TX75 and -TX100.  You will need X11R5 or newer to install it (it
might work on X11R4, but I haven't tested it.)

Previously, gxditview was installed in the usual place for X binaries
(e.g., /usr/bin/X11); you have to remove it manually.

xditview is copyrighted by MIT under the usual X terms (see
gxditview.man); the changes to that which come along with the groff package
are in the public domain.

Please report bugs to bug-groff@gnu.org.
This is grn from the Berkeley ditroff distribution.  It has no
AT&T code and is therefore freely distributable.

Tim Theisen <tim@cs.wisc.edu>

=====================================================================

This is the modified code for the groff.  It uses the different
devxxx format that is ascii rather than binary as in the
Berkeley distribution.  Since groff does not have the \Ds option
for line drawing (dotted, dashed, etc.), this version includes
the routines for drawing curves and arcs, so it does not use the
\D~, \Da nor \Dc.  Although also included in here is a routine
for drawing the optional gremlin style curves, it is not used
because the gremlin editor uses the conventional spline
algorithm.  The Berkeley grn has the choice of different
stipples.  Here, only different shades of gray will be painted
depending on the gremlin file.  It is possible to upgrade this at
a later time.  (Daniel Senderowicz <daniel@synchrods.com> 12/28/99)

=====================================================================

Gremlin produces three types of curves: B-Splines, interpolated
curves and Bezier.  As the original Berkeley grn, now groff grn
will honor B-Splines and interpolated curves.  Bezier curves will
be printed as B-Splines.  (Daniel Senderowicz <daniel@synchrods.com>
10/04/02)

=====================================================================

It has been further modified by Werner Lemberg <wl@gnu.org> to fit
better into the groff package.

  . Replaced Makefile with Makefile.sub.

  . Removed dev.h since it is unused.

  . Renamed grn.1 to grn.man; this man page has been extensively
    revised.

  . Used error() and fatal() from libgroff for all source files.

  . Renamed *.c to *.cpp; updates as needed for C++ (prototypes, proper
    casts, standard header files etc).  Heavy formatting.

  . main.cpp:

      Using groff's default values instead of DEVDIR, DEFAULTDEV, PRINTER,
      TYPESETTER, and GREMLIB.

      `res' is now an integer.

      Added `-C' command flag (for compatibility mode) as with other
      preprocessors.

      Added `-F' and `-v' option (similar to troff).

      Renamed `-L' option to `-M' for consistence.

      Removed `-P' option.

      Using font::load_desc() for scanning DESC files.

      Removed SYSV-specific code.

      Using macro_path.open_file() for getting gremlin graphic files.

      Added usage().
The file snprintf.c is not part of groff but is used by groff; it has been
written by Mark Martinec <mark.martinec@ijs.si>.

Please look into snprintf.c for the copyright message.

The complete snprintf package together with documentation is available from

  http://www.ijs.si/software/snprintf/
This is a port of GNU Groff to DJGPP v2.03 or later.
Groff is the GNU version of document formatting tools related to
`troff'.

This README file describes how to build and install Groff on MS-DOS or
MS-Windows systems using the DJGPP port of GNU C/C++ compiler and
development tools.


I.  Installing the pre-compiled binary package
    ------------------------------------------

    1. Unzip the file groNNNb.zip (where NNN is the version number)
       preserving the directory structure (-d switch to PKUNZIP) from
       the main DJGPP installation directory.  If you will use Groff
       on Windows 9X or Windows2000, use an unzip program which
       supports long filenames.

    2. Groff binaries were configured so that they will look for their
       standard directories under the directory pointed to by the
       DJDIR environment variable, so it should work automatically if
       you have DJGPP installed.  If you don't have a standard DJGPP
       installation, set the variable DJDIR to point to the directory
       where you unzip Groff.  In this latter case, you will need to
       set additional environment variables:

        GROFF_TMAC_PATH=%DJDIR%/share/groff/<version>/tmac:%DJDIR%/share/groff/site-tmac
        GROFF_TYPESETTER=ascii
        GROFF_FONT_PATH=%DJDIR%/share/groff/<version>/font

       <version> is something like `1.16.1' or `1.17'.

       In addition, you can set the variable GROFF_TMPDIR to point to
       a directory where you want Groff to create temporary files it
       needs for running its jobs (these files are automatically
       deleted when Groff exits).

       All of those variables are automatically set in the file
       DJGPP.ENV that is part of the standard DJGPP distribution
       djdevNNN.zip (where NNN is the DJGPP version number), so you
       only need to set them manually if you don't have DJGPP
       installed.

       Note that the GROFF_TYPESETTER variable sets the default Groff
       device to be `ascii', which is suitable for formatting man
       pages to be viewed on the terminal.  Use the -T switch to
       generate output for other devices (e.g., -Tps for PostScript).

    3. If your TMPDIR environment variable points to a RAM drive, you
       might consider changing GROFF_TMPDIR to point to a directory on
       a real disk drive, especially if you intend to generate
       PostScript output, because RAM disks are typically small (2-3
       MBytes) which might be not enough for formatting large
       documents.

    4. Read the docs.  It comes as formatted manual pages called *.1,
       *.5 and *.7 which unzip into your man/ subdirectory.  You
       can read them with a pager such as GNU Less (recommended, as
       Less will use colors for bold and underlined text) or with
       Info (which will remove the bold/underline attributes).
       Another alternative is to use Emacs built-in man page reader;
       the DJGPP FAQ lists other possibilities.

       Beginning with version 1.15, Groff comes with an Info manual;
       type "info -f groff" to read it.  The Info manual is still
       under construction, so some sections are empty.

       If you want to add a Groff entry to the main Info menu in the
       file DIR, chdir to the `info' subdirectory of the main Groff
       installation directory and run this command:

         install-info --dir-file=dir groff.info

       After you do that, "info groff" will also work.

    5. For those who only need Groff to format man pages and don't
       like reading the docs, here's a minimal cookbook:

                  groff -man -s foo.1 > foo.man

       where `foo.1' is the troff source of the man page and `foo.man'
       is the formatted page.  If you need to view the man page, say
       this:

                  groff -man -s foo.1 | less

       You can also use the DJGPP clone of the Unix `man' command, in
       which case `man' runs the above command for you automatically.

       Here's how you print man pages on a PostScript printer:

                  groff -man -s -Tps foo.1 > prn

       And this is for a LaserJet4 printer:

                  groff -man -s -Tlj4 foo.1 > prn

       Printing the documents produced by Groff is possible either by
       redirecting Groff's standard output to the local printer
       device, like shown above, or by using the `-l' switch to Groff.
       The latter possibility causes Groff to pipe its output to a
       program whose name and arguments appear in the files named
       `DESC' in each of the `devFOO' subdirectories of the
       %DJDIR%/share/groff/<version>/font directory; for example, the
       file devps/DESC is used by "groff -Tps".  The relevant line in
       these files begins with the word "print".

       As configured, when invoked with the `-l' switch, Groff will
       call `cat' (from GNU Textutils) to pipe its output to the
       default printer device for -Tps, -Tlbp and -Tlj4 options, and
       it will call `dvilj4' (from the dvljNNNb.zip package) for -Tdvi
       option.  If you don't have these programs installed, you can
       edit the respective `DESC' files to replace these commands with
       something else.  The replacement program must be able to read
       its standard input and send its output to whatever printer you
       want.  The "print" entry is assumed to be a shell command, so
       it can use redirection, pipes, and other shell features.

       Beginning with version 1.15, Groff can create HTML output, like
       this:

                  groff -man -s -Thtml foo.1 > foo.html

       Producing HTML files sometimes requires Ghostscript to be
       installed, and, for gif images, ppmquant and ppmtogif as well.
       If you do not have these programs installed, Groff will print
       an error message, and the produced file will have links which
       will fail to resolve when you view it with a Web browser.

    6. Some programs in the package are supplied as Unix shell
       scripts.  While it is relatively easy to write a DOS batch file
       which will do the same, DOS doesn't allow to redirect input and
       output of a batch file.  Since Groff tools are meant to be
       invoked in a pipe, the batch files are not very useful.  The
       batch files are included in the binary distribution
       nonetheless.

       These scripts need the following utilities to run (in addition
       to the Groff programs they invoke):

                 - bash
                 - gawk
                 - egrep
                 - sed

       The `afmtodit' and `mmroff' utilities are Perl scripts, so you
       will need a Perl port to run them.

       All of these ports should be available from the DJGPP sites.

       If you need to run these scripts and batch files, you have to
       install the port of bash (or another Unix-like shell) and the
       above-mentioned utilities called by the script.  Alternatively,
       just look inside the shell script and invoke the programs it
       calls manually.

       To run the scripts with redirection, invoke them via the shell,
       like this: "sh mmroff > foo".

    7. Note that Groff programs use floating point, so you will need
       an FP emulator if your machine doesn't have an FPU.  The binary
       distribution includes the emulator, in case you don't have the
       DJGPP development environment installed.  Please refer to the
       DJGPP FAQ list in case you have any problems with the emulator.

    8. The package does not include the directories under
       share/groff/<version>/font whose names begin with "devX": these
       are needed on X-Windows for running the gxditview program, which
       is not supported by this port.

    9. Due to 8+3 limitations of DOS filesystems, several files were
       renamed:

         - groff_mdoc.samples.7 was renamed to groff-mdoc_samples.7
           and groff_mmse.7 to groff-mmse.7.  The latter was also
           converted from Latin-1 encoding to codepage 437.



II. Building Groff from sources
    ---------------------------

    1. To build Groff, you will need the following tools (the file
       name in parentheses is what you need to download from one of
       the DJGPP sites):

          - Standard DJGPP development environment (djdev203.zip)
          - GNU C compiler (gcc2721b.zip)
          - GNU C++ compiler (gpp2721b.zip)
          - GNU Make 3.79 (mak379b.zip)
          - Bash v2.03 (bsh203b.zip)
          - Fileutils 3.16 (fil316b.zip)
          - Textutils 2.0 (txt20b.zip)
          - Sh-utils 1.12 (shl112b.zip)
          - Sed 3.02 (sed302b.zip)
          - Gawk 3.04 (gwk304b.zip)
          - Grep 2.4 (grep24b.zip)
          - Bison (only if you change one of the *.y files)

       Note that you don't need to install libg++ (lgpNNNb.zip) since
       Groff doesn't use any C++ classes except its own.

       Any versions of the utilities later than what's mentioned above
       should also do; in particular, GCC 2.95.2 was tested and Groff
       built okay with it.  Versions older than in the above list
       might also work, but I don't guarantee that; you are on your
       own.

       Special considerations apply if you have GCC 2.8.1 installed,
       and cannot upgrade to a later version.  See paragraph 4 below.

       After you install these tools, make sure you have a ``symlink''
       to bash.exe called sh.exe and a ``symlink'' to gawk.exe called
       awk.exe.  If not, go to the DJGPP bin/ subdirectory and type
       the following words of wisdom from the DOS prompt:

                     ln -s bash.exe sh.exe
                     ln -s gawk.exe awk.exe

       (`ln' is part of GNU Fileutils, see above.)

    2. Unzip the source distribution groXYZs.zip (where XYZ is the
       version number) preserving the directory structure (-d switch
       to PKUNZIP) from the main DJGPP installation directory.  (If
       you are building Groff on Windows 9X or Windows 2000, use an
       unzip program which supports long filenames.)  This creates
       directory gnu/groff-X.YZ and unzips the sources there.

       If you are building from the official GNU distribution, unpack
       the .tar.gz archive like this:

                   djtar -x groff-X.YZ.tar.gz

       (DJTAR is part of the standard DJGPP development distribution.)

    3. Groff sources on DJGPP sites are already configured for the
       current version of DJGPP.  If that is the version you have,
       then you can just chdir to gnu/groff-X.YZ and say "make" to
       build the entire package (if you have GCC 2.8.1, see the next
       paragraph).

       If you have version of DJGPP other than the current one, or if
       you build the official GNU distribution, or if you prefer to
       configure the package so that it defaults to the directory
       structure on your machine, or need to change some options
       (e.g., compile with different optimization options), you will
       have to reconfigure Groff.  To this end, use the CONFIG.BAT
       batch file in the DJGPP subdirectory:

                     arch\djgpp\config

       You can configure and build Groff from outside its source
       directory.  In that case, you need to pass the full path to the
       source directory as an argument to CONFIG.BAT, like this:

          d:\gnu\groff-1.16\arch\djgpp\config d:/gnu/groff-1.16

       Note that you MUST use forward slashes in the path you pass to
       CONFIG.BAT, or else it may fail.  (For versions of Groff other
       than 1.16, change the above command accordingly.)

    4. If your version of GCC is 2.8.1, you cannot build the
       preconfigured package without some tinkering.  The DJGPP port
       of GCC 2.8.1 had a bug in its C++ configuration, whereby the
       file _G_config.h erroneously indicated that the header
       <sys/socket.h> is available, and also undefined the symbol
       NULL.  This causes several files in the Groff distribution to
       fail to compile.

       The easiest way to solve this is to upgrade to a later version
       of GCC; then you can simply say "make" to build the
       preconfigured package.  If this is not an option, you will have
       to edit the file lang/cxx/_G_config.h and change this line:

          #define _G_HAVE_SYS_SOCKET 1

       to say this instead:

          #define _G_HAVE_SYS_SOCKET 0

       The problem with redefining NULL should not happen with DJGPP
       v2.03 or later.  But if you still see compilation errors which
       say "`NULL' undeclared", comment out the line in _G_config.h
       that says this:

          #undef NULL

       Alternatively, you can reconfigure the package as described in
       the previous paragraph, before building it.

    5. After the configure script exits, say "make" to build Groff.
       Groff is a large package, and it might take a few minutes to
       build, depending on your CPU, so you might as well go for a
       coffee while it grinds away.

    6. Test the package that you have built.  A batch file T-GROFF.BAT
       in the DJGPP subdirectory is supplied for that purpose.  Most
       of the commands there are commented out, since I cannot
       possibly know what kind of printer do you have and which
       additional programs, such as Less, do you have installed.  The
       only command that runs by default will format a large document
       and print it to the screen.  Read the comments in the batch
       file, uncomment additional lines as you see fit and run the
       batch file to see that you get the document printed as you'd
       expect.  (Btw, the document that the batch file prints is an
       introduction to the entire Groff package, so you might as well
       read it to make yourself familiar with the programs.)

       Note that the batch file sets a lot of environment variables;
       if you get ``Out of environment space'' messages, launch a
       subsidiary COMMAND.COM with plenty of environment space, like
       so:

                     command.com /e:3000

       then invoke T-GROFF.BAT from that COMMAND.COM.

    7. Install the package by typing "make install".  This will copy
       all the binaries, the auxiliary files (fonts, macros, etc.) and
       the docs into their places.  If you configured the package for
       your system, these are precisely the directories where the
       files should remain (with the exception of the man pages, see
       below).  If you use the default configuration, the files will
       be installed under the top DJGPP installation directory.

       Alternatively, you could instruct Make explicitly where to
       install the package by setting the `prefix' variable.  For
       example:

               make install prefix=c:/groff

       "make install" doesn't format the man pages, it just copies
       them into subdirectories of the %DJDIR%\MAN directory.  If you
       need to keep formatted pages in your man/ subdirectory, you
       will need to format them.  Use the commands shown in chapter I,
       section 5 above to do that, and redirect its output to the
       appropriate catN subdirectory.  Alternatively, you could format
       the pages when you need to view them (the DJGPP clone of `man'
       will automatically format them).

       Consult the installation instructions for pre-compiled binaries
       above, for more info about installing and using Groff.

    8. You can safely delete the directories under
       share/groff/<version>/font whose names begin with "devX": these
       are needed on X-Windows which is not supported by this port.
The file `hyphen.us' is identical to the file `hyphen.tex', part of the TeX
system written by Donald E. Knuth; the master file can be found at

  ftp://labrea.stanford.edu/pub/tex/dist/lib/hyphen.tex    .

It has been renamed for consistency, i.e., to make patterns available under
the filenames `hyphen.<language>', e.g. `hyphen.de' or `hyphen.uk'.

See the file itself for a copyright notice.
	send-pr - sends bug reports to a central support site

`send-pr' uses electronic mail to submit support questions and
software bugs to a central site.  No piece of software is perfect, and
software organizations understand this `send-pr' is designed to allow
users who have problems to submit reports of these problems to sites
responsible for supporting the software in question, in a defined form
which can be read by an electronically managed database.

`send-pr' is part of a suite of programs known collectively as GNATS,
an acronym for Problem Report Management System.  GNATS consists of
several programs which, used in concert, formulate and partially
administer a database of Problem Reports, or PRs, at a central support
site.  A PR goes through several states in its lifetime; GNATS tracks
the PR and all information associated with it through each state and
finally acts as an archive for PRs which have been resolved.

The same engine can be used to submit bugs to any number of support
sites by setting up aliases for each of them; `send-pr' error-checks
each PR as it is sent to be sure that the category specified matches a
category supported by the site in question.

`send-pr' invokes an editor on a problem report template (after trying
to fill in some fields with reasonable default values).  When you exit
the editor, `send-pr' sends the completed form to the support site.
At the support site, the PR is assigned a unique number and is stored
in the GNATS database according to its category and customer-id.  GNATS
automatically replies with an acknowledgement, citing the category and
the PR number.

See the Texinfo file `send-pr.texi' or the Info file `send-pr.info'
for detailed installation and usage information.

See the file MANIFEST for a list of the files which should have
accompanied this distribution.

See `send-pr.texi', `send-pr.info', or the file INSTALL for the
installation procedure for `send-pr'.


Copyright (c) 1993, Free Software Foundation, Inc.  
See the file COPYING for copyright information concerning this
distribution and all its components.
Licensing and contribution policy of dtc and libfdt
===================================================

This dtc package contains two pieces of software: dtc itself, and
libfdt which comprises the files in the libfdt/ subdirectory.  These
two pieces of software, although closely related, are quite distinct.
dtc does not incoporate or rely on libfdt for its operation, nor vice
versa.  It is important that these two pieces of software have
different license conditions.

As the copyright banners in each source file attest, dtc is licensed
under the GNU GPL.  The full text of the GPL can be found in the file
entitled 'GPL' which should be included in this package.  dtc code,
therefore, may not be incorporated into works which do not have a GPL
compatible license.

libfdt, however, is GPL/BSD dual-licensed.  That is, it may be used
either under the terms of the GPL, or under the terms of the 2-clause
BSD license (aka the ISC license).  The full terms of that license are
given in the copyright banners of each of the libfdt source files.
This is, in practice, equivalent to being BSD licensed, since the
terms of the BSD license are strictly more permissive than the GPL.

I made the decision to license libfdt in this way because I want to
encourage widespread and correct usage of flattened device trees,
including by proprietary or otherwise GPL-incompatible firmware or
tools.  Allowing libfdt to be used under the terms of the BSD license
makes that it easier for vendors or authors of such software to do so.

This does mean that libfdt code could be "stolen" - say, included in a
proprietary fimware and extended without contributing those extensions
back to the libfdt mainline.  While I hope that doesn't happen, I
believe the goal of allowing libfdt to be widely used is more
important than avoiding that.  libfdt is quite small, and hardly
rocket science; so the incentive for such impolite behaviour is small,
and the inconvenience caused therby is not dire.

Licenses such as the LGPL which would allow code to be used in non-GPL
software, but also require contributions to be returned were
considered.  However, libfdt is designed to be used in firmwares and
other environments with unusual technical constraints.  It's difficult
to anticipate all possible changes which might be needed to meld
libfdt into such environments and so difficult to suitably word a
license that puts the boundary between what is and isn't permitted in
the intended place.  Again, I judged encouraging widespread use of
libfdt by keeping the license terms simple and familiar to be the more
important goal.

**IMPORTANT** It's intended that all of libfdt as released remain
permissively licensed this way.  Therefore only contributions which
are released under these terms can be merged into the libfdt mainline.


David Gibson <david@gibson.dropbear.id.au>
(principal original author of dtc and libfdt)
2 November 2007
The source tree contains the Device Tree Compiler (dtc) toolchain for
working with device tree source and binary files and also libfdt, a
utility library for reading and manipulating the binary format.

DTC and LIBFDT are maintained by:

David Gibson <david@gibson.dropbear.id.au>
Jon Loeliger <jdl@jdl.com>

Mailing list
------------
The following list is for discussion about dtc and libfdt implementation
mailto:devicetree-compiler@vger.kernel.org

Core device tree bindings are discussed on the devicetree-spec list:
mailto:devicetree-spec@vger.kernel.org
#	$NetBSD: README,v 1.1 2009/06/25 18:21:51 tron Exp $

Example Postfix configuration files can be found in:

	/usr/share/examples/postfix
See http://www.openspf.org/Software for the current version of the
SPF policy daemon for Postfix.

SPF support is also available via MILTER plugins, such as sid-milter
at http://sourceforge.net/projects/sid-milter/ which implements both
SenderID and SPF.
Purpose of this document
========================

This document describes how to add your own SASL implementation to
Postfix.  You don't have to provide both the server and client side.
You can provide just one and omit the other. The examples below
assume you do both.

The plug-in API is described in cyrus_server.c and cyrus_client.c.
It was unavoidably contaminated^h^h^h^h^h^h^h^h^h^h^h^hinfluenced
by Cyrus SASL and may need revision as other implementations are
added.

For an example of how the plug-in interface is implemented, have a
look at the xsasl/xsasl_cyrus_client.c and xsasl/xsasl_cyrus_server.c.

Configuration features
======================

There are two configuration parameters that allow you to pass
information from main.cf into the plug_in:

    smtpd_sasl_path, smtpd_sasl_security_options
    smtp_sasl_path, smtp_sasl_security_options
    lmtp_sasl_path, lmtp_sasl_security_options

As usual, newline characters are removed from multi-line parameter
values, and $name is expanded recursively.  The parameter values
are passed to the plug-in without any further processing. The
following restrictions are imposed by the main.cf file parser:

- parameter values never contain newlines,

- parameter values never start or end with whitespace characters.

The _path parameter value is passed only once during process
initialization (i.e. it is a class variable). The path typically
specifies the location of a configuration file or rendez-vous point.
The _security_options parameter value is passed each time SASL is
turned on for a connection (i.e. it is an instance variable).  The
options may depend on whether or not TLS encryption is turned on.
Remember that one Postfix process may perform up to 100 mail
transactions during its life time. Things that happen in one
transaction must not affect later transactions.

Adding Postfix support for your own SASL implementation
=======================================================

To add your own SASL implementation, say, FOOBAR:

- Copy xsasl/xsasl_cyrus.h to xsasl/xsasl_foobar.h and replace
  CYRUS by FOOBAR:

 #if defined(USE_SASL_AUTH) && defined(USE_FOOBAR_SASL)
  /*
   * SASL protocol interface
   */
 #define XSASL_TYPE_FOOBAR "foobar"
 extern XSASL_SERVER_IMPL *xsasl_foobar_server_init(const char *, const char *);
 extern XSASL_CLIENT_IMPL *xsasl_foobar_client_init(const char *, const char *);
 #endif

- Edit xsasl/xsasl_server.c, add your #include <xsasl_foobar.h> line
  under #include <xsasl_cyrus.h> at the top, and add your initialization
  function in the table at the bottom as shown below:

  static XSASL_SERVER_IMPL_INFO server_impl_info[] = {
  #ifdef XSASL_TYPE_CYRUS
      XSASL_TYPE_CYRUS, xsasl_cyrus_server_init,
  #endif
  #ifdef XSASL_TYPE_FOOBAR
      XSASL_TYPE_FOOBAR, xsasl_foobar_server_init,
  #endif
      0,
  };

- Repeat the (almost) same procedure for xsasl/xsasl_client.c.

- Create your own xsasl/xsasl_foobar_{client,server}.c and support
  files. Perhaps it's convenient to copy the cyrus files, rip out
  the function bodies, and replace CYRUS by FOOBAR.

- List your source files in Makefile.in. Don't forget to do "make
  depend" after you do "make makefiles" in the step that follows
  after this one.

  SRCS    = xsasl_server.c xsasl_cyrus_server.c xsasl_cyrus_log.c \
          xsasl_cyrus_security.c xsasl_client.c xsasl_cyrus_client.c \
          xsasl_foobar_client.c xsasl_foobar_server.c
  OBJS    = xsasl_server.o xsasl_cyrus_server.o xsasl_cyrus_log.o \
          xsasl_cyrus_security.o xsasl_client.o xsasl_cyrus_client.o \
          xsasl_foobar_client.o xsasl_foobar_server.o

- Create the Postfix makefiles from the top-level directory:

  % make makefiles CCARGS='-DUSE_SASL_AUTH -DUSE_FOOBAR_SASL \
      -DDEF_CLIENT_SASL_TYPE=\"foobar\" -DDEF_SERVER_SASL_TYPE=\"foobar\" \
      -I/some/where/include' AUXLIBS='-L/some/where/lib -lfoobar'

  Yes, you can have different default SASL implementation types for
  the client and server plug-ins.

  Of course you don't have to override the default SASL implementation
  type; it is shown here as an example.
  

- Don't forget to do "make depend" in the xsasl directory.

- Document your build and configuration with a README document.
This directory contains source for manual pages that aren't generated
from C source code comments, and for README files.

Tools for these conversions are found in the ../mantools subdirectory.
The most important tool is postlink, which adds all the hyperlinks
between manual pages, configuration parameters and README files.

Manual pages are in the form of "shell" style comments.  They are
converted into nroff(1) input with the srctoman utility, converted
to HTML with man2html, and hyperlinked with postlink.

The README files are in the form of HTML. The are converted into
hyperlinked HTML with postlink, and are converted into ASCII files
with html2readme.

The format of the README source files is a little tricky, because
of the way postlink works.

- postlink hyperlinks all the references to Postfix manual pages,
configuration parameters, README file names, RFC documents, Postfix
address class names, and URLs.  Therefore you should not hyperlink
those elements yourself.

- URLs (such as "http://www.example.com") cannot contain quote, comma,
or space characters. 

- An URL that appears at the end of a line must be followed by one
other character.

- Text must go between <p> and </p>, especially within lists, for
consistency of vertical space.

- Code fragments must go between <pre> and </pre>, again for
consistency of vertical space.
Scripts and tools to format embedded manual pages, or to format C
source code files. Each has an embedded man page in the source.

ccformat 	c code formatter
		usage: ccformat (copy stdin to stdout)
		usage: ccformat files... (format files in place)

enter		set project-specific environment
		usage: enter project-name

mansect		extract manual page section from source file
		usage: mansect file.suffix
		usage: mansect -type file

srctoman	extract man page from source file
		usage: srctoman file.suffix
		usage: srctoman -type file

man2html	quick script to htmlize nroff -man output

postlink	quick script to hyperlink HTML text

See the proto/README file for the following tools that generate
HTML and ASCII forms of README documents and of some manual pages.

fixman		quick hack to patch postconf.proto text into C sorce

makereadme	create README_FILES table of contents (AAAREADME)

html2readme	convert HTML to README file

postconf2html	postconf.proto -> postconf.5.html

postconf2man	postconf.proto -> postconf.5 (nroff input)

xpostconf	extract selected sections from postconf.proto

xpostdef	re-compute the defaults in postconf.proto
pigz 2.3.1 (9 Oct 2013) by Mark Adler

pigz, which stands for Parallel Implementation of GZip, is a fully functional
replacement for gzip that exploits multiple processors and multiple cores to
the hilt when compressing data.

pigz was written by Mark Adler, and uses the zlib and pthread libraries.

This version of pigz is written to be portable across Unix-style operating
systems that provide the zlib and pthread libraries.

Type "make" in this directory to build the "pigz" executable.  You can then
install the executable wherever you like in your path (e.g. /usr/local/bin/).
Type "pigz" to see the command help and all of the command options.

The latest version of pigz can be found at http://zlib.net/pigz/ .  You need
zlib version 1.2.3 or later to compile pigz.  zlib version 1.2.6 or later is
recommended, which reduces the overhead between blocks.  You can find the
latest version of zlib at http://zlib.net/ .  You can look in pigz.c for the
change history.

Questions, comments, bug reports, fixes, etc. can be emailed to Mark at his
address in the license below.

The license from pigz.c is copied here:

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the author be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Mark Adler
  madler@alumni.caltech.edu

  Mark accepts donations for providing this software.  Donations are not
  required or expected.  Any amount that you feel is appropriate would be
  appreciated.  You can use this link:

  https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=536055
Zopfli Compression Algorithm is a compression library programmed in C to perform
very good, but slow, deflate or zlib compression.

zopfli.c is separate from the library and contains an example program to create
very well compressed gzip files.

The basic functions to compress data are ZopfliDeflate in deflate.h,
ZopfliZlibCompress in zlib_container.h and ZopfliGzipCompress in
gzip_container.h. Use the ZopfliOptions object to set parameters that affect the
speed and compression. Use the ZopfliInitOptions function to place the default
values in the ZopfliOptions first.

Deflate creates a valid deflate stream in memory, see:
http://www.ietf.org/rfc/rfc1951.txt
ZlibCompress creates a valid zlib stream in memory, see:
http://www.ietf.org/rfc/rfc1950.txt
GzipCompress creates a valid gzip stream in memory, see:
http://www.ietf.org/rfc/rfc1952.txt

This library can only compress, not decompress. Existing zlib or deflate
libraries can decompress the data.

Zopfli Compression Algorithm was created by Lode Vandevenne and Jyrki
Alakuijala, based on an algorithm by Jyrki Alakuijala.

This is Lua 5.3.4, released on 12 Jan 2017.

For installation instructions, license details, and
further information about Lua, see doc/readme.html.


                        Expat, Release 2.2.1

This is Expat, a C library for parsing XML, written by James Clark.
Expat is a stream-oriented XML parser.  This means that you register
handlers with the parser before starting the parse.  These handlers
are called when the parser discovers the associated structures in the
document being parsed.  A start tag is an example of the kind of
structures for which you may register handlers.

Windows users should use the expat_win32bin package, which includes
both precompiled libraries and executables, and source code for
developers.

Expat is free software.  You may copy, distribute, and modify it under
the terms of the License contained in the file COPYING distributed
with this package.  This license is the same as the MIT/X Consortium
license.

Versions of Expat that have an odd minor version (the middle number in
the release above), are development releases and should be considered
as beta software.  Releases with even minor version numbers are
intended to be production grade software.

If you are building Expat from a check-out from the CVS repository,
you need to run a script that generates the configure script using the
GNU autoconf and libtool tools.  To do this, you need to have
autoconf 2.58 or newer. Run the script like this:

        ./buildconf.sh

Once this has been done, follow the same instructions as for building
from a source distribution.

To build Expat from a source distribution, you first run the
configuration shell script in the top level distribution directory:

        ./configure

There are many options which you may provide to configure (which you
can discover by running configure with the --help option).  But the
one of most interest is the one that sets the installation directory.
By default, the configure script will set things up to install
libexpat into /usr/local/lib, expat.h into /usr/local/include, and
xmlwf into /usr/local/bin.  If, for example, you'd prefer to install
into /home/me/mystuff/lib, /home/me/mystuff/include, and
/home/me/mystuff/bin, you can tell configure about that with:

        ./configure --prefix=/home/me/mystuff
        
Another interesting option is to enable 64-bit integer support for
line and column numbers and the over-all byte index:

        ./configure CPPFLAGS=-DXML_LARGE_SIZE
        
However, such a modification would be a breaking change to the ABI
and is therefore not recommended for general use - e.g. as part of
a Linux distribution - but rather for builds with special requirements.

After running the configure script, the "make" command will build
things and "make install" will install things into their proper
location.  Have a look at the "Makefile" to learn about additional
"make" options.  Note that you need to have write permission into
the directories into which things will be installed.

If you are interested in building Expat to provide document
information in UTF-16 encoding rather than the default UTF-8, follow
these instructions (after having run "make distclean"):

        1. For UTF-16 output as unsigned short (and version/error
           strings as char), run:

               ./configure CPPFLAGS=-DXML_UNICODE

           For UTF-16 output as wchar_t (incl. version/error strings),
           run:

               ./configure CFLAGS="-g -O2 -fshort-wchar" \
                           CPPFLAGS=-DXML_UNICODE_WCHAR_T

        2. Edit the MakeFile, changing:

               LIBRARY = libexpat.la

           to:

               LIBRARY = libexpatw.la

           (Note the additional "w" in the library name.)

        3. Run "make buildlib" (which builds the library only).
           Or, to save step 2, run "make buildlib LIBRARY=libexpatw.la".

        4. Run "make installlib" (which installs the library only).
           Or, if step 2 was omitted, run "make installlib LIBRARY=libexpatw.la".
           
Using DESTDIR or INSTALL_ROOT is enabled, with INSTALL_ROOT being the default
value for DESTDIR, and the rest of the make file using only DESTDIR.
It works as follows:
   $ make install DESTDIR=/path/to/image
overrides the in-makefile set DESTDIR, while both
   $ INSTALL_ROOT=/path/to/image make install
   $ make install INSTALL_ROOT=/path/to/image
use DESTDIR=$(INSTALL_ROOT), even if DESTDIR eventually is defined in the
environment, because variable-setting priority is
1) commandline
2) in-makefile
3) environment  

Note: This only applies to the Expat library itself, building UTF-16 versions
of xmlwf and the tests is currently not supported.         

Note for Solaris users:  The "ar" command is usually located in
"/usr/ccs/bin", which is not in the default PATH.  You will need to
add this to your path for the "make" command, and probably also switch
to GNU make (the "make" found in /usr/ccs/bin does not seem to work
properly -- apparently it does not understand .PHONY directives).  If
you're using ksh or bash, use this command to build:

        PATH=/usr/ccs/bin:$PATH make

When using Expat with a project using autoconf for configuration, you
can use the probing macro in conftools/expat.m4 to determine how to
include Expat.  See the comments at the top of that file for more
information.

A reference manual is available in the file doc/reference.html in this
distribution.

The homepage for this project is http://www.libexpat.org/.  There
are links there to connect you to the bug reports page.  If you need
to report a bug when you don't have access to a browser, you may also
send a bug report by email to expat-bugs@mail.libexpat.org.

Discussion related to the direction of future expat development takes
place on expat-discuss@mail.libexpat.org.  Archives of this list and
other Expat-related lists may be found at:

        http://mail.libexpat.org/mailman/listinfo/
This directory contains the (fledgling) test suite for Expat.  The
tests provide general unit testing and regression coverage.  The tests
are not expected to be useful examples of Expat usage; see the
examples/ directory for that.

The Expat tests use a partial internal implementation of the "Check"
unit testing framework for C. More information on Check can be found at:

        http://check.sourceforge.net/

Expat must be built and, depending on platform, must be installed, before "make check" can be executed.

This test suite can all change in a later version.
Use this benchmark command line utility as follows:

  benchmark [-n] <file name> <buffer size> <# iterations>

The command line arguments are:

  -n             ... optional; if supplied, namespace processing is turned on
  <file name>    ... name/path of test xml file
  <buffer size>  ... size of processing buffer;
                     the file is parsed in chunks of this size
  <# iterations> ... how often will the file be parsed

Returns:

  The time (in seconds) it takes to parse the test file,
  averaged over the number of iterations.@

Expat can be built on Windows in two ways:
  using MS Visual Studio .NET or Cygwin.

* Cygwin:
  This follows the Unix build procedures.

* MS Visual Studio 2013, 2015 and 2017:
  A solution file for Visual Studio 2013 is provided: expat.sln.
  The associated project files (*.vcxproj) reside in the appropriate
  project directories. This solution file can be opened in VS 2015 or VS 2017
  and should be upgraded automatically if VS 2013 is not also installed.
  Note: Tests have their own solution files.

* All MS C/C++ compilers:
  The output for all projects will be generated in the win32\bin
  directory, intermediate files will be located in project-specific
  subdirectories of win32\tmp.
  
* Creating MinGW dynamic libraries from MS VC++ DLLs:
  
  On the command line, execute these steps:
  pexports libexpat.dll > expat.def
  pexports libexpatw.dll > expatw.def
  dlltool -d expat.def -l libexpat.a
  dlltool -d expatw.def -l libexpatw.a
  
  The *.a files are mingw libraries.

* Special note about MS VC++ and runtime libraries:

  There are three possible configurations: using the
  single threaded or multithreaded run-time library,
  or using the multi-threaded run-time Dll. That is, 
  one can build three different Expat libraries depending
  on the needs of the application.

  Dynamic Linking:

  By default the Expat Dlls are built to link statically
  with the multi-threaded run-time library. 
  The libraries are named
  - libexpat(w).dll 
  - libexpat(w).lib (import library)
  The "w" indicates the UTF-16 version of the library.

  One rarely uses other versions of the Dll, but they can
  be built easily by specifying a different RTL linkage in
  the IDE on the C/C++ tab under the category Code Generation.

  Static Linking:

  The libraries should be named like this:
  Single-theaded:     libexpat(w)ML.lib
  Multi-threaded:     libexpat(w)MT.lib
  Multi-threaded Dll: libexpat(w)MD.lib
  The suffixes conform to the compiler switch settings
  /ML, /MT and /MD for MS VC++.
  
  Note: In Visual Studio 2005 (Visual C++ 8.0) and later, the
  single-threaded runtime library is not supported anymore.

  By default, the expat-static and expatw-static projects are set up
  to link statically against the multithreaded run-time library,
  so they will build libexpatMT.lib or libexpatwMT.lib files.

  To build the other versions of the static library, 
  go to Project - Settings:
  - specify a different RTL linkage on the C/C++ tab
    under the category Code Generation.
  - then, on the Library tab, change the output file name
    accordingly, as described above

  An application linking to the static libraries must
  have the global macro XML_STATIC defined.
$NetBSD: README,v 1.1 2010/04/01 14:13:25 reed Exp $

The code within the src/external/gplv3 directories may have serious
legal impacts if you are a company and redistributing or changing
this code (as a company holding patents). We recommend you contact
your lawyer before using it.

Please do not import new GPLv3 projects without Board approval.

--------------------------------------------------------------------

Statement for The NetBSD Foundation's Position on the GPLv3

NetBSD provides source code with the goal for anyone to be able
to use it for whatever they want, as long as they follow the simple
licensing terms. Historically, most of the original code used
Berkeley-style licensing and NetBSD's own code uses a simple
two-clause Berkeley-style license. To summarize: modifications are
allowed, the source code may be redistributed and the binaries (or
executables) may be distributed as long as the copyright and
disclaimer is included.  NetBSD's code may be extended and sold
without sharing back the source code changes.

NetBSD also uses and redistributes source code and binaries from
source code obtained from external third parties. This source code
is segregated by placing it in the src/external and sys/src/external
directories which are categorized per license. Examples of this
include:  ISC BIND, Solaris ZFS, CVS, GNU Binutils, Postfix, X.org
X Windowing System, and other software that are primarily maintained
outside of NetBSD.

In some cases, the third-party software is licensed under terms
that conflict with NetBSD's own goals. For example, the GPLv2 is
a "copyleft" license  -- it requires that anyone who distributes
executable or object code based on the source code, also make the
source code and modifications available to the public.  (NetBSD's
own code doesn't require companies to share their changes.)

The GPLv3 (GNU General Public License Version 3) includes clauses
that may cause additional burdens to developers or companies who
may modify the source code or ship products based on the source
code. The following summarizes some of these issues:

- The license allows the user to circumvent measures preventing
software changes (#3).  This is known as the Tivoization clause.
In addition, this same clause is an anti-DRM, anti-DMCA clause --
as the developer allows the end-user to attempt to circumvent or
break the technological protection measures. Also, any information
or authorization keys required to install or run modified versions
must also be provided (#6).

- The patent clause (#11) says the copyright holders grant a
non-exclusive, worldwide, royalty-free patent license.  You may be
required to extend the royalty-free patent license(s) to all
recipients or future users and developers who use the code.  In
addition, you may not initiate litigation for a patent infringement
(#10).

We recommend companies redistributing GPLv3 licensed code to
consult their lawyer before using it.

It is the intent of the NetBSD project to use as little GPL licensed
software as possible to provide maximum freedom for development
and distribution of NetBSD derived products.


                Notes on enabling maintainer mode

Note that if you configure with --enable-maintainer-mode, you will need
special versions of automake, autoconf, libtool and gettext. You will
find the sources for these in the respective upstream directories:

  ftp://ftp.gnu.org/gnu/autoconf
  ftp://ftp.gnu.org/gnu/automake
  ftp://ftp.gnu.org/gnu/libtool
  ftp://ftp.gnu.org/gnu/gettext

The required versions of the tools for this tree are
  autoconf 2.64
  automake 1.11
  libtool 2.2.6
  gettext 0.14.5

Note - "make distclean" does not work with maintainer mode enabled.
The Makefiles in the some of the po/ subdirectories depend upon the
Makefiles in their parent directories, and distclean will delete the
Makefiles in the parent directories before running the Makefiles in
the child directories.  There is no easy way around this (short of
changing the automake macros) as these dependencies need to exist in
order to correctly build the NLS files.
		   README for GNU development tools

This directory contains various GNU compilers, assemblers, linkers, 
debuggers, etc., plus their support routines, definitions, and documentation.

If you are receiving this as part of a GDB release, see the file gdb/README.
If with a binutils release, see binutils/README;  if with a libg++ release,
see libg++/README, etc.  That'll give you info about this
package -- supported targets, how to use it, how to report bugs, etc.

It is now possible to automatically configure and build a variety of
tools with one command.  To build all of the tools contained herein,
run the ``configure'' script here, e.g.:

	./configure 
	make

To install them (by default in /usr/local/bin, /usr/local/lib, etc),
then do:
	make install

(If the configure script can't determine your type of computer, give it
the name as an argument, for instance ``./configure sun4''.  You can
use the script ``config.sub'' to test whether a name is recognized; if
it is, config.sub translates it to a triplet specifying CPU, vendor,
and OS.)

If you have more than one compiler on your system, it is often best to
explicitly set CC in the environment before running configure, and to
also set CC when running make.  For example (assuming sh/bash/ksh):

	CC=gcc ./configure
	make

A similar example using csh:

	setenv CC gcc
	./configure
	make

Much of the code and documentation enclosed is copyright by
the Free Software Foundation, Inc.  See the file COPYING or
COPYING.LIB in the various directories, for a description of the
GNU General Public License terms under which you can copy the files.

REPORTING BUGS: Again, see gdb/README, binutils/README, etc., for info
on where and how to report problems.
		     README for GDB release

This is GDB, the GNU source-level debugger.

A summary of new features is in the file `gdb/NEWS'.

Check the GDB home page at http://www.gnu.org/software/gdb/ for up to
date release information, mailing list links and archives, etc.

The file `gdb/PROBLEMS' contains information on problems identified
late in the release cycle.  GDB's bug tracking data base at
http://www.gnu.org/software/gdb/bugs/ contains a more complete list of
bugs.


Unpacking and Installation -- quick overview
==========================

   The release is provided as a gzipped tar file called
'gdb-VERSION.tar.gz', where VERSION is the version of GDB.

   The GDB debugger sources, the generic GNU include
files, the BFD ("binary file description") library, the readline
library, and other libraries all have directories of their own
underneath the gdb-VERSION directory.  The idea is that a variety of GNU
tools can share a common copy of these things.  Be aware of variation
over time--for example don't try to build GDB with a copy of bfd from
a release other than the GDB release (such as a binutils release),
especially if the releases are more than a few weeks apart.
Configuration scripts and makefiles exist to cruise up and down this
directory tree and automatically build all the pieces in the right
order.

   When you unpack the gdb-VERSION.tar.gz file, it will create a
source directory called `gdb-VERSION'.

You can build GDB right in the source directory:

      cd gdb-VERSION
      ./configure
      make
      cp gdb/gdb /usr/local/bin/gdb	(or wherever you want)

However, we recommend that an empty directory be used instead.
This way you do not clutter your source tree with binary files
and will be able to create different builds with different 
configuration options.

You can build GDB in any empty build directory:

      mkdir build
      cd build
      <full path to your sources>/gdb-VERSION/configure
      make
      cp gdb/gdb /usr/local/bin/gdb	(or wherever you want)

(Building GDB with DJGPP tools for MS-DOS/MS-Windows is slightly
different; see the file gdb-VERSION/gdb/config/djgpp/README for details.)

   This will configure and build all the libraries as well as GDB.  If
`configure' can't determine your system type, specify one as its
argument, e.g., `./configure sun4' or `./configure decstation'.

   Make sure that your 'configure' line ends in 'gdb-VERSION/configure':

      /berman/migchain/source/gdb-VERSION/configure      # RIGHT
      /berman/migchain/source/gdb-VERSION/gdb/configure  # WRONG

   The GDB package contains several subdirectories, such as 'gdb',
'bfd', and 'readline'.  If your 'configure' line ends in
'gdb-VERSION/gdb/configure', then you are configuring only the gdb
subdirectory, not the whole GDB package.  This leads to build errors
such as:

      make: *** No rule to make target `../bfd/bfd.h', needed by `gdb.o'.  Stop.

   If you get other compiler errors during this stage, see the `Reporting
Bugs' section below; there are a few known problems.

   GDB requires an ISO C (ANSI C) compiler.  If you do not have an ISO
C compiler for your system, you may be able to download and install
the GNU CC compiler.  It is available via anonymous FTP from the
directory `ftp://ftp.gnu.org/pub/gnu/gcc'.  GDB also requires an ISO
C standard library.  The GDB remote server, GDBserver, builds with some
non-ISO standard libraries - e.g. for Windows CE.

   GDB uses Expat, an XML parsing library, to implement some target-specific
features.  Expat will be linked in if it is available at build time, or
those features will be disabled.  The latest version of Expat should be
available from `http://expat.sourceforge.net'.

   GDB can be used as a cross-debugger, running on a machine of one
type while debugging a program running on a machine of another type.
See below.


More Documentation
******************

   All the documentation for GDB comes as part of the machine-readable
distribution.  The documentation is written in Texinfo format, which
is a documentation system that uses a single source file to produce
both on-line information and a printed manual.  You can use one of the
Info formatting commands to create the on-line version of the
documentation and TeX (or `texi2roff') to typeset the printed version.

   GDB includes an already formatted copy of the on-line Info version
of this manual in the `gdb/doc' subdirectory.  The main Info file is
`gdb-VERSION/gdb/doc/gdb.info', and it refers to subordinate files
matching `gdb.info*' in the same directory.  If necessary, you can
print out these files, or read them with any editor; but they are
easier to read using the `info' subsystem in GNU Emacs or the
standalone `info' program, available as part of the GNU Texinfo
distribution.

   If you want to format these Info files yourself, you need one of the
Info formatting programs, such as `texinfo-format-buffer' or
`makeinfo'.

   If you have `makeinfo' installed, and are in the top level GDB
source directory (`gdb-VERSION'), you can make the Info file by
typing:

      cd gdb/doc
      make info

   If you want to typeset and print copies of this manual, you need
TeX, a program to print its DVI output files, and `texinfo.tex', the
Texinfo definitions file.  This file is included in the GDB
distribution, in the directory `gdb-VERSION/texinfo'.

   TeX is a typesetting program; it does not print files directly, but
produces output files called DVI files.  To print a typeset document,
you need a program to print DVI files.  If your system has TeX
installed, chances are it has such a program.  The precise command to
use depends on your system; `lpr -d' is common; another (for PostScript
devices) is `dvips'.  The DVI print command may require a file name
without any extension or a `.dvi' extension.

   TeX also requires a macro definitions file called `texinfo.tex'. 
This file tells TeX how to typeset a document written in Texinfo
format.  On its own, TeX cannot read, much less typeset a Texinfo file.
 `texinfo.tex' is distributed with GDB and is located in the
`gdb-VERSION/texinfo' directory.

   If you have TeX and a DVI printer program installed, you can typeset
and print this manual.  First switch to the `gdb' subdirectory of
the main source directory (for example, to `gdb-VERSION/gdb') and then type:

      make doc/gdb.dvi

   If you prefer to have the manual in PDF format, type this from the
`gdb/doc' subdirectory of the main source directory:

      make gdb.pdf

For this to work, you will need the PDFTeX package to be installed.


Installing GDB
**************

   GDB comes with a `configure' script that automates the process of
preparing GDB for installation; you can then use `make' to build the
`gdb' program.

   The GDB distribution includes all the source code you need for GDB in
a single directory.  That directory contains:

`gdb-VERSION/{COPYING,COPYING.LIB}'
     Standard GNU license files.  Please read them.

`gdb-VERSION/bfd'
     source for the Binary File Descriptor library

`gdb-VERSION/config*'
     script for configuring GDB, along with other support files

`gdb-VERSION/gdb'
     the source specific to GDB itself

`gdb-VERSION/include'
     GNU include files

`gdb-VERSION/libiberty'
     source for the `-liberty' free software library

`gdb-VERSION/opcodes'
     source for the library of opcode tables and disassemblers

`gdb-VERSION/readline'
     source for the GNU command-line interface
     NOTE:  The readline library is compiled for use by GDB, but will
     not be installed on your system when "make install" is issued.

`gdb-VERSION/sim'
     source for some simulators (ARM, D10V, SPARC, M32R, MIPS, PPC, V850, etc)

`gdb-VERSION/texinfo'
     The `texinfo.tex' file, which you need in order to make a printed
     manual using TeX.

`gdb-VERSION/etc'
     Coding standards, useful files for editing GDB, and other
     miscellanea.

   Note: the following instructions are for building GDB on Unix or
Unix-like systems.  Instructions for building with DJGPP for
MS-DOS/MS-Windows are in the file gdb/config/djgpp/README.

   The simplest way to configure and build GDB is to run `configure'
from the `gdb-VERSION' directory.

   First switch to the `gdb-VERSION' source directory if you are
not already in it; then run `configure'.

   For example:

      cd gdb-VERSION
      ./configure
      make

   Running `configure' followed by `make' builds the `bfd',
`readline', `mmalloc', and `libiberty' libraries, then `gdb' itself.
The configured source files, and the binaries, are left in the
corresponding source directories.

   `configure' is a Bourne-shell (`/bin/sh') script; if your system
does not recognize this automatically when you run a different shell,
you may need to run `sh' on it explicitly:

      sh configure

   If you run `configure' from a directory that contains source
directories for multiple libraries or programs, `configure' creates
configuration files for every directory level underneath (unless
you tell it not to, with the `--norecursion' option).

   You can install `gdb' anywhere; it has no hardwired paths. However,
you should make sure that the shell on your path (named by the `SHELL'
environment variable) is publicly readable.  Remember that GDB uses the
shell to start your program--some systems refuse to let GDB debug child
processes whose programs are not readable.


Compiling GDB in another directory
==================================

   If you want to run GDB versions for several host or target machines,
you need a different `gdb' compiled for each combination of host and
target.  `configure' is designed to make this easy by allowing you to
generate each configuration in a separate subdirectory, rather than in
the source directory.  If your `make' program handles the `VPATH'
feature correctly (GNU `make' and SunOS 'make' are two that should),
running `make' in each of these directories builds the `gdb' program
specified there.

   To build `gdb' in a separate directory, run `configure' with the
`--srcdir' option to specify where to find the source. (You also need
to specify a path to find `configure' itself from your working
directory.  If the path to `configure' would be the same as the
argument to `--srcdir', you can leave out the `--srcdir' option; it
will be assumed.)

   For example, you can build GDB in a separate
directory for a Sun 4 like this:

     cd gdb-VERSION
     mkdir ../gdb-sun4
     cd ../gdb-sun4
     ../gdb-VERSION/configure
     make

   When `configure' builds a configuration using a remote source
directory, it creates a tree for the binaries with the same structure
(and using the same names) as the tree under the source directory.  In
the example, you'd find the Sun 4 library `libiberty.a' in the
directory `gdb-sun4/libiberty', and GDB itself in `gdb-sun4/gdb'.

   One popular reason to build several GDB configurations in separate
directories is to configure GDB for cross-compiling (where GDB runs on
one machine--the host--while debugging programs that run on another
machine--the target).  You specify a cross-debugging target by giving
the `--target=TARGET' option to `configure'.

   When you run `make' to build a program or library, you must run it
in a configured directory--whatever directory you were in when you
called `configure' (or one of its subdirectories).

   The `Makefile' that `configure' generates in each source directory
also runs recursively.  If you type `make' in a source directory such
as `gdb-VERSION' (or in a separate configured directory configured with
`--srcdir=PATH/gdb-VERSION'), you will build all the required libraries,
and then build GDB.

   When you have multiple hosts or targets configured in separate
directories, you can run `make' on them in parallel (for example, if
they are NFS-mounted on each of the hosts); they will not interfere
with each other.


Specifying names for hosts and targets
======================================

   The specifications used for hosts and targets in the `configure'
script are based on a three-part naming scheme, but some short
predefined aliases are also supported.  The full naming scheme encodes
three pieces of information in the following pattern:

     ARCHITECTURE-VENDOR-OS

   For example, you can use the alias `sun4' as a HOST argument or in a
`--target=TARGET' option.  The equivalent full name is
`sparc-sun-sunos4'.

   The `configure' script accompanying GDB does not provide any query
facility to list all supported host and target names or aliases. 
`configure' calls the Bourne shell script `config.sub' to map
abbreviations to full names; you can read the script, if you wish, or
you can use it to test your guesses on abbreviations--for example:

     % sh config.sub sun4
     sparc-sun-sunos4.1.1
     % sh config.sub sun3
     m68k-sun-sunos4.1.1
     % sh config.sub decstation
     mips-dec-ultrix4.2
     % sh config.sub hp300bsd
     m68k-hp-bsd
     % sh config.sub i386v
     i386-pc-sysv
     % sh config.sub i786v
     Invalid configuration `i786v': machine `i786v' not recognized

`config.sub' is also distributed in the GDB source directory.


`configure' options
===================

   Here is a summary of the `configure' options and arguments that are
most often useful for building GDB.  `configure' also has several other
options not listed here.  *note : (configure.info)What Configure Does,
for a full explanation of `configure'.

     configure [--help]
               [--prefix=DIR]
               [--srcdir=PATH]
               [--norecursion] [--rm]
	       [--enable-build-warnings]
               [--target=TARGET]
	       [--host=HOST]
	       [HOST]

You may introduce options with a single `-' rather than `--' if you
prefer; but you may abbreviate option names if you use `--'.

`--help'
     Display a quick summary of how to invoke `configure'.

`-prefix=DIR'
     Configure the source to install programs and files under directory
     `DIR'.

`--srcdir=PATH'
     *Warning: using this option requires GNU `make', or another `make'
     that compatibly implements the `VPATH' feature.*
     Use this option to make configurations in directories separate
     from the GDB source directories.  Among other things, you can use
     this to build (or maintain) several configurations simultaneously,
     in separate directories.  `configure' writes configuration
     specific files in the current directory, but arranges for them to
     use the source in the directory PATH.  `configure' will create
     directories under the working directory in parallel to the source
     directories below PATH.

`--host=HOST'
     Configure GDB to run on the specified HOST.

     There is no convenient way to generate a list of all available
     hosts.

`HOST ...'
     Same as `--host=HOST'.  If you omit this, GDB will guess; it's
     quite accurate.

`--norecursion'
     Configure only the directory level where `configure' is executed;
     do not propagate configuration to subdirectories.

`--rm'
     Remove the configuration that the other arguments specify.

`--enable-build-warnings'
     When building the GDB sources, ask the compiler to warn about any
     code which looks even vaguely suspicious.  You should only using
     this feature if you're compiling with GNU CC.  It passes the
     following flags:
	-Wimplicit
	-Wreturn-type
	-Wcomment
	-Wtrigraphs
	-Wformat
	-Wparentheses
	-Wpointer-arith

`--enable-werror'
     Treat compiler warnings as werrors.  Use this only with GCC.  It
     adds the -Werror flag to the compiler, which will fail the
     compilation if the compiler outputs any warning messages.

`--target=TARGET'
     Configure GDB for cross-debugging programs running on the specified
     TARGET.  Without this option, GDB is configured to debug programs
     that run on the same machine (HOST) as GDB itself.

     There is no convenient way to generate a list of all available
     targets.

`--with-gdb-datadir=PATH'
     Set the GDB-specific data directory.  GDB will look here for
     certain supporting files or scripts.  This defaults to the `gdb'
     subdirectory of `datadir' (which can be set using `--datadir').

`--with-relocated-sources=DIR'
     Sets up the default source path substitution rule so that
     directory names recorded in debug information will be
     automatically adjusted for any directory under DIR.  DIR should
     be a subdirectory of GDB's configured prefix, the one mentioned
     in the `--prefix' or `--exec-prefix' options to configure.  This
     option is useful if GDB is supposed to be moved to a different
     place after it is built.

`--enable-64-bit-bfd'
     Enable 64-bit support in BFD on 32-bit hosts.

`--disable-gdbmi'
     Build GDB without the GDB/MI machine interface.

`--enable-tui'
     Build GDB with the text-mode full-screen user interface (TUI).
     Requires a curses library (ncurses and cursesX are also
     supported).

`--enable-gdbtk'
     Build GDB with the gdbtk GUI interface.  Requires TCL/Tk to be
     installed.

`--with-libunwind-ia64'
     Use the libunwind library for unwinding function call stack on ia64
     target platforms.
     See http://www.nongnu.org/libunwind/index.html for details.

`--with-curses'
     Use the curses library instead of the termcap library, for
     text-mode terminal operations.

`--enable-profiling' Enable profiling of GDB itself.  Necessary if you
     want to use the "maint set profile" command for profiling GDB.
     Requires the functions `monstartup' and `_mcleanup' to be present
     in the standard C library used to build GDB, and also requires a
     compiler that supports the `-pg' option.

`--with-system-readline'
     Use the readline library installed on the host, rather than the
     library supplied as part of GDB tarball.

`--with-expat'
     Build GDB with the libexpat library.  (Done by default if
     libexpat is installed and found at configure time.)  This library
     is used to read XML files supplied with GDB.  If it is
     unavailable, some features, such as remote protocol memory maps,
     target descriptions, and shared library lists, that are based on
     XML files, will not be available in GDB.  If your host does not
     have libexpat installed, you can  get the latest version from
     http://expat.sourceforge.net.

`--with-python[=PATH]'
     Build GDB with Python scripting support.  (Done by default if
     libpython is present and found at configure time.)  Python makes
     GDB scripting much more powerful than the restricted CLI
     scripting language.  If your host does not have Python installed,
     you can find it on http://www.python.org/download/.  The oldest
     version of Python supported by GDB is 2.4.  The optional argument
     PATH says where to find the Python headers and libraries; the
     configure script will look in PATH/include for headers and in
     PATH/lib for the libraries.

`--without-included-regex'
     Don't use the regex library included with GDB (as part of the
     libiberty library).  This is the default on hosts with version 2
     of the GNU C library.

`--with-sysroot=DIR'
     Use DIR as the default system root directory for libraries whose
     file names begin with `/lib' or `/usr/lib'.  (The value of DIR
     can be modified at run time by using the "set sysroot" command.)
     If DIR is under the GDB configured prefix (set with `--prefix' or
     `--exec-prefix' options), the default system root will be
     automatically adjusted if and when GDB is moved to a different
     location.

`--with-system-gdbinit=FILE'
     Configure GDB to automatically load a system-wide init file.
     FILE should be an absolute file name.  If FILE is in a directory
     under the configured prefix, and GDB is moved to another location
     after being built, the location of the system-wide init file will
     be adjusted accordingly. 

`configure' accepts other options, for compatibility with configuring
other GNU tools recursively; but these are the only options that affect
GDB or its supporting libraries.


Remote debugging
=================

   The files m68k-stub.c, i386-stub.c, and sparc-stub.c are examples
of remote stubs to be used with remote.c.  They are designed to run
standalone on an m68k, i386, or SPARC cpu and communicate properly
with the remote.c stub over a serial line.

   The directory gdb/gdbserver/ contains `gdbserver', a program that
allows remote debugging for Unix applications.  GDBserver is only
supported for some native configurations, including Sun 3, Sun 4, and
Linux.
The file gdb/gdbserver/README includes further notes on GDBserver; in
particular, it explains how to build GDBserver for cross-debugging
(where GDBserver runs on the target machine, which is of a different
architecture than the host machine running GDB).

   There are a number of remote interfaces for talking to existing ROM
monitors and other hardware:

	remote-mips.c	 MIPS remote debugging protocol
	remote-sds.c	 PowerPC SDS monitor
	remote-sim.c	 Generalized simulator protocol


Reporting Bugs in GDB
=====================

   There are several ways of reporting bugs in GDB.  The prefered
method is to use the World Wide Web:

      http://www.gnu.org/software/gdb/bugs/

As an alternative, the bug report can be submitted, via e-mail, to the
address "bug-gdb@gnu.org".

   When submitting a bug, please include the GDB version number, and
how you configured it (e.g., "sun4" or "mach386 host,
i586-intel-synopsys target").  Since GDB now supports so many
different configurations, it is important that you be precise about
this.  If at all possible, you should include the actual banner
that GDB prints when it starts up, or failing that, the actual
configure command that you used when configuring GDB.

   For more information on how/whether to report bugs, see the
Reporting Bugs chapter of the GDB manual (gdb/doc/gdb.texinfo).


Graphical interface to GDB -- X Windows, MS Windows
==========================

   Several graphical interfaces to GDB are available.  You should
check:

	http://www.gnu.org/software/gdb/links/

for an up-to-date list.

   Emacs users will very likely enjoy the Grand Unified Debugger mode;
try typing `M-x gdb RET'.


Writing Code for GDB
=====================

   There is information about writing code for GDB in the file
`CONTRIBUTE' and at the website:

	http://www.gnu.org/software/gdb/

in particular in the wiki.

   If you are pondering writing anything but a short patch, especially
take note of the information about copyrights and copyright assignment.
It can take quite a while to get all the paperwork done, so
we encourage you to start that process as soon as you decide you are
planning to work on something, or at least well ahead of when you
think you will be ready to submit the patches.


GDB Testsuite
=============

   Included with the GDB distribution is a DejaGNU based testsuite
that can either be used to test your newly built GDB, or for
regression testing a GDB with local modifications.

   Running the testsuite requires the prior installation of DejaGNU,
which is generally available via ftp.  The directory
ftp://sources.redhat.com/pub/dejagnu/ will contain a recent snapshot.
Once DejaGNU is installed, you can run the tests in one of the
following ways:

  (1)	cd gdb-VERSION
	make check-gdb

or

  (2)	cd gdb-VERSION/gdb
	make check

or

  (3)	cd gdb-VERSION/gdb/testsuite
	make site.exp	(builds the site specific file)
	runtest -tool gdb GDB=../gdb    (or GDB=<somepath> as appropriate)

When using a `make'-based method, you can use the Makefile variable
`RUNTESTFLAGS' to pass flags to `runtest', e.g.:

	make RUNTESTFLAGS=--directory=gdb.cp check

If you use GNU make, you can use its `-j' option to run the testsuite
in parallel.  This can greatly reduce the amount of time it takes for
the testsuite to run.  In this case, if you set `RUNTESTFLAGS' then,
by default, the tests will be run serially even under `-j'.  You can
override this and force a parallel run by setting the `make' variable
`FORCE_PARALLEL' to any non-empty value.  Note that the parallel `make
check' assumes that you want to run the entire testsuite, so it is not
compatible with some dejagnu options, like `--directory'.

The last method gives you slightly more control in case of problems
with building one or more test executables or if you are using the
testsuite `standalone', without it being part of the GDB source tree.

See the DejaGNU documentation for further details.


Copyright and License Notices
=============================

Most files maintained by the GDB Project contain a copyright notice
as well as a license notice, usually at the start of the file.

To reduce the length of copyright notices, consecutive years in the
copyright notice can be combined into a single range.  For instance,
the following list of copyright years...

    1986, 1988, 1989, 1991-1993, 1999, 2000, 2007, 2008, 2009, 2010, 2011

... is abbreviated into:

    1986, 1988-1989, 1991-1993, 1999-2000, 2007-2011

Every year of each range, inclusive, is a copyrightable year that
could be listed individually.


(this is for editing this file with GNU emacs)
Local Variables:
mode: text
End:
This is a collection of tests for GDB.

The file gdb/README contains basic instructions on how to run the
testsuite, while this file documents additional options and controls
that are available.  The GDB wiki may also have some pages with ideas
and suggestions.


Running the Testsuite
*********************

There are two ways to run the testsuite and pass additional parameters
to DejaGnu.  The first is to do `make check' in the main build
directory and specifying the makefile variable `RUNTESTFLAGS':

	 make check RUNTESTFLAGS='TRANSCRIPT=y gdb.base/a2-run.exp'

The second is to cd to the testsuite directory and invoke the DejaGnu
`runtest' command directly.

	cd testsuite
	make site.exp
	runtest TRANSCRIPT=y

(The `site.exp' file contains a handful of useful variables like host
and target triplets, and pathnames.)

Parallel testing
****************

If not testing with a remote host (in DejaGnu's sense), you can run
the GDB test suite in a fully parallel mode.  In this mode, each .exp
file runs separately and maybe simultaneously.  The test suite ensures
that all the temporary files created by the test suite do not clash,
by putting them into separate directories.  This mode is primarily
intended for use by the Makefile.

For GNU make, the Makefile tries to run the tests in parallel mode if
any -j option is given.  For a non-GNU make, tests are not
parallelized.

If RUNTESTFLAGS is not empty, then by default the tests are
serialized.  This can be overridden by either using the
`check-parallel' target in the Makefile, or by setting FORCE_PARALLEL
to any non-empty value:

	make check-parallel RUNTESTFLAGS="--target_board=native-gdbserver"
	make check RUNTESTFLAGS="--target_board=native-gdbserver" FORCE_PARALLEL=1

If you want to use runtest directly instead of using the Makefile, see
the description of GDB_PARALLEL below.

Racy testcases
**************

Sometimes, new testcases are added to the testsuite that are not
entirely deterministic, and can randomly pass or fail.  We call them
"racy testcases", and they can be bothersome when one is comparing
different testsuite runs.  In order to help identifying them, it is
possible to run the tests several times in a row and ask the testsuite
machinery to analyze the results.  To do that, you need to specify the
RACY_ITER environment variable to make:

	make check RACY_ITER=5 -j4

The value assigned to RACY_ITER represents the number of times you
wish to run the tests in sequence (in the example above, the entire
testsuite will be executed 5 times in a row, in parallel).  It is also
possible to check just a specific test:

	make check TESTS='gdb.base/default.exp' RACY_ITER=3

One can also decide to call the Makefile rules by hand inside the
gdb/testsuite directory, e.g.:

	make check-paralell-racy -j4

In which case the value of the DEFAULT_RACY_ITER variable (inside
gdb/testsuite/Makefile.in) will be used to determine how many
iterations will be run.

After running the tests, you shall see a file name 'racy.sum' in the
gdb/testsuite directory.  You can also inspect the generated *.log and
*.sum files by looking into the gdb/testsuite/racy_ouputs directory.

If you already have *.sum files generated from previous testsuite runs
and you would like to analyze them without having to run the testsuite
again, you can also use the 'analyze-racy-logs.py' script directly.
It is located in the gdb/testsuite/ directory, and it expects a list
of two or more *.sum files to be provided as its argument.  For
example:

	./gdb/testsuite/analyze-racy-logs.py testsuite-01/gdb.sum \
	  testsuite-02/gdb.sum testsuite-03/gdb.sum

The script will output its analysis report to the standard output.

Running the Performance Tests
*****************************

GDB Testsuite includes performance test cases, which are not run together
with other test cases, because performance test cases are slow and need
a quiet system.  There are two ways to run the performance test cases.
The first is to do `make check-perf' in the main build directory:

	make check-perf RUNTESTFLAGS="solib.exp SOLIB_COUNT=8"

The second is to cd to the testsuite directory and invoke the DejaGnu
`runtest' command directly.

	cd testsuite
	make site.exp
	runtest GDB_PERFTEST_MODE=both GDB_PERFTEST_TIMEOUT=4000 --directory=gdb.perf solib.exp SOLIB_COUNT=8

Only "compile", "run" and "both" are valid to GDB_PERFTEST_MODE.  They
stand for "compile tests only", "run tests only", and "compile and run
tests" respectively.  "both" is the default.  GDB_PERFTEST_TIMEOUT
specify the timeout, which is 3000 in default.  The result of
performance test is appended in `testsuite/perftest.log'.

Testsuite Parameters
********************

The following parameters are DejaGNU variables that you can set to
affect the testsuite run globally.

TRANSCRIPT

You may find it useful to have a transcript of the commands that the
testsuite sends to GDB, for instance if GDB crashes during the run,
and you want to reconstruct the sequence of commands.

If the DejaGNU variable TRANSCRIPT is set (to any value), each
invocation of GDB during the test run will get a transcript file
written into the DejaGNU output directory.  The file will have the
name transcript.<n>, where <n> is an integer.  The first line of the
file shows the invocation command with all the options passed to it,
while subsequent lines are the GDB commands.  A `make check' might
look like this:

      make check RUNTESTFLAGS=TRANSCRIPT=y

The transcript may not be complete, as for instance tests of command
completion may show only partial command lines.

GDB

By default, the testsuite exercises the GDB in the build directory,
but you can set GDB to be a pathname to a different version.  For
instance,

    make check RUNTESTFLAGS=GDB=/usr/bin/gdb

runs the testsuite on the GDB in /usr/bin.

GDBSERVER

You can set GDBSERVER to be a particular GDBserver of interest, so for
instance

    make check RUNTESTFLAGS="GDB=/usr/bin/gdb GDBSERVER=/usr/bin/gdbserver"

checks both the installed GDB and GDBserver.

INTERNAL_GDBFLAGS

Command line options passed to all GDB invocations.

The default is "-nw -nx".

`-nw' disables any of the windowed interfaces.
`-nx' disables ~/.gdbinit, so that it doesn't interfere with
the tests.

This is actually considered an internal variable, and you
won't normally want to change it.  However, in some situations,
this may be tweaked as a last resort if the testsuite doesn't
have direct support for the specifics of your environment.
The testsuite does not override a value provided by the user.

As an example, when testing an installed GDB that has been
configured with `--with-system-gdbinit', like by default,
you do not want ~/.gdbinit to interfere with tests, but, you
may want the system .gdbinit file loaded.  As there's no way to
ask the testsuite, or GDB, to load the system gdbinit but
not ~/.gdbinit, a workaround is then to remove `-nx' from
INTERNAL_GDBFLAGS, and point $HOME at a directory without
a .gdbinit.  For example:

	cd testsuite
	HOME=`pwd` runtest \
	  GDB=/usr/bin/gdb \
	  GDBSERVER=/usr/bin/gdbserver \
	  INTERNAL_GDBFLAGS=-nw

GDB_PARALLEL

To use parallel testing mode without using the the Makefile, set
GDB_PARALLEL on the runtest command line to "yes".  Before starting
the tests, you must ensure that the directories cache, outputs, and
temp in the test suite build directory are either empty or have been
deleted.  cache in particular is used to share data across invocations
of runtest, and files there may affect the test results.  The Makefile
automatically does these deletions.

FORCE_PARALLEL

Setting FORCE_PARALLEL to any non-empty value forces parallel testing
mode even if RUNTESTFLAGS is not empty.

FORCE_SEPARATE_MI_TTY

Setting FORCE_MI_SEPARATE_UI to 1 forces all MI testing to start GDB
in console mode, with MI running on a separate TTY, on a secondary UI
started with "new-ui".

GDB_INOTIFY

For debugging parallel mode, it is handy to be able to see when a test
case writes to a file outside of its designated output directory.

If you have the inotify-tools package installed, you can set the
GDB_INOTIFY variable on the runtest command line.  This will cause the
test suite to watch for parallel-unsafe file creations and report
them, both to stdout and in the test suite log file.

This setting is only meaningful in conjunction with GDB_PARALLEL.

TESTS

This variable is used to specify which set of tests to run.
It is passed to make (not runtest) and its contents are a space separated
list of tests to run.

If using GNU make then the contents are wildcard-expanded using
GNU make's $(wildcard) function.  Test paths must be fully specified,
relative to the "testsuite" subdirectory.  This allows one to run all
tests in a subdirectory by passing "gdb.subdir/*.exp", or more simply
by using the check-gdb.subdir target in the Makefile.

If for some strange reason one wanted to run all tests that begin with
the letter "d" that is also possible: TESTS="*/d*.exp".

Do not write */*.exp to specify all tests (assuming all tests are only
nested one level deep, which is not necessarily true).  This will pick up
.exp files in ancillary directories like "lib" and "config".
Instead write gdb.*/*.exp.

Example:

	make -j10 check TESTS="gdb.server/[s-w]*.exp */x*.exp"

If not using GNU make then the value is passed directly to runtest.
If not specified, all tests are run.

READ1

This make (not runtest) variable is used to specify whether the
testsuite preloads the read1.so library into expect.  Any non-empty
value means true.  See "Race detection" below.

Race detection
**************

The testsuite includes a mechanism that helps detect test races.

For example, say the program running under expect outputs "abcd", and
a test does something like this:

  expect {
    "a.*c" {
    }
    "b" {
    }
    "a" {
    }
  }

Which case happens to match depends on what expect manages to read
into its internal buffer in one go.  If it manages to read three bytes
or more, then the first case matches.  If it manages to read two
bytes, then the second case matches.  If it manages to read only one
byte, then the third case matches.

To help detect these cases, the race detection mechanism preloads a
library into expect that forces the `read' system call to always
return at most 1 byte.

To enable this, either pass a non-empty value in the READ1 make
variable, or use the check-read1 make target instead of check.

Examples:

	make -j10 check-read1 TESTS="*/paginate-*.exp"
	make -j10 check READ1="1"

Testsuite Configuration
***********************

It is possible to adjust the behavior of the testsuite by defining
the global variables listed below, either in a `site.exp' file,
or in a board file.

gdb_test_timeout

Defining this variable changes the default timeout duration used
during communication with GDB.  More specifically, the global variable
used during testing is `timeout', but this variable gets reset to
`gdb_test_timeout' at the beginning of each testcase, which ensures
that any local change to `timeout' in a testcase does not affect
subsequent testcases.

This global variable comes in handy when the debugger is slower than
normal due to the testing environment, triggering unexpected `TIMEOUT'
test failures.  Examples include when testing on a remote machine, or
against a system where communications are slow.

If not specifically defined, this variable gets automatically defined
to the same value as `timeout' during the testsuite initialization.
The default value of the timeout is defined in the file
`testsuite/config/unix.exp' (at least for Unix hosts; board files may
have their own values).

gdb_reverse_timeout

Defining this variable changes the default timeout duration when tests
under gdb.reverse directory are running.  Process record and reverse
debugging is so slow that its tests have unexpected `TIMEOUT' test
failures.  This global variable is useful to bump up the value of
`timeout' for gdb.reverse tests and doesn't cause any delay where
actual failures happen in the rest of the testsuite.


Board Settings
**************

DejaGNU includes the concept of a "board file", which specifies
testing details for a particular target (which are often bare circuit
boards, thus the name).

In the GDB testsuite specifically, the board file may include a
number of "board settings" that test cases may check before deciding
whether to exercise a particular feature.  For instance, a board
lacking any I/O devices, or perhaps simply having its I/O devices
not wired up, should set `noinferiorio'.

Here are the supported board settings:

gdb,cannot_call_functions

  The board does not support inferior call, that is, invoking inferior
  functions in GDB.

gdb,can_reverse

  The board supports reverse execution.

gdb,no_hardware_watchpoints

  The board does not support hardware watchpoints.

gdb,nofileio

  GDB is unable to intercept target file operations in remote and
  perform them on the host.

gdb,noinferiorio

  The board is unable to provide I/O capability to the inferior.

gdb,noresults

  A program will not return an exit code or result code (or the value
  of the result is undefined, and should not be looked at).

gdb,nosignals

  The board does not support signals.

gdb,skip_huge_test

  Skip time-consuming tests on the board with slow connection.

gdb,skip_float_tests

  Skip tests related to floating point.

gdb,use_precord

  The board supports process record.

gdb_init_command
gdb_init_commands

  Commands to send to GDB every time a program is about to be run.  The
  first of these settings defines a single command as a string.  The
  second defines a TCL list of commands being a string each.  The commands
  are sent one by one in a sequence, first from `gdb_init_command', if any,
  followed by individual commands from `gdb_init_command', if any, in this
  list's order.

gdb_server_prog

  The location of GDBserver.  If GDBserver somewhere other than its
  default location is used in test, specify the location of GDBserver in
  this variable.  The location is a file name for GDBserver, and may be
  either absolute or relative to the testsuite subdirectory of the build
  directory.

in_proc_agent

  The location of the in-process agent (used for fast tracepoints and
  other special tests).  If the in-process agent of interest is anywhere
  other than its default location, set this variable.  The location is a
  filename, and may be either absolute or relative to the testsuite
  subdirectory of the build directory.

noargs

  GDB does not support argument passing for inferior.

no_long_long

  The board does not support type long long.

use_cygmon

  The board is running the monitor Cygmon.

use_gdb_stub

  The tests are running with a GDB stub.

exit_is_reliable

  Set to true if GDB can assume that letting the program run to end
  reliably results in program exits being reported as such, as opposed
  to, e.g., the program ending in an infinite loop or the board
  crashing/resetting.  If not set, this defaults to $use_gdb_stub.  In
  other words, native targets are assumed reliable by default, and
  remote stubs assumed unreliable.

gdb,predefined_tsv

  The predefined trace state variables the board has.

gdb,no_thread_names

  The target doesn't support thread names.

Testsuite Organization
**********************

The testsuite is entirely contained in `gdb/testsuite'.  The main
directory of the testsuite includes some makefiles and configury, but
these are minimal, and used for little besides cleaning up, since the
tests themselves handle the compilation of the programs that GDB will
run.

The file `testsuite/lib/gdb.exp' contains common utility procs useful
for all GDB tests, while the directory testsuite/config contains
configuration-specific files, typically used for special-purpose
definitions of procs like `gdb_load' and `gdb_start'.

The tests themselves are to be found in directories named
'testsuite/gdb.* and subdirectories of those.  The names of the test
files must always end with ".exp".  DejaGNU collects the test files by
wildcarding in the test directories, so both subdirectories and
individual files typically get chosen and run in alphabetical order.

The following lists some notable types of subdirectories and what they
are for.  Since DejaGNU finds test files no matter where they are
located, and since each test file sets up its own compilation and
execution environment, this organization is simply for convenience and
intelligibility.

gdb.base

This is the base testsuite.  The tests in it should apply to all
configurations of GDB (but generic native-only tests may live here).
The test programs should be in the subset of C that is both valid
ANSI/ISO C, and C++.

gdb.<lang>

Language-specific tests for any language besides C.  Examples are
gdb.cp for C++ and gdb.java for Java.

gdb.<platform>

Non-portable tests.  The tests are specific to a specific
configuration (host or target), such as eCos.

gdb.arch

Architecture-specific tests that are (usually) cross-platform.

gdb.<subsystem>

Tests that exercise a specific GDB subsystem in more depth.  For
instance, gdb.disasm exercises various disassemblers, while
gdb.stabs tests pathways through the stabs symbol reader.

gdb.perf

GDB performance tests.

Writing Tests
*************

In many areas, the GDB tests are already quite comprehensive; you
should be able to copy existing tests to handle new cases.  Be aware
that older tests may use obsolete practices but have not yet been
updated.

You should try to use `gdb_test' whenever possible, since it includes
cases to handle all the unexpected errors that might happen.  However,
it doesn't cost anything to add new test procedures; for instance,
gdb.base/exprs.exp defines a `test_expr' that calls `gdb_test'
multiple times.

Only use `send_gdb' and `gdb_expect' when absolutely necessary.  Even
if GDB has several valid responses to a command, you can use
`gdb_test_multiple'.  Like `gdb_test', `gdb_test_multiple' recognizes
internal errors and unexpected prompts.

Do not write tests which expect a literal tab character from GDB.  On
some operating systems (e.g. OpenBSD) the TTY layer expands tabs to
spaces, so by the time GDB's output reaches `expect' the tab is gone.

The source language programs do *not* need to be in a consistent
style.  Since GDB is used to debug programs written in many different
styles, it's worth having a mix of styles in the testsuite; for
instance, some GDB bugs involving the display of source lines might
never manifest themselves if the test programs used GNU coding style
uniformly.

Some testcase results need more detailed explanation:

KFAIL

Use KFAIL for known problem of GDB itself.  You must specify the GDB
bug report number, as in these sample tests:

	kfail "gdb/13392" "continue to marker 2"

or

	setup_kfail gdb/13392 "*-*-*"
	kfail "continue to marker 2"


XFAIL

Short for "expected failure", this indicates a known problem with the
environment.  This could include limitations of the operating system,
compiler version, and other components.

This example from gdb.base/attach-pie-misread.exp is a sanity check
for the target environment:

	# On x86_64 it is commonly about 4MB.
	if {$stub_size > 25000000} {
	    xfail "stub size $stub_size is too large"
	    return
	}

You should provide bug report number for the failing component of the
environment, if such bug report is available, as with this example
referring to a GCC problem:

	  if {[test_compiler_info {gcc-[0-3]-*}]
	      || [test_compiler_info {gcc-4-[0-5]-*}]} {
	      setup_xfail "gcc/46955" *-*-*
	  }
	  gdb_test "python print ttype.template_argument(2)" "&C::c"

Note that it is also acceptable, and often preferable, to avoid
running the test at all.  This is the better option if the limitation
is intrinsic to the environment, rather than a bug expected to be
fixed in the near future.
The GDB Performance Testsuite
=============================

This README contains notes on hacking on GDB's performance testsuite.
For notes on GDB's regular testsuite or how to run the performance testsuite,
see ../README.

Generated tests
***************

The testcase generator lets us easily test GDB on large programs.
The "monster" tests are mocks of real programs where GDB's
performance has been a problem.  Often it is difficult to build
these monster programs, but when measuring performance one doesn't
need the "real" program, all one needs is something that looks like
the real program along the axis one is measuring; for example, the
number of CUs (compilation units).

Structure of generated tests
****************************

Generated tests consist of a binary and potentially any number of
shared libraries.  One of these shared libraries, called "tail", is
special.  It is used to provide mocks of system provided code, and
contains no generated code.  Typically system-provided libraries
are searched last which can have significant performance consequences,
so we provide a means to exercise that.

The binary and the generated shared libraries can have a mix of
manually written and generated code.  Manually written code is
specified with the {binary,gen_shlib}_extra_sources config parameters,
which are lists of source files in testsuite/gdb.perf.  Generated
files are controlled with various configuration knobs.

Once a large test program is built, it makes sense to use it as much
as possible (i.e., with multiple tests).  Therefore perf data collection
for generated tests is split into two passes: the first pass builds
all the generated tests, and the second pass runs all the performance
tests.  The first pass is called "build-perf" and the second pass is
called "check-perf".  See ../README for instructions on running the tests.

Generated test directory layout
*******************************

All output lives under testsuite/gdb.perf in the build directory.

Because some of the tests can get really large (and take potentially
minutes to compile), parallelism is built into their compilation.
Note however that we don't run the tests in parallel as it can skew
the results.

To keep things simple and stay consistent, we use the same
mechanism used by "make check-parallel".  There is one catch: we need
one .exp for each "worker" but the .exp file must come from the source
tree.  To avoid generating .exp files for each worker we invoke
lib/build-piece.exp for each worker with different arguments.
The file build.piece.exp lives in "lib" to prevent dejagnu from finding
it when it goes to look for .exp scripts to run.

Another catch is that each parallel build worker needs its own directory
so that their gdb.{log,sum} files don't collide.  On the other hand
its easier if their output (all the object files and shared libraries)
are in the same directory.

The above considerations yield the resulting layout:

$objdir/testsuite/gdb.perf/

	gdb.log, gdb.sum: result of doing final link and running tests

	workers/

		gdb.log, gdb.sum: result of gen-workers step

		$program_name/

			${program_name}-0.worker
			...
			${program_name}-N.worker: input to build-pieces step

	outputs/

		${program_name}/

			${program_name}-0/
			...
			${program_name}-N/

				gdb.log, gdb.sum: for each build-piece worker

			pieces/

				generated sources, object files, shlibs

			${run_name_1}: binary for test config #1
			...
			${run_name_N}: binary for test config #N

Generated test configuration knobs
**********************************

The monster program generator provides various knobs for building various
kinds of monster programs.  For a list of the knobs see function
GenPerfTest::init_testcase in testsuite/lib/perftest.exp.
Most knobs are self-explanatory.
Here is a description of the less obvious ones.

binary_extra_sources

	This is the list of non-machine generated sources that go
	into the test binary.  There must be at least one: the one
	with main.

class_specs

	List of pairs of keys and values.
	Supported keys are:
	count: number of classes
	  Default: 1
	name: list of namespaces and class name prefix
	  E.g., { ns0 ns1 foo } -> ns0::ns1::foo_<cu#>_{0,1,...}
	  There is no default, this value must be specified.
	nr_members: number of members
	  Default: 0
	nr_static_members: number of static members
	  Default: 0
	nr_methods: number of methods
	  Default: 0
	nr_inline_methods: number of inline methods
	  Default: 0
	nr_static_methods: number of static methods
	  Default: 0
	nr_static_inline_methods: number of static inline methods
	  Default: 0

	E.g.,
	class foo {};
	namespace ns1 { class bar {}; }
	would be represented as:
	{
	  { count 1 name { foo } }
	  { count 1 name { ns1 bar } }
	}

	The naming of each class is "class_<cu_nr>_<class_nr>",
	where <cu_nr> is the number of the compilation unit the
	class is defined in.

	There's currently no support for nesting classes in classes,
	or for specifying baseclasses or templates.

Misc. configuration knobs
*************************

These knobs control building or running of the test and are specified
like any global Tcl variable.

CAT_PROGRAM

	Default is /bin/cat, you shouldn't need to change this.

SHA1SUM_PROGRAM

	Default is /usr/bin/sha1sum.

PERF_TEST_COMPILE_PARALLELISM

	An integer, specifies the amount of parallelism in the builds.
	Akin to make's -j flag.  The default is 10.

Writing a generated test program
********************************

The best way to write a generated test program is to take an existing
one as boilerplate.  Two good examples are gmonster1.exp and gmonster2.exp.
gmonster1.exp builds a big binary with various custom manually written
code, and gmonster2 is (essentially) the equivalent binary split up over
several shared libraries.

Writing a performance test that uses a generated program
********************************************************

The best way to write a test is to take an existing one as boilerplate.
Good examples are gmonster1-*.exp and gmonster2-*.exp.

The naming used thus far is that "foo.exp" builds the test program
and there is one "foo-bar.exp" file for each performance test
that uses test program "foo".

In addition to writing the test driver .exp script, one must also
write a python script that is used to run the test.
This contents of this script is defined by the performance testsuite
harness.  It defines a class, which is a subclass of one of the
classes in gdb.perf/lib/perftest/perftest.py.
See gmonster-null-lookup.py for an example.

Note: Since gmonster1 and gmonster2 are treated as being variations of
the same program, each test shares the same python script.
E.g., gmonster1-null-lookup.exp and gmonster2-null-lookup.exp
both use gmonster-null-lookup.py.

Running performance tests for generated programs
************************************************

There are two steps: build and run.

Example:

bash$ make -j10 build-perf RUNTESTFLAGS="gmonster1.exp"
bash$ make -j10 check-perf RUNTESTFLAGS="gmonster1-null-lookup.exp" \
    GDB_PERFTEST_MODE=run

       How to build and install the DJGPP native version of GDB
       ********************************************************

General
=======

GDB built with DJGPP supports native DJGPP debugging, whereby you run
gdb.exe and the program being debugged on the same machine.  In
addition, this version supports remote debugging via a serial port,
provided that the target machine has a GDB-compatible debugging stub
which can be linked with the target program (see the section "Remote
Serial" in the GDB manual for more details).


Installation of the binary distribution
=======================================

Simply unzip the gdbNNNb.zip file (where NNN is the version number)
from the top DJGPP installation directory.  Be sure to preserve the
directory structure while you unzip (use -d switch if you do this with
PKUNZIP).  On Windows 9X and Windows 2000, use an unzip program which
supports long file names; one such program is unzip32.exe, available
from the DJGPP sites.

If you need the libraries which are built as part of GDB, install the
companion file gdbNNNa.zip.  This allows to develop applications which
use the same functions as GDB.  For example, you can build your own
front end to the debugger.


Rebuilding GDB from sources
===========================

1. Prerequisites
   -------------
To build the package, you will need the DJGPP development environment
(GCC, header files, and the libraries), and also DJGPP ports of the
following tools:

	- GNU Make 3.79.1 or later
	- Bash 2.03 or later
	- GNU Sed
	- GNU Fileutils
	- GNU Textutils 2.0 or later
	- GNU Sh-utils
	- GNU Grep 2.4 or later
	- GNU Findutils
	- GNU Awk 3.04 or later
	- GNU Bison (only if you change one of the gdb/*.y files)
	- Groff (only if you need to format the man pages)
	- GNU Diffutils (only if you run the test suite)

These programs should be available from the DJGPP sites, in the v2gnu
directory.  In addition, the configuration script invokes the `update'
and `utod' utilities which are part of the basic DJGPP development kit
(djdevNNN.zip).


2. Unpacking the sources
   ---------------------
If you download the source distribution from one of the DJGPP sites,
just unzip it while preserving the directory structure (I suggest to
use unzip32.exe available with the rest of DJGPP), and proceed to the
section "How to build", below.

Source distributions downloaded from one of the GNU FTP sites need
some more work to unpack.  First, you MUST use the `djunpack' batch
file to unzip the package.  That's because some file names in the
official distributions need to be changed to avoid problems on the
various platforms supported by DJGPP.  `djunpack' invokes the `djtar'
program (that is part of the basic DJGPP development kit) to rename
these files on the fly given a file with name mappings; the
distribution includes a file `gdb/config/djgpp/fnchange.lst' with the
necessary mappings.  So you need first to retrieve that batch file,
and then invoke it to unpack the distribution.  Here's how:

 djtar -x -p -o gdb-5.2/djunpack.bat gdb-5.2.tar.gz > djunpack.bat
 djunpack gdb-5.2.tar.gz

(The name of the distribution archive and the leading directory of the
path to `djunpack.bat' in the distribution will be different for
versions of GDB other than 5.2.)

If the argument to `djunpack.bat' include leading directories, it MUST
be given with the DOS-style backslashes; Unix-style forward slashes
will NOT work.

If the distribution comes as a .tar.bz2 archive, and your version of
`djtar' doesn't support bzip2 decompression, you need to unpack it as
follows:

 bunzip2 gdb-6.4.tar.bz2
 djtar -x -p -o gdb-6.4/djunpack.bat gdb-6.4.tar > djunpack.bat
 djunpack gdb-6.4.tar


3. How to build
   ------------

If the source distribution available from DJGPP archives is already
configured for DJGPP v2.x (if it is, you will find files named
`Makefile' in each subdirectory), then just invoke Make:

		make

To build a package that is not yet configured, or if you downloaded
GDB from a GNU FTP site, you will need to configure it first.  You
will also need to configure it if you want to change the configuration
options (e.g., compile without support for the GDBMI interface).  To
configure GDB, type this command:

		sh ./gdb/config/djgpp/djconfig.sh

This script checks the unpacked distribution, then edits the configure
scripts in the various subdirectories, to make them suitable for
DJGPP, and finally invokes the top-level configure script, which
recursively configures all the subdirectories.

You may pass optional switches to djconfig.sh.  It accepts all the
switches accepted by the original GDB configure script.  These
switches are described in the file gdb/README, and their full list can
be displayed by running the following command:

		sh ./gdb/configure --help

NOTE: if you *do* use optional command-line switches, you MUST pass
to the script the name of the directory where GDB sources are
unpacked--even if you are building GDB in-place!  For example:

	sh ./gdb/config/djgpp/djconfig.sh . --disable-gdbmi

It is also possible to build GDB in a directory that is different from
the one where the sources were unpacked.  In that case, you have to
pass the source directory as the first argument to the script:

	sh ./gdb/config/djgpp/djconfig.sh d:/gnu/gdb-6.4

You MUST use forward slashes in the first argument.

After the configure script finishes, run Make:

	make

If you want to produce the documentation (for example, if you changed
some of the Texinfo sources), type this:

	make info

When Make finishes, you can install the package:

	make install prefix='${DJDIR}' INSTALL='ginstall -c'

The above doesn't install the docs; for that you will need to say
this:

	make install-info prefix='${DJDIR}' INSTALL='ginstall -c'

The test suite has been made to work with DJGPP.  If you make a change
in some of the programs, or want to be sure you have a fully
functional GDB executable, it is a good idea to run the test suite.
You cannot use "make check" for that, since it will want to run the
`dejagnu' utility which DJGPP doesn't support.  Instead, use the special
script gdb/config/djgpp/djcheck.sh, like this:

		cd gdb/testsuite
		sh ../config/djgpp/djcheck.sh

This will run for a while and should not print anything, except the
messages "Running tests in DIR", where DIR is one of the
subdirectories of the testsuite.  Any test that fails to produce the
expected output will cause the diffs between the expected and the
actual output be printed, and in addition will leave behind a file
SOMETHING.tst (where SOMETHING is the name of the failed test).  You
should compare each of the *.tst files with the corresponding *.out
file and convince yourself that the differences do not indicate a real
problem.  Examples of differences you can disregard are changes in the
copyright blurb printed by GDB, values of unitialized variables,
addresses of global variables like argv[] and envp[] (which depend on
the size of your environment), etc.

Note that djcheck.sh only recurses into those of the subdirectories of
the test suite which test features supported by the DJGPP port of GDB.
For example, the tests in the gdb.gdbtk, and gdb.threads directories
are not run.


Enjoy,
                                    Eli Zaretskii <eliz@gnu.org>
README for gdb/guile
====================

This file contains important notes for gdb/guile developers.
["gdb/guile" refers to the directory you found this file in]

Nomenclature:

  In the implementation we use "Scheme" or "Guile" depending on context.
  And sometimes it doesn't matter.
  Guile is Scheme, and for the most part this is what we present to the user
  as well.  However, to highlight the fact that it is Guile, the GDB commands
  that invoke Scheme functions are named "guile" and "guile-repl",
  abbreviated "gu" and "gr" respectively.

Co-existence with Python:

  Keep the user interfaces reasonably consistent, but don't shy away from
  providing a clearer (or more Scheme-friendly/consistent) user interface
  where appropriate.

  Additions to Python support or Scheme support don't require corresponding
  changes in the other scripting language.

  Scheme-wrapped breakpoints are created lazily so that if the user
  doesn't use Scheme s/he doesn't pay any cost.

Importing the gdb module into Scheme:

  To import the gdb module:
  (gdb) guile (use-modules (gdb))

  If you want to add a prefix to gdb module symbols:
  (gdb) guile (use-modules ((gdb) #:renamer (symbol-prefix-proc 'gdb:)))
  This gives every symbol a "gdb:" prefix which is a common convention.
  OTOH it's more to type.

Implementation/Hacking notes:

  Don't use scm_is_false.
  For this C function, () == #f (a la Lisp) and it's not clear how treating
  them as equivalent for truth values will affect the GDB interface.
  Until the effect is clear avoid them.
  Instead use gdbscm_is_false, gdbscm_is_true, gdbscm_is_bool.
  There are macros in guile-internal.h to enforce this.

  Use gdbscm_foo as the name of functions that implement Scheme procedures
  to provide consistent naming in error messages.  The user can see "gdbscm"
  in the name and immediately know where the function came from.

  All smobs contain gdb_smob or chained_gdb_smob as the first member.
  This provides a mechanism for extending them in the Scheme side without
  tying GDB to the details.

  The lifetime of a smob, AIUI, is decided by the containing SCM.
  When there is no longer a reference to the containing SCM then the
  smob can be GC'd.  Objects that have references from outside of Scheme,
  e.g., breakpoints, need to be protected from GC.

  Don't do something that can cause a Scheme exception inside a TRY_CATCH,
  and, in code that can be called from Scheme, don't do something that can
  cause a GDB exception outside a TRY_CATCH.
  This makes the code a little tricky to write sometimes, but it is a
  rule imposed by the programming environment.  Bugs often happen because
  this rule is broken.  Learn it, follow it.

Coding style notes:

  - If you find violations to these rules, let's fix the code.
    Some attempt has been made to be consistent, but it's early.
    Over time we want things to be more consistent, not less.

  - None of this really needs to be read.  Instead, do not be creative:
    Monkey-See-Monkey-Do hacking should generally Just Work.

  - Absence of the word "typically" means the rule is reasonably strict.

  - The gdbscm_initialize_foo function (e.g., gdbscm_initialize_values)
    is the last thing to appear in the file, immediately preceded by any
    tables of exported variables and functions.

  - In addition to these of course, follow GDB coding conventions.

General naming rules:

  - The word "object" absent any modifier (like "GOOPS object") means a
    Scheme object (of any type), and is never used otherwise.
    If you want to refer to, e.g., a GOOPS object, say "GOOPS object".

  - Do not begin any function, global variable, etc. name with scm_.
    That's what the Guile implementation uses.
    (kinda obvious, just being complete).

  - The word "invalid" carries a specific connotation.  Try not to use it
    in a different way.  It means the underlying GDB object has disappeared.
    For example, a <gdb:objfile> smob becomes "invalid" when the underlying
    objfile is removed from GDB.

  - We typically use the word "exception" to mean Scheme exceptions,
    and we typically use the word "error" to mean GDB errors.

Comments:

  - function comments for functions implementing Scheme procedures begin with
    a description of the Scheme usage.  Example:
    /* (gsmob-aux gsmob) -> object */

  - the following comment appears after the copyright header:
    /* See README file in this directory for implementation notes, coding
       conventions, et.al.  */

Smob naming:

  - gdb smobs are named, internally, "gdb:foo"
  - in Guile they become <gdb:foo>, that is the convention for naming classes
    and smobs have rudimentary GOOPS support (they can't be inherited from,
    but generics can work with them)
  - in comments use the Guile naming for smobs,
    i.e., <gdb:foo> instead of gdb:foo.
    Note: This only applies to smobs.  Exceptions are also named gdb:foo,
    but since they are not "classes" they are not wrapped in <>.
  - smob names are stored in a global, and for simplicity we pass this
    global as the "expected type" parameter to SCM_ASSERT_TYPE, thus in
    this instance smob types are printed without the <>.
    [Hmmm, this rule seems dated now.  Plus I18N rules in GDB are not always
    clear, sometimes we pass the smob name through _(), however it's not
    clear that's actually a good idea.]

Type naming:

  - smob structs are typedefs named foo_smob

Variable naming:

  - "scm" by itself is reserved for arbitrary Scheme objects

  - variables that are pointers to smob structs are named <char>_smob or
    <char><char>_smob, e.g., f_smob for a pointer to a frame smob

  - variables that are gdb smob objects are typically named <char>_scm or
    <char><char>_scm, e.g., f_scm for a <gdb:frame> object

  - the name of the first argument for method-like functions is "self"

Function naming:

  General:

  - all non-static functions have a prefix,
    either gdbscm_ or <char><char>scm_ [or <char><char><char>scm_]

  - all functions that implement Scheme procedures have a gdbscm_ prefix,
    this is for consistency and readability of Scheme exception text

  - static functions typically have a prefix
    - the prefix is typically <char><char>scm_ where the first two letters
      are unique to the file or class the function works with.
      E.g., the scm-arch.c prefix is arscm_.
      This follows something used in gdb/python in some places,
      we make it formal.

  - if the function is of a general nature, or no other prefix works,
    use gdbscm_

  Conversion functions:

  - the from/to in function names follows from libguile's existing style
  - conversions from/to Scheme objects are named:
      prefix_scm_from_foo: converts from foo to scm
      prefix_scm_to_foo: converts from scm to foo

  Exception handling:

  - functions that may throw a Scheme exception have an _unsafe suffix
    - This does not apply to functions that implement Scheme procedures.
    - This does not apply to functions whose explicit job is to throw
      an exception.  Adding _unsafe to gdbscm_throw is kinda superfluous. :-)
  - functions that can throw a GDB error aren't adorned with _unsafe

  - "_safe" in a function name means it will never throw an exception
    - Generally unnecessary, since the convention is to mark the ones that
      *can* throw an exception.  But sometimes it's useful to highlight the
      fact that the function is safe to call without worrying about exception
      handling.

  - except for functions that implement Scheme procedures, all functions
    that can throw exceptions (GDB or Scheme) say so in their function comment

  - functions that don't throw an exception, but still need to indicate to
    the caller that one happened (i.e., "safe" functions), either return
    a <gdb:exception> smob as a result or pass it back via a parameter.
    For this reason don't pass back <gdb:exception> smobs for any other
    reason.  There are functions that explicitly construct <gdb:exception>
    smobs.  They're obviously the, umm, exception.

  Internal functions:

  - internal Scheme functions begin with "%" and are intentionally undocumented
    in the manual

  Standard Guile/Scheme conventions:

  - predicates that return Scheme values have the suffix _p and have suffix "?"
    in the Scheme procedure's name
  - functions that implement Scheme procedures that modify state have the
    suffix _x and have suffix "!" in the Scheme procedure's name
  - object predicates that return a C truth value are named prefix_is_foo
  - functions that set something have "set" at the front (except for a prefix)
    write this: gdbscm_set_gsmob_aux_x implements (set-gsmob-aux! ...)
    not this: gdbscm_gsmob_set_aux_x implements (gsmob-set-aux! ...)

Doc strings:

  - there are lots of existing examples, they should be pretty consistent,
    use them as boilerplate/examples
  - begin with a one line summary (can be multiple lines if necessary)
  - if the arguments need description:
    - blank line
    - "  Arguments: arg1 arg2"
      "    arg1: blah ..."
      "    arg2: blah ..."
  - if the result requires more description:
    - blank line
    - "  Returns:"
      "    Blah ..."
  - if it's important to list exceptions that can be thrown:
    - blank line
    - "  Throws:"
      "    exception-name: blah ..."
		   README for GDBserver & GDBreplay
		    by Stu Grossman and Fred Fish

Introduction:

This is GDBserver, a remote server for Un*x-like systems.  It can be used to
control the execution of a program on a target system from a GDB on a different
host.  GDB and GDBserver communicate using the standard remote serial protocol
implemented in remote.c, and various *-stub.c files.  They communicate via
either a serial line or a TCP connection.

For more information about GDBserver, see the GDB manual.

Usage (server (target) side):

First, you need to have a copy of the program you want to debug put onto
the target system.  The program can be stripped to save space if needed, as
GDBserver doesn't care about symbols.  All symbol handling is taken care of by
the GDB running on the host system.

To use the server, you log on to the target system, and run the `gdbserver'
program.  You must tell it (a) how to communicate with GDB, (b) the name of
your program, and (c) its arguments.  The general syntax is:

	target> gdbserver COMM PROGRAM [ARGS ...]

For example, using a serial port, you might say:

	target> gdbserver /dev/com1 emacs foo.txt

This tells GDBserver to debug emacs with an argument of foo.txt, and to
communicate with GDB via /dev/com1.  GDBserver now waits patiently for the
host GDB to communicate with it.

To use a TCP connection, you could say:

	target> gdbserver host:2345 emacs foo.txt

This says pretty much the same thing as the last example, except that we are
going to communicate with the host GDB via TCP.  The `host:2345' argument means
that we are expecting to see a TCP connection from `host' to local TCP port
2345.  (Currently, the `host' part is ignored.)  You can choose any number you
want for the port number as long as it does not conflict with any existing TCP
ports on the target system.  This same port number must be used in the host
GDBs `target remote' command, which will be described shortly.  Note that if
you chose a port number that conflicts with another service, GDBserver will
print an error message and exit.

On some targets, GDBserver can also attach to running programs.  This is
accomplished via the --attach argument.  The syntax is:

	target> gdbserver --attach COMM PID

PID is the process ID of a currently running process.  It isn't necessary
to point GDBserver at a binary for the running process.

Usage (host side):

You need an unstripped copy of the target program on your host system, since
GDB needs to examine it's symbol tables and such.  Start up GDB as you normally
would, with the target program as the first argument.  (You may need to use the
--baud option if the serial line is running at anything except 9600 baud.)
Ie: `gdb TARGET-PROG', or `gdb --baud BAUD TARGET-PROG'.  After that, the only
new command you need to know about is `target remote'.  It's argument is either
a device name (usually a serial device, like `/dev/ttyb'), or a HOST:PORT
descriptor.  For example:

	(gdb) target remote /dev/ttyb

communicates with the server via serial line /dev/ttyb, and:

	(gdb) target remote the-target:2345

communicates via a TCP connection to port 2345 on host `the-target', where
you previously started up GDBserver with the same port number.  Note that for
TCP connections, you must start up GDBserver prior to using the `target remote'
command, otherwise you may get an error that looks something like
`Connection refused'.

Building GDBserver:

The supported targets as of November 2006 are:
	arm-*-linux*
	bfin-*-uclinux
	bfin-*-linux-uclibc
	crisv32-*-linux*
	cris-*-linux*
	i[34567]86-*-cygwin*
	i[34567]86-*-linux*
	i[34567]86-*-mingw*
	ia64-*-linux*
	m32r*-*-linux*
	m68*-*-linux*
	m68*-*-uclinux*
	mips*64*-*-linux*
	mips*-*-linux*
	powerpc[64]-*-linux*
	s390[x]-*-linux*
	sh-*-linux*
	spu*-*-*
	x86_64-*-linux*

Configuring GDBserver you should specify the same machine for host and
target (which are the machine that GDBserver is going to run on.  This
is not the same as the machine that GDB is going to run on; building
GDBserver automatically as part of building a whole tree of tools does
not currently work if cross-compilation is involved (we don't get the
right CC in the Makefile, to start with)).

Building GDBserver for your target is very straightforward.  If you build
GDB natively on a target which GDBserver supports, it will be built
automatically when you build GDB.  You can also build just GDBserver:

	% mkdir obj
	% cd obj
	% path-to-gdbserver-sources/configure
	% make

If you prefer to cross-compile to your target, then you can also build
GDBserver that way.  In a Bourne shell, for example:

	% export CC=your-cross-compiler
	% path-to-gdbserver-sources/configure your-target-name
	% make

Using GDBreplay:

A special hacked down version of GDBserver can be used to replay remote
debug log files created by GDB.  Before using the GDB "target" command to
initiate a remote debug session, use "set remotelogfile <filename>" to tell
GDB that you want to make a recording of the serial or tcp session.  Note
that when replaying the session, GDB communicates with GDBreplay via tcp,
regardless of whether the original session was via a serial link or tcp.

Once you are done with the remote debug session, start GDBreplay and
tell it the name of the log file and the host and port number that GDB
should connect to (typically the same as the host running GDB):

	$ gdbreplay logfile host:port

Then start GDB (preferably in a different screen or window) and use the
"target" command to connect to GDBreplay:

	(gdb) target remote host:port

Repeat the same sequence of user commands to GDB that you gave in the
original debug session.  GDB should not be able to tell that it is talking
to GDBreplay rather than a real target, all other things being equal.  Note
that GDBreplay echos the command lines to stderr, as well as the contents of
the packets it sends and receives.  The last command echoed by GDBreplay is
the next command that needs to be typed to GDB to continue the session in
sync with the original session.
GNU toolchain edition of GNU libintl 0.12.1

Most of the content of this directory is taken from gettext 0.12.1
and is owned by that project.  Patches should be directed to the
gettext developers first.  However, note the following:

* libintl.h comes from gettext, but is named libgnuintl.h.in in that
  project's source tree.

* The files COPYING.LIB-2.0 and COPYING.LIB-2.1 are redundant with the
  top-level COPYING.LIB and have therefore been removed.

* The files config.charset, ref-add.sin, ref-del.sin, os2compat.c,
  and os2compat.h are not used in this setup and have therefore been 
  removed.

* aclocal.m4 was constructed using automake's "aclocal -I ../config".

* configure.ac, config.intl.in, and Makefile.in were written for this
  directory layout, by Zack Weinberg <zack@codesourcery.com>.  Please
  direct patches for these files to gcc-patches@gcc.gnu.org.
BFD is an object file library.  It permits applications to use the
same routines to process object files regardless of their format.

BFD is used by the GNU debugger, assembler, linker, and the binary
utilities.

The documentation on using BFD is scanty and may be occasionally
incorrect.  Pointers to documentation problems, or an entirely
rewritten manual, would be appreciated.

There is some BFD internals documentation in doc/bfdint.texi which may
help programmers who want to modify BFD.

BFD is normally built as part of another package.  See the build
instructions for that package, probably in a README file in the
appropriate directory.

BFD supports the following configure options:

  --target=TARGET
	The default target for which to build the library.  TARGET is
	a configuration target triplet, such as sparc-sun-solaris.
  --enable-targets=TARGET,TARGET,TARGET...
	Additional targets the library should support.  To include
	support for all known targets, use --enable-targets=all.
  --enable-64-bit-bfd
	Include support for 64 bit targets.  This is automatically
	turned on if you explicitly request a 64 bit target, but not
	for --enable-targets=all.  This requires a compiler with a 64
	bit integer type, such as gcc.
  --enable-shared
	Build BFD as a shared library.
  --with-mmap
	Use mmap when accessing files.  This is faster on some hosts,
	but slower on others.  It may not work on all hosts.

Report bugs with BFD to bug-binutils@gnu.org.

Patches are encouraged.  When sending patches, always send the output
of diff -u or diff -c from the original file to the new file.  Do not
send default diff output.  Do not make the diff from the new file to
the original file.  Remember that any patch must not break other
systems.  Remember that BFD must support cross compilation from any
host to any target, so patches which use ``#ifdef HOST'' are not
acceptable.  Please also read the ``Reporting Bugs'' section of the
gcc manual.

Bug reports without patches will be remembered, but they may never get
fixed until somebody volunteers to fix them.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
This directory contains the -liberty library of free software.
It is a collection of subroutines used by various GNU programs.
Current members include:

	getopt -- get options from command line
	obstack -- stacks of arbitrarily-sized objects
	strerror -- error message strings corresponding to errno
	strtol -- string-to-long conversion
	strtoul -- string-to-unsigned-long conversion

We expect many of the GNU subroutines that are floating around to
eventually arrive here.

The library must be configured from the top source directory.  Don't
try to run configure in this directory.  Follow the configuration
instructions in ../README.

Please report bugs to "gcc-bugs@gcc.gnu.org" and send fixes to
"gcc-patches@gcc.gnu.org".  Thank you.

ADDING A NEW FILE
=================

There are two sets of files:  Those that are "required" will be
included in the library for all configurations, while those
that are "optional" will be included in the library only if "needed."

To add a new required file, edit Makefile.in to add the source file
name to CFILES and the object file to REQUIRED_OFILES.

To add a new optional file, it must provide a single function, and the
name of the function must be the same as the name of the file.

    * Add the source file name to CFILES in Makefile.in and the object
      file to CONFIGURED_OFILES.

    * Add the function to name to the funcs shell variable in
      configure.ac.

    * Add the function to the AC_CHECK_FUNCS lists just after the
      setting of the funcs shell variable.  These AC_CHECK_FUNCS calls
      are never executed; they are there to make autoheader work
      better.

    * Consider the special cases of building libiberty; as of this
      writing, the special cases are newlib and VxWorks.  If a
      particular special case provides the function, you do not need
      to do anything.  If it does not provide the function, add the
      object file to LIBOBJS, and add the function name to the case
      controlling whether to define HAVE_func.

Finally, in the build directory of libiberty, configure with
"--enable-maintainer-mode", run "make maint-deps" to update
Makefile.in, and run 'make stamp-functions' to regenerate
functions.texi.

The optional file you've added (e.g. getcwd.c) should compile and work
on all hosts where it is needed.  It does not have to work or even
compile on hosts where it is not needed.

ADDING A NEW CONFIGURATION
==========================

On most hosts you should be able to use the scheme for automatically
figuring out which files are needed.  In that case, you probably
don't need a special Makefile stub for that configuration.

If the fully automatic scheme doesn't work, you may be able to get
by with defining EXTRA_OFILES in your Makefile stub.  This is
a list of object file names that should be treated as required
for this configuration - they will be included in libiberty.a,
regardless of whatever might be in the C library.
This is a loose collection of notes for people hacking on simulators.
If this document gets big enough it can be prettied up then.

Contents

- The "common" directory
- Common Makefile Support
- TAGS support
- Generating "configure" files
- C Language Assumptions
- "dump" commands under gdb

The "common" directory
======================

The common directory contains:

- common documentation files (e.g. run.1, and maybe in time .texi files)
- common source files (e.g. run.c)
- common Makefile fragment and configury (e.g. Make-common.in, aclocal.m4).

In addition "common" contains portions of the system call support
(e.g. callback.c, nltvals.def).

Even though no files are built in this directory, it is still configured
so support for regenerating nltvals.def is present.

Common Makefile Support
=======================

A common configuration framework is available for simulators that want
to use it.  The common framework exists to remove a lot of duplication
in configure.ac and Makefile.in, and it also provides a foundation for
enhancing the simulators uniformly (e.g. the more they share in common
the easier a feature added to one is added to all).

The configure.ac of a simulator using the common framework should look like:

--- snip ---
dnl Process this file with autoconf to produce a configure script.
sinclude(../common/aclocal.m4)
AC_PREREQ(2.5)dnl
AC_INIT(Makefile.in)

SIM_AC_COMMON

... target specific additions ...

SIM_AC_OUTPUT
--- snip ---

SIM_AC_COMMON:

- invokes the autoconf macros most often used by the simulators
- defines --enable/--with options usable by all simulators
- initializes sim_link_files/sim_link_links as the set of symbolic links
  to set up

SIM_AC_OUTPUT:

- creates the symbolic links defined in sim_link_{files,links}
- creates config.h
- creates the Makefile

The Makefile.in of a simulator using the common framework should look like:

--- snip ---
# Makefile for blah ...
# Copyright blah ...

## COMMON_PRE_CONFIG_FRAG

# These variables are given default values in COMMON_PRE_CONFIG_FRAG.
# We override the ones we need to here.
# Not all of these need to be mentioned, only the necessary ones.
# In fact it is better to *not* mention ones if the value is the default.

# List of object files, less common parts.
SIM_OBJS =
# List of extra dependencies.
# Generally this consists of simulator specific files included by sim-main.h.
SIM_EXTRA_DEPS =
# List of flags to always pass to $(CC).
SIM_EXTRA_CFLAGS =
# List of extra libraries to link with.
SIM_EXTRA_LIBS =
# List of extra program dependencies.
SIM_EXTRA_LIBDEPS =
# List of main object files for `run'.
SIM_RUN_OBJS = run.o
# Dependency of `all' to build any extra files.
SIM_EXTRA_ALL =
# Dependency of `install' to install any extra files.
SIM_EXTRA_INSTALL =
# Dependency of `clean' to clean any extra files.
SIM_EXTRA_CLEAN =

## COMMON_POST_CONFIG_FRAG

# Rules need to build $(SIM_OBJS), plus whatever else the target wants.

... target specific rules ...
--- snip ---

COMMON_{PRE,POST}_CONFIG_FRAG are markers for SIM_AC_OUTPUT to tell it
where to insert the two pieces of common/Make-common.in.
The resulting Makefile is created by doing autoconf substitions on
both the target's Makefile.in and Make-common.in, and inserting
the two pieces of Make-common.in into the target's Makefile.in at
COMMON_{PRE,POST}_CONFIG_FRAG.

Note that SIM_EXTRA_{INSTALL,CLEAN} could be removed and "::" targets
could be used instead.  However, it's not clear yet whether "::" targets
are portable enough.

TAGS support
============

Many files generate program symbols at compile time.
Such symbols can't be found with grep nor do they normally appear in
the TAGS file.  To get around this, source files can add the comment

/* TAGS: foo1 foo2 */

where foo1, foo2 are program symbols.  Symbols found in such comments
are greppable and appear in the TAGS file.

Generating "configure" files
============================

For targets using the common framework, "configure" can be generated
by running `autoconf'.

To regenerate the configure files for all targets using the common framework:

	$  cd devo/sim
	$  make -f Makefile.in SHELL=/bin/sh autoconf-common

To add a change-log entry to the ChangeLog file for each updated
directory (WARNING - check the modified new-ChangeLog files before
renaming):

	$  make -f Makefile.in SHELL=/bin/sh autoconf-changelog
	$  more */new-ChangeLog
	$  make -f Makefile.in SHELL=/bin/sh autoconf-install

In a similar vein, both the configure and config.in files can be
updated using the sequence:

	$  cd devo/sim
	$  make -f Makefile.in SHELL=/bin/sh autoheader-common
	$  make -f Makefile.in SHELL=/bin/sh autoheader-changelog
	$  more */new-ChangeLog
	$  make -f Makefile.in SHELL=/bin/sh autoheader-install

To add the entries to an alternative ChangeLog file, use:

	$  make ChangeLog=MyChangeLog ....


C Language Assumptions
======================

The programmer may assume that the simulator is being built using an
ANSI C compiler that supports a 64 bit data type.  Consequently:

	o	prototypes can be used

	o	If sim-types.h is included, the two
		types signed64 and unsigned64 are
		available.

	o	The type `unsigned' is valid.

However, the user should be aware of the following:

	o	GCC's `<number>LL' is NOT acceptable.
		Microsoft-C doesn't reconize it.

	o	MSC's `<number>i64' is NOT acceptable.
		GCC doesn't reconize it.

	o	GCC's `long long' MSC's `_int64' can
		NOT be used to define 64 bit integer data
		types.

	o	An empty array (eg int a[0]) is not valid.

When building with GCC it is effectivly a requirement that
--enable-build-warnings=,-Werror be specified during configuration.

"dump" commands under gdb
=========================

gdbinit.in contains the following

define dump
set sim_debug_dump ()
end

Simulators that define the sim_debug_dump function can then have their
internal state pretty printed from gdb.

FIXME: This can obviously be made more elaborate.  As needed it will be.

Rebuilding nltvals.def
======================

Checkout a copy of the SIM and LIBGLOSS modules (Unless you've already
got one to hand):

	$  mkdir /tmp/$$
	$  cd /tmp/$$
	$  cvs checkout sim-no-testsuite libgloss-no-testsuite newlib-no-testsuite

Configure things for an arbitrary simulator target (I've d10v for
convenience):

	$  mkdir /tmp/$$/build
	$  cd /tmp/$$/build
	$  /tmp/$$/devo/configure --target=d10v-elf

In the sim/common directory rebuild the headers:

	$  cd sim/common
	$  make headers

To add a new target:

	devo/sim/common/gennltvals.sh

		Add your new processor target (you'll need to grub
		around to find where your syscall.h lives).

	devo/sim/<processor>/Makefile.in

		Add the definition:

			``NL_TARGET = -DNL_TARGET_d10v''

		just before the line COMMON_POST_CONFIG_FRAG.

	devo/sim/<processor>/*.[ch]

		Include targ-vals.h instead of syscall.h.

Tracing
=======

For ports based on CGEN, tracing instrumentation should largely be for free,
so we will cover the basic non-CGEN setup here.  The assumption is that your
target is using the common autoconf macros and so the build system already
includes the sim-trace configure flag.

The full tracing API is covered in sim-trace.h, so this section is an overview.

Before calling any trace function, you should make a call to the trace_prefix()
function.  This is usually done in the main sim_engine_run() loop before
simulating the next instruction.  You should make this call before every
simulated insn.  You can probably copy & paste this:
  if (TRACE_ANY_P (cpu))
    trace_prefix (sd, cpu, NULL_CIA, oldpc, TRACE_LINENUM_P (cpu), NULL, 0, "");

You will then need to instrument your simulator code with calls to the
trace_generic() function with the appropriate trace index.  Typically, this
will take a form similar to the above snippet.  So to trace instructions, you
would use something like:
  if (TRACE_INSN_P (cpu))
    trace_generic (sd, cpu, TRACE_INSN_IDX, "NOP;");

The exact output format is up to you.  See the trace index enum in sim-trace.h
to see the different tracing info available.

To utilize the tracing features at runtime, simply use the --trace-xxx flags.
  run --trace-insn ./some-program

Profiling
=========

Similar to the tracing section, this is merely an overview for non-CGEN based
ports.  The full API may be found in sim-profile.h.  Its API is also similar
to the tracing API.

Note that unlike the tracing command line options, in addition to the profile
flags, you have to use the --verbose option to view the summary report after
execution.  Tracing output is displayed on the fly, but the profile output is
only summarized.

To profile core accesses (such as data reads/writes and insn fetches), add
calls to PROFILE_COUNT_CORE() to your read/write functions.  So in your data
fetch function, you'd use something like:
  PROFILE_COUNT_CORE (cpu, target_addr, size_in_bytes, map_read);
Then in your data write function:
  PROFILE_COUNT_CORE (cpu, target_addr, size_in_bytes, map_write);
And in your insn fetcher:
  PROFILE_COUNT_CORE (cpu, target_addr, size_in_bytes, map_exec);

To use the PC profiling code, you simply have to tell the system where to find
your simulator's PC and its size.  So in your sim_open() function:
  STATE_WATCHPOINTS (sd)->pc = address_of_cpu0_pc;
  STATE_WATCHPOINTS (sd)->sizeof_pc = number_of_bytes_for_pc_storage;
In a typical 32bit system, the sizeof_pc will be 4 bytes.

To profile branches, in every location where a branch insn is executed, call
one of the related helpers:
  PROFILE_BRANCH_TAKEN (cpu);
  PROFILE_BRANCH_UNTAKEN (cpu);
If you have stall information, you can utilize the other helpers too.

Environment Simulation
======================

The simplest simulator doesn't include environment support -- it merely
simulates the Instruction Set Architecture (ISA).  Once you're ready to move
on to the next level, call the common macro in your configure.ac:
SIM_AC_OPTION_ENVIRONMENT

This will support for the user, virtual, and operating environments.  See the
sim-config.h header for a more detailed description of them.  The former are
pretty straight forward as things like exceptions (making system calls) are
handled in the simulator.  Which is to say, an exception does not trigger an
exception handler in the simulator target -- that is what the operating env
is about.  See the following userspace section for more information.

Userspace System Calls
======================

By default, the libgloss userspace is simulated.  That means the system call
numbers and calling convention matches that of libgloss.  Simulating other
userspaces (such as Linux) is pretty straightforward, but let's first focus
on the basics.  The basic API is covered in include/gdb/callback.h.

When an instruction is simulated that invokes the system call method (such as
forcing a hardware trap or exception), your simulator code should set up the
CB_SYSCALL data structure before calling the common cb_syscall() function.
For example:
static int
syscall_read_mem (host_callback *cb, struct cb_syscall *sc,
		  unsigned long taddr, char *buf, int bytes)
{
  SIM_DESC sd = (SIM_DESC) sc->p1;
  SIM_CPU *cpu = (SIM_CPU *) sc->p2;
  return sim_core_read_buffer (sd, cpu, read_map, buf, taddr, bytes);
}
static int
syscall_write_mem (host_callback *cb, struct cb_syscall *sc,
		  unsigned long taddr, const char *buf, int bytes)
{
  SIM_DESC sd = (SIM_DESC) sc->p1;
  SIM_CPU *cpu = (SIM_CPU *) sc->p2;
  return sim_core_write_buffer (sd, cpu, write_map, buf, taddr, bytes);
}
void target_sim_syscall (SIM_CPU *cpu)
{
  SIM_DESC sd = CPU_STATE (cpu);
  host_callback *cb = STATE_CALLBACK (sd);
  CB_SYSCALL sc;

  CB_SYSCALL_INIT (&sc);

  sc.func = <fetch system call number>;
  sc.arg1 = <fetch first system call argument>;
  sc.arg2 = <fetch second system call argument>;
  sc.arg3 = <fetch third system call argument>;
  sc.arg4 = <fetch fourth system call argument>;
  sc.p1 = (PTR) sd;
  sc.p2 = (PTR) cpu;
  sc.read_mem = syscall_read_mem;
  sc.write_mem = syscall_write_mem;

  cb_syscall (cb, &sc);

  <store system call result from sc.result>;
  <store system call error from sc.errcode>;
}
Some targets store the result and error code in different places, while others
only store the error code when the result is an error.

Keep in mind that the CB_SYS_xxx defines are normalized values with no real
meaning with respect to the target.  They provide a unique map on the host so
that it can parse things sanely.  For libgloss, the common/nltvals.def file
creates the target's system call numbers to the CB_SYS_xxx values.

To simulate other userspace targets, you really only need to update the maps
pointers that are part of the callback interface.  So create CB_TARGET_DEFS_MAP
arrays for each set (system calls, errnos, open bits, etc...) and in a place
you find useful, do something like:

...
static CB_TARGET_DEFS_MAP cb_linux_syscall_map[] = {
# define TARGET_LINUX_SYS_open 5
  { CB_SYS_open, TARGET_LINUX_SYS_open },
  ...
  { -1, -1 },
};
...
  host_callback *cb = STATE_CALLBACK (sd);
  cb->syscall_map = cb_linux_syscall_map;
  cb->errno_map = cb_linux_errno_map;
  cb->open_map = cb_linux_open_map;
  cb->signal_map = cb_linux_signal_map;
  cb->stat_map = cb_linux_stat_map;
...

Each of these cb_linux_*_map's are manually declared by the arch target.

The target_sim_syscall() example above will then work unchanged (ignoring the
system call convention) because all of the callback functions go through these
mapping arrays.

Events
======

Events are scheduled and executed on behalf of either a cpu or hardware devices.
The API is pretty much the same and can be found in common/sim-events.h and
common/hw-events.h.

For simulator targets, you really just have to worry about the schedule and
deschedule functions.

Device Trees
============

The device tree model is based on the OpenBoot specification.  Since this is
largely inherited from the psim code, consult the existing psim documentation
for some in-depth details.
	http://sourceware.org/psim/manual/

Hardware Devices
================

The simplest simulator doesn't include hardware device support.  Once you're
ready to move on to the next level, call the common macro in your configure.ac:
SIM_AC_OPTION_HARDWARE(yes,,devone devtwo devthree)

The basic hardware API is documented in common/hw-device.h.

Each device has to have a matching file name with a "dv-" prefix.  So there has
to be a dv-devone.c, dv-devtwo.c, and dv-devthree.c files.  Further, each file
has to have a matching hw_descriptor structure.  So the dv-devone.c file has to
have something like:
  const struct hw_descriptor dv_devone_descriptor[] = {
    {"devone", devone_finish,},
    {NULL, NULL},
  };

The "devone" string as well as the "devone_finish" function are not hard
requirements, just common conventions.  The structure name is a hard
requirement.

The devone_finish() callback function is used to instantiate this device by
parsing the corresponding properties in the device tree.

Hardware devices typically attach address ranges to themselves.  Then when
accesses to those addresses are made, the hardware will have its callback
invoked.  The exact callback could be a normal I/O read/write access, as
well as a DMA access.  This makes it easy to simulate memory mapped registers.

Keep in mind that like a proper device driver, it may be instantiated many
times over.  So any device state it needs to be maintained should be allocated
during the finish callback and attached to the hardware device via set_hw_data.
Any hardware functions can access this private data via the hw_data function.

Ports (Interrupts / IRQs)
=========================

First, a note on terminology.  A "port" is an aspect of a hardware device that
accepts or generates interrupts.  So devices with input ports may be the target
of an interrupt (accept it), and/or they have output ports so that they may be
the source of an interrupt (generate it).

Each port has a symbolic name and a unique number.  These are used to identify
the port in different contexts.  The output port name has no hard relationship
to the input port name (same for the unique number).  The callback that accepts
the interrupt uses the name/id of its input port, while the generator function
uses the name/id of its output port.

The device tree is used to connect the output port of a device to the input
port of another device.  There are no limits on the number of inputs connected
to an output, or outputs to an input, or the devices attached to the ports.
In other words, the input port and output port could be the same device.

The basics are:
 - each hardware device declares an array of ports (hw_port_descriptor).
   any mix of input and output ports is allowed.
 - when setting up the device, attach the array (set_hw_ports).
 - if the device accepts interrupts, it will have to attach a port callback
   function (set_hw_port_event)
 - connect ports with the device tree
 - handle incoming interrupts with the callback
 - generate outgoing interrupts with hw_port_event
This is the m32r simulator directory.

It is still work-in-progress.  The current sources are reasonably
well tested and lots of features are in.  However, there's lots
more yet to come.

There are lots of machine generated files in the source directory!
They are only generated if you configure with --enable-cgen-maint,
similar in behaviour to Makefile.in, configure under automake/autoconf.

For details on the generator, see ../../cgen.

devo/cgen isn't part of the comp-tools module yet.
You'll need to check it out manually (also akin to automake/autoconf).
The RX simulator offers two rx-specific configure options:

--enable-cycle-accurate  (default)
--disable-cycle-accurate

If enabled, the simulator will keep track of how many cycles each
instruction takes.  While not 100% accurate, it is very close,
including modelling fetch stalls and register latency.

--enable-cycle-stats  (default)
--disable-cycle-stats

If enabled, specifying "-v" twice on the simulator command line causes
the simulator to print statistics on how much time was used by each
type of opcode, and what pairs of opcodes tend to happen most
frequently, as well as how many times various pipeline stalls
happened.



The RX simulator offers many command line options:

-v - verbose output.  This prints some information about where the
program is being loaded and its starting address, as well as
information about how much memory was used and how many instructions
were executed during the run.  If specified twice, pipeline and cycle
information are added to the report.

-d - disassemble output.  Each instruction executed is printed.

-t - trace output.  Causes a *lot* of printed information about what
  every instruction is doing, from math results down to register
  changes.

--ignore-*
--warn-*
--error-*

  The RX simulator can detect certain types of memory corruption, and
  either ignore them, warn the user about them, or error and exit.
  Note that valid GCC code may trigger some of these, for example,
  writing a bitfield involves reading the existing value, which may
  not have been set yet.  The options for * are:

    null-deref - memory access to address zero.  You must modify your
      linker script to avoid putting anything at location zero, of
      course.

    unwritten-pages - attempts to read a page of memory (see below)
      before it is written.  This is much faster than the next option.

    unwritten-bytes - attempts to read individual bytes before they're
      written.

    corrupt-stack - On return from a subroutine, the memory location
      where $pc was stored is checked to see if anything other than
      $pc had been written to it most recently.

-i -w -e - these three options change the settings for all of the
  above.  For example, "-i" tells the simulator to ignore all memory
  corruption.

-E - end of options.  Any remaining options (after the program name)
  are considered to be options for the simulated program, although
  such functionality is not supported.



The RX simulator simulates a small number of peripherals, mostly in
order to provide I/O capabilities for testing and such.  The supported
peripherals, and their limitations, are documented here.

Memory

Memory for the simulator is stored in a hierarchical tree, much like
the i386's page directory and page tables.  The simulator can allocate
memory to individual pages as needed, allowing the simulated program
to act as if it had a full 4 Gb of RAM at its disposal, without
actually allocating more memory from the host operating system than
the simulated program actually uses.  Note that for each page of
memory, there's a corresponding page of memory *types* (for tracking
memory corruption).  Memory is initially filled with all zeros.

GPIO Port A

PA.DR is configured as an output-only port (regardless of PA.DDR).
When written to, a row of colored @ and * symbols are printed,
reflecting a row of eight LEDs being either on or off.

GPIO Port B

PB.DR controls the pipeline statistics.  Writing a 0 to PB.DR disables
statistics gathering.  Writing a non-0 to PB.DR resets all counters
and enables (even if already enabled) statistics gathering.  The
simulator starts with statistics enabled, so writing to PB.DR is not
needed if you want statistics on the entire program's run.

SCI4

SCI4.TDR is connected to the simulator's stdout.  Any byte written to
SCI4.TDR is written to stdout.  If the simulated program writes the
bytes 3, 3, and N in sequence, the simulator exits with an exit value
of N.

SCI4.SSR always returns "transmitter empty".


TPU1.TCNT
TPU2.TCNT

TPU1 and TPU2 are configured as a chained 32-bit counter which counts
machine cycles.  It always runs at "ICLK speed", regardless of the
clock control settings.  Writing to either of these 16-bit registers
zeros the counter, regardless of the value written.  Reading from
these registers returns the elapsed cycle count, with TPU1 holding the
most significant word and TPU2 holding the least significant word.

Note that, much like the hardware, these values may (TPU2.CNT *will*)
change between reads, so you must read TPU1.CNT, then TPU2.CNT, and
then TPU1.CNT again, and only trust the values if both reads of
TPU1.CNT were the same.

This directory contains the standard release of the ARMulator from
Advanced RISC Machines, and was ftp'd from.

ftp.cl.cam.ac.uk:/arm/gnu

It likes to use TCP/IP between the simulator and the host, which is
nice, but is a pain to use under anything non-unix.

I've added created a new Makefile.in (the original in Makefile.orig)
to build a version of the simulator without the TCP/IP stuff, and a
wrapper.c to link directly into gdb and the run command.

It should be possible (barring major changes in the layout of
the armulator) to upgrade the simulator by copying all the files
out of a release into this directory and renaming the Makefile.

(Except that I changed armos.c to work more simply with our
simulator rigs)

Steve

sac@cygnus.com

Mon May 15 12:03:28 PDT 1995




		PSIM 1.0.1 - Model of the PowerPC Environments


    Copyright (C) 1994-1996, Andrew Cagney <cagney@highland.com.au>.

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
 
    You should have received a copy of the GNU General Public License
    along with this program; if not, see <http://www.gnu.org/licenses/>.
 

    ----------------------------------------------------------------------


PSIM is a program written in extended ANSI-C that implements an
instruction level simulation of the PowerPC environment.  It is freely
available in source code form under the terms of the GNU General
Public License (version 3 or later).

The PowerPC Architecture is described as having three levels of
compliance:

	UEA - User Environment Architecture
	VEA - Virtual Environment Architecture
	OEA - Operating Environment Architecture

PSIM both implements all three levels of the PowerPC and includes (for
each level) a corresponding simulated run-time environment.

In addition, PSIM, to the execution unit level, models the performance
of most of the current PowerPC implementations (contributed by Michael
Meissner).  This detailed performance monitoring (unlike many other
simulators) resulting in only a relatively marginal reduction in the
simulators performance.


A description of how to build PSIM is contained in the file:

		ftp://ftp.ci.com.au/pub/psim/INSTALL
	or	ftp://cambridge.cygnus.com/pub/psim/INSTALL

while an overview of how to use PSIM is in:

	ftp://ftp.ci.com.au/pub/psim/RUN
or	ftp://cambridge.cygnus.com/pub/psim/RUN

This file is found in:

	ftp://ftp.ci.com.au/pub/psim/README
or	ftp://cambridge.cygnus.com/pub/psim/README


Thanks goes firstly to:

	Corinthian Engineering Pty Ltd
	Cygnus Support
	Highland Logic Pty Ltd

who provided the resources needed for making this software available
on the Internet.

More importantly I'd like to thank the following individuals who each
contributed in their own unique way:

	Allen Briggs, Bett Koch, David Edelsohn, Gordon Irlam,
	Michael Meissner, Bob Mercier, Richard Perini, Dale Rahn,
	Richard Stallman, Mitchele Walker


				Andrew Cagney
				Feb, 1995


    ----------------------------------------------------------------------


    What features does PSIM include?

	Monitoring and modeling

		PSIM includes (thanks to Michael Meissner)
		a detailed model of most of the PowerPC
		implementations to the functional unit level.


	SMP
		
		The PowerPC ISA defines SMP synchronizing instructions.
		This simulator implements a limited, but functional,
		subset of the PowerPC synchronization instructions
		behaviour.  Programs that restrict their synchronization
		primitives to those that work with this functional
		sub-set (eg P() and V()) are able to run on the SMP
		version of PSIM.

		People intending to use this system should study
		the code implementing the lwarx instruction.
		
	ENDIAN SUPPORT

		PSIM implements the PowerPC's big and little (xor
		endian) modes and correctly simulates code that
		switches between these two modes.

		In addition, psim can model a true little-endian
		machine.

	ISA (Instruction Set Architecture) models

		PSIM includes a model of the UEA, VEA and OEA.  This
		includes the time base registers (VEA) and HTAB
		and BATS (OEA).

		In addition, a preliminary model of the 64 bit
		PowerPC architecture is implemented.

	IO Hardware

		PSIM's internals are based around the concept
		of a Device Tree.  This tree intentionally
		resembles that of the Device Tree found in
		OpenBoot firmware.  PSIM is flexible enough
		to allow the user to fully configure this device
		tree (and consequently the hardware model) at
		run time.

	Run-time environments:

		PSIM's UEA model includes emulation for BSD
		based UNIX system calls.

		PSIM's OEA model includes emulation of either:

			o	OpenBoot client interface

			o	MOTO's BUG interface.


	Floating point

		Preliminary support for floating point is included.


    Who would be interested in PSIM?

	o	the curious

		Using psim, gdb, gcc and binutils the curious
		user can construct an environment that allows
		them to play with PowerPC Environment without
		the need for real hardware.


	o	the analyst

		PSIM includes many (contributed) monitoring
		features which (unlike many other simulators)
		do not come with a great penalty in performance.

		Thus the performance analyst is able to use
		this simulator to analyse the performance of
		the system under test.

		If PSIM doesn't monitor a components of interest,
		the source code is freely available, and hence
		there is no hinderance to changing things
		to meet a specific analysts needs.


	o	the serious SW developer

		PSIM models all three levels of the PowerPC
		Architecture: UEA, VEA and OEA.  Further,
		the internal design is such that PSIM can
		be extended to support additional requirements.


    What performance analysis measurements can PSIM perform?

	Below is the output from a recent analysis run
	(contributed by Michael Meissner):

	For the following program:

	long
	simple_rand ()
	{
	  static unsigned long seed = 47114711;
	  unsigned long this = seed * 1103515245 + 12345;
	  seed = this;
	/* cut-cut-cut - see the file RUN.psim */
	}

	Here is the current output generated with the -I switch on a P90
	(the compiler used is the development version of GCC with a new
	scheduler replacing the old one):
	
	CPU #1 executed     41,994 AND instructions.
	CPU #1 executed    519,785 AND Immediate instructions.
	.
	.
	.
	CPU #1 executed          1 System Call instruction.
	CPU #1 executed    207,746 XOR instructions.
	
	CPU #1 executed 23,740,856 cycles.
	CPU #1 executed 10,242,780 stalls waiting for data.
	CPU #1 executed          1 stall waiting for a function unit.
	.
	.
	.
	CPU #1 executed  3,136,229 branch functional unit instructions.
	CPU #1 executed 16,949,396 instructions that were accounted for in timing info.
	CPU #1 executed    871,920 data reads.
	CPU #1 executed    971,926 data writes.
	CPU #1 executed        221 icache misses.
	CPU #1 executed 16,949,396 instructions in total.
	
	Simulator speed was 250,731 instructions/second


    What motivated PSIM?

	As an idea, psim was first discussed seriously during mid
	1994.  At that time its main objectives were:


		o	good performance

			Many simulators loose out by only providing
			a binary interface to the internals.  This
			interface eventually becomes a bottle neck
			in the simulators performance.

			It was intended that PSIM would avoid this
			problem by giving the user access to the
			full source code.

			Further, by exploiting the power of modern
			compilers it was hoped that PSIM would achieve
			good performance with out having to compromise
			its internal design.


		o	practical portability

			Rather than try to be portable to every
			C compiler on every platform, it was decided
			that PSIM would restrict its self to supporting
			ANSI compilers that included the extension
			of a long long type.

			GCC is one such compiler, consequently PSIM
			should be portable to any machine running GCC.


		o	flexibility in its design

			PSIM should allow the user to select the
			features required and customise the build
			accordingly.  By having the source code,
			the compiler is able to eliminate any un
			used features of the simulator.

			After all, let the compiler do the work.


		o	SMP

			A model that allowed the simulation of
			SMP platforms with out the large overhead
			often encountered with such models.


	PSIM achieves each of these objectives.


    Is PSIM PowerPC Platform (PPCP) (nee CHRP) Compliant?

	No.

	Among other things it does not have an Apple ROM socket.


    Could PSIM be extended so that it models a CHRP machine?

	Yes.

	PSIM has been designed with the CHRP spec in mind. To model
	a CHRP desktop the following would need to be added:

		o	An apple ROM socket :-)

		o	Model of each of the desktop IO devices

		o	An OpenPIC device.

		o	RTAS (Run Time Abstraction Services).

		o	A fully populated device tree.


    Is the source code available?

	Yes.

	The source code to PSIM is available under the terms of
	the GNU Public Licence.  This allows you to distribute
	the source code for free but with certain conditions.

	See the file:

		ftp://archie.au/gnu/COPYING

	For details of the terms and conditions.


    Where do I send bugs or report problems?

	There is a mailing list (subscribe through majordomo@ci.com.au) at:

	powerpc-psim@ci.com.au

	If I get the ftp archive updated I post a note to that mailing list.
	In addition your welcome to send bugs or problems either to me or to
	that e-mail list.

	This list currently averages zero articles a day.


     Does PSIM have any limitations or problems?

	PSIM can't run rs6000/AIX binaries - At present PSIM can only
	simulate static executables.  Since an AIX executable is
	never static, PSIM is unable to simulate its execution.

	PSIM is still under development - consequently there are going
	to be bugs.

	See the file BUGS (included in the distribution) for any
	other outstanding issues.


1. MEC and ERC32 emulation

The file 'erc32.c' contains a model of the MEC, 512 K rom and 4 M ram.

The following paragraphs outline the implemented MEC functions.

1.1 UARTs

The UARTs are connected to two pseudo-devices, /dev/ttypc and /dev/ttypd.
The following registers are implemeted:

- UART A RX and TX register	(0x01f800e0)
- UART B RX and TX register	(0x01f800e4)
- UART status register		(0x01f800e8)

To speed up simulation, the UARTs operate at approximately 115200 baud. 
The UARTs generate interrupt 4 and 5 after each received or transmitted 
character.  The error interrupt is generated if overflow occurs - other
errors cannot occure.

1.2 Real-time clock and general pupose timer A

The following registers are implemeted:

- Real-time clock timer				(0x01f80080, read-only)
- Real-time clock scaler program register 	(0x01f80084, write-only)
- Real-time clock counter program register 	(0x01f80080, write-only)

- Genearl pupose timer 				(0x01f80088, read-only)
- Real-time clock scaler program register 	(0x01f8008c, write-only)
- General purpose timer counter prog. register 	(0x01f80088, write-only)

- Timer control register			(0x01f80098, write-only)

1.3 Interrupt controller

The interrupt controller is implemented as in the MEC specification with
the exception of the interrupt shape register. Since external interrupts
are not possible, the interrupt shape register is not implemented. The
only internal interrupts that are generated are the real-time clock, 
the general purpose timer and UARTs. However, all 15 interrupts
can be tested via the interrupt force register.

The following registers are implemeted:

- Interrupt pending register		       (0x01f80048, read-only)
- Interrupt mask register		       (0x01f8004c, read-write)
- Interrupt clear register		       (0x01f80050, write-only)
- Interrupt force register		       (0x01f80054, read-write)

1.4 Breakpoint and watchpoint register

The breakpoint and watchpoint functions are implemented as in the MEC
specification. Traps are correctly generated, and the system fault status
register is updated accordingly. Implemeted registers are:

- Debug control register			(0x01f800c0, read-write)
- Breakpoint register				(0x01f800c4, write-only)
- Watchpoint register				(0x01f800c8, write-only)
- System fault status register			(0x01f800a0, read-write)
- Firts failing address register		(0x01f800a4, read-write)


1.5 Memory interface

The following memory areas are valid for the ERC32 simulator:

0x00000000 - 0x00080000		ROM (512 Kbyte, loaded at start-up)
0x02000000 - 0x02400000		RAM (4 Mbyte, initialised to 0x0)
0x01f80000 - 0x01f800ff		MEC registers

Access to unimplemented MEC registers or non-existing memory will result
in a memory exception trap. However, access to unimplemented MEC registers
in the area 0x01f80000 - 0x01f80100 will not cause a memory exception trap.
The written value will be stored in a register and can be read back. It
does however not affect the function in any way. 

The memory configuartion register is used to define available memory
in the system. The fields RSIZ and PSIZ are used to set RAM and ROM
size, the remaining fields are not used.  NOTE: after reset, the MEC 
is set to decode 4 Kbyte of ROM and 256 Kbyte of RAM. The memory 
configuration register has to be updated to reflect the available memory. 

The waitstate configuration register is used to generate waitstates. 
This register must also be updated with the correct configuration after 
reset.

The memory protection scheme is implemented - it is enabled through bit 3
in the MEC control register.

The following registers are implemeted:

- MEC control register (bit 3 only)		(0x01f80000, read-write)
- Memory control register			(0x01f80010, read-write)
- Waitstate configuration register		(0x01f80018, read-write)
- Memory access register 0			(0x01f80020, read-write)
- Memory access register 1			(0x01f80024, read-write)

1.6 Watchdog

The watchdog is implemented as in the specification. The input clock is
always the system clock regardsless of WDCS bit in mec configuration 
register.

The following registers are implemeted:
 
- Watchdog program and acknowledge register	(0x01f80060, write-only)
- Watchdog trap door set register		(0x01f80064, write-only)

1.7 Software reset register

Implemented as in the specification (0x01f800004, write-only).

1.8 Power-down mode

The power-down register (0x01f800008) is implemented as in the specification.
However, if the simulator event queue is empty, power-down mode is not
entered since no interrupt would be generated to exit from the mode. A
Ctrl-C in the simulator window will exit the power-down mode.

1.9 MEC control register

The following bits are implemented in the MEC control register:

Bit	Name	Function
0	PRD	Power-down mode enable
1	SWR	Soft reset enable
3	APR	Access protection enable

How to use SIS with GDB
-----------------------

1. Building GDB with SIS

To build GDB with the SIS/ERC32 simulator, configure with option
'--target sparc-erc32-aout' and build as usual.

2. Attaching the simulator

To attach GDB to the simulator, use:

target sim [options] [files]

The following options are supported:

 -nfp		Disable FPU. FPops will cause an FPU disabled trap.

 -freq <f>	Set the simulated "system clock" to <f> MHz.

 -v		Verbose mode.

 -nogdb		Disable GDB breakpoint handling (see below)

The listed [files] are expected to be in aout format and will be
loaded in the simulator memory prior. This could be used to load
a boot block at address 0x0 if the application is linked to run
from RAM (0x2000000).

To start debugging a program type 'load <program>' and debug as
usual. 

The native simulator commands can be reached using the GDB 'sim'
command:

sim <sis_command>

Direct simulator commands during a GDB session must be issued
with care not to disturb GDB's operation ... 

For info on supported ERC32 functionality, see README.sis.


3. Loading aout files

The GDB load command loads an aout file into the simulator
memory with the data section starting directly after the text
section regardless of wich start address was specified for the data
at link time! This means that your applications either has to include
a routine that initialise the data segment at the proper address or
link with the data placed directly after the text section.

A copying routine is fairly simple, just copy all data between
_etext and _data to a memory loaction starting at _environ. This
should be done at the same time as the bss is cleared (in srt0.s).


4. GDB breakpoint handling

GDB inserts breakpoint in the form of the 'ta 1' instruction. The
GDB-integrated simulator will therefore recognize the breakpoint
instruction and return control to GDB. If the application uses
'ta 1', the breakpoint detection can be disabled with the -nogdb
switch. In this case however, GDB breakpoints will not work.


Report problems to Jiri Gaisler ESA/ESTEC (jgais@wd.estec.esa.nl)

SIS - Sparc Instruction Simulator README file  (v2.0, 05-02-1996)
-------------------------------------------------------------------

1. Introduction

The SIS is a SPARC V7 architecture simulator. It consist of two parts,
the simulator core and a user defined memory module. The simulator
core executes the instructions while the memory module emulates memory
and peripherals. 

2. Usage

The simulator is started as follows: 

sis [-uart1 uart_device1] [-uart2 uart_device2] 
    [-nfp] [-freq frequency] [-c batch_file] [files] 

The default uart devices for SIS are /dev/ptypc and /dev/ptypd. The
-uart[1,2] switch can be used to connect the uarts to other devices.
Use 'tip /dev/ttypc'  to connect a terminal emulator to the uarts.
The '-nfp' will disable the simulated FPU, so each FPU instruction will
generate a FPU disabled trap. The '-freq' switch can be used to define
which "frequency" the simulator runs at. This is used by the 'perf'
command to calculated the MIPS figure for a particular configuration.
The give frequency must be an integer indicating the frequency in MHz.

The -c option indicates that sis commands should be read from 'batch_file' 
at startup.

Files to be loaded must be in one of the supported formats (see INSTALLATION),
and will be loaded into the simulated memory. The file formats are
automatically recognised.

The script 'startsim' will start the simulator in one xterm window and
open a terminal emulator (tip) connected to the UART A in a second
xterm window. Below is description of commands  that are recognized by 
the simulator. The command-line is parsed using GNU readline. A command
history of 64 commands is maintained. Use the up/down arrows to recall
previous commands. For more details, see the readline documentation.

batch <file>

Execute a batch file of SIS commands.

+bp <address>

Adds an breakpoint at address <address>.

bp

Prints all breakpoints

-bp <num>

Deletes breakpoint <num>. Use 'bp' to see which number is assigned to the 
breakpoints.

cont [inst_count]

Continue execution at present position, optionally for [inst_count] 
instructions.

dis [addr] [count]

Disassemble [count] instructions at address [addr]. Default values for
count is 16 and addr is the present address.

echo <string>

Print <string> to the simulator window.

float

Prints the FPU registers

go <address> [inst_count]

The go command will set pc to <address> and npc to <address> + 4, and start
execution. No other initialisation will be done. If inst_count is given, 
execution will stop after the specified number of instructions.

help

Print a small help menu for the SIS commands.

hist [trace_length]

Enable the instruction trace buffer. The 'trace_length' last executed 
instructions will be placed in the trace buffer. A 'hist' command without 
a trace_length will display the trace buffer. Specifying a zero trace 
length will disable the trace buffer.

load  <file_name>

Loads a file into simulator memory. 

mem [addr] [count]

Display memory at [addr] for [count] bytes. Same default values as above.

quit

Exits the simulator.

perf [reset]

The 'perf' command will display various execution statistics. A 'perf reset' 
command will reset the statistics. This can be used if statistics shall 
be calculated only over a part of the program. The 'run' and 'reset' 
command also resets the statistic information.

reg [reg_name] [value]

Prints and sets the IU regiters. 'reg' without parameters prints the IU
registers. 'reg [reg_name] [value]' sets the corresponding register to
[value]. Valid register names are psr, tbr, wim, y, g1-g7, o0-o7 and
l0-l7.

reset

Performs a power-on reset. This command is equal to 'run 0'.

run [inst_count]

Resets the simulator and starts execution from address 0. If an instruction
count is given (inst_count), the simulator will stop after the specified 
number of instructions. The event queue is emptied but any set breakpoints
remain.

step

Equal to 'trace 1'

tra [inst_count]

Starts the simulator at the present position and prints each instruction
it executes. If an instruction count is given (inst_count), the simulator 
will stop after the specified number of instructions.

Typing a 'Ctrl-C' will interrupt a running simulator. 

Short forms of the commands are allowed, e.g 'c' 'co' or 'con' are all
interpreted as 'cont'. 


3. Simulator core

The SIS emulates the behavior of the 90C601E and 90C602E sparc IU and
FPU from Matra MHS. These are roughly equivalent to the Cypress C601
and C602.  The simulator is cycle true, i.e a simulator time is
maintained and inremented according the IU and FPU instruction timing.
The parallel execution between the IU and FPU is modelled, as well as
stalls due to operand dependencies (FPU). The core interacts with the
user-defined memory modules through a number of functions. The memory
module must provide the following functions:

int memory_read(asi,addr,data,ws)
int asi;
unsigned int addr;
unsigned int *data;
int *ws;

int memory_write(asi,addr,data,sz,ws)
int asi;
unsigned int addr;
unsigned int *data;
int sz;
int *ws;

int sis_memory_read(addr, data, length)
unsigned int addr;
char   *data;
unsigned int length;

int sis_memory_write(addr, data, length)
unsigned int addr;
char    *data;
unsigned int length;

int init_sim()

int reset()

int error_mode(pc)
unsigned int pc;

memory_read() is used by the simulator to fetch instructions and
operands.  The address space identifier (asi) and address is passed as
parameters. The read data should be assigned to the data pointer
(*data) and the number of waitstate to *ws. 'memory_read' should return
0 on success and 1 on failure. A failure will cause a data or
instruction fetch trap. memory_read() always reads one 32-bit word.

sis_memory_read() is used by the simulator to display and disassemble
memory contants. The function should copy 'length' bytes of the simulated
memory starting at 'addr' to '*data'.
The sis_memory_read() should return 1 on success and 0 on failure.
Failure should only be indicated if access to unimplemented memory is attempted.

memory_write() is used to write to memory. In addition to the asi
and address parameters, the size of the written data is given by 'sz'.
The pointer *data points to the data to be written. The 'sz' is coded
as follows:

  sz	access type
  0	  byte
  1	  halfword
  2	  word
  3	  double-word

If a double word is written, the most significant word is in data[0] and
the least significant in data[1].

sis_memory_write() is used by the simulator during loading of programs.
The function should copy 'length' bytes from *data to the simulated
memory starting at 'addr'. sis_memory_write() should return 1 on 
success and 0 on failure. Failure should only be indicated if access 
to unimplemented memory is attempted. See erc32.c for more details 
on how to define the memory emulation functions.

The 'init_sim' is called once when the simulator is started. This function
should be used to perform initialisations of user defined memory or 
peripherals that only have to be done once, such as opening files etc.

The 'reset' is called every time the simulator is reset, i.e. when a
'run' command is given. This function should be used to simulate a power
on reset of memory and peripherals.

error_mode() is called by the simulator when the IU goes into error mode,
typically if a trap is caused when traps are disabled. The memory module
can then take actions, such as issue a reset.

sys_reset() can be called by the memory module to reset the simulator. A
reset will empty the event queue and perform a power-on reset.

4. Events and interrupts

The simulator supports an event queue and the generation of processor
interrupts. The following functions are available to the user-defined
memory module:

event(cfunc,arg,delta)
void (*cfunc)();
int arg;
unsigned int delta;

set_int(level,callback,arg)
int level;
void (*callback)();
int arg;

clear_int(level)
int level;

sim_stop()

The 'event' functions will schedule the execution of the function 'cfunc'
at time 'now + delta' clock cycles. The parameter 'arg' is passed as a 
parameter to 'cfunc'.

The 'set_int' function set the processor interrupt 'level'. When the interrupt
is taken, the function 'callback' is called with the argument 'arg'. This
will also clear the interrupt. An interrupt can be cleared before it is
taken by calling 'clear_int' with the appropriate interrupt level.

The sim_stop function is called each time the simulator stops execution.
It can be used to flush buffered devices to get a clean state during
single stepping etc.

See 'erc32.c' for examples on how to use events and interrupts.

5. Memory module

The supplied memory module (erc32.c) emulates the functions of memory and
the MEC asic developed for the 90C601/2. It includes the following functions:

* UART A & B
* Real-time clock
* General purpose timer
* Interrupt controller
* Breakpoint register
* Watchpoint register
* 512 Kbyte ROM
* 4 Mbyte RAM

See README.erc32 on how the MEC functions are emulated.  For a detailed MEC
specification, look at the ERC32 home page at URL:

http://www.estec.esa.nl/wsmwww/erc32

6. Compile and linking programs

The directory 'examples' contain some code fragments for SIS.
The script gccx indicates how the native sunos gcc and linker can be used
to produce executables for the simulator. To compile and link the provided
'hello.c', type 'gccx hello.c'. This will build the executable 'hello'.
Start the simulator by running 'startsim hello', and issue the command 'run.
After the program is terminated, the IU will be force to error mode through
a software trap and halt. 

The programs are linked with a start-up file, srt0.S. This file includes
the traptable and window underflow/overflow trap routines.

7. IU and FPU instruction timing.

The simulator provides cycle true simulation. The following table shows
the emulated instruction timing for 90C601E & 90C602E:

Instructions	      Cycles

jmpl, rett		2
load			2
store			3
load double		3
store double		4
other integer ops	1
fabs			2
fadds			4
faddd			4
fcmps			4
fcmpd			4
fdivs			20
fdivd			35
fmovs			2
fmuls			5
fmuld			9
fnegs			2
fsqrts			37
fsqrtd			65
fsubs			4
fsubd			4
fdtoi			7
fdots			3
fitos			6
fitod			6
fstoi			6
fstod			2

The parallel operation between the IU and FPU is modelled. This means
that a FPU instruction will execute in parallel with other instructions as
long as no data or resource dependency is detected. See the 90C602E data
sheet for the various types of dependencies. Tracing using the 'trace'
command will display the current simulator time in the left column. This
time indicates when the instruction is fetched. If a dependency is detetected,
the following fetch will be delayed until the conflict is resolved.

The load dependency in the 90C601E is also modelled - if the destination 
register of a load instruction is used by the following instruction, an 
idle cycle is inserted.

8. FPU implementation

The simulator maps floating-point operations on the hosts floating point
capabilities. This means that accuracy and generation of IEEE exceptions is 
host dependent.
This is the frv simulator directory.

It is still work-in-progress.  The current sources are
well tested and lots of features are in.

There are lots of machine generated files in the source directory!
They are only generated if you configure with --enable-cgen-maint,
similar in behaviour to Makefile.in, configure under automake/autoconf.

For details on the generator, see ../../cgen.
This directory contains the zlib package, which is not part of GCC but
shipped with GCC as convenience.

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.8 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://tools.ietf.org/html/rfc1950 (zlib format), rfc1951 (deflate format) and
rfc1952 (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  A usage example
of the library is given in the file test/example.c which also tests that
the library is working correctly.  Another example is given in the file
test/minigzip.c.  The compression library itself is composed of all source
files in the root directory.

To compile all files and run the test program, follow the instructions given at
the top of Makefile.in.  In short "./configure; make test", and if that goes
well, "make install" should work for most flavors of Unix.  For Windows, use
one of the special makefiles in win32/ or contrib/vstudio/ .  For VMS, use
make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version.  The zlib home page is
http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read the zlib FAQ http://zlib.net/zlib_faq.html before asking for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan.  1997
issue of Dr.  Dobb's Journal; a copy of the article is available at
http://marknelson.us/1997/01/01/zlib-engine/ .

The changes made in version 1.2.8 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory contrib/ .

zlib is available in Java using the java.util.zip package, documented at
http://java.sun.com/developer/technicalArticles/Programming/compression/ .

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is available
at CPAN (Comprehensive Perl Archive Network) sites, including
http://search.cpan.org/~pmqs/IO-Compress-Zlib/ .

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://docs.python.org/library/zlib.html .

zlib is built into tcl: http://wiki.tcl.tk/4610 .

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS or BEOS.

- For PalmOs, see http://palmzlib.sourceforge.net/


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz.  The deflate and
  zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
  people who reported problems and suggested various improvements in zlib; they
  are too numerous to cite here.

Copyright notice:

 (C) 1995-2013 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
This directory contains examples of the use of zlib and other relevant
programs and documentation.

enough.c
    calculation and justification of ENOUGH parameter in inftrees.h
    - calculates the maximum table space used in inflate tree
      construction over all possible Huffman codes

fitblk.c
    compress just enough input to nearly fill a requested output size
    - zlib isn't designed to do this, but fitblk does it anyway

gun.c
    uncompress a gzip file
    - illustrates the use of inflateBack() for high speed file-to-file
      decompression using call-back functions
    - is approximately twice as fast as gzip -d
    - also provides Unix uncompress functionality, again twice as fast

gzappend.c
    append to a gzip file
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of deflatePrime() to start at any bit

gzjoin.c
    join gzip files without recalculating the crc or recompressing
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of crc32_combine()

gzlog.c
gzlog.h
    efficiently and robustly maintain a message log file in gzip format
    - illustrates use of raw deflate, Z_PARTIAL_FLUSH, deflatePrime(),
      and deflateSetDictionary()
    - illustrates use of a gzip header extra field

zlib_how.html
    painfully comprehensive description of zpipe.c (see below)
    - describes in excruciating detail the use of deflate() and inflate()

zpipe.c
    reads and writes zlib streams from stdin to stdout
    - illustrates the proper use of deflate() and inflate()
    - deeply commented in zlib_how.html (see above)

zran.c
    index a zlib or gzip stream and randomly access it
    - illustrates the use of Z_BLOCK, inflatePrime(), and
      inflateSetDictionary() to provide random access
Puff -- A Simple Inflate
3 Mar 2003
Mark Adler
madler@alumni.caltech.edu

What this is --

puff.c provides the routine puff() to decompress the deflate data format.  It
does so more slowly than zlib, but the code is about one-fifth the size of the
inflate code in zlib, and written to be very easy to read.

Why I wrote this --

puff.c was written to document the deflate format unambiguously, by virtue of
being working C code.  It is meant to supplement RFC 1951, which formally
describes the deflate format.  I have received many questions on details of the
deflate format, and I hope that reading this code will answer those questions.
puff.c is heavily commented with details of the deflate format, especially
those little nooks and cranies of the format that might not be obvious from a
specification.

puff.c may also be useful in applications where code size or memory usage is a
very limited resource, and speed is not as important.

How to use it --

Well, most likely you should just be reading puff.c and using zlib for actual
applications, but if you must ...

Include puff.h in your code, which provides this prototype:

int puff(unsigned char *dest,           /* pointer to destination pointer */
         unsigned long *destlen,        /* amount of output space */
         unsigned char *source,         /* pointer to source data pointer */
         unsigned long *sourcelen);     /* amount of input available */

Then you can call puff() to decompress a deflate stream that is in memory in
its entirety at source, to a sufficiently sized block of memory for the
decompressed data at dest.  puff() is the only external symbol in puff.c  The
only C library functions that puff.c needs are setjmp() and longjmp(), which
are used to simplify error checking in the code to improve readabilty.  puff.c
does no memory allocation, and uses less than 2K bytes off of the stack.

If destlen is not enough space for the uncompressed data, then inflate will
return an error without writing more than destlen bytes.  Note that this means
that in order to decompress the deflate data successfully, you need to know
the size of the uncompressed data ahead of time.

If needed, puff() can determine the size of the uncompressed data with no
output space.  This is done by passing dest equal to (unsigned char *)0.  Then
the initial value of *destlen is ignored and *destlen is set to the length of
the uncompressed data.  So if the size of the uncompressed data is not known,
then two passes of puff() can be used--first to determine the size, and second
to do the actual inflation after allocating the appropriate memory.  Not
pretty, but it works.  (This is one of the reasons you should be using zlib.)

The deflate format is self-terminating.  If the deflate stream does not end
in *sourcelen bytes, puff() will return an error without reading at or past
endsource.

On return, *sourcelen is updated to the amount of input data consumed, and
*destlen is updated to the size of the uncompressed data.  See the comments
in puff.c for the possible return codes for puff().
These classes provide a C++ stream interface to the zlib library. It allows you
to do things like:

  gzofstream outf("blah.gz");
  outf << "These go into the gzip file " << 123 << endl;

It does this by deriving a specialized stream buffer for gzipped files, which is
the way Stroustrup would have done it. :->

The gzifstream and gzofstream classes were originally written by Kevin Ruland
and made available in the zlib contrib/iostream directory. The older version still
compiles under gcc 2.xx, but not under gcc 3.xx, which sparked the development of
this version.

The new classes are as standard-compliant as possible, closely following the
approach of the standard library's fstream classes. It compiles under gcc versions
3.2 and 3.3, but not under gcc 2.xx. This is mainly due to changes in the standard
library naming scheme. The new version of gzifstream/gzofstream/gzfilebuf differs
from the previous one in the following respects:
- added showmanyc
- added setbuf, with support for unbuffered output via setbuf(0,0)
- a few bug fixes of stream behavior
- gzipped output file opened with default compression level instead of maximum level
- setcompressionlevel()/strategy() members replaced by single setcompression()

The code is provided "as is", with the permission to use, copy, modify, distribute
and sell it for any purpose without fee.

Ludwig Schwardt
<schwardt@sun.ac.za>

DSP Lab
Electrical & Electronic Engineering Department
University of Stellenbosch
South Africa
This is a patched version of zlib, modified to use
Pentium-Pro-optimized assembly code in the deflation algorithm. The
files changed/added by this patch are:

README.686
match.S

The speedup that this patch provides varies, depending on whether the
compiler used to build the original version of zlib falls afoul of the
PPro's speed traps. My own tests show a speedup of around 10-20% at
the default compression level, and 20-30% using -9, against a version
compiled using gcc 2.7.2.3. Your mileage may vary.

Note that this code has been tailored for the PPro/PII in particular,
and will not perform particuarly well on a Pentium.

If you are using an assembler other than GNU as, you will have to
translate match.S to use your assembler's syntax. (Have fun.)

Brian Raiter
breadbox@muppetlabs.com
April, 1998


Added for zlib 1.1.3:

The patches come from
http://www.muppetlabs.com/~breadbox/software/assembly.html

To compile zlib with this asm file, copy match.S to the zlib directory
then do:

CFLAGS="-O3 -DASMV" ./configure
make OBJA=match.o


Update:

I've been ignoring these assembly routines for years, believing that
gcc's generated code had caught up with it sometime around gcc 2.95
and the major rearchitecting of the Pentium 4. However, I recently
learned that, despite what I believed, this code still has some life
in it. On the Pentium 4 and AMD64 chips, it continues to run about 8%
faster than the code produced by gcc 4.1.

In acknowledgement of its continuing usefulness, I've altered the
license to match that of the rest of zlib. Share and Enjoy!

Brian Raiter
breadbox@muppetlabs.com
April, 2007
Read blast.h for purpose and usage.

Mark Adler
madler@alumni.caltech.edu
See infback9.h for what this is and how to use it.
This directory contains files that have not been updated for zlib 1.2.x

(Volunteers are encouraged to help clean this up.  Thanks.)
This Makefile requires devkitARM (http://www.devkitpro.org/category/devkitarm/) and works inside "contrib/nds". It is based on a devkitARM template.

Eduardo Costa <eduardo.m.costa@gmail.com>
January 3, 2009

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.8 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://www.ietf.org/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate format)
and rfc1952.txt (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  Two compiled
examples are distributed in this package, example and minigzip.  The example_d
and minigzip_d flavors validate that the zlib1.dll file is working correctly.

Questions about zlib should be sent to <zlib@gzip.org>.  The zlib home page
is http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read DLL_FAQ.txt, and the the zlib FAQ http://zlib.net/zlib_faq.html
before asking for help.


Manifest:

The package zlib-1.2.8-win32-x86.zip will contain the following files:

  README-WIN32.txt This document
  ChangeLog        Changes since previous zlib packages
  DLL_FAQ.txt      Frequently asked questions about zlib1.dll
  zlib.3.pdf       Documentation of this library in Adobe Acrobat format

  example.exe      A statically-bound example (using zlib.lib, not the dll)
  example.pdb      Symbolic information for debugging example.exe

  example_d.exe    A zlib1.dll bound example (using zdll.lib)
  example_d.pdb    Symbolic information for debugging example_d.exe

  minigzip.exe     A statically-bound test program (using zlib.lib, not the dll)
  minigzip.pdb     Symbolic information for debugging minigzip.exe

  minigzip_d.exe   A zlib1.dll bound test program (using zdll.lib)
  minigzip_d.pdb   Symbolic information for debugging minigzip_d.exe

  zlib.h           Install these files into the compilers' INCLUDE path to
  zconf.h          compile programs which use zlib.lib or zdll.lib

  zdll.lib         Install these files into the compilers' LIB path if linking
  zdll.exp         a compiled program to the zlib1.dll binary

  zlib.lib         Install these files into the compilers' LIB path to link zlib
  zlib.pdb         into compiled programs, without zlib1.dll runtime dependency
                   (zlib.pdb provides debugging info to the compile time linker)

  zlib1.dll        Install this binary shared library into the system PATH, or
                   the program's runtime directory (where the .exe resides)
  zlib1.pdb        Install in the same directory as zlib1.dll, in order to debug
                   an application crash using WinDbg or similar tools.

All .pdb files above are entirely optional, but are very useful to a developer
attempting to diagnose program misbehavior or a crash.  Many additional
important files for developers can be found in the zlib127.zip source package
available from http://zlib.net/ - review that package's README file for details.


Acknowledgments:

The deflate format used by zlib was defined by Phil Katz.  The deflate and
zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
people who reported problems and suggested various improvements in zlib; they
are too numerous to cite here.


Copyright notice:

  (C) 1995-2012 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
Introduction
============

This is the Gnu Readline library, version 6.2.

The Readline library provides a set of functions for use by applications
that allow users to edit command lines as they are typed in.  Both
Emacs and vi editing modes are available.  The Readline library includes
additional functions to maintain a list of previously-entered command
lines, to recall and perhaps reedit those lines, and perform csh-like
history expansion on previous commands.

The history facilites are also placed into a separate library, the
History library, as part of the build process.  The History library
may be used without Readline in applications which desire its
capabilities.

The Readline library is free software, distributed under the terms of
the [GNU] General Public License as published by the Free Software
Foundation, version 3 of the License.  For more information, see the
file COPYING.

To build the library, try typing `./configure', then `make'.  The
configuration process is automated, so no further intervention should
be necessary.  Readline builds with `gcc' by default if it is
available.  If you want to use `cc' instead, type

        CC=cc ./configure

if you are using a Bourne-style shell.  If you are not, the following
may work:

        env CC=cc ./configure

Read the file INSTALL in this directory for more information about how
to customize and control the build process.

The file rlconf.h contains C preprocessor defines that enable and disable
certain Readline features.

The special make target `everything' will build the static and shared
libraries (if the target platform supports them) and the examples.

Examples
========

There are several example programs that use Readline features in the
examples directory.  The `rl' program is of particular interest.  It
is a command-line interface to Readline, suitable for use in shell
scripts in place of `read'.

Shared Libraries
================

There is skeletal support for building shared versions of the
Readline and History libraries.  The configure script creates
a Makefile in the `shlib' subdirectory, and typing `make shared'
will cause shared versions of the Readline and History libraries
to be built on supported platforms.

If `configure' is given the `--enable-shared' option, it will attempt
to build the shared libraries by default on supported platforms.

Configure calls the script support/shobj-conf to test whether or
not shared library creation is supported and to generate the values
of variables that are substituted into shlib/Makefile.  If you
try to build shared libraries on an unsupported platform, `make'
will display a message asking you to update support/shobj-conf for
your platform.

If you need to update support/shobj-conf, you will need to create
a `stanza' for your operating system and compiler.  The script uses
the value of host_os and ${CC} as determined by configure.  For
instance, FreeBSD 4.2 with any version of gcc is identified as
`freebsd4.2-gcc*'.

In the stanza for your operating system-compiler pair, you will need to
define several variables.  They are:

SHOBJ_CC	The C compiler used to compile source files into shareable
		object files.  This is normally set to the value of ${CC}
		by configure, and should not need to be changed.

SHOBJ_CFLAGS	Flags to pass to the C compiler ($SHOBJ_CC) to create
		position-independent code.  If you are using gcc, this
		should probably be set to `-fpic'.

SHOBJ_LD	The link editor to be used to create the shared library from
		the object files created by $SHOBJ_CC.  If you are using
		gcc, a value of `gcc' will probably work.

SHOBJ_LDFLAGS	Flags to pass to SHOBJ_LD to enable shared object creation.
		If you are using gcc, `-shared' may be all that is necessary.
		These should be the flags needed for generic shared object
		creation.

SHLIB_XLDFLAGS	Additional flags to pass to SHOBJ_LD for shared library
		creation.  Many systems use the -R option to the link
		editor to embed a path within the library for run-time
		library searches.  A reasonable value for such systems would
		be `-R$(libdir)'.

SHLIB_LIBS	Any additional libraries that shared libraries should be
		linked against when they are created.

SHLIB_LIBPREF	The prefix to use when generating the filename of the shared
		library.  The default is `lib'; Cygwin uses `cyg'.

SHLIB_LIBSUFF	The suffix to add to `libreadline' and `libhistory' when
		generating the filename of the shared library.  Many systems
		use `so'; HP-UX uses `sl'.

SHLIB_LIBVERSION The string to append to the filename to indicate the version
		of the shared library.  It should begin with $(SHLIB_LIBSUFF),
		and possibly include version information that allows the
		run-time loader to load the version of the shared library
		appropriate for a particular program.  Systems using shared
		libraries similar to SunOS 4.x use major and minor library
		version numbers; for those systems a value of
		`$(SHLIB_LIBSUFF).$(SHLIB_MAJOR)$(SHLIB_MINOR)' is appropriate.
		Systems based on System V Release 4 don't use minor version
		numbers; use `$(SHLIB_LIBSUFF).$(SHLIB_MAJOR)' on those systems.
		Other Unix versions use different schemes.

SHLIB_DLLVERSION The version number for shared libraries that determines API
		compatibility between readline versions and the underlying
		system.  Used only on Cygwin.  Defaults to $SHLIB_MAJOR, but
		can be overridden at configuration time by defining DLLVERSION
		in the environment.

SHLIB_DOT	The character used to separate the name of the shared library
		from the suffix and version information.  The default is `.';
		systems like Cygwin which don't separate version information
		from the library name should set this to the empty string.

SHLIB_STATUS	Set this to `supported' when you have defined the other
		necessary variables.  Make uses this to determine whether
		or not shared library creation should be attempted.

You should look at the existing stanzas in support/shobj-conf for ideas.

Once you have updated support/shobj-conf, re-run configure and type
`make shared'.  The shared libraries will be created in the shlib
subdirectory.

If shared libraries are created, `make install' will install them. 
You may install only the shared libraries by running `make
install-shared' from the top-level build directory.  Running `make
install' in the shlib subdirectory will also work.  If you don't want
to install any created shared libraries, run `make install-static'. 

Documentation
=============

The documentation for the Readline and History libraries appears in
the `doc' subdirectory.  There are three texinfo files and a
Unix-style manual page describing the facilities available in the
Readline library.  The texinfo files include both user and
programmer's manuals.  HTML versions of the manuals appear in the
`doc' subdirectory as well. 

Reporting Bugs
==============

Bug reports for Readline should be sent to:

        bug-readline@gnu.org

When reporting a bug, please include the following information:

        * the version number and release status of Readline (e.g., 4.2-release)
        * the machine and OS that it is running on
        * a list of the compilation flags or the contents of `config.h', if
          appropriate
        * a description of the bug
        * a recipe for recreating the bug reliably
        * a fix for the bug if you have one!

If you would like to contact the Readline maintainer directly, send mail
to bash-maintainers@gnu.org.

Since Readline is developed along with bash, the bug-bash@gnu.org mailing
list (mirrored to the Usenet newsgroup gnu.bash.bug) often contains
Readline bug reports and fixes. 

Chet Ramey
chet.ramey@case.edu
rlfe (ReadLine Front-End) is a "universal wrapper" around readline.
You specify an interactive program to run (typically a shell), and
readline is used to edit input lines.

There are other such front-ends; what distinguishes this one is that
it monitors the state of the inferior pty, and if the inferior program
switches its terminal to raw mode, then rlfe passes your characters
through directly.  This basically means you can run your entire
session (including bash and terminal-mode emacs) under rlfe.

FEATURES

* Can use all readline commands (and history) in commands that
read input lines in "canonical mode" - even 'cat'!

* Automatically switches between "readline-editing mode" and "raw mode"
depending on the terminal mode.  If the inferior program invokes
readline itself, it will do its own line editing.  (The inferior
readline will not know about rlfe, and it will have its own history.)
You can even run programs like 'emavs -nw' and 'vi' under rlfe.
The goal is you could leave rlfe always on without even knowing
about it.  (We're not quite there, but it works tolerably well.)

* The input line (after any prompt) is changed to bold-face.

INSTALL

The usual: ./configure && make && make install

Note so far rlfe has only been tested on GNU Linux (Fedora Core 2)
and Mac OS X (10.3).

This assumes readline header files and libraries are in the default
places.  If not, you can create a link named readline pointing to the
readline sources.  To link with libreadline.a and libhistory.a
you can copy or link them, or add LDFLAGS='-/path/to/readline' to
the make command-line.

USAGE

Just run it.  That by default runs bash.  You can run some other
command by giving it as command-line arguments.

There are a few tweaks:  -h allows you to name the history file,
and -s allows you to specify its size.  It default to "emacs" mode,
but if the the environment variable EDITOR is set to "vi" that
mode is chosen.

ISSUES

* The mode switching depends on the terminal mode set by the inferior
program.  Thus ssh/telnet/screen-type programs will typically be in
raw mode, so rlfe won't be much use, even if remote programs run in
canonical mode.  The work-around is to run rlfe on the remote end.

* Echo supression and prompt recognition are somewhat fragile.
(A protocol so that the o/s tty code can reliably communicate its
state to rlfe could solve this problem, and the previous one.)

* See the intro to rlfe.c for more notes.

* Assumes a VT100-compatible terminal, though that could be generalized
if anybody cares.

* Requires ncurses.

* It would be useful to integrate rlfe's logic in a terminal emulator.
That would make it easier to reposition the edit position with a mouse,
integrate cut-and-paste with the system clipboard, and more robustly
handle escape sequence and multi-byte characters more robustly.

AUTHOR

Per Bothner <per@bothner.com>

LICENSE

GPL.

                Notes on enabling maintainer mode

Note that if you configure with --enable-maintainer-mode, you will need
special versions of automake, autoconf, libtool and gettext. You will
find the sources for these in the respective upstream directories:

  ftp://ftp.gnu.org/gnu/autoconf
  ftp://ftp.gnu.org/gnu/automake
  ftp://ftp.gnu.org/gnu/libtool
  ftp://ftp.gnu.org/gnu/gettext

The required versions of the tools for this tree are
  autoconf 2.64
  automake 1.11
  libtool 2.2.6
  gettext 0.14.5

Note - "make distclean" does not work with maintainer mode enabled.
The Makefiles in the some of the po/ subdirectories depend upon the
Makefiles in their parent directories, and distclean will delete the
Makefiles in the parent directories before running the Makefiles in
the child directories.  There is no easy way around this (short of
changing the automake macros) as these dependencies need to exist in
order to correctly build the NLS files.
		   README for GNU development tools

This directory contains various GNU compilers, assemblers, linkers, 
debuggers, etc., plus their support routines, definitions, and documentation.

If you are receiving this as part of a GDB release, see the file gdb/README.
If with a binutils release, see binutils/README;  if with a libg++ release,
see libg++/README, etc.  That'll give you info about this
package -- supported targets, how to use it, how to report bugs, etc.

It is now possible to automatically configure and build a variety of
tools with one command.  To build all of the tools contained herein,
run the ``configure'' script here, e.g.:

	./configure 
	make

To install them (by default in /usr/local/bin, /usr/local/lib, etc),
then do:
	make install

(If the configure script can't determine your type of computer, give it
the name as an argument, for instance ``./configure sun4''.  You can
use the script ``config.sub'' to test whether a name is recognized; if
it is, config.sub translates it to a triplet specifying CPU, vendor,
and OS.)

If you have more than one compiler on your system, it is often best to
explicitly set CC in the environment before running configure, and to
also set CC when running make.  For example (assuming sh/bash/ksh):

	CC=gcc ./configure
	make

A similar example using csh:

	setenv CC gcc
	./configure
	make

Much of the code and documentation enclosed is copyright by
the Free Software Foundation, Inc.  See the file COPYING or
COPYING.LIB in the various directories, for a description of the
GNU General Public License terms under which you can copy the files.

REPORTING BUGS: Again, see gdb/README, binutils/README, etc., for info
on where and how to report problems.
		     README for GDB release

This is GDB, the GNU source-level debugger.

A summary of new features is in the file `gdb/NEWS'.

Check the GDB home page at http://www.gnu.org/software/gdb/ for up to
date release information, mailing list links and archives, etc.

The file `gdb/PROBLEMS' contains information on problems identified
late in the release cycle.  GDB's bug tracking data base at
http://www.gnu.org/software/gdb/bugs/ contains a more complete list of
bugs.


Unpacking and Installation -- quick overview
==========================

   The release is provided as a gzipped tar file called
'gdb-VERSION.tar.gz', where VERSION is the version of GDB.

   The GDB debugger sources, the generic GNU include
files, the BFD ("binary file description") library, the readline
library, and other libraries all have directories of their own
underneath the gdb-VERSION directory.  The idea is that a variety of GNU
tools can share a common copy of these things.  Be aware of variation
over time--for example don't try to build GDB with a copy of bfd from
a release other than the GDB release (such as a binutils release),
especially if the releases are more than a few weeks apart.
Configuration scripts and makefiles exist to cruise up and down this
directory tree and automatically build all the pieces in the right
order.

   When you unpack the gdb-VERSION.tar.gz file, it will create a
source directory called `gdb-VERSION'.

You can build GDB right in the source directory:

      cd gdb-VERSION
      ./configure
      make
      cp gdb/gdb /usr/local/bin/gdb	(or wherever you want)

However, we recommend that an empty directory be used instead.
This way you do not clutter your source tree with binary files
and will be able to create different builds with different 
configuration options.

You can build GDB in any empty build directory:

      mkdir build
      cd build
      <full path to your sources>/gdb-VERSION/configure
      make
      cp gdb/gdb /usr/local/bin/gdb	(or wherever you want)

(Building GDB with DJGPP tools for MS-DOS/MS-Windows is slightly
different; see the file gdb-VERSION/gdb/config/djgpp/README for details.)

   This will configure and build all the libraries as well as GDB.  If
`configure' can't determine your system type, specify one as its
argument, e.g., `./configure sun4' or `./configure decstation'.

   Make sure that your 'configure' line ends in 'gdb-VERSION/configure':

      /berman/migchain/source/gdb-VERSION/configure      # RIGHT
      /berman/migchain/source/gdb-VERSION/gdb/configure  # WRONG

   The GDB package contains several subdirectories, such as 'gdb',
'bfd', and 'readline'.  If your 'configure' line ends in
'gdb-VERSION/gdb/configure', then you are configuring only the gdb
subdirectory, not the whole GDB package.  This leads to build errors
such as:

      make: *** No rule to make target `../bfd/bfd.h', needed by `gdb.o'.  Stop.

   If you get other compiler errors during this stage, see the `Reporting
Bugs' section below; there are a few known problems.

   GDB requires an ISO C (ANSI C) compiler.  If you do not have an ISO
C compiler for your system, you may be able to download and install
the GNU CC compiler.  It is available via anonymous FTP from the
directory `ftp://ftp.gnu.org/pub/gnu/gcc'.  GDB also requires an ISO
C standard library.  The GDB remote server, GDBserver, builds with some
non-ISO standard libraries - e.g. for Windows CE.

   GDB uses Expat, an XML parsing library, to implement some target-specific
features.  Expat will be linked in if it is available at build time, or
those features will be disabled.  The latest version of Expat should be
available from `http://expat.sourceforge.net'.

   GDB can be used as a cross-debugger, running on a machine of one
type while debugging a program running on a machine of another type.
See below.


More Documentation
******************

   All the documentation for GDB comes as part of the machine-readable
distribution.  The documentation is written in Texinfo format, which
is a documentation system that uses a single source file to produce
both on-line information and a printed manual.  You can use one of the
Info formatting commands to create the on-line version of the
documentation and TeX (or `texi2roff') to typeset the printed version.

   GDB includes an already formatted copy of the on-line Info version
of this manual in the `gdb/doc' subdirectory.  The main Info file is
`gdb-VERSION/gdb/doc/gdb.info', and it refers to subordinate files
matching `gdb.info*' in the same directory.  If necessary, you can
print out these files, or read them with any editor; but they are
easier to read using the `info' subsystem in GNU Emacs or the
standalone `info' program, available as part of the GNU Texinfo
distribution.

   If you want to format these Info files yourself, you need one of the
Info formatting programs, such as `texinfo-format-buffer' or
`makeinfo'.

   If you have `makeinfo' installed, and are in the top level GDB
source directory (`gdb-VERSION'), you can make the Info file by
typing:

      cd gdb/doc
      make info

   If you want to typeset and print copies of this manual, you need
TeX, a program to print its DVI output files, and `texinfo.tex', the
Texinfo definitions file.  This file is included in the GDB
distribution, in the directory `gdb-VERSION/texinfo'.

   TeX is a typesetting program; it does not print files directly, but
produces output files called DVI files.  To print a typeset document,
you need a program to print DVI files.  If your system has TeX
installed, chances are it has such a program.  The precise command to
use depends on your system; `lpr -d' is common; another (for PostScript
devices) is `dvips'.  The DVI print command may require a file name
without any extension or a `.dvi' extension.

   TeX also requires a macro definitions file called `texinfo.tex'. 
This file tells TeX how to typeset a document written in Texinfo
format.  On its own, TeX cannot read, much less typeset a Texinfo file.
 `texinfo.tex' is distributed with GDB and is located in the
`gdb-VERSION/texinfo' directory.

   If you have TeX and a DVI printer program installed, you can typeset
and print this manual.  First switch to the `gdb' subdirectory of
the main source directory (for example, to `gdb-VERSION/gdb') and then type:

      make doc/gdb.dvi

   If you prefer to have the manual in PDF format, type this from the
`gdb/doc' subdirectory of the main source directory:

      make gdb.pdf

For this to work, you will need the PDFTeX package to be installed.


Installing GDB
**************

   GDB comes with a `configure' script that automates the process of
preparing GDB for installation; you can then use `make' to build the
`gdb' program.

   The GDB distribution includes all the source code you need for GDB in
a single directory.  That directory contains:

`gdb-VERSION/{COPYING,COPYING.LIB}'
     Standard GNU license files.  Please read them.

`gdb-VERSION/bfd'
     source for the Binary File Descriptor library

`gdb-VERSION/config*'
     script for configuring GDB, along with other support files

`gdb-VERSION/gdb'
     the source specific to GDB itself

`gdb-VERSION/include'
     GNU include files

`gdb-VERSION/libiberty'
     source for the `-liberty' free software library

`gdb-VERSION/opcodes'
     source for the library of opcode tables and disassemblers

`gdb-VERSION/readline'
     source for the GNU command-line interface
     NOTE:  The readline library is compiled for use by GDB, but will
     not be installed on your system when "make install" is issued.

`gdb-VERSION/sim'
     source for some simulators (ARM, D10V, SPARC, M32R, MIPS, PPC, V850, etc)

`gdb-VERSION/texinfo'
     The `texinfo.tex' file, which you need in order to make a printed
     manual using TeX.

`gdb-VERSION/etc'
     Coding standards, useful files for editing GDB, and other
     miscellanea.

   Note: the following instructions are for building GDB on Unix or
Unix-like systems.  Instructions for building with DJGPP for
MS-DOS/MS-Windows are in the file gdb/config/djgpp/README.

   The simplest way to configure and build GDB is to run `configure'
from the `gdb-VERSION' directory.

   First switch to the `gdb-VERSION' source directory if you are
not already in it; then run `configure'.

   For example:

      cd gdb-VERSION
      ./configure
      make

   Running `configure' followed by `make' builds the `bfd',
`readline', `mmalloc', and `libiberty' libraries, then `gdb' itself.
The configured source files, and the binaries, are left in the
corresponding source directories.

   `configure' is a Bourne-shell (`/bin/sh') script; if your system
does not recognize this automatically when you run a different shell,
you may need to run `sh' on it explicitly:

      sh configure

   If you run `configure' from a directory that contains source
directories for multiple libraries or programs, `configure' creates
configuration files for every directory level underneath (unless
you tell it not to, with the `--norecursion' option).

   You can install `gdb' anywhere; it has no hardwired paths. However,
you should make sure that the shell on your path (named by the `SHELL'
environment variable) is publicly readable.  Remember that GDB uses the
shell to start your program--some systems refuse to let GDB debug child
processes whose programs are not readable.


Compiling GDB in another directory
==================================

   If you want to run GDB versions for several host or target machines,
you need a different `gdb' compiled for each combination of host and
target.  `configure' is designed to make this easy by allowing you to
generate each configuration in a separate subdirectory, rather than in
the source directory.  If your `make' program handles the `VPATH'
feature correctly (GNU `make' and SunOS 'make' are two that should),
running `make' in each of these directories builds the `gdb' program
specified there.

   To build `gdb' in a separate directory, run `configure' with the
`--srcdir' option to specify where to find the source. (You also need
to specify a path to find `configure' itself from your working
directory.  If the path to `configure' would be the same as the
argument to `--srcdir', you can leave out the `--srcdir' option; it
will be assumed.)

   For example, you can build GDB in a separate
directory for a Sun 4 like this:

     cd gdb-VERSION
     mkdir ../gdb-sun4
     cd ../gdb-sun4
     ../gdb-VERSION/configure
     make

   When `configure' builds a configuration using a remote source
directory, it creates a tree for the binaries with the same structure
(and using the same names) as the tree under the source directory.  In
the example, you'd find the Sun 4 library `libiberty.a' in the
directory `gdb-sun4/libiberty', and GDB itself in `gdb-sun4/gdb'.

   One popular reason to build several GDB configurations in separate
directories is to configure GDB for cross-compiling (where GDB runs on
one machine--the host--while debugging programs that run on another
machine--the target).  You specify a cross-debugging target by giving
the `--target=TARGET' option to `configure'.

   When you run `make' to build a program or library, you must run it
in a configured directory--whatever directory you were in when you
called `configure' (or one of its subdirectories).

   The `Makefile' that `configure' generates in each source directory
also runs recursively.  If you type `make' in a source directory such
as `gdb-VERSION' (or in a separate configured directory configured with
`--srcdir=PATH/gdb-VERSION'), you will build all the required libraries,
and then build GDB.

   When you have multiple hosts or targets configured in separate
directories, you can run `make' on them in parallel (for example, if
they are NFS-mounted on each of the hosts); they will not interfere
with each other.


Specifying names for hosts and targets
======================================

   The specifications used for hosts and targets in the `configure'
script are based on a three-part naming scheme, but some short
predefined aliases are also supported.  The full naming scheme encodes
three pieces of information in the following pattern:

     ARCHITECTURE-VENDOR-OS

   For example, you can use the alias `sun4' as a HOST argument or in a
`--target=TARGET' option.  The equivalent full name is
`sparc-sun-sunos4'.

   The `configure' script accompanying GDB does not provide any query
facility to list all supported host and target names or aliases. 
`configure' calls the Bourne shell script `config.sub' to map
abbreviations to full names; you can read the script, if you wish, or
you can use it to test your guesses on abbreviations--for example:

     % sh config.sub sun4
     sparc-sun-sunos4.1.1
     % sh config.sub sun3
     m68k-sun-sunos4.1.1
     % sh config.sub decstation
     mips-dec-ultrix4.2
     % sh config.sub hp300bsd
     m68k-hp-bsd
     % sh config.sub i386v
     i386-pc-sysv
     % sh config.sub i786v
     Invalid configuration `i786v': machine `i786v' not recognized

`config.sub' is also distributed in the GDB source directory.


`configure' options
===================

   Here is a summary of the `configure' options and arguments that are
most often useful for building GDB.  `configure' also has several other
options not listed here.  *note : (configure.info)What Configure Does,
for a full explanation of `configure'.

     configure [--help]
               [--prefix=DIR]
               [--srcdir=PATH]
               [--norecursion] [--rm]
	       [--enable-build-warnings]
               [--target=TARGET]
	       [--host=HOST]
	       [HOST]

You may introduce options with a single `-' rather than `--' if you
prefer; but you may abbreviate option names if you use `--'.

`--help'
     Display a quick summary of how to invoke `configure'.

`-prefix=DIR'
     Configure the source to install programs and files under directory
     `DIR'.

`--srcdir=PATH'
     *Warning: using this option requires GNU `make', or another `make'
     that compatibly implements the `VPATH' feature.*
     Use this option to make configurations in directories separate
     from the GDB source directories.  Among other things, you can use
     this to build (or maintain) several configurations simultaneously,
     in separate directories.  `configure' writes configuration
     specific files in the current directory, but arranges for them to
     use the source in the directory PATH.  `configure' will create
     directories under the working directory in parallel to the source
     directories below PATH.

`--host=HOST'
     Configure GDB to run on the specified HOST.

     There is no convenient way to generate a list of all available
     hosts.

`HOST ...'
     Same as `--host=HOST'.  If you omit this, GDB will guess; it's
     quite accurate.

`--norecursion'
     Configure only the directory level where `configure' is executed;
     do not propagate configuration to subdirectories.

`--rm'
     Remove the configuration that the other arguments specify.

`--enable-build-warnings'
     When building the GDB sources, ask the compiler to warn about any
     code which looks even vaguely suspicious.  You should only using
     this feature if you're compiling with GNU CC.  It passes the
     following flags:
	-Wimplicit
	-Wreturn-type
	-Wcomment
	-Wtrigraphs
	-Wformat
	-Wparentheses
	-Wpointer-arith

`--enable-werror'
     Treat compiler warnings as werrors.  Use this only with GCC.  It
     adds the -Werror flag to the compiler, which will fail the
     compilation if the compiler outputs any warning messages.

`--target=TARGET'
     Configure GDB for cross-debugging programs running on the specified
     TARGET.  Without this option, GDB is configured to debug programs
     that run on the same machine (HOST) as GDB itself.

     There is no convenient way to generate a list of all available
     targets.

`--with-gdb-datadir=PATH'
     Set the GDB-specific data directory.  GDB will look here for
     certain supporting files or scripts.  This defaults to the `gdb'
     subdirectory of `datadir' (which can be set using `--datadir').

`--with-relocated-sources=DIR'
     Sets up the default source path substitution rule so that
     directory names recorded in debug information will be
     automatically adjusted for any directory under DIR.  DIR should
     be a subdirectory of GDB's configured prefix, the one mentioned
     in the `--prefix' or `--exec-prefix' options to configure.  This
     option is useful if GDB is supposed to be moved to a different
     place after it is built.

`--enable-64-bit-bfd'
     Enable 64-bit support in BFD on 32-bit hosts.

`--disable-gdbmi'
     Build GDB without the GDB/MI machine interface.

`--enable-tui'
     Build GDB with the text-mode full-screen user interface (TUI).
     Requires a curses library (ncurses and cursesX are also
     supported).

`--enable-gdbtk'
     Build GDB with the gdbtk GUI interface.  Requires TCL/Tk to be
     installed.

`--with-libunwind-ia64'
     Use the libunwind library for unwinding function call stack on ia64
     target platforms.
     See http://www.nongnu.org/libunwind/index.html for details.

`--with-curses'
     Use the curses library instead of the termcap library, for
     text-mode terminal operations.

`--enable-profiling' Enable profiling of GDB itself.  Necessary if you
     want to use the "maint set profile" command for profiling GDB.
     Requires the functions `monstartup' and `_mcleanup' to be present
     in the standard C library used to build GDB, and also requires a
     compiler that supports the `-pg' option.

`--with-system-readline'
     Use the readline library installed on the host, rather than the
     library supplied as part of GDB tarball.

`--with-expat'
     Build GDB with the libexpat library.  (Done by default if
     libexpat is installed and found at configure time.)  This library
     is used to read XML files supplied with GDB.  If it is
     unavailable, some features, such as remote protocol memory maps,
     target descriptions, and shared library lists, that are based on
     XML files, will not be available in GDB.  If your host does not
     have libexpat installed, you can  get the latest version from
     http://expat.sourceforge.net.

`--with-python[=PATH]'
     Build GDB with Python scripting support.  (Done by default if
     libpython is present and found at configure time.)  Python makes
     GDB scripting much more powerful than the restricted CLI
     scripting language.  If your host does not have Python installed,
     you can find it on http://www.python.org/download/.  The oldest
     version of Python supported by GDB is 2.4.  The optional argument
     PATH says where to find the Python headers and libraries; the
     configure script will look in PATH/include for headers and in
     PATH/lib for the libraries.

`--without-included-regex'
     Don't use the regex library included with GDB (as part of the
     libiberty library).  This is the default on hosts with version 2
     of the GNU C library.

`--with-sysroot=DIR'
     Use DIR as the default system root directory for libraries whose
     file names begin with `/lib' or `/usr/lib'.  (The value of DIR
     can be modified at run time by using the "set sysroot" command.)
     If DIR is under the GDB configured prefix (set with `--prefix' or
     `--exec-prefix' options), the default system root will be
     automatically adjusted if and when GDB is moved to a different
     location.

`--with-system-gdbinit=FILE'
     Configure GDB to automatically load a system-wide init file.
     FILE should be an absolute file name.  If FILE is in a directory
     under the configured prefix, and GDB is moved to another location
     after being built, the location of the system-wide init file will
     be adjusted accordingly. 

`configure' accepts other options, for compatibility with configuring
other GNU tools recursively; but these are the only options that affect
GDB or its supporting libraries.


Remote debugging
=================

   The files m68k-stub.c, i386-stub.c, and sparc-stub.c are examples
of remote stubs to be used with remote.c.  They are designed to run
standalone on an m68k, i386, or SPARC cpu and communicate properly
with the remote.c stub over a serial line.

   The directory gdb/gdbserver/ contains `gdbserver', a program that
allows remote debugging for Unix applications.  GDBserver is only
supported for some native configurations, including Sun 3, Sun 4, and
Linux.
The file gdb/gdbserver/README includes further notes on GDBserver; in
particular, it explains how to build GDBserver for cross-debugging
(where GDBserver runs on the target machine, which is of a different
architecture than the host machine running GDB).

   There are a number of remote interfaces for talking to existing ROM
monitors and other hardware:

	remote-mips.c	 MIPS remote debugging protocol
	remote-sds.c	 PowerPC SDS monitor
	remote-sim.c	 Generalized simulator protocol


Reporting Bugs in GDB
=====================

   There are several ways of reporting bugs in GDB.  The prefered
method is to use the World Wide Web:

      http://www.gnu.org/software/gdb/bugs/

As an alternative, the bug report can be submitted, via e-mail, to the
address "bug-gdb@gnu.org".

   When submitting a bug, please include the GDB version number, and
how you configured it (e.g., "sun4" or "mach386 host,
i586-intel-synopsys target").  Since GDB now supports so many
different configurations, it is important that you be precise about
this.  If at all possible, you should include the actual banner
that GDB prints when it starts up, or failing that, the actual
configure command that you used when configuring GDB.

   For more information on how/whether to report bugs, see the
Reporting Bugs chapter of the GDB manual (gdb/doc/gdb.texinfo).


Graphical interface to GDB -- X Windows, MS Windows
==========================

   Several graphical interfaces to GDB are available.  You should
check:

	http://www.gnu.org/software/gdb/links/

for an up-to-date list.

   Emacs users will very likely enjoy the Grand Unified Debugger mode;
try typing `M-x gdb RET'.


Writing Code for GDB
=====================

   There is information about writing code for GDB in the file
`CONTRIBUTE' and at the website:

	http://www.gnu.org/software/gdb/

in particular in the wiki.

   If you are pondering writing anything but a short patch, especially
take note of the information about copyrights and copyright assignment.
It can take quite a while to get all the paperwork done, so
we encourage you to start that process as soon as you decide you are
planning to work on something, or at least well ahead of when you
think you will be ready to submit the patches.


GDB Testsuite
=============

   Included with the GDB distribution is a DejaGNU based testsuite
that can either be used to test your newly built GDB, or for
regression testing a GDB with local modifications.

   Running the testsuite requires the prior installation of DejaGNU,
which is generally available via ftp.  The directory
ftp://sources.redhat.com/pub/dejagnu/ will contain a recent snapshot.
Once DejaGNU is installed, you can run the tests in one of the
following ways:

  (1)	cd gdb-VERSION
	make check-gdb

or

  (2)	cd gdb-VERSION/gdb
	make check

or

  (3)	cd gdb-VERSION/gdb/testsuite
	make site.exp	(builds the site specific file)
	runtest -tool gdb GDB=../gdb    (or GDB=<somepath> as appropriate)

When using a `make'-based method, you can use the Makefile variable
`RUNTESTFLAGS' to pass flags to `runtest', e.g.:

	make RUNTESTFLAGS=--directory=gdb.cp check

If you use GNU make, you can use its `-j' option to run the testsuite
in parallel.  This can greatly reduce the amount of time it takes for
the testsuite to run.  In this case, if you set `RUNTESTFLAGS' then,
by default, the tests will be run serially even under `-j'.  You can
override this and force a parallel run by setting the `make' variable
`FORCE_PARALLEL' to any non-empty value.  Note that the parallel `make
check' assumes that you want to run the entire testsuite, so it is not
compatible with some dejagnu options, like `--directory'.

The last method gives you slightly more control in case of problems
with building one or more test executables or if you are using the
testsuite `standalone', without it being part of the GDB source tree.

See the DejaGNU documentation for further details.


Copyright and License Notices
=============================

Most files maintained by the GDB Project contain a copyright notice
as well as a license notice, usually at the start of the file.

To reduce the length of copyright notices, consecutive years in the
copyright notice can be combined into a single range.  For instance,
the following list of copyright years...

    1986, 1988, 1989, 1991-1993, 1999, 2000, 2007, 2008, 2009, 2010, 2011

... is abbreviated into:

    1986, 1988-1989, 1991-1993, 1999-2000, 2007-2011

Every year of each range, inclusive, is a copyrightable year that
could be listed individually.


(this is for editing this file with GNU emacs)
Local Variables:
mode: text
End:
This is a collection of tests for GDB.

The file gdb/README contains basic instructions on how to run the
testsuite, while this file documents additional options and controls
that are available.  The GDB wiki may also have some pages with ideas
and suggestions.


Running the Testsuite
*********************

There are two ways to run the testsuite and pass additional parameters
to DejaGnu.  The first is to do `make check' in the main build
directory and specifying the makefile variable `RUNTESTFLAGS':

	 make check RUNTESTFLAGS='TRANSCRIPT=y gdb.base/a2-run.exp'

The second is to cd to the testsuite directory and invoke the DejaGnu
`runtest' command directly.

	cd testsuite
	make site.exp
	runtest TRANSCRIPT=y

(The `site.exp' file contains a handful of useful variables like host
and target triplets, and pathnames.)

Running the Performance Tests
*****************************

GDB Testsuite includes performance test cases, which are not run together
with other test cases, because performance test cases are slow and need
a quiet system.  There are two ways to run the performance test cases.
The first is to do `make check-perf' in the main build directory:

	make check-perf RUNTESTFLAGS="solib.exp SOLIB_COUNT=8"

The second is to cd to the testsuite directory and invoke the DejaGnu
`runtest' command directly.

	cd testsuite
	make site.exp
	runtest GDB_PERFTEST_MODE=both GDB_PERFTEST_TIMEOUT=4000 --directory=gdb.perf solib.exp SOLIB_COUNT=8

Only "compile", "run" and "both" are valid to GDB_PERFTEST_MODE.  They
stand for "compile tests only", "run tests only", and "compile and run
tests" respectively.  "both" is the default.  GDB_PERFTEST_TIMEOUT
specify the timeout, which is 3000 in default.  The result of
performance test is appended in `testsuite/perftest.log'.

Testsuite Parameters
********************

The following parameters are DejaGNU variables that you can set to
affect the testsuite run globally.

TRANSCRIPT

You may find it useful to have a transcript of the commands that the
testsuite sends to GDB, for instance if GDB crashes during the run,
and you want to reconstruct the sequence of commands.

If the DejaGNU variable TRANSCRIPT is set (to any value), each
invocation of GDB during the test run will get a transcript file
written into the DejaGNU output directory.  The file will have the
name transcript.<n>, where <n> is an integer.  The first line of the
file shows the invocation command with all the options passed to it,
while subsequent lines are the GDB commands.  A `make check' might
look like this:

      make check RUNTESTFLAGS=TRANSCRIPT=y

The transcript may not be complete, as for instance tests of command
completion may show only partial command lines.

GDB

By default, the testsuite exercises the GDB in the build directory,
but you can set GDB to be a pathname to a different version.  For
instance,

    make check RUNTESTFLAGS=GDB=/usr/bin/gdb

runs the testsuite on the GDB in /usr/bin.

GDBSERVER

You can set GDBSERVER to be a particular GDBserver of interest, so for
instance

    make check RUNTESTFLAGS="GDB=/usr/bin/gdb GDBSERVER=/usr/bin/gdbserver"

checks both the installed GDB and GDBserver.

INTERNAL_GDBFLAGS

Command line options passed to all GDB invocations.

The default is "-nw -nx".

`-nw' disables any of the windowed interfaces.
`-nx' disables ~/.gdbinit, so that it doesn't interfere with
the tests.

This is actually considered an internal variable, and you
won't normally want to change it.  However, in some situations,
this may be tweaked as a last resort if the testsuite doesn't
have direct support for the specifics of your environment.
The testsuite does not override a value provided by the user.

As an example, when testing an installed GDB that has been
configured with `--with-system-gdbinit', like by default,
you do not want ~/.gdbinit to interfere with tests, but, you
may want the system .gdbinit file loaded.  As there's no way to
ask the testsuite, or GDB, to load the system gdbinit but
not ~/.gdbinit, a workaround is then to remove `-nx' from
INTERNAL_GDBFLAGS, and point $HOME at a directory without
a .gdbinit.  For example:

	cd testsuite
	HOME=`pwd` runtest \
	  GDB=/usr/bin/gdb \
	  GDBSERVER=/usr/bin/gdbserver \
	  INTERNAL_GDBFLAGS=-nw

GDB_PARALLEL

When testing natively (that is, not with a remote host), you can run
the GDB test suite in a fully parallel mode.  In this mode, each .exp
file runs separately and maybe simultaneously.  The test suite will
ensure that all the temporary files created by the test suite do not
clash, by putting them into separate directories.  This mode is
primarily intended for use by the Makefile.

To use this mode, set the GDB_PARALLEL on the runtest command line.
Before starting the tests, you must ensure that the directories cache,
outputs, and temp in the test suite build directory are either empty
or have been deleted.  cache in particular is used to share data
across invocations of runtest, and files there may affect the test
results.  Note that the Makefile automatically does these deletions.

GDB_INOTIFY

For debugging parallel mode, it is handy to be able to see when a test
case writes to a file outside of its designated output directory.

If you have the inotify-tools package installed, you can set the
GDB_INOTIFY variable on the runtest command line.  This will cause the
test suite to watch for parallel-unsafe file creations and report
them, both to stdout and in the test suite log file.

This setting is only meaningful in conjunction with GDB_PARALLEL.

TESTS

This variable is used to specify which set of tests to run.
It is passed to make (not runtest) and its contents are a space separated
list of tests to run.

If using GNU make then the contents are wildcard-expanded using
GNU make's $(wildcard) function.  Test paths must be fully specified,
relative to the "testsuite" subdirectory.  This allows one to run all
tests in a subdirectory by passing "gdb.subdir/*.exp", or more simply
by using the check-gdb.subdir target in the Makefile.

If for some strange reason one wanted to run all tests that begin with
the letter "d" that is also possible: TESTS="*/d*.exp".

Do not write */*.exp to specify all tests (assuming all tests are only
nested one level deep, which is not necessarily true).  This will pick up
.exp files in ancillary directories like "lib" and "config".
Instead write gdb.*/*.exp.

Example:

	make -j10 check TESTS="gdb.server/[s-w]*.exp */x*.exp"

If not using GNU make then the value is passed directly to runtest.
If not specified, all tests are run.

READ1

This make (not runtest) variable is used to specify whether the
testsuite preloads the read1.so library into expect.  Any non-empty
value means true.  See "Race detection" below.

Race detection
**************

The testsuite includes a mechanism that helps detect test races.

For example, say the program running under expect outputs "abcd", and
a test does something like this:

  expect {
    "a.*c" {
    }
    "b" {
    }
    "a" {
    }
  }

Which case happens to match depends on what expect manages to read
into its internal buffer in one go.  If it manages to read three bytes
or more, then the first case matches.  If it manages to read two
bytes, then the second case matches.  If it manages to read only one
byte, then the third case matches.

To help detect these cases, the race detection mechanism preloads a
library into expect that forces the `read' system call to always
return at most 1 byte.

To enable this, either pass a non-empty value in the READ1 make
variable, or use the check-read1 make target instead of check.

Examples:

	make -j10 check-read1 TESTS="*/paginate-*.exp"
	make -j10 check READ1="1"

Testsuite Configuration
***********************

It is possible to adjust the behavior of the testsuite by defining
the global variables listed below, either in a `site.exp' file,
or in a board file.

gdb_test_timeout

Defining this variable changes the default timeout duration used
during communication with GDB.  More specifically, the global variable
used during testing is `timeout', but this variable gets reset to
`gdb_test_timeout' at the beginning of each testcase, which ensures
that any local change to `timeout' in a testcase does not affect
subsequent testcases.

This global variable comes in handy when the debugger is slower than
normal due to the testing environment, triggering unexpected `TIMEOUT'
test failures.  Examples include when testing on a remote machine, or
against a system where communications are slow.

If not specifically defined, this variable gets automatically defined
to the same value as `timeout' during the testsuite initialization.
The default value of the timeout is defined in the file
`testsuite/config/unix.exp' (at least for Unix hosts; board files may
have their own values).

gdb_reverse_timeout

Defining this variable changes the default timeout duration when tests
under gdb.reverse directory are running.  Process record and reverse
debugging is so slow that its tests have unexpected `TIMEOUT' test
failures.  This global variable is useful to bump up the value of
`timeout' for gdb.reverse tests and doesn't cause any delay where
actual failures happen in the rest of the testsuite.


Board Settings
**************

DejaGNU includes the concept of a "board file", which specifies
testing details for a particular target (which are often bare circuit
boards, thus the name).

In the GDB testsuite specifically, the board file may include a
number of "board settings" that test cases may check before deciding
whether to exercise a particular feature.  For instance, a board
lacking any I/O devices, or perhaps simply having its I/O devices
not wired up, should set `noinferiorio'.

Here are the supported board settings:

gdb,cannot_call_functions

  The board does not support inferior call, that is, invoking inferior
  functions in GDB.

gdb,can_reverse

  The board supports reverse execution.

gdb,no_hardware_watchpoints

  The board does not support hardware watchpoints.

gdb,nofileio

  GDB is unable to intercept target file operations in remote and
  perform them on the host.

gdb,noinferiorio

  The board is unable to provide I/O capability to the inferior.

gdb,noresults

  A program will not return an exit code or result code (or the value
  of the result is undefined, and should not be looked at).

gdb,nosignals

  The board does not support signals.

gdb,skip_huge_test

  Skip time-consuming tests on the board with slow connection.

gdb,skip_float_tests

  Skip tests related to floating point.

gdb,use_precord

  The board supports process record.

gdb_init_command
gdb_init_commands

  Commands to send to GDB every time a program is about to be run.  The
  first of these settings defines a single command as a string.  The
  second defines a TCL list of commands being a string each.  The commands
  are sent one by one in a sequence, first from `gdb_init_command', if any,
  followed by individual commands from `gdb_init_command', if any, in this
  list's order.

gdb_server_prog

  The location of GDBserver.  If GDBserver somewhere other than its
  default location is used in test, specify the location of GDBserver in
  this variable.  The location is a file name for GDBserver, and may be
  either absolute or relative to the testsuite subdirectory of the build
  directory.

in_proc_agent

  The location of the in-process agent (used for fast tracepoints and
  other special tests).  If the in-process agent of interest is anywhere
  other than its default location, set this variable.  The location is a
  filename, and may be either absolute or relative to the testsuite
  subdirectory of the build directory.

noargs

  GDB does not support argument passing for inferior.

no_long_long

  The board does not support type long long.

use_cygmon

  The board is running the monitor Cygmon.

use_gdb_stub

  The tests are running with a GDB stub.

exit_is_reliable

  Set to true if GDB can assume that letting the program run to end
  reliably results in program exits being reported as such, as opposed
  to, e.g., the program ending in an infinite loop or the board
  crashing/resetting.  If not set, this defaults to $use_gdb_stub.  In
  other words, native targets are assumed reliable by default, and
  remote stubs assumed unreliable.

gdb,predefined_tsv

  The predefined trace state variables the board has.


Testsuite Organization
**********************

The testsuite is entirely contained in `gdb/testsuite'.  The main
directory of the testsuite includes some makefiles and configury, but
these are minimal, and used for little besides cleaning up, since the
tests themselves handle the compilation of the programs that GDB will
run.

The file `testsuite/lib/gdb.exp' contains common utility procs useful
for all GDB tests, while the directory testsuite/config contains
configuration-specific files, typically used for special-purpose
definitions of procs like `gdb_load' and `gdb_start'.

The tests themselves are to be found in directories named
'testsuite/gdb.* and subdirectories of those.  The names of the test
files must always end with ".exp".  DejaGNU collects the test files by
wildcarding in the test directories, so both subdirectories and
individual files typically get chosen and run in alphabetical order.

The following lists some notable types of subdirectories and what they
are for.  Since DejaGNU finds test files no matter where they are
located, and since each test file sets up its own compilation and
execution environment, this organization is simply for convenience and
intelligibility.

gdb.base

This is the base testsuite.  The tests in it should apply to all
configurations of GDB (but generic native-only tests may live here).
The test programs should be in the subset of C that is both valid
ANSI/ISO C, and C++.

gdb.<lang>

Language-specific tests for any language besides C.  Examples are
gdb.cp for C++ and gdb.java for Java.

gdb.<platform>

Non-portable tests.  The tests are specific to a specific
configuration (host or target), such as eCos.

gdb.arch

Architecture-specific tests that are (usually) cross-platform.

gdb.<subsystem>

Tests that exercise a specific GDB subsystem in more depth.  For
instance, gdb.disasm exercises various disassemblers, while
gdb.stabs tests pathways through the stabs symbol reader.

gdb.perf

GDB performance tests.

Writing Tests
*************

In many areas, the GDB tests are already quite comprehensive; you
should be able to copy existing tests to handle new cases.  Be aware
that older tests may use obsolete practices but have not yet been
updated.

You should try to use `gdb_test' whenever possible, since it includes
cases to handle all the unexpected errors that might happen.  However,
it doesn't cost anything to add new test procedures; for instance,
gdb.base/exprs.exp defines a `test_expr' that calls `gdb_test'
multiple times.

Only use `send_gdb' and `gdb_expect' when absolutely necessary.  Even
if GDB has several valid responses to a command, you can use
`gdb_test_multiple'.  Like `gdb_test', `gdb_test_multiple' recognizes
internal errors and unexpected prompts.

Do not write tests which expect a literal tab character from GDB.  On
some operating systems (e.g. OpenBSD) the TTY layer expands tabs to
spaces, so by the time GDB's output reaches `expect' the tab is gone.

The source language programs do *not* need to be in a consistent
style.  Since GDB is used to debug programs written in many different
styles, it's worth having a mix of styles in the testsuite; for
instance, some GDB bugs involving the display of source lines might
never manifest themselves if the test programs used GNU coding style
uniformly.

Some testcase results need more detailed explanation:

KFAIL

Use KFAIL for known problem of GDB itself.  You must specify the GDB
bug report number, as in these sample tests:

	kfail "gdb/13392" "continue to marker 2"

or

	setup_kfail gdb/13392 "*-*-*"
	kfail "continue to marker 2"


XFAIL

Short for "expected failure", this indicates a known problem with the
environment.  This could include limitations of the operating system,
compiler version, and other components.

This example from gdb.base/attach-pie-misread.exp is a sanity check
for the target environment:

	# On x86_64 it is commonly about 4MB.
	if {$stub_size > 25000000} {
	    xfail "stub size $stub_size is too large"
	    return
	}

You should provide bug report number for the failing component of the
environment, if such bug report is available, as with this example
referring to a GCC problem:

	  if {[test_compiler_info {gcc-[0-3]-*}]
	      || [test_compiler_info {gcc-4-[0-5]-*}]} {
	      setup_xfail "gcc/46955" *-*-*
	  }
	  gdb_test "python print ttype.template_argument(2)" "&C::c"

Note that it is also acceptable, and often preferable, to avoid
running the test at all.  This is the better option if the limitation
is intrinsic to the environment, rather than a bug expected to be
fixed in the near future.

       How to build and install the DJGPP native version of GDB
       ********************************************************

General
=======

GDB built with DJGPP supports native DJGPP debugging, whereby you run
gdb.exe and the program being debugged on the same machine.  In
addition, this version supports remote debugging via a serial port,
provided that the target machine has a GDB-compatible debugging stub
which can be linked with the target program (see the section "Remote
Serial" in the GDB manual for more details).


Installation of the binary distribution
=======================================

Simply unzip the gdbNNNb.zip file (where NNN is the version number)
from the top DJGPP installation directory.  Be sure to preserve the
directory structure while you unzip (use -d switch if you do this with
PKUNZIP).  On Windows 9X and Windows 2000, use an unzip program which
supports long file names; one such program is unzip32.exe, available
from the DJGPP sites.

If you need the libraries which are built as part of GDB, install the
companion file gdbNNNa.zip.  This allows to develop applications which
use the same functions as GDB.  For example, you can build your own
front end to the debugger.


Rebuilding GDB from sources
===========================

1. Prerequisites
   -------------
To build the package, you will need the DJGPP development environment
(GCC, header files, and the libraries), and also DJGPP ports of the
following tools:

	- GNU Make 3.79.1 or later
	- Bash 2.03 or later
	- GNU Sed
	- GNU Fileutils
	- GNU Textutils 2.0 or later
	- GNU Sh-utils
	- GNU Grep 2.4 or later
	- GNU Findutils
	- GNU Awk 3.04 or later
	- GNU Bison (only if you change one of the gdb/*.y files)
	- Groff (only if you need to format the man pages)
	- GNU Diffutils (only if you run the test suite)

These programs should be available from the DJGPP sites, in the v2gnu
directory.  In addition, the configuration script invokes the `update'
and `utod' utilities which are part of the basic DJGPP development kit
(djdevNNN.zip).


2. Unpacking the sources
   ---------------------
If you download the source distribution from one of the DJGPP sites,
just unzip it while preserving the directory structure (I suggest to
use unzip32.exe available with the rest of DJGPP), and proceed to the
section "How to build", below.

Source distributions downloaded from one of the GNU FTP sites need
some more work to unpack.  First, you MUST use the `djunpack' batch
file to unzip the package.  That's because some file names in the
official distributions need to be changed to avoid problems on the
various platforms supported by DJGPP.  `djunpack' invokes the `djtar'
program (that is part of the basic DJGPP development kit) to rename
these files on the fly given a file with name mappings; the
distribution includes a file `gdb/config/djgpp/fnchange.lst' with the
necessary mappings.  So you need first to retrieve that batch file,
and then invoke it to unpack the distribution.  Here's how:

 djtar -x -p -o gdb-5.2/djunpack.bat gdb-5.2.tar.gz > djunpack.bat
 djunpack gdb-5.2.tar.gz

(The name of the distribution archive and the leading directory of the
path to `djunpack.bat' in the distribution will be different for
versions of GDB other than 5.2.)

If the argument to `djunpack.bat' include leading directories, it MUST
be given with the DOS-style backslashes; Unix-style forward slashes
will NOT work.

If the distribution comes as a .tar.bz2 archive, and your version of
`djtar' doesn't support bzip2 decompression, you need to unpack it as
follows:

 bunzip2 gdb-6.4.tar.bz2
 djtar -x -p -o gdb-6.4/djunpack.bat gdb-6.4.tar > djunpack.bat
 djunpack gdb-6.4.tar


3. How to build
   ------------

If the source distribution available from DJGPP archives is already
configured for DJGPP v2.x (if it is, you will find files named
`Makefile' in each subdirectory), then just invoke Make:

		make

To build a package that is not yet configured, or if you downloaded
GDB from a GNU FTP site, you will need to configure it first.  You
will also need to configure it if you want to change the configuration
options (e.g., compile without support for the GDBMI interface).  To
configure GDB, type this command:

		sh ./gdb/config/djgpp/djconfig.sh

This script checks the unpacked distribution, then edits the configure
scripts in the various subdirectories, to make them suitable for
DJGPP, and finally invokes the top-level configure script, which
recursively configures all the subdirectories.

You may pass optional switches to djconfig.sh.  It accepts all the
switches accepted by the original GDB configure script.  These
switches are described in the file gdb/README, and their full list can
be displayed by running the following command:

		sh ./gdb/configure --help

NOTE: if you *do* use optional command-line switches, you MUST pass
to the script the name of the directory where GDB sources are
unpacked--even if you are building GDB in-place!  For example:

	sh ./gdb/config/djgpp/djconfig.sh . --disable-gdbmi

It is also possible to build GDB in a directory that is different from
the one where the sources were unpacked.  In that case, you have to
pass the source directory as the first argument to the script:

	sh ./gdb/config/djgpp/djconfig.sh d:/gnu/gdb-6.4

You MUST use forward slashes in the first argument.

After the configure script finishes, run Make:

	make

If you want to produce the documentation (for example, if you changed
some of the Texinfo sources), type this:

	make info

When Make finishes, you can install the package:

	make install prefix='${DJDIR}' INSTALL='ginstall -c'

The above doesn't install the docs; for that you will need to say
this:

	make install-info prefix='${DJDIR}' INSTALL='ginstall -c'

The test suite has been made to work with DJGPP.  If you make a change
in some of the programs, or want to be sure you have a fully
functional GDB executable, it is a good idea to run the test suite.
You cannot use "make check" for that, since it will want to run the
`dejagnu' utility which DJGPP doesn't support.  Instead, use the special
script gdb/config/djgpp/djcheck.sh, like this:

		cd gdb/testsuite
		sh ../config/djgpp/djcheck.sh

This will run for a while and should not print anything, except the
messages "Running tests in DIR", where DIR is one of the
subdirectories of the testsuite.  Any test that fails to produce the
expected output will cause the diffs between the expected and the
actual output be printed, and in addition will leave behind a file
SOMETHING.tst (where SOMETHING is the name of the failed test).  You
should compare each of the *.tst files with the corresponding *.out
file and convince yourself that the differences do not indicate a real
problem.  Examples of differences you can disregard are changes in the
copyright blurb printed by GDB, values of unitialized variables,
addresses of global variables like argv[] and envp[] (which depend on
the size of your environment), etc.

Note that djcheck.sh only recurses into those of the subdirectories of
the test suite which test features supported by the DJGPP port of GDB.
For example, the tests in the gdb.gdbtk, and gdb.threads directories
are not run.


Enjoy,
                                    Eli Zaretskii <eliz@gnu.org>
README for gdb/guile
====================

This file contains important notes for gdb/guile developers.
["gdb/guile" refers to the directory you found this file in]

Nomenclature:

  In the implementation we use "Scheme" or "Guile" depending on context.
  And sometimes it doesn't matter.
  Guile is Scheme, and for the most part this is what we present to the user
  as well.  However, to highlight the fact that it is Guile, the GDB commands
  that invoke Scheme functions are named "guile" and "guile-repl",
  abbreviated "gu" and "gr" respectively.

Co-existence with Python:

  Keep the user interfaces reasonably consistent, but don't shy away from
  providing a clearer (or more Scheme-friendly/consistent) user interface
  where appropriate.

  Additions to Python support or Scheme support don't require corresponding
  changes in the other scripting language.

  Scheme-wrapped breakpoints are created lazily so that if the user
  doesn't use Scheme s/he doesn't pay any cost.

Importing the gdb module into Scheme:

  To import the gdb module:
  (gdb) guile (use-modules (gdb))

  If you want to add a prefix to gdb module symbols:
  (gdb) guile (use-modules ((gdb) #:renamer (symbol-prefix-proc 'gdb:)))
  This gives every symbol a "gdb:" prefix which is a common convention.
  OTOH it's more to type.

Implementation/Hacking notes:

  Don't use scm_is_false.
  For this C function, () == #f (a la Lisp) and it's not clear how treating
  them as equivalent for truth values will affect the GDB interface.
  Until the effect is clear avoid them.
  Instead use gdbscm_is_false, gdbscm_is_true, gdbscm_is_bool.
  There are macros in guile-internal.h to enforce this.

  Use gdbscm_foo as the name of functions that implement Scheme procedures
  to provide consistent naming in error messages.  The user can see "gdbscm"
  in the name and immediately know where the function came from.

  All smobs contain gdb_smob or chained_gdb_smob as the first member.
  This provides a mechanism for extending them in the Scheme side without
  tying GDB to the details.

  The lifetime of a smob, AIUI, is decided by the containing SCM.
  When there is no longer a reference to the containing SCM then the
  smob can be GC'd.  Objects that have references from outside of Scheme,
  e.g., breakpoints, need to be protected from GC.

  Don't do something that can cause a Scheme exception inside a TRY_CATCH,
  and, in code that can be called from Scheme, don't do something that can
  cause a GDB exception outside a TRY_CATCH.
  This makes the code a little tricky to write sometimes, but it is a
  rule imposed by the programming environment.  Bugs often happen because
  this rule is broken.  Learn it, follow it.

Coding style notes:

  - If you find violations to these rules, let's fix the code.
    Some attempt has been made to be consistent, but it's early.
    Over time we want things to be more consistent, not less.

  - None of this really needs to be read.  Instead, do not be creative:
    Monkey-See-Monkey-Do hacking should generally Just Work.

  - Absence of the word "typically" means the rule is reasonably strict.

  - The gdbscm_initialize_foo function (e.g., gdbscm_initialize_values)
    is the last thing to appear in the file, immediately preceded by any
    tables of exported variables and functions.

  - In addition to these of course, follow GDB coding conventions.

General naming rules:

  - The word "object" absent any modifier (like "GOOPS object") means a
    Scheme object (of any type), and is never used otherwise.
    If you want to refer to, e.g., a GOOPS object, say "GOOPS object".

  - Do not begin any function, global variable, etc. name with scm_.
    That's what the Guile implementation uses.
    (kinda obvious, just being complete).

  - The word "invalid" carries a specific connotation.  Try not to use it
    in a different way.  It means the underlying GDB object has disappeared.
    For example, a <gdb:objfile> smob becomes "invalid" when the underlying
    objfile is removed from GDB.

  - We typically use the word "exception" to mean Scheme exceptions,
    and we typically use the word "error" to mean GDB errors.

Comments:

  - function comments for functions implementing Scheme procedures begin with
    a description of the Scheme usage.  Example:
    /* (gsmob-aux gsmob) -> object */

  - the following comment appears after the copyright header:
    /* See README file in this directory for implementation notes, coding
       conventions, et.al.  */

Smob naming:

  - gdb smobs are named, internally, "gdb:foo"
  - in Guile they become <gdb:foo>, that is the convention for naming classes
    and smobs have rudimentary GOOPS support (they can't be inherited from,
    but generics can work with them)
  - in comments use the Guile naming for smobs,
    i.e., <gdb:foo> instead of gdb:foo.
    Note: This only applies to smobs.  Exceptions are also named gdb:foo,
    but since they are not "classes" they are not wrapped in <>.
  - smob names are stored in a global, and for simplicity we pass this
    global as the "expected type" parameter to SCM_ASSERT_TYPE, thus in
    this instance smob types are printed without the <>.
    [Hmmm, this rule seems dated now.  Plus I18N rules in GDB are not always
    clear, sometimes we pass the smob name through _(), however it's not
    clear that's actually a good idea.]

Type naming:

  - smob structs are typedefs named foo_smob

Variable naming:

  - "scm" by itself is reserved for arbitrary Scheme objects

  - variables that are pointers to smob structs are named <char>_smob or
    <char><char>_smob, e.g., f_smob for a pointer to a frame smob

  - variables that are gdb smob objects are typically named <char>_scm or
    <char><char>_scm, e.g., f_scm for a <gdb:frame> object

  - the name of the first argument for method-like functions is "self"

Function naming:

  General:

  - all non-static functions have a prefix,
    either gdbscm_ or <char><char>scm_ [or <char><char><char>scm_]

  - all functions that implement Scheme procedures have a gdbscm_ prefix,
    this is for consistency and readability of Scheme exception text

  - static functions typically have a prefix
    - the prefix is typically <char><char>scm_ where the first two letters
      are unique to the file or class the function works with.
      E.g., the scm-arch.c prefix is arscm_.
      This follows something used in gdb/python in some places,
      we make it formal.

  - if the function is of a general nature, or no other prefix works,
    use gdbscm_

  Conversion functions:

  - the from/to in function names follows from libguile's existing style
  - conversions from/to Scheme objects are named:
      prefix_scm_from_foo: converts from foo to scm
      prefix_scm_to_foo: converts from scm to foo

  Exception handling:

  - functions that may throw a Scheme exception have an _unsafe suffix
    - This does not apply to functions that implement Scheme procedures.
    - This does not apply to functions whose explicit job is to throw
      an exception.  Adding _unsafe to gdbscm_throw is kinda superfluous. :-)
  - functions that can throw a GDB error aren't adorned with _unsafe

  - "_safe" in a function name means it will never throw an exception
    - Generally unnecessary, since the convention is to mark the ones that
      *can* throw an exception.  But sometimes it's useful to highlight the
      fact that the function is safe to call without worrying about exception
      handling.

  - except for functions that implement Scheme procedures, all functions
    that can throw exceptions (GDB or Scheme) say so in their function comment

  - functions that don't throw an exception, but still need to indicate to
    the caller that one happened (i.e., "safe" functions), either return
    a <gdb:exception> smob as a result or pass it back via a parameter.
    For this reason don't pass back <gdb:exception> smobs for any other
    reason.  There are functions that explicitly construct <gdb:exception>
    smobs.  They're obviously the, umm, exception.

  Internal functions:

  - internal Scheme functions begin with "%" and are intentionally undocumented
    in the manual

  Standard Guile/Scheme conventions:

  - predicates that return Scheme values have the suffix _p and have suffix "?"
    in the Scheme procedure's name
  - functions that implement Scheme procedures that modify state have the
    suffix _x and have suffix "!" in the Scheme procedure's name
  - object predicates that return a C truth value are named prefix_is_foo
  - functions that set something have "set" at the front (except for a prefix)
    write this: gdbscm_set_gsmob_aux_x implements (set-gsmob-aux! ...)
    not this: gdbscm_gsmob_set_aux_x implements (gsmob-set-aux! ...)

Doc strings:

  - there are lots of existing examples, they should be pretty consistent,
    use them as boilerplate/examples
  - begin with a one line summary (can be multiple lines if necessary)
  - if the arguments need description:
    - blank line
    - "  Arguments: arg1 arg2"
      "    arg1: blah ..."
      "    arg2: blah ..."
  - if the result requires more description:
    - blank line
    - "  Returns:"
      "    Blah ..."
  - if it's important to list exceptions that can be thrown:
    - blank line
    - "  Throws:"
      "    exception-name: blah ..."
		   README for GDBserver & GDBreplay
		    by Stu Grossman and Fred Fish

Introduction:

This is GDBserver, a remote server for Un*x-like systems.  It can be used to
control the execution of a program on a target system from a GDB on a different
host.  GDB and GDBserver communicate using the standard remote serial protocol
implemented in remote.c, and various *-stub.c files.  They communicate via
either a serial line or a TCP connection.

For more information about GDBserver, see the GDB manual.

Usage (server (target) side):

First, you need to have a copy of the program you want to debug put onto
the target system.  The program can be stripped to save space if needed, as
GDBserver doesn't care about symbols.  All symbol handling is taken care of by
the GDB running on the host system.

To use the server, you log on to the target system, and run the `gdbserver'
program.  You must tell it (a) how to communicate with GDB, (b) the name of
your program, and (c) its arguments.  The general syntax is:

	target> gdbserver COMM PROGRAM [ARGS ...]

For example, using a serial port, you might say:

	target> gdbserver /dev/com1 emacs foo.txt

This tells GDBserver to debug emacs with an argument of foo.txt, and to
communicate with GDB via /dev/com1.  GDBserver now waits patiently for the
host GDB to communicate with it.

To use a TCP connection, you could say:

	target> gdbserver host:2345 emacs foo.txt

This says pretty much the same thing as the last example, except that we are
going to communicate with the host GDB via TCP.  The `host:2345' argument means
that we are expecting to see a TCP connection from `host' to local TCP port
2345.  (Currently, the `host' part is ignored.)  You can choose any number you
want for the port number as long as it does not conflict with any existing TCP
ports on the target system.  This same port number must be used in the host
GDBs `target remote' command, which will be described shortly.  Note that if
you chose a port number that conflicts with another service, GDBserver will
print an error message and exit.

On some targets, GDBserver can also attach to running programs.  This is
accomplished via the --attach argument.  The syntax is:

	target> gdbserver --attach COMM PID

PID is the process ID of a currently running process.  It isn't necessary
to point GDBserver at a binary for the running process.

Usage (host side):

You need an unstripped copy of the target program on your host system, since
GDB needs to examine it's symbol tables and such.  Start up GDB as you normally
would, with the target program as the first argument.  (You may need to use the
--baud option if the serial line is running at anything except 9600 baud.)
Ie: `gdb TARGET-PROG', or `gdb --baud BAUD TARGET-PROG'.  After that, the only
new command you need to know about is `target remote'.  It's argument is either
a device name (usually a serial device, like `/dev/ttyb'), or a HOST:PORT
descriptor.  For example:

	(gdb) target remote /dev/ttyb

communicates with the server via serial line /dev/ttyb, and:

	(gdb) target remote the-target:2345

communicates via a TCP connection to port 2345 on host `the-target', where
you previously started up GDBserver with the same port number.  Note that for
TCP connections, you must start up GDBserver prior to using the `target remote'
command, otherwise you may get an error that looks something like
`Connection refused'.

Building GDBserver:

The supported targets as of November 2006 are:
	arm-*-linux*
	bfin-*-uclinux
	bfin-*-linux-uclibc
	crisv32-*-linux*
	cris-*-linux*
	i[34567]86-*-cygwin*
	i[34567]86-*-linux*
	i[34567]86-*-mingw*
	ia64-*-linux*
	m32r*-*-linux*
	m68*-*-linux*
	m68*-*-uclinux*
	mips*64*-*-linux*
	mips*-*-linux*
	powerpc[64]-*-linux*
	s390[x]-*-linux*
	sh-*-linux*
	spu*-*-*
	x86_64-*-linux*

Configuring GDBserver you should specify the same machine for host and
target (which are the machine that GDBserver is going to run on.  This
is not the same as the machine that GDB is going to run on; building
GDBserver automatically as part of building a whole tree of tools does
not currently work if cross-compilation is involved (we don't get the
right CC in the Makefile, to start with)).

Building GDBserver for your target is very straightforward.  If you build
GDB natively on a target which GDBserver supports, it will be built
automatically when you build GDB.  You can also build just GDBserver:

	% mkdir obj
	% cd obj
	% path-to-gdbserver-sources/configure
	% make

If you prefer to cross-compile to your target, then you can also build
GDBserver that way.  In a Bourne shell, for example:

	% export CC=your-cross-compiler
	% path-to-gdbserver-sources/configure your-target-name
	% make

Using GDBreplay:

A special hacked down version of GDBserver can be used to replay remote
debug log files created by GDB.  Before using the GDB "target" command to
initiate a remote debug session, use "set remotelogfile <filename>" to tell
GDB that you want to make a recording of the serial or tcp session.  Note
that when replaying the session, GDB communicates with GDBreplay via tcp,
regardless of whether the original session was via a serial link or tcp.

Once you are done with the remote debug session, start GDBreplay and
tell it the name of the log file and the host and port number that GDB
should connect to (typically the same as the host running GDB):

	$ gdbreplay logfile host:port

Then start GDB (preferably in a different screen or window) and use the
"target" command to connect to GDBreplay:

	(gdb) target remote host:port

Repeat the same sequence of user commands to GDB that you gave in the
original debug session.  GDB should not be able to tell that it is talking
to GDBreplay rather than a real target, all other things being equal.  Note
that GDBreplay echos the command lines to stderr, as well as the contents of
the packets it sends and receives.  The last command echoed by GDBreplay is
the next command that needs to be typed to GDB to continue the session in
sync with the original session.
GNU toolchain edition of GNU libintl 0.12.1

Most of the content of this directory is taken from gettext 0.12.1
and is owned by that project.  Patches should be directed to the
gettext developers first.  However, note the following:

* libintl.h comes from gettext, but is named libgnuintl.h.in in that
  project's source tree.

* The files COPYING.LIB-2.0 and COPYING.LIB-2.1 are redundant with the
  top-level COPYING.LIB and have therefore been removed.

* The files config.charset, ref-add.sin, ref-del.sin, os2compat.c,
  and os2compat.h are not used in this setup and have therefore been 
  removed.

* aclocal.m4 was constructed using automake's "aclocal -I ../config".

* configure.ac, config.intl.in, and Makefile.in were written for this
  directory layout, by Zack Weinberg <zack@codesourcery.com>.  Please
  direct patches for these files to gcc-patches@gcc.gnu.org.
BFD is an object file library.  It permits applications to use the
same routines to process object files regardless of their format.

BFD is used by the GNU debugger, assembler, linker, and the binary
utilities.

The documentation on using BFD is scanty and may be occasionally
incorrect.  Pointers to documentation problems, or an entirely
rewritten manual, would be appreciated.

There is some BFD internals documentation in doc/bfdint.texi which may
help programmers who want to modify BFD.

BFD is normally built as part of another package.  See the build
instructions for that package, probably in a README file in the
appropriate directory.

BFD supports the following configure options:

  --target=TARGET
	The default target for which to build the library.  TARGET is
	a configuration target triplet, such as sparc-sun-solaris.
  --enable-targets=TARGET,TARGET,TARGET...
	Additional targets the library should support.  To include
	support for all known targets, use --enable-targets=all.
  --enable-64-bit-bfd
	Include support for 64 bit targets.  This is automatically
	turned on if you explicitly request a 64 bit target, but not
	for --enable-targets=all.  This requires a compiler with a 64
	bit integer type, such as gcc.
  --enable-shared
	Build BFD as a shared library.
  --with-mmap
	Use mmap when accessing files.  This is faster on some hosts,
	but slower on others.  It may not work on all hosts.

Report bugs with BFD to bug-binutils@gnu.org.

Patches are encouraged.  When sending patches, always send the output
of diff -u or diff -c from the original file to the new file.  Do not
send default diff output.  Do not make the diff from the new file to
the original file.  Remember that any patch must not break other
systems.  Remember that BFD must support cross compilation from any
host to any target, so patches which use ``#ifdef HOST'' are not
acceptable.  Please also read the ``Reporting Bugs'' section of the
gcc manual.

Bug reports without patches will be remembered, but they may never get
fixed until somebody volunteers to fix them.

Copyright (C) 2012-2015 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
This directory contains the -liberty library of free software.
It is a collection of subroutines used by various GNU programs.
Current members include:

	getopt -- get options from command line
	obstack -- stacks of arbitrarily-sized objects
	strerror -- error message strings corresponding to errno
	strtol -- string-to-long conversion
	strtoul -- string-to-unsigned-long conversion

We expect many of the GNU subroutines that are floating around to
eventually arrive here.

The library must be configured from the top source directory.  Don't
try to run configure in this directory.  Follow the configuration
instructions in ../README.

Please report bugs to "gcc-bugs@gcc.gnu.org" and send fixes to
"gcc-patches@gcc.gnu.org".  Thank you.

ADDING A NEW FILE
=================

There are two sets of files:  Those that are "required" will be
included in the library for all configurations, while those
that are "optional" will be included in the library only if "needed."

To add a new required file, edit Makefile.in to add the source file
name to CFILES and the object file to REQUIRED_OFILES.

To add a new optional file, it must provide a single function, and the
name of the function must be the same as the name of the file.

    * Add the source file name to CFILES in Makefile.in and the object
      file to CONFIGURED_OFILES.

    * Add the function to name to the funcs shell variable in
      configure.ac.

    * Add the function to the AC_CHECK_FUNCS lists just after the
      setting of the funcs shell variable.  These AC_CHECK_FUNCS calls
      are never executed; they are there to make autoheader work
      better.

    * Consider the special cases of building libiberty; as of this
      writing, the special cases are newlib and VxWorks.  If a
      particular special case provides the function, you do not need
      to do anything.  If it does not provide the function, add the
      object file to LIBOBJS, and add the function name to the case
      controlling whether to define HAVE_func.

Finally, in the build directory of libiberty, configure with
"--enable-maintainer-mode", run "make maint-deps" to update
Makefile.in, and run 'make stamp-functions' to regenerate
functions.texi.

The optional file you've added (e.g. getcwd.c) should compile and work
on all hosts where it is needed.  It does not have to work or even
compile on hosts where it is not needed.

ADDING A NEW CONFIGURATION
==========================

On most hosts you should be able to use the scheme for automatically
figuring out which files are needed.  In that case, you probably
don't need a special Makefile stub for that configuration.

If the fully automatic scheme doesn't work, you may be able to get
by with defining EXTRA_OFILES in your Makefile stub.  This is
a list of object file names that should be treated as required
for this configuration - they will be included in libiberty.a,
regardless of whatever might be in the C library.
This is a loose collection of notes for people hacking on simulators.
If this document gets big enough it can be prettied up then.

Contents

- The "common" directory
- Common Makefile Support
- TAGS support
- Generating "configure" files
- tconfig.h
- C Language Assumptions
- "dump" commands under gdb

The "common" directory
======================

The common directory contains:

- common documentation files (e.g. run.1, and maybe in time .texi files)
- common source files (e.g. run.c)
- common Makefile fragment and configury (e.g. Make-common.in, aclocal.m4).

In addition "common" contains portions of the system call support
(e.g. callback.c, nltvals.def).

Even though no files are built in this directory, it is still configured
so support for regenerating nltvals.def is present.

Common Makefile Support
=======================

A common configuration framework is available for simulators that want
to use it.  The common framework exists to remove a lot of duplication
in configure.ac and Makefile.in, and it also provides a foundation for
enhancing the simulators uniformly (e.g. the more they share in common
the easier a feature added to one is added to all).

The configure.ac of a simulator using the common framework should look like:

--- snip ---
dnl Process this file with autoconf to produce a configure script.
sinclude(../common/aclocal.m4)
AC_PREREQ(2.5)dnl
AC_INIT(Makefile.in)

SIM_AC_COMMON

... target specific additions ...

SIM_AC_OUTPUT
--- snip ---

SIM_AC_COMMON:

- invokes the autoconf macros most often used by the simulators
- defines --enable/--with options usable by all simulators
- initializes sim_link_files/sim_link_links as the set of symbolic links
  to set up

SIM_AC_OUTPUT:

- creates the symbolic links defined in sim_link_{files,links}
- creates config.h
- creates the Makefile

The Makefile.in of a simulator using the common framework should look like:

--- snip ---
# Makefile for blah ...
# Copyright blah ...

## COMMON_PRE_CONFIG_FRAG

# These variables are given default values in COMMON_PRE_CONFIG_FRAG.
# We override the ones we need to here.
# Not all of these need to be mentioned, only the necessary ones.
# In fact it is better to *not* mention ones if the value is the default.

# List of object files, less common parts.
SIM_OBJS =
# List of extra dependencies.
# Generally this consists of simulator specific files included by sim-main.h.
SIM_EXTRA_DEPS =
# List of flags to always pass to $(CC).
SIM_EXTRA_CFLAGS =
# List of extra libraries to link with.
SIM_EXTRA_LIBS =
# List of extra program dependencies.
SIM_EXTRA_LIBDEPS =
# List of main object files for `run'.
SIM_RUN_OBJS = run.o
# Dependency of `all' to build any extra files.
SIM_EXTRA_ALL =
# Dependency of `install' to install any extra files.
SIM_EXTRA_INSTALL =
# Dependency of `clean' to clean any extra files.
SIM_EXTRA_CLEAN =

## COMMON_POST_CONFIG_FRAG

# Rules need to build $(SIM_OBJS), plus whatever else the target wants.

... target specific rules ...
--- snip ---

COMMON_{PRE,POST}_CONFIG_FRAG are markers for SIM_AC_OUTPUT to tell it
where to insert the two pieces of common/Make-common.in.
The resulting Makefile is created by doing autoconf substitions on
both the target's Makefile.in and Make-common.in, and inserting
the two pieces of Make-common.in into the target's Makefile.in at
COMMON_{PRE,POST}_CONFIG_FRAG.

Note that SIM_EXTRA_{INSTALL,CLEAN} could be removed and "::" targets
could be used instead.  However, it's not clear yet whether "::" targets
are portable enough.

TAGS support
============

Many files generate program symbols at compile time.
Such symbols can't be found with grep nor do they normally appear in
the TAGS file.  To get around this, source files can add the comment

/* TAGS: foo1 foo2 */

where foo1, foo2 are program symbols.  Symbols found in such comments
are greppable and appear in the TAGS file.

Generating "configure" files
============================

For targets using the common framework, "configure" can be generated
by running `autoconf'.

To regenerate the configure files for all targets using the common framework:

	$  cd devo/sim
	$  make -f Makefile.in SHELL=/bin/sh autoconf-common

To add a change-log entry to the ChangeLog file for each updated
directory (WARNING - check the modified new-ChangeLog files before
renaming):

	$  make -f Makefile.in SHELL=/bin/sh autoconf-changelog
	$  more */new-ChangeLog
	$  make -f Makefile.in SHELL=/bin/sh autoconf-install

In a similar vein, both the configure and config.in files can be
updated using the sequence:

	$  cd devo/sim
	$  make -f Makefile.in SHELL=/bin/sh autoheader-common
	$  make -f Makefile.in SHELL=/bin/sh autoheader-changelog
	$  more */new-ChangeLog
	$  make -f Makefile.in SHELL=/bin/sh autoheader-install

To add the entries to an alternative ChangeLog file, use:

	$  make ChangeLog=MyChangeLog ....


tconfig.h
==========

File tconfig.h defines one or more target configuration macros
(e.g. a tm.h file).  There are very few that need defining.
For a list of all of them, see common/tconfig.h.
It contains them all, commented out.
The intent is that a new port can just copy this file and
define the ones it needs.

C Language Assumptions
======================

The programmer may assume that the simulator is being built using an
ANSI C compiler that supports a 64 bit data type.  Consequently:

	o	prototypes can be used

	o	If sim-types.h is included, the two
		types signed64 and unsigned64 are
		available.

	o	The type `unsigned' is valid.

However, the user should be aware of the following:

	o	GCC's `<number>LL' is NOT acceptable.
		Microsoft-C doesn't reconize it.

	o	MSC's `<number>i64' is NOT acceptable.
		GCC doesn't reconize it.

	o	GCC's `long long' MSC's `_int64' can
		NOT be used to define 64 bit integer data
		types.

	o	An empty array (eg int a[0]) is not valid.

When building with GCC it is effectivly a requirement that
--enable-build-warnings=,-Werror be specified during configuration.

"dump" commands under gdb
=========================

gdbinit.in contains the following

define dump
set sim_debug_dump ()
end

Simulators that define the sim_debug_dump function can then have their
internal state pretty printed from gdb.

FIXME: This can obviously be made more elaborate.  As needed it will be.

Rebuilding nltvals.def
======================

Checkout a copy of the SIM and LIBGLOSS modules (Unless you've already
got one to hand):

	$  mkdir /tmp/$$
	$  cd /tmp/$$
	$  cvs checkout sim-no-testsuite libgloss-no-testsuite newlib-no-testsuite

Configure things for an arbitrary simulator target (I've d10v for
convenience):

	$  mkdir /tmp/$$/build
	$  cd /tmp/$$/build
	$  /tmp/$$/devo/configure --target=d10v-elf

In the sim/common directory rebuild the headers:

	$  cd sim/common
	$  make headers

To add a new target:

	devo/sim/common/gennltvals.sh

		Add your new processor target (you'll need to grub
		around to find where your syscall.h lives).

	devo/sim/<processor>/Makefile.in

		Add the definition:

			``NL_TARGET = -DNL_TARGET_d10v''

		just before the line COMMON_POST_CONFIG_FRAG.

	devo/sim/<processor>/*.[ch]

		Include targ-vals.h instead of syscall.h.

Tracing
=======

For ports based on CGEN, tracing instrumentation should largely be for free,
so we will cover the basic non-CGEN setup here.  The assumption is that your
target is using the common autoconf macros and so the build system already
includes the sim-trace configure flag.

The full tracing API is covered in sim-trace.h, so this section is an overview.

Before calling any trace function, you should make a call to the trace_prefix()
function.  This is usually done in the main sim_engine_run() loop before
simulating the next instruction.  You should make this call before every
simulated insn.  You can probably copy & paste this:
  if (TRACE_ANY_P (cpu))
    trace_prefix (sd, cpu, NULL_CIA, oldpc, TRACE_LINENUM_P (cpu), NULL, 0, "");

You will then need to instrument your simulator code with calls to the
trace_generic() function with the appropriate trace index.  Typically, this
will take a form similar to the above snippet.  So to trace instructions, you
would use something like:
  if (TRACE_INSN_P (cpu))
    trace_generic (sd, cpu, TRACE_INSN_IDX, "NOP;");

The exact output format is up to you.  See the trace index enum in sim-trace.h
to see the different tracing info available.

To utilize the tracing features at runtime, simply use the --trace-xxx flags.
  run --trace-insn ./some-program

Profiling
=========

Similar to the tracing section, this is merely an overview for non-CGEN based
ports.  The full API may be found in sim-profile.h.  Its API is also similar
to the tracing API.

Note that unlike the tracing command line options, in addition to the profile
flags, you have to use the --verbose option to view the summary report after
execution.  Tracing output is displayed on the fly, but the profile output is
only summarized.

To profile core accesses (such as data reads/writes and insn fetches), add
calls to PROFILE_COUNT_CORE() to your read/write functions.  So in your data
fetch function, you'd use something like:
  PROFILE_COUNT_CORE (cpu, target_addr, size_in_bytes, map_read);
Then in your data write function:
  PROFILE_COUNT_CORE (cpu, target_addr, size_in_bytes, map_write);
And in your insn fetcher:
  PROFILE_COUNT_CORE (cpu, target_addr, size_in_bytes, map_exec);

To use the PC profiling code, you simply have to tell the system where to find
your simulator's PC and its size.  So in your sim_open() function:
  STATE_WATCHPOINTS (sd)->pc = address_of_cpu0_pc;
  STATE_WATCHPOINTS (sd)->sizeof_pc = number_of_bytes_for_pc_storage;
In a typical 32bit system, the sizeof_pc will be 4 bytes.

To profile branches, in every location where a branch insn is executed, call
one of the related helpers:
  PROFILE_BRANCH_TAKEN (cpu);
  PROFILE_BRANCH_UNTAKEN (cpu);
If you have stall information, you can utilize the other helpers too.

Environment Simulation
======================

The simplest simulator doesn't include environment support -- it merely
simulates the Instruction Set Architecture (ISA).  Once you're ready to move
on to the next level, call the common macro in your configure.ac:
SIM_AC_OPTION_ENVIRONMENT

This will support for the user, virtual, and operating environments.  See the
sim-config.h header for a more detailed description of them.  The former are
pretty straight forward as things like exceptions (making system calls) are
handled in the simulator.  Which is to say, an exception does not trigger an
exception handler in the simulator target -- that is what the operating env
is about.  See the following userspace section for more information.

Userspace System Calls
======================

By default, the libgloss userspace is simulated.  That means the system call
numbers and calling convention matches that of libgloss.  Simulating other
userspaces (such as Linux) is pretty straightforward, but let's first focus
on the basics.  The basic API is covered in include/gdb/callback.h.

When an instruction is simulated that invokes the system call method (such as
forcing a hardware trap or exception), your simulator code should set up the
CB_SYSCALL data structure before calling the common cb_syscall() function.
For example:
static int
syscall_read_mem (host_callback *cb, struct cb_syscall *sc,
		  unsigned long taddr, char *buf, int bytes)
{
  SIM_DESC sd = (SIM_DESC) sc->p1;
  SIM_CPU *cpu = (SIM_CPU *) sc->p2;
  return sim_core_read_buffer (sd, cpu, read_map, buf, taddr, bytes);
}
static int
syscall_write_mem (host_callback *cb, struct cb_syscall *sc,
		  unsigned long taddr, const char *buf, int bytes)
{
  SIM_DESC sd = (SIM_DESC) sc->p1;
  SIM_CPU *cpu = (SIM_CPU *) sc->p2;
  return sim_core_write_buffer (sd, cpu, write_map, buf, taddr, bytes);
}
void target_sim_syscall (SIM_CPU *cpu)
{
  SIM_DESC sd = CPU_STATE (cpu);
  host_callback *cb = STATE_CALLBACK (sd);
  CB_SYSCALL sc;

  CB_SYSCALL_INIT (&sc);

  sc.func = <fetch system call number>;
  sc.arg1 = <fetch first system call argument>;
  sc.arg2 = <fetch second system call argument>;
  sc.arg3 = <fetch third system call argument>;
  sc.arg4 = <fetch fourth system call argument>;
  sc.p1 = (PTR) sd;
  sc.p2 = (PTR) cpu;
  sc.read_mem = syscall_read_mem;
  sc.write_mem = syscall_write_mem;

  cb_syscall (cb, &sc);

  <store system call result from sc.result>;
  <store system call error from sc.errcode>;
}
Some targets store the result and error code in different places, while others
only store the error code when the result is an error.

Keep in mind that the CB_SYS_xxx defines are normalized values with no real
meaning with respect to the target.  They provide a unique map on the host so
that it can parse things sanely.  For libgloss, the common/nltvals.def file
creates the target's system call numbers to the CB_SYS_xxx values.

To simulate other userspace targets, you really only need to update the maps
pointers that are part of the callback interface.  So create CB_TARGET_DEFS_MAP
arrays for each set (system calls, errnos, open bits, etc...) and in a place
you find useful, do something like:

...
static CB_TARGET_DEFS_MAP cb_linux_syscall_map[] = {
# define TARGET_LINUX_SYS_open 5
  { CB_SYS_open, TARGET_LINUX_SYS_open },
  ...
  { -1, -1 },
};
...
  host_callback *cb = STATE_CALLBACK (sd);
  cb->syscall_map = cb_linux_syscall_map;
  cb->errno_map = cb_linux_errno_map;
  cb->open_map = cb_linux_open_map;
  cb->signal_map = cb_linux_signal_map;
  cb->stat_map = cb_linux_stat_map;
...

Each of these cb_linux_*_map's are manually declared by the arch target.

The target_sim_syscall() example above will then work unchanged (ignoring the
system call convention) because all of the callback functions go through these
mapping arrays.

Events
======

Events are scheduled and executed on behalf of either a cpu or hardware devices.
The API is pretty much the same and can be found in common/sim-events.h and
common/hw-events.h.

For simulator targets, you really just have to worry about the schedule and
deschedule functions.

Device Trees
============

The device tree model is based on the OpenBoot specification.  Since this is
largely inherited from the psim code, consult the existing psim documentation
for some in-depth details.
	http://sourceware.org/psim/manual/

Hardware Devices
================

The simplest simulator doesn't include hardware device support.  Once you're
ready to move on to the next level, call the common macro in your configure.ac:
SIM_AC_OPTION_HARDWARE(yes,,devone devtwo devthree)

The basic hardware API is documented in common/hw-device.h.

Each device has to have a matching file name with a "dv-" prefix.  So there has
to be a dv-devone.c, dv-devtwo.c, and dv-devthree.c files.  Further, each file
has to have a matching hw_descriptor structure.  So the dv-devone.c file has to
have something like:
  const struct hw_descriptor dv_devone_descriptor[] = {
    {"devone", devone_finish,},
    {NULL, NULL},
  };

The "devone" string as well as the "devone_finish" function are not hard
requirements, just common conventions.  The structure name is a hard
requirement.

The devone_finish() callback function is used to instantiate this device by
parsing the corresponding properties in the device tree.

Hardware devices typically attach address ranges to themselves.  Then when
accesses to those addresses are made, the hardware will have its callback
invoked.  The exact callback could be a normal I/O read/write access, as
well as a DMA access.  This makes it easy to simulate memory mapped registers.

Keep in mind that like a proper device driver, it may be instantiated many
times over.  So any device state it needs to be maintained should be allocated
during the finish callback and attached to the hardware device via set_hw_data.
Any hardware functions can access this private data via the hw_data function.

Ports (Interrupts / IRQs)
=========================

First, a note on terminology.  A "port" is an aspect of a hardware device that
accepts or generates interrupts.  So devices with input ports may be the target
of an interrupt (accept it), and/or they have output ports so that they may be
the source of an interrupt (generate it).

Each port has a symbolic name and a unique number.  These are used to identify
the port in different contexts.  The output port name has no hard relationship
to the input port name (same for the unique number).  The callback that accepts
the interrupt uses the name/id of its input port, while the generator function
uses the name/id of its output port.

The device tree is used to connect the output port of a device to the input
port of another device.  There are no limits on the number of inputs connected
to an output, or outputs to an input, or the devices attached to the ports.
In other words, the input port and output port could be the same device.

The basics are:
 - each hardware device declares an array of ports (hw_port_descriptor).
   any mix of input and output ports is allowed.
 - when setting up the device, attach the array (set_hw_ports).
 - if the device accepts interrupts, it will have to attach a port callback
   function (set_hw_port_event)
 - connect ports with the device tree
 - handle incoming interrupts with the callback
 - generate outgoing interrupts with hw_port_event
This is the m32r simulator directory.

It is still work-in-progress.  The current sources are reasonably
well tested and lots of features are in.  However, there's lots
more yet to come.

There are lots of machine generated files in the source directory!
They are only generated if you configure with --enable-cgen-maint,
similar in behaviour to Makefile.in, configure under automake/autoconf.

For details on the generator, see ../../cgen.

devo/cgen isn't part of the comp-tools module yet.
You'll need to check it out manually (also akin to automake/autoconf).
The RX simulator offers two rx-specific configure options:

--enable-cycle-accurate  (default)
--disable-cycle-accurate

If enabled, the simulator will keep track of how many cycles each
instruction takes.  While not 100% accurate, it is very close,
including modelling fetch stalls and register latency.

--enable-cycle-stats  (default)
--disable-cycle-stats

If enabled, specifying "-v" twice on the simulator command line causes
the simulator to print statistics on how much time was used by each
type of opcode, and what pairs of opcodes tend to happen most
frequently, as well as how many times various pipeline stalls
happened.



The RX simulator offers many command line options:

-v - verbose output.  This prints some information about where the
program is being loaded and its starting address, as well as
information about how much memory was used and how many instructions
were executed during the run.  If specified twice, pipeline and cycle
information are added to the report.

-d - disassemble output.  Each instruction executed is printed.

-t - trace output.  Causes a *lot* of printed information about what
  every instruction is doing, from math results down to register
  changes.

--ignore-*
--warn-*
--error-*

  The RX simulator can detect certain types of memory corruption, and
  either ignore them, warn the user about them, or error and exit.
  Note that valid GCC code may trigger some of these, for example,
  writing a bitfield involves reading the existing value, which may
  not have been set yet.  The options for * are:

    null-deref - memory access to address zero.  You must modify your
      linker script to avoid putting anything at location zero, of
      course.

    unwritten-pages - attempts to read a page of memory (see below)
      before it is written.  This is much faster than the next option.

    unwritten-bytes - attempts to read individual bytes before they're
      written.

    corrupt-stack - On return from a subroutine, the memory location
      where $pc was stored is checked to see if anything other than
      $pc had been written to it most recently.

-i -w -e - these three options change the settings for all of the
  above.  For example, "-i" tells the simulator to ignore all memory
  corruption.

-E - end of options.  Any remaining options (after the program name)
  are considered to be options for the simulated program, although
  such functionality is not supported.



The RX simulator simulates a small number of peripherals, mostly in
order to provide I/O capabilities for testing and such.  The supported
peripherals, and their limitations, are documented here.

Memory

Memory for the simulator is stored in a hierarchical tree, much like
the i386's page directory and page tables.  The simulator can allocate
memory to individual pages as needed, allowing the simulated program
to act as if it had a full 4 Gb of RAM at its disposal, without
actually allocating more memory from the host operating system than
the simulated program actually uses.  Note that for each page of
memory, there's a corresponding page of memory *types* (for tracking
memory corruption).  Memory is initially filled with all zeros.

GPIO Port A

PA.DR is configured as an output-only port (regardless of PA.DDR).
When written to, a row of colored @ and * symbols are printed,
reflecting a row of eight LEDs being either on or off.

GPIO Port B

PB.DR controls the pipeline statistics.  Writing a 0 to PB.DR disables
statistics gathering.  Writing a non-0 to PB.DR resets all counters
and enables (even if already enabled) statistics gathering.  The
simulator starts with statistics enabled, so writing to PB.DR is not
needed if you want statistics on the entire program's run.

SCI4

SCI4.TDR is connected to the simulator's stdout.  Any byte written to
SCI4.TDR is written to stdout.  If the simulated program writes the
bytes 3, 3, and N in sequence, the simulator exits with an exit value
of N.

SCI4.SSR always returns "transmitter empty".


TPU1.TCNT
TPU2.TCNT

TPU1 and TPU2 are configured as a chained 32-bit counter which counts
machine cycles.  It always runs at "ICLK speed", regardless of the
clock control settings.  Writing to either of these 16-bit registers
zeros the counter, regardless of the value written.  Reading from
these registers returns the elapsed cycle count, with TPU1 holding the
most significant word and TPU2 holding the least significant word.

Note that, much like the hardware, these values may (TPU2.CNT *will*)
change between reads, so you must read TPU1.CNT, then TPU2.CNT, and
then TPU1.CNT again, and only trust the values if both reads of
TPU1.CNT were the same.

This directory contains the standard release of the ARMulator from
Advanced RISC Machines, and was ftp'd from.

ftp.cl.cam.ac.uk:/arm/gnu

It likes to use TCP/IP between the simulator and the host, which is
nice, but is a pain to use under anything non-unix.

I've added created a new Makefile.in (the original in Makefile.orig)
to build a version of the simulator without the TCP/IP stuff, and a
wrapper.c to link directly into gdb and the run command.

It should be possible (barring major changes in the layout of
the armulator) to upgrade the simulator by copying all the files
out of a release into this directory and renaming the Makefile.

(Except that I changed armos.c to work more simply with our
simulator rigs)

Steve

sac@cygnus.com

Mon May 15 12:03:28 PDT 1995




		PSIM 1.0.1 - Model of the PowerPC Environments


    Copyright (C) 1994-1996, Andrew Cagney <cagney@highland.com.au>.

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
 
    You should have received a copy of the GNU General Public License
    along with this program; if not, see <http://www.gnu.org/licenses/>.
 

    ----------------------------------------------------------------------


PSIM is a program written in extended ANSI-C that implements an
instruction level simulation of the PowerPC environment.  It is freely
available in source code form under the terms of the GNU General
Public License (version 3 or later).

The PowerPC Architecture is described as having three levels of
compliance:

	UEA - User Environment Architecture
	VEA - Virtual Environment Architecture
	OEA - Operating Environment Architecture

PSIM both implements all three levels of the PowerPC and includes (for
each level) a corresponding simulated run-time environment.

In addition, PSIM, to the execution unit level, models the performance
of most of the current PowerPC implementations (contributed by Michael
Meissner).  This detailed performance monitoring (unlike many other
simulators) resulting in only a relatively marginal reduction in the
simulators performance.


A description of how to build PSIM is contained in the file:

		ftp://ftp.ci.com.au/pub/psim/INSTALL
	or	ftp://cambridge.cygnus.com/pub/psim/INSTALL

while an overview of how to use PSIM is in:

	ftp://ftp.ci.com.au/pub/psim/RUN
or	ftp://cambridge.cygnus.com/pub/psim/RUN

This file is found in:

	ftp://ftp.ci.com.au/pub/psim/README
or	ftp://cambridge.cygnus.com/pub/psim/README


Thanks goes firstly to:

	Corinthian Engineering Pty Ltd
	Cygnus Support
	Highland Logic Pty Ltd

who provided the resources needed for making this software available
on the Internet.

More importantly I'd like to thank the following individuals who each
contributed in their own unique way:

	Allen Briggs, Bett Koch, David Edelsohn, Gordon Irlam,
	Michael Meissner, Bob Mercier, Richard Perini, Dale Rahn,
	Richard Stallman, Mitchele Walker


				Andrew Cagney
				Feb, 1995


    ----------------------------------------------------------------------


    What features does PSIM include?

	Monitoring and modeling

		PSIM includes (thanks to Michael Meissner)
		a detailed model of most of the PowerPC
		implementations to the functional unit level.


	SMP
		
		The PowerPC ISA defines SMP synchronizing instructions.
		This simulator implements a limited, but functional,
		subset of the PowerPC synchronization instructions
		behaviour.  Programs that restrict their synchronization
		primitives to those that work with this functional
		sub-set (eg P() and V()) are able to run on the SMP
		version of PSIM.

		People intending to use this system should study
		the code implementing the lwarx instruction.
		
	ENDIAN SUPPORT

		PSIM implements the PowerPC's big and little (xor
		endian) modes and correctly simulates code that
		switches between these two modes.

		In addition, psim can model a true little-endian
		machine.

	ISA (Instruction Set Architecture) models

		PSIM includes a model of the UEA, VEA and OEA.  This
		includes the time base registers (VEA) and HTAB
		and BATS (OEA).

		In addition, a preliminary model of the 64 bit
		PowerPC architecture is implemented.

	IO Hardware

		PSIM's internals are based around the concept
		of a Device Tree.  This tree intentionally
		resembles that of the Device Tree found in
		OpenBoot firmware.  PSIM is flexible enough
		to allow the user to fully configure this device
		tree (and consequently the hardware model) at
		run time.

	Run-time environments:

		PSIM's UEA model includes emulation for BSD
		based UNIX system calls.

		PSIM's OEA model includes emulation of either:

			o	OpenBoot client interface

			o	MOTO's BUG interface.


	Floating point

		Preliminary support for floating point is included.


    Who would be interested in PSIM?

	o	the curious

		Using psim, gdb, gcc and binutils the curious
		user can construct an environment that allows
		them to play with PowerPC Environment without
		the need for real hardware.


	o	the analyst

		PSIM includes many (contributed) monitoring
		features which (unlike many other simulators)
		do not come with a great penalty in performance.

		Thus the performance analyst is able to use
		this simulator to analyse the performance of
		the system under test.

		If PSIM doesn't monitor a components of interest,
		the source code is freely available, and hence
		there is no hinderance to changing things
		to meet a specific analysts needs.


	o	the serious SW developer

		PSIM models all three levels of the PowerPC
		Architecture: UEA, VEA and OEA.  Further,
		the internal design is such that PSIM can
		be extended to support additional requirements.


    What performance analysis measurements can PSIM perform?

	Below is the output from a recent analysis run
	(contributed by Michael Meissner):

	For the following program:

	long
	simple_rand ()
	{
	  static unsigned long seed = 47114711;
	  unsigned long this = seed * 1103515245 + 12345;
	  seed = this;
	/* cut-cut-cut - see the file RUN.psim */
	}

	Here is the current output generated with the -I switch on a P90
	(the compiler used is the development version of GCC with a new
	scheduler replacing the old one):
	
	CPU #1 executed     41,994 AND instructions.
	CPU #1 executed    519,785 AND Immediate instructions.
	.
	.
	.
	CPU #1 executed          1 System Call instruction.
	CPU #1 executed    207,746 XOR instructions.
	
	CPU #1 executed 23,740,856 cycles.
	CPU #1 executed 10,242,780 stalls waiting for data.
	CPU #1 executed          1 stall waiting for a function unit.
	.
	.
	.
	CPU #1 executed  3,136,229 branch functional unit instructions.
	CPU #1 executed 16,949,396 instructions that were accounted for in timing info.
	CPU #1 executed    871,920 data reads.
	CPU #1 executed    971,926 data writes.
	CPU #1 executed        221 icache misses.
	CPU #1 executed 16,949,396 instructions in total.
	
	Simulator speed was 250,731 instructions/second


    What motivated PSIM?

	As an idea, psim was first discussed seriously during mid
	1994.  At that time its main objectives were:


		o	good performance

			Many simulators loose out by only providing
			a binary interface to the internals.  This
			interface eventually becomes a bottle neck
			in the simulators performance.

			It was intended that PSIM would avoid this
			problem by giving the user access to the
			full source code.

			Further, by exploiting the power of modern
			compilers it was hoped that PSIM would achieve
			good performance with out having to compromise
			its internal design.


		o	practical portability

			Rather than try to be portable to every
			C compiler on every platform, it was decided
			that PSIM would restrict its self to supporting
			ANSI compilers that included the extension
			of a long long type.

			GCC is one such compiler, consequently PSIM
			should be portable to any machine running GCC.


		o	flexibility in its design

			PSIM should allow the user to select the
			features required and customise the build
			accordingly.  By having the source code,
			the compiler is able to eliminate any un
			used features of the simulator.

			After all, let the compiler do the work.


		o	SMP

			A model that allowed the simulation of
			SMP platforms with out the large overhead
			often encountered with such models.


	PSIM achieves each of these objectives.


    Is PSIM PowerPC Platform (PPCP) (nee CHRP) Compliant?

	No.

	Among other things it does not have an Apple ROM socket.


    Could PSIM be extended so that it models a CHRP machine?

	Yes.

	PSIM has been designed with the CHRP spec in mind. To model
	a CHRP desktop the following would need to be added:

		o	An apple ROM socket :-)

		o	Model of each of the desktop IO devices

		o	An OpenPIC device.

		o	RTAS (Run Time Abstraction Services).

		o	A fully populated device tree.


    Is the source code available?

	Yes.

	The source code to PSIM is available under the terms of
	the GNU Public Licence.  This allows you to distribute
	the source code for free but with certain conditions.

	See the file:

		ftp://archie.au/gnu/COPYING

	For details of the terms and conditions.


    Where do I send bugs or report problems?

	There is a mailing list (subscribe through majordomo@ci.com.au) at:

	powerpc-psim@ci.com.au

	If I get the ftp archive updated I post a note to that mailing list.
	In addition your welcome to send bugs or problems either to me or to
	that e-mail list.

	This list currently averages zero articles a day.


     Does PSIM have any limitations or problems?

	PSIM can't run rs6000/AIX binaries - At present PSIM can only
	simulate static executables.  Since an AIX executable is
	never static, PSIM is unable to simulate its execution.

	PSIM is still under development - consequently there are going
	to be bugs.

	See the file BUGS (included in the distribution) for any
	other outstanding issues.


1. MEC and ERC32 emulation

The file 'erc32.c' contains a model of the MEC, 512 K rom and 4 M ram.

The following paragraphs outline the implemented MEC functions.

1.1 UARTs

The UARTs are connected to two pseudo-devices, /dev/ttypc and /dev/ttypd.
The following registers are implemeted:

- UART A RX and TX register	(0x01f800e0)
- UART B RX and TX register	(0x01f800e4)
- UART status register		(0x01f800e8)

To speed up simulation, the UARTs operate at approximately 115200 baud. 
The UARTs generate interrupt 4 and 5 after each received or transmitted 
character.  The error interrupt is generated if overflow occurs - other
errors cannot occure.

1.2 Real-time clock and general pupose timer A

The following registers are implemeted:

- Real-time clock timer				(0x01f80080, read-only)
- Real-time clock scaler program register 	(0x01f80084, write-only)
- Real-time clock counter program register 	(0x01f80080, write-only)

- Genearl pupose timer 				(0x01f80088, read-only)
- Real-time clock scaler program register 	(0x01f8008c, write-only)
- General purpose timer counter prog. register 	(0x01f80088, write-only)

- Timer control register			(0x01f80098, write-only)

1.3 Interrupt controller

The interrupt controller is implemented as in the MEC specification with
the exception of the interrupt shape register. Since external interrupts
are not possible, the interrupt shape register is not implemented. The
only internal interrupts that are generated are the real-time clock, 
the general purpose timer and UARTs. However, all 15 interrupts
can be tested via the interrupt force register.

The following registers are implemeted:

- Interrupt pending register		       (0x01f80048, read-only)
- Interrupt mask register		       (0x01f8004c, read-write)
- Interrupt clear register		       (0x01f80050, write-only)
- Interrupt force register		       (0x01f80054, read-write)

1.4 Breakpoint and watchpoint register

The breakpoint and watchpoint functions are implemented as in the MEC
specification. Traps are correctly generated, and the system fault status
register is updated accordingly. Implemeted registers are:

- Debug control register			(0x01f800c0, read-write)
- Breakpoint register				(0x01f800c4, write-only)
- Watchpoint register				(0x01f800c8, write-only)
- System fault status register			(0x01f800a0, read-write)
- Firts failing address register		(0x01f800a4, read-write)


1.5 Memory interface

The following memory areas are valid for the ERC32 simulator:

0x00000000 - 0x00080000		ROM (512 Kbyte, loaded at start-up)
0x02000000 - 0x02400000		RAM (4 Mbyte, initialised to 0x0)
0x01f80000 - 0x01f800ff		MEC registers

Access to unimplemented MEC registers or non-existing memory will result
in a memory exception trap. However, access to unimplemented MEC registers
in the area 0x01f80000 - 0x01f80100 will not cause a memory exception trap.
The written value will be stored in a register and can be read back. It
does however not affect the function in any way. 

The memory configuartion register is used to define available memory
in the system. The fields RSIZ and PSIZ are used to set RAM and ROM
size, the remaining fields are not used.  NOTE: after reset, the MEC 
is set to decode 4 Kbyte of ROM and 256 Kbyte of RAM. The memory 
configuration register has to be updated to reflect the available memory. 

The waitstate configuration register is used to generate waitstates. 
This register must also be updated with the correct configuration after 
reset.

The memory protection scheme is implemented - it is enabled through bit 3
in the MEC control register.

The following registers are implemeted:

- MEC control register (bit 3 only)		(0x01f80000, read-write)
- Memory control register			(0x01f80010, read-write)
- Waitstate configuration register		(0x01f80018, read-write)
- Memory access register 0			(0x01f80020, read-write)
- Memory access register 1			(0x01f80024, read-write)

1.6 Watchdog

The watchdog is implemented as in the specification. The input clock is
always the system clock regardsless of WDCS bit in mec configuration 
register.

The following registers are implemeted:
 
- Watchdog program and acknowledge register	(0x01f80060, write-only)
- Watchdog trap door set register		(0x01f80064, write-only)

1.7 Software reset register

Implemented as in the specification (0x01f800004, write-only).

1.8 Power-down mode

The power-down register (0x01f800008) is implemented as in the specification.
However, if the simulator event queue is empty, power-down mode is not
entered since no interrupt would be generated to exit from the mode. A
Ctrl-C in the simulator window will exit the power-down mode.

1.9 MEC control register

The following bits are implemented in the MEC control register:

Bit	Name	Function
0	PRD	Power-down mode enable
1	SWR	Soft reset enable
3	APR	Access protection enable

How to use SIS with GDB
-----------------------

1. Building GDB with SIS

To build GDB with the SIS/ERC32 simulator, configure with option
'--target sparc-erc32-aout' and build as usual.

2. Attaching the simulator

To attach GDB to the simulator, use:

target sim [options] [files]

The following options are supported:

 -nfp		Disable FPU. FPops will cause an FPU disabled trap.

 -freq <f>	Set the simulated "system clock" to <f> MHz.

 -v		Verbose mode.

 -nogdb		Disable GDB breakpoint handling (see below)

The listed [files] are expected to be in aout format and will be
loaded in the simulator memory prior. This could be used to load
a boot block at address 0x0 if the application is linked to run
from RAM (0x2000000).

To start debugging a program type 'load <program>' and debug as
usual. 

The native simulator commands can be reached using the GDB 'sim'
command:

sim <sis_command>

Direct simulator commands during a GDB session must be issued
with care not to disturb GDB's operation ... 

For info on supported ERC32 functionality, see README.sis.


3. Loading aout files

The GDB load command loads an aout file into the simulator
memory with the data section starting directly after the text
section regardless of wich start address was specified for the data
at link time! This means that your applications either has to include
a routine that initialise the data segment at the proper address or
link with the data placed directly after the text section.

A copying routine is fairly simple, just copy all data between
_etext and _data to a memory loaction starting at _environ. This
should be done at the same time as the bss is cleared (in srt0.s).


4. GDB breakpoint handling

GDB inserts breakpoint in the form of the 'ta 1' instruction. The
GDB-integrated simulator will therefore recognize the breakpoint
instruction and return control to GDB. If the application uses
'ta 1', the breakpoint detection can be disabled with the -nogdb
switch. In this case however, GDB breakpoints will not work.


Report problems to Jiri Gaisler ESA/ESTEC (jgais@wd.estec.esa.nl)

SIS - Sparc Instruction Simulator README file  (v2.0, 05-02-1996)
-------------------------------------------------------------------

1. Introduction

The SIS is a SPARC V7 architecture simulator. It consist of two parts,
the simulator core and a user defined memory module. The simulator
core executes the instructions while the memory module emulates memory
and peripherals. 

2. Usage

The simulator is started as follows: 

sis [-uart1 uart_device1] [-uart2 uart_device2] 
    [-nfp] [-freq frequency] [-c batch_file] [files] 

The default uart devices for SIS are /dev/ptypc and /dev/ptypd. The
-uart[1,2] switch can be used to connect the uarts to other devices.
Use 'tip /dev/ttypc'  to connect a terminal emulator to the uarts.
The '-nfp' will disable the simulated FPU, so each FPU instruction will
generate a FPU disabled trap. The '-freq' switch can be used to define
which "frequency" the simulator runs at. This is used by the 'perf'
command to calculated the MIPS figure for a particular configuration.
The give frequency must be an integer indicating the frequency in MHz.

The -c option indicates that sis commands should be read from 'batch_file' 
at startup.

Files to be loaded must be in one of the supported formats (see INSTALLATION),
and will be loaded into the simulated memory. The file formats are
automatically recognised.

The script 'startsim' will start the simulator in one xterm window and
open a terminal emulator (tip) connected to the UART A in a second
xterm window. Below is description of commands  that are recognized by 
the simulator. The command-line is parsed using GNU readline. A command
history of 64 commands is maintained. Use the up/down arrows to recall
previous commands. For more details, see the readline documentation.

batch <file>

Execute a batch file of SIS commands.

+bp <address>

Adds an breakpoint at address <address>.

bp

Prints all breakpoints

-bp <num>

Deletes breakpoint <num>. Use 'bp' to see which number is assigned to the 
breakpoints.

cont [inst_count]

Continue execution at present position, optionally for [inst_count] 
instructions.

dis [addr] [count]

Disassemble [count] instructions at address [addr]. Default values for
count is 16 and addr is the present address.

echo <string>

Print <string> to the simulator window.

float

Prints the FPU registers

go <address> [inst_count]

The go command will set pc to <address> and npc to <address> + 4, and start
execution. No other initialisation will be done. If inst_count is given, 
execution will stop after the specified number of instructions.

help

Print a small help menu for the SIS commands.

hist [trace_length]

Enable the instruction trace buffer. The 'trace_length' last executed 
instructions will be placed in the trace buffer. A 'hist' command without 
a trace_length will display the trace buffer. Specifying a zero trace 
length will disable the trace buffer.

load  <file_name>

Loads a file into simulator memory. 

mem [addr] [count]

Display memory at [addr] for [count] bytes. Same default values as above.

quit

Exits the simulator.

perf [reset]

The 'perf' command will display various execution statistics. A 'perf reset' 
command will reset the statistics. This can be used if statistics shall 
be calculated only over a part of the program. The 'run' and 'reset' 
command also resets the statistic information.

reg [reg_name] [value]

Prints and sets the IU regiters. 'reg' without parameters prints the IU
registers. 'reg [reg_name] [value]' sets the corresponding register to
[value]. Valid register names are psr, tbr, wim, y, g1-g7, o0-o7 and
l0-l7.

reset

Performs a power-on reset. This command is equal to 'run 0'.

run [inst_count]

Resets the simulator and starts execution from address 0. If an instruction
count is given (inst_count), the simulator will stop after the specified 
number of instructions. The event queue is emptied but any set breakpoints
remain.

step

Equal to 'trace 1'

tra [inst_count]

Starts the simulator at the present position and prints each instruction
it executes. If an instruction count is given (inst_count), the simulator 
will stop after the specified number of instructions.

Typing a 'Ctrl-C' will interrupt a running simulator. 

Short forms of the commands are allowed, e.g 'c' 'co' or 'con' are all
interpreted as 'cont'. 


3. Simulator core

The SIS emulates the behavior of the 90C601E and 90C602E sparc IU and
FPU from Matra MHS. These are roughly equivalent to the Cypress C601
and C602.  The simulator is cycle true, i.e a simulator time is
maintained and inremented according the IU and FPU instruction timing.
The parallel execution between the IU and FPU is modelled, as well as
stalls due to operand dependencies (FPU). The core interacts with the
user-defined memory modules through a number of functions. The memory
module must provide the following functions:

int memory_read(asi,addr,data,ws)
int asi;
unsigned int addr;
unsigned int *data;
int *ws;

int memory_write(asi,addr,data,sz,ws)
int asi;
unsigned int addr;
unsigned int *data;
int sz;
int *ws;

int sis_memory_read(addr, data, length)
unsigned int addr;
char   *data;
unsigned int length;

int sis_memory_write(addr, data, length)
unsigned int addr;
char    *data;
unsigned int length;

int init_sim()

int reset()

int error_mode(pc)
unsigned int pc;

memory_read() is used by the simulator to fetch instructions and
operands.  The address space identifier (asi) and address is passed as
parameters. The read data should be assigned to the data pointer
(*data) and the number of waitstate to *ws. 'memory_read' should return
0 on success and 1 on failure. A failure will cause a data or
instruction fetch trap. memory_read() always reads one 32-bit word.

sis_memory_read() is used by the simulator to display and disassemble
memory contants. The function should copy 'length' bytes of the simulated
memory starting at 'addr' to '*data'.
The sis_memory_read() should return 1 on success and 0 on failure.
Failure should only be indicated if access to unimplemented memory is attempted.

memory_write() is used to write to memory. In addition to the asi
and address parameters, the size of the written data is given by 'sz'.
The pointer *data points to the data to be written. The 'sz' is coded
as follows:

  sz	access type
  0	  byte
  1	  halfword
  2	  word
  3	  double-word

If a double word is written, the most significant word is in data[0] and
the least significant in data[1].

sis_memory_write() is used by the simulator during loading of programs.
The function should copy 'length' bytes from *data to the simulated
memory starting at 'addr'. sis_memory_write() should return 1 on 
success and 0 on failure. Failure should only be indicated if access 
to unimplemented memory is attempted. See erc32.c for more details 
on how to define the memory emulation functions.

The 'init_sim' is called once when the simulator is started. This function
should be used to perform initialisations of user defined memory or 
peripherals that only have to be done once, such as opening files etc.

The 'reset' is called every time the simulator is reset, i.e. when a
'run' command is given. This function should be used to simulate a power
on reset of memory and peripherals.

error_mode() is called by the simulator when the IU goes into error mode,
typically if a trap is caused when traps are disabled. The memory module
can then take actions, such as issue a reset.

sys_reset() can be called by the memory module to reset the simulator. A
reset will empty the event queue and perform a power-on reset.

4. Events and interrupts

The simulator supports an event queue and the generation of processor
interrupts. The following functions are available to the user-defined
memory module:

event(cfunc,arg,delta)
void (*cfunc)();
int arg;
unsigned int delta;

set_int(level,callback,arg)
int level;
void (*callback)();
int arg;

clear_int(level)
int level;

sim_stop()

The 'event' functions will schedule the execution of the function 'cfunc'
at time 'now + delta' clock cycles. The parameter 'arg' is passed as a 
parameter to 'cfunc'.

The 'set_int' function set the processor interrupt 'level'. When the interrupt
is taken, the function 'callback' is called with the argument 'arg'. This
will also clear the interrupt. An interrupt can be cleared before it is
taken by calling 'clear_int' with the appropriate interrupt level.

The sim_stop function is called each time the simulator stops execution.
It can be used to flush buffered devices to get a clean state during
single stepping etc.

See 'erc32.c' for examples on how to use events and interrupts.

5. Memory module

The supplied memory module (erc32.c) emulates the functions of memory and
the MEC asic developed for the 90C601/2. It includes the following functions:

* UART A & B
* Real-time clock
* General purpose timer
* Interrupt controller
* Breakpoint register
* Watchpoint register
* 512 Kbyte ROM
* 4 Mbyte RAM

See README.erc32 on how the MEC functions are emulated.  For a detailed MEC
specification, look at the ERC32 home page at URL:

http://www.estec.esa.nl/wsmwww/erc32

6. Compile and linking programs

The directory 'examples' contain some code fragments for SIS.
The script gccx indicates how the native sunos gcc and linker can be used
to produce executables for the simulator. To compile and link the provided
'hello.c', type 'gccx hello.c'. This will build the executable 'hello'.
Start the simulator by running 'startsim hello', and issue the command 'run.
After the program is terminated, the IU will be force to error mode through
a software trap and halt. 

The programs are linked with a start-up file, srt0.S. This file includes
the traptable and window underflow/overflow trap routines.

7. IU and FPU instruction timing.

The simulator provides cycle true simulation. The following table shows
the emulated instruction timing for 90C601E & 90C602E:

Instructions	      Cycles

jmpl, rett		2
load			2
store			3
load double		3
store double		4
other integer ops	1
fabs			2
fadds			4
faddd			4
fcmps			4
fcmpd			4
fdivs			20
fdivd			35
fmovs			2
fmuls			5
fmuld			9
fnegs			2
fsqrts			37
fsqrtd			65
fsubs			4
fsubd			4
fdtoi			7
fdots			3
fitos			6
fitod			6
fstoi			6
fstod			2

The parallel operation between the IU and FPU is modelled. This means
that a FPU instruction will execute in parallel with other instructions as
long as no data or resource dependency is detected. See the 90C602E data
sheet for the various types of dependencies. Tracing using the 'trace'
command will display the current simulator time in the left column. This
time indicates when the instruction is fetched. If a dependency is detetected,
the following fetch will be delayed until the conflict is resolved.

The load dependency in the 90C601E is also modelled - if the destination 
register of a load instruction is used by the following instruction, an 
idle cycle is inserted.

8. FPU implementation

The simulator maps floating-point operations on the hosts floating point
capabilities. This means that accuracy and generation of IEEE exceptions is 
host dependent.
This is the frv simulator directory.

It is still work-in-progress.  The current sources are
well tested and lots of features are in.

There are lots of machine generated files in the source directory!
They are only generated if you configure with --enable-cgen-maint,
similar in behaviour to Makefile.in, configure under automake/autoconf.

For details on the generator, see ../../cgen.
This directory contains the zlib package, which is not part of GCC but
shipped with GCC as convenience.

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.7 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://tools.ietf.org/html/rfc1950 (zlib format), rfc1951 (deflate format) and
rfc1952 (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  A usage example
of the library is given in the file test/example.c which also tests that
the library is working correctly.  Another example is given in the file
test/minigzip.c.  The compression library itself is composed of all source
files in the root directory.

To compile all files and run the test program, follow the instructions given at
the top of Makefile.in.  In short "./configure; make test", and if that goes
well, "make install" should work for most flavors of Unix.  For Windows, use
one of the special makefiles in win32/ or contrib/vstudio/ .  For VMS, use
make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version.  The zlib home page is
http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read the zlib FAQ http://zlib.net/zlib_faq.html before asking for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan.  1997
issue of Dr.  Dobb's Journal; a copy of the article is available at
http://marknelson.us/1997/01/01/zlib-engine/ .

The changes made in version 1.2.7 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory contrib/ .

zlib is available in Java using the java.util.zip package, documented at
http://java.sun.com/developer/technicalArticles/Programming/compression/ .

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is available
at CPAN (Comprehensive Perl Archive Network) sites, including
http://search.cpan.org/~pmqs/IO-Compress-Zlib/ .

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://docs.python.org/library/zlib.html .

zlib is built into tcl: http://wiki.tcl.tk/4610 .

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS or BEOS.

- For PalmOs, see http://palmzlib.sourceforge.net/


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz.  The deflate and
  zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
  people who reported problems and suggested various improvements in zlib; they
  are too numerous to cite here.

Copyright notice:

 (C) 1995-2012 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
This directory contains examples of the use of zlib and other relevant
programs and documentation.

enough.c
    calculation and justification of ENOUGH parameter in inftrees.h
    - calculates the maximum table space used in inflate tree
      construction over all possible Huffman codes

fitblk.c
    compress just enough input to nearly fill a requested output size
    - zlib isn't designed to do this, but fitblk does it anyway

gun.c
    uncompress a gzip file
    - illustrates the use of inflateBack() for high speed file-to-file
      decompression using call-back functions
    - is approximately twice as fast as gzip -d
    - also provides Unix uncompress functionality, again twice as fast

gzappend.c
    append to a gzip file
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of deflatePrime() to start at any bit

gzjoin.c
    join gzip files without recalculating the crc or recompressing
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of crc32_combine()

gzlog.c
gzlog.h
    efficiently and robustly maintain a message log file in gzip format
    - illustrates use of raw deflate, Z_PARTIAL_FLUSH, deflatePrime(),
      and deflateSetDictionary()
    - illustrates use of a gzip header extra field

zlib_how.html
    painfully comprehensive description of zpipe.c (see below)
    - describes in excruciating detail the use of deflate() and inflate()

zpipe.c
    reads and writes zlib streams from stdin to stdout
    - illustrates the proper use of deflate() and inflate()
    - deeply commented in zlib_how.html (see above)

zran.c
    index a zlib or gzip stream and randomly access it
    - illustrates the use of Z_BLOCK, inflatePrime(), and
      inflateSetDictionary() to provide random access
Puff -- A Simple Inflate
3 Mar 2003
Mark Adler
madler@alumni.caltech.edu

What this is --

puff.c provides the routine puff() to decompress the deflate data format.  It
does so more slowly than zlib, but the code is about one-fifth the size of the
inflate code in zlib, and written to be very easy to read.

Why I wrote this --

puff.c was written to document the deflate format unambiguously, by virtue of
being working C code.  It is meant to supplement RFC 1951, which formally
describes the deflate format.  I have received many questions on details of the
deflate format, and I hope that reading this code will answer those questions.
puff.c is heavily commented with details of the deflate format, especially
those little nooks and cranies of the format that might not be obvious from a
specification.

puff.c may also be useful in applications where code size or memory usage is a
very limited resource, and speed is not as important.

How to use it --

Well, most likely you should just be reading puff.c and using zlib for actual
applications, but if you must ...

Include puff.h in your code, which provides this prototype:

int puff(unsigned char *dest,           /* pointer to destination pointer */
         unsigned long *destlen,        /* amount of output space */
         unsigned char *source,         /* pointer to source data pointer */
         unsigned long *sourcelen);     /* amount of input available */

Then you can call puff() to decompress a deflate stream that is in memory in
its entirety at source, to a sufficiently sized block of memory for the
decompressed data at dest.  puff() is the only external symbol in puff.c  The
only C library functions that puff.c needs are setjmp() and longjmp(), which
are used to simplify error checking in the code to improve readabilty.  puff.c
does no memory allocation, and uses less than 2K bytes off of the stack.

If destlen is not enough space for the uncompressed data, then inflate will
return an error without writing more than destlen bytes.  Note that this means
that in order to decompress the deflate data successfully, you need to know
the size of the uncompressed data ahead of time.

If needed, puff() can determine the size of the uncompressed data with no
output space.  This is done by passing dest equal to (unsigned char *)0.  Then
the initial value of *destlen is ignored and *destlen is set to the length of
the uncompressed data.  So if the size of the uncompressed data is not known,
then two passes of puff() can be used--first to determine the size, and second
to do the actual inflation after allocating the appropriate memory.  Not
pretty, but it works.  (This is one of the reasons you should be using zlib.)

The deflate format is self-terminating.  If the deflate stream does not end
in *sourcelen bytes, puff() will return an error without reading at or past
endsource.

On return, *sourcelen is updated to the amount of input data consumed, and
*destlen is updated to the size of the uncompressed data.  See the comments
in puff.c for the possible return codes for puff().
These classes provide a C++ stream interface to the zlib library. It allows you
to do things like:

  gzofstream outf("blah.gz");
  outf << "These go into the gzip file " << 123 << endl;

It does this by deriving a specialized stream buffer for gzipped files, which is
the way Stroustrup would have done it. :->

The gzifstream and gzofstream classes were originally written by Kevin Ruland
and made available in the zlib contrib/iostream directory. The older version still
compiles under gcc 2.xx, but not under gcc 3.xx, which sparked the development of
this version.

The new classes are as standard-compliant as possible, closely following the
approach of the standard library's fstream classes. It compiles under gcc versions
3.2 and 3.3, but not under gcc 2.xx. This is mainly due to changes in the standard
library naming scheme. The new version of gzifstream/gzofstream/gzfilebuf differs
from the previous one in the following respects:
- added showmanyc
- added setbuf, with support for unbuffered output via setbuf(0,0)
- a few bug fixes of stream behavior
- gzipped output file opened with default compression level instead of maximum level
- setcompressionlevel()/strategy() members replaced by single setcompression()

The code is provided "as is", with the permission to use, copy, modify, distribute
and sell it for any purpose without fee.

Ludwig Schwardt
<schwardt@sun.ac.za>

DSP Lab
Electrical & Electronic Engineering Department
University of Stellenbosch
South Africa
This is a patched version of zlib, modified to use
Pentium-Pro-optimized assembly code in the deflation algorithm. The
files changed/added by this patch are:

README.686
match.S

The speedup that this patch provides varies, depending on whether the
compiler used to build the original version of zlib falls afoul of the
PPro's speed traps. My own tests show a speedup of around 10-20% at
the default compression level, and 20-30% using -9, against a version
compiled using gcc 2.7.2.3. Your mileage may vary.

Note that this code has been tailored for the PPro/PII in particular,
and will not perform particuarly well on a Pentium.

If you are using an assembler other than GNU as, you will have to
translate match.S to use your assembler's syntax. (Have fun.)

Brian Raiter
breadbox@muppetlabs.com
April, 1998


Added for zlib 1.1.3:

The patches come from
http://www.muppetlabs.com/~breadbox/software/assembly.html

To compile zlib with this asm file, copy match.S to the zlib directory
then do:

CFLAGS="-O3 -DASMV" ./configure
make OBJA=match.o


Update:

I've been ignoring these assembly routines for years, believing that
gcc's generated code had caught up with it sometime around gcc 2.95
and the major rearchitecting of the Pentium 4. However, I recently
learned that, despite what I believed, this code still has some life
in it. On the Pentium 4 and AMD64 chips, it continues to run about 8%
faster than the code produced by gcc 4.1.

In acknowledgement of its continuing usefulness, I've altered the
license to match that of the rest of zlib. Share and Enjoy!

Brian Raiter
breadbox@muppetlabs.com
April, 2007
Read blast.h for purpose and usage.

Mark Adler
madler@alumni.caltech.edu
See infback9.h for what this is and how to use it.
This directory contains files that have not been updated for zlib 1.2.x

(Volunteers are encouraged to help clean this up.  Thanks.)
This Makefile requires devkitARM (http://www.devkitpro.org/category/devkitarm/) and works inside "contrib/nds". It is based on a devkitARM template.

Eduardo Costa <eduardo.m.costa@gmail.com>
January 3, 2009

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.7 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://www.ietf.org/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate format)
and rfc1952.txt (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  Two compiled
examples are distributed in this package, example and minigzip.  The example_d
and minigzip_d flavors validate that the zlib1.dll file is working correctly.

Questions about zlib should be sent to <zlib@gzip.org>.  The zlib home page
is http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read DLL_FAQ.txt, and the the zlib FAQ http://zlib.net/zlib_faq.html
before asking for help.


Manifest:

The package zlib-1.2.7-win32-x86.zip will contain the following files:

  README-WIN32.txt This document
  ChangeLog        Changes since previous zlib packages
  DLL_FAQ.txt      Frequently asked questions about zlib1.dll
  zlib.3.pdf       Documentation of this library in Adobe Acrobat format

  example.exe      A statically-bound example (using zlib.lib, not the dll)
  example.pdb      Symbolic information for debugging example.exe

  example_d.exe    A zlib1.dll bound example (using zdll.lib)
  example_d.pdb    Symbolic information for debugging example_d.exe

  minigzip.exe     A statically-bound test program (using zlib.lib, not the dll)
  minigzip.pdb     Symbolic information for debugging minigzip.exe

  minigzip_d.exe   A zlib1.dll bound test program (using zdll.lib)
  minigzip_d.pdb   Symbolic information for debugging minigzip_d.exe

  zlib.h           Install these files into the compilers' INCLUDE path to
  zconf.h          compile programs which use zlib.lib or zdll.lib

  zdll.lib         Install these files into the compilers' LIB path if linking
  zdll.exp         a compiled program to the zlib1.dll binary

  zlib.lib         Install these files into the compilers' LIB path to link zlib
  zlib.pdb         into compiled programs, without zlib1.dll runtime dependency
                   (zlib.pdb provides debugging info to the compile time linker)

  zlib1.dll        Install this binary shared library into the system PATH, or
                   the program's runtime directory (where the .exe resides)
  zlib1.pdb        Install in the same directory as zlib1.dll, in order to debug
                   an application crash using WinDbg or similar tools.

All .pdb files above are entirely optional, but are very useful to a developer
attempting to diagnose program misbehavior or a crash.  Many additional
important files for developers can be found in the zlib127.zip source package
available from http://zlib.net/ - review that package's README file for details.


Acknowledgments:

The deflate format used by zlib was defined by Phil Katz.  The deflate and
zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
people who reported problems and suggested various improvements in zlib; they
are too numerous to cite here.


Copyright notice:

  (C) 1995-2012 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
Introduction
============

This is the Gnu Readline library, version 6.2.

The Readline library provides a set of functions for use by applications
that allow users to edit command lines as they are typed in.  Both
Emacs and vi editing modes are available.  The Readline library includes
additional functions to maintain a list of previously-entered command
lines, to recall and perhaps reedit those lines, and perform csh-like
history expansion on previous commands.

The history facilites are also placed into a separate library, the
History library, as part of the build process.  The History library
may be used without Readline in applications which desire its
capabilities.

The Readline library is free software, distributed under the terms of
the [GNU] General Public License as published by the Free Software
Foundation, version 3 of the License.  For more information, see the
file COPYING.

To build the library, try typing `./configure', then `make'.  The
configuration process is automated, so no further intervention should
be necessary.  Readline builds with `gcc' by default if it is
available.  If you want to use `cc' instead, type

        CC=cc ./configure

if you are using a Bourne-style shell.  If you are not, the following
may work:

        env CC=cc ./configure

Read the file INSTALL in this directory for more information about how
to customize and control the build process.

The file rlconf.h contains C preprocessor defines that enable and disable
certain Readline features.

The special make target `everything' will build the static and shared
libraries (if the target platform supports them) and the examples.

Examples
========

There are several example programs that use Readline features in the
examples directory.  The `rl' program is of particular interest.  It
is a command-line interface to Readline, suitable for use in shell
scripts in place of `read'.

Shared Libraries
================

There is skeletal support for building shared versions of the
Readline and History libraries.  The configure script creates
a Makefile in the `shlib' subdirectory, and typing `make shared'
will cause shared versions of the Readline and History libraries
to be built on supported platforms.

If `configure' is given the `--enable-shared' option, it will attempt
to build the shared libraries by default on supported platforms.

Configure calls the script support/shobj-conf to test whether or
not shared library creation is supported and to generate the values
of variables that are substituted into shlib/Makefile.  If you
try to build shared libraries on an unsupported platform, `make'
will display a message asking you to update support/shobj-conf for
your platform.

If you need to update support/shobj-conf, you will need to create
a `stanza' for your operating system and compiler.  The script uses
the value of host_os and ${CC} as determined by configure.  For
instance, FreeBSD 4.2 with any version of gcc is identified as
`freebsd4.2-gcc*'.

In the stanza for your operating system-compiler pair, you will need to
define several variables.  They are:

SHOBJ_CC	The C compiler used to compile source files into shareable
		object files.  This is normally set to the value of ${CC}
		by configure, and should not need to be changed.

SHOBJ_CFLAGS	Flags to pass to the C compiler ($SHOBJ_CC) to create
		position-independent code.  If you are using gcc, this
		should probably be set to `-fpic'.

SHOBJ_LD	The link editor to be used to create the shared library from
		the object files created by $SHOBJ_CC.  If you are using
		gcc, a value of `gcc' will probably work.

SHOBJ_LDFLAGS	Flags to pass to SHOBJ_LD to enable shared object creation.
		If you are using gcc, `-shared' may be all that is necessary.
		These should be the flags needed for generic shared object
		creation.

SHLIB_XLDFLAGS	Additional flags to pass to SHOBJ_LD for shared library
		creation.  Many systems use the -R option to the link
		editor to embed a path within the library for run-time
		library searches.  A reasonable value for such systems would
		be `-R$(libdir)'.

SHLIB_LIBS	Any additional libraries that shared libraries should be
		linked against when they are created.

SHLIB_LIBPREF	The prefix to use when generating the filename of the shared
		library.  The default is `lib'; Cygwin uses `cyg'.

SHLIB_LIBSUFF	The suffix to add to `libreadline' and `libhistory' when
		generating the filename of the shared library.  Many systems
		use `so'; HP-UX uses `sl'.

SHLIB_LIBVERSION The string to append to the filename to indicate the version
		of the shared library.  It should begin with $(SHLIB_LIBSUFF),
		and possibly include version information that allows the
		run-time loader to load the version of the shared library
		appropriate for a particular program.  Systems using shared
		libraries similar to SunOS 4.x use major and minor library
		version numbers; for those systems a value of
		`$(SHLIB_LIBSUFF).$(SHLIB_MAJOR)$(SHLIB_MINOR)' is appropriate.
		Systems based on System V Release 4 don't use minor version
		numbers; use `$(SHLIB_LIBSUFF).$(SHLIB_MAJOR)' on those systems.
		Other Unix versions use different schemes.

SHLIB_DLLVERSION The version number for shared libraries that determines API
		compatibility between readline versions and the underlying
		system.  Used only on Cygwin.  Defaults to $SHLIB_MAJOR, but
		can be overridden at configuration time by defining DLLVERSION
		in the environment.

SHLIB_DOT	The character used to separate the name of the shared library
		from the suffix and version information.  The default is `.';
		systems like Cygwin which don't separate version information
		from the library name should set this to the empty string.

SHLIB_STATUS	Set this to `supported' when you have defined the other
		necessary variables.  Make uses this to determine whether
		or not shared library creation should be attempted.

You should look at the existing stanzas in support/shobj-conf for ideas.

Once you have updated support/shobj-conf, re-run configure and type
`make shared'.  The shared libraries will be created in the shlib
subdirectory.

If shared libraries are created, `make install' will install them. 
You may install only the shared libraries by running `make
install-shared' from the top-level build directory.  Running `make
install' in the shlib subdirectory will also work.  If you don't want
to install any created shared libraries, run `make install-static'. 

Documentation
=============

The documentation for the Readline and History libraries appears in
the `doc' subdirectory.  There are three texinfo files and a
Unix-style manual page describing the facilities available in the
Readline library.  The texinfo files include both user and
programmer's manuals.  HTML versions of the manuals appear in the
`doc' subdirectory as well. 

Reporting Bugs
==============

Bug reports for Readline should be sent to:

        bug-readline@gnu.org

When reporting a bug, please include the following information:

        * the version number and release status of Readline (e.g., 4.2-release)
        * the machine and OS that it is running on
        * a list of the compilation flags or the contents of `config.h', if
          appropriate
        * a description of the bug
        * a recipe for recreating the bug reliably
        * a fix for the bug if you have one!

If you would like to contact the Readline maintainer directly, send mail
to bash-maintainers@gnu.org.

Since Readline is developed along with bash, the bug-bash@gnu.org mailing
list (mirrored to the Usenet newsgroup gnu.bash.bug) often contains
Readline bug reports and fixes. 

Chet Ramey
chet.ramey@case.edu
rlfe (ReadLine Front-End) is a "universal wrapper" around readline.
You specify an interactive program to run (typically a shell), and
readline is used to edit input lines.

There are other such front-ends; what distinguishes this one is that
it monitors the state of the inferior pty, and if the inferior program
switches its terminal to raw mode, then rlfe passes your characters
through directly.  This basically means you can run your entire
session (including bash and terminal-mode emacs) under rlfe.

FEATURES

* Can use all readline commands (and history) in commands that
read input lines in "canonical mode" - even 'cat'!

* Automatically switches between "readline-editing mode" and "raw mode"
depending on the terminal mode.  If the inferior program invokes
readline itself, it will do its own line editing.  (The inferior
readline will not know about rlfe, and it will have its own history.)
You can even run programs like 'emavs -nw' and 'vi' under rlfe.
The goal is you could leave rlfe always on without even knowing
about it.  (We're not quite there, but it works tolerably well.)

* The input line (after any prompt) is changed to bold-face.

INSTALL

The usual: ./configure && make && make install

Note so far rlfe has only been tested on GNU Linux (Fedora Core 2)
and Mac OS X (10.3).

This assumes readline header files and libraries are in the default
places.  If not, you can create a link named readline pointing to the
readline sources.  To link with libreadline.a and libhistory.a
you can copy or link them, or add LDFLAGS='-/path/to/readline' to
the make command-line.

USAGE

Just run it.  That by default runs bash.  You can run some other
command by giving it as command-line arguments.

There are a few tweaks:  -h allows you to name the history file,
and -s allows you to specify its size.  It default to "emacs" mode,
but if the the environment variable EDITOR is set to "vi" that
mode is chosen.

ISSUES

* The mode switching depends on the terminal mode set by the inferior
program.  Thus ssh/telnet/screen-type programs will typically be in
raw mode, so rlfe won't be much use, even if remote programs run in
canonical mode.  The work-around is to run rlfe on the remote end.

* Echo supression and prompt recognition are somewhat fragile.
(A protocol so that the o/s tty code can reliably communicate its
state to rlfe could solve this problem, and the previous one.)

* See the intro to rlfe.c for more notes.

* Assumes a VT100-compatible terminal, though that could be generalized
if anybody cares.

* Requires ncurses.

* It would be useful to integrate rlfe's logic in a terminal emulator.
That would make it easier to reposition the edit position with a mouse,
integrate cut-and-paste with the system clipboard, and more robustly
handle escape sequence and multi-byte characters more robustly.

AUTHOR

Per Bothner <per@bothner.com>

LICENSE

GPL.
-*- text -*-

Autoconf

Autoconf is an extensible package of M4 macros that produce shell
scripts to automatically configure software source code packages.
These scripts can adapt the packages to many kinds of UNIX-like
systems without manual user intervention.  Autoconf creates a
configuration script for a package from a template file that lists the
operating system features that the package can use, in the form of M4
macro calls.

Producing configuration scripts using Autoconf requires GNU M4 and
Perl.  You should install GNU M4 (version 1.4.6 or later is required;
1.4.14 or later is recommended) and Perl (5.006 or later) before
configuring Autoconf, so that Autoconf's configure script can find
them.  The configuration scripts produced by Autoconf are
self-contained, so their users do not need to have Autoconf (or GNU
M4, Perl, etc.).

You can get GNU M4 here:

ftp://ftp.gnu.org/gnu/m4/

The file INSTALL should be distributed with packages that use
Autoconf-generated configure scripts and Makefiles that conform to the
GNU coding standards.  The package's README can just give an overview
of the package, where to report bugs, and a pointer to INSTALL for
instructions on compilation and installation.  This removes the need
to maintain many similar sets of installation instructions.

Be sure to read BUGS and INSTALL.

Mail suggestions to autoconf@gnu.org, report bugs to
bug-autoconf@gnu.org, and submit patches to autoconf-patches@gnu.org.
All changes can be tracked at the read-only autoconf-commit@gnu.org.
Always include the Autoconf version number, which you can get by
running `autoconf --version'.  Archives of bug-autoconf@gnu.org can be
found in <http://lists.gnu.org/archive/html/bug-autoconf/>, and
similarly for the other mailing lists.

Licensing

Autoconf is released under the General Public License version 3 (GPLv3+).
Additionally, Autoconf includes a licensing exception in some of its
source files; see the respective copyright notices for how your
project is impacted by including scripts generated by Autoconf, and the
COPYING.EXCEPTION file for the exception in terms of the Additional
Permissions as described in section 7 of GPLv3.

For more licensing information, see
<http://www.gnu.org/licenses/gpl-faq.html> and
<http://www.gnu.org/licenses/exceptions.html>.

For any copyright year range specified as YYYY-ZZZZ in this package
note that the range specifies every single year in that closed interval.

-----

Copyright (C) 1992-1994, 1998, 2000-2012 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.  This file is offered as-is,
without warranty of any kind.
$NetBSD: README.gcc53,v 1.21 2016/06/09 23:36:53 mrg Exp $

lib:
	libasan is disabled currently (haven't tried yet)

new stuff:
	cc1objcplus
	collect2 -- need to re-introduce?
	libcilkrts
	libmpx
	liboffloadmic
	libvtv
	libitm

other changes to look at:

Index: gcc/targhooks.c
	vs new binutils

+           doc/gcov-tool.1


arch/feature list.  anything not here has been switched already:

tools:		does build.sh tools work?
kernels:	does a kernel run?
libgcc:		does libgcc build?
native-gcc:	does a full mknative-gcc complete?
make release:	does build.sh release complete? 
runs:		does the system boot with a full world?
atf:		does atf run / compare well


architecture	tools	kernels	libgcc	native-gcc	make release	runs	atf
------------	-----	-------	------	----------	------------	----	---
arm		y	?	y	y		n[3]
armeb		y	?	y	y		n[3]
coldfire	y	N/A	y	y		?		N/A	N/A
earm		y	?	y	y		y[3,13]
earmeb		y	?	y	y		y[3,13]
earmhf		y	?	y	y		y[3]
earmhfeb	y	?	y	y		y[3]
earmv4		y	y	y	y		y[3]		y	y
earmv4eb	y	?	y	y		y[3]
earmv6		y	?	y	y		y[3]
earmv6eb	y	?	y	y		y[3]
earmv6hf	y	?	y	y		y[3]
earmv6hfeb	y	?	y	y		y[3]
earmv7		y	?	y	y		y[3]
earmv7eb	y	?	y	y		y[3]
earmv7hf	y	?	y	y		y[3]
earmv7hfeb	y	?	y	y		y[3]
m68000		y	?	y	y		y
m68k		y	y[16]	y	y		y[13]
mipseb		y	y	y	y		y		y
mipsel		y	y	y	y		y		y	y
mips64eb	y	y	y	y		y[8]		y	y
mips64el	y	?	y	y		y		y	y
powerpc		y	y	y	y		y		y[6]	
powerpc64	y	?	y	y		y
sh3eb		y	?	y	y		y[5]
sh3el		y	?	y	y		y
sparc		y	y	y	y		y		y[6]
sparc64		y	y	y	y		y		y[6,2]	y
vax		y	y	y	y		y		y[15]	n
--
or1k		n[10]
riscv32		n[10]
riscv64		n[10]
ia64
------------	-----	-------	------	----------	------------	----	---
architecture	tools	kernels	libgcc	native-gcc	make release	runs	atf


[2] - -O2 and -Os kernels hang, -O1 SIR reset
	-- may be a SMP issue; ultra10 works, ultra45 SMP does not
		-- try sb2000 (particularly with UP kernel.  u45 UP is shitty.)
[3] - MKCOMPAT=no enabled currently, infact OABI seems to be broken.  the docs say it was only deprecated, but the options make broken stuff.
[5] - sh3eb tries to use sh specific atomic config which doesn't work, has hand edited mknative output
[6] - crtbeginS.o builds incorrectly with GCC 5.3 and -O2.  a hack for -O1 has been added.
[8] - sgimips64 release build fails, mdsetimage'd gz'd kernels don't get built?
[10] - needs to be re-merged with GCC 5.3 versions of these not-merged-to-mainline-gcc ports.
[13] - builds tested:
       - m68k: mvm68k amiga atari
       - earm : shark evbarm*
       - mipseb: sgimips evbmips
       - mipsel: evbmips
       - mips64eb: sgimips[fail] evbmips
       - mips64el: evbmips
[14] - machines tested:
       - sparc ss20 (OK)
       - sparc64 ultra45 (FAIL), ultra10 (OK), sb2000 (OK UP, FAIL SMP.)
       - powerpc pegasosII (OK)
       - alpha UP1000 (OK)
       - arm shark (OK)
       - evbmips gxemul MALTA
		- had to implement some MIPS32 in gxemul to make this work
       - pmax gxemul (OK)
       - sgimips O2 (OK kernel -- but my O2 is not stable)
       - i386/amd64 (OK)
       - hppa (OK)
       - amiga (OK)
[15] - vax -- switched to GCC 5.3 already as it is less broken
	- ssh is broken, was broken with 4.8
	- there are too many -O0's we added because of mis-handled rtl
	- the eh_frame stuff is probably busted, but it never worked?
[16] - kernels tested:
	- m68k: amiga
This directory contains the GNU Compiler Collection (GCC).

The GNU Compiler Collection is free software.  See the files whose
names start with COPYING for copying permission.  The manuals, and
some of the runtime libraries, are under different terms; see the
individual source files for details.

The directory INSTALL contains copies of the installation information
as HTML and plain text.  The source of this information is
gcc/doc/install.texi.  The installation information includes details
of what is included in the GCC sources and what files GCC installs.

See the file gcc/doc/gcc.texi (together with other files that it
includes) for usage and porting information.  An online readable
version of the manual is in the files gcc/doc/gcc.info*.

See http://gcc.gnu.org/bugs/ for how to report bugs usefully.

Copyright years on GCC source files may be listed using range
notation, e.g., 1987-2012, indicating that every year in the range,
inclusive, is a copyrightable year that could otherwise be listed
individually.
This directory has been obsoleted for GCC snapshots and SVN access.

For releases the installation documentation is generated from
gcc/doc/install.texi and copied into this directory.

To read this documentation, please point your HTML browser to "index.html".
The latest version is always available at http://gcc.gnu.org/install/ .
GNU toolchain edition of GNU libintl 0.12.1

Most of the content of this directory is taken from gettext 0.12.1
and is owned by that project.  Patches should be directed to the
gettext developers first.  However, note the following:

* libintl.h comes from gettext, but is named libgnuintl.h.in in that
  project's source tree.

* The files COPYING.LIB-2.0 and COPYING.LIB-2.1 are redundant with the
  top-level COPYING.LIB and have therefore been removed.

* The files config.charset, ref-add.sin, ref-del.sin, os2compat.c,
  and os2compat.h are not used in this setup and have therefore been 
  removed.

* aclocal.m4 was constructed using automake's "aclocal -I ../config".

* configure.ac, config.intl.in, and Makefile.in were written for this
  directory layout, by Zack Weinberg <zack@codesourcery.com>.  Please
  direct patches for these files to gcc-patches@gcc.gnu.org.
file: libstdc++-v3/README

New users may wish to point their web browsers to the file
index.html in the 'doc/html' subdirectory.  It contains brief
building instructions and notes on how to configure the library in
interesting ways.
The HTML documentation in this folder is generated from the XML sources.

To change or edit, please edit the XML sources in the ../xml directory.
This directory contains scripts that are used by the regression
tester, <http://gcc.gnu.org/regtest/>

The primary script is 'btest-gcc.sh'.  This is the script that is run
to actually test the compiler.

'objs-gcc.sh' takes a combined tree and builds (but does not test) the
tools required for 'btest-gcc.sh'.  It is run periodically to update
the tools.  This script is followed by running 'btest-gcc.sh' using
the newly-build tools to check that they will not cause regressions.

'site.exp' is what $DEJAGNU points to when the regression tester runs
these scripts.

'GCC_Regression_Tester.wdgt' is a Dashboard widget that displays the
current state of the tester using Javascript.  You can use it without
needing Dashboard by pointing your web browser at
'GCC_Regression_Tester.wdgt/widget.html', if your browser supports
and permits it.

Note that any changes made here need to be approved by the regression
tester's maintainer (see MAINTAINERS).  The changes will be used on
the tester's next run, so `experimental' changes are very strongly
discouraged :-).
This directory contains files from examples of regression hunts, cut
down to smaller ranges to save space and time.  Try these out before
using the tools on your own tests.  First, update gcc-svn-env and
common.config for your own environment.

Each of the examples has multiple files:

  *.list files were created using gcc-svn-patchlist

  *.config files were written by hand based on earlier config files;
  the commented-out pieces are left as templates in case they're needed

  *.c, *.c++ are source files for the test, usually taken directly from
  the PR

  *.test files are tests specific to a bug when an existing gcc-test-*
  script can't be used

  *.log files are output from various scripts

Examples, where the identifier is the PR number:

  28970   wrong-code
  29106   special test, 4.1 branch
  29578   bogus-warning
  29906a  ice-on-valid-code, break
  29906b  ice-on-valid-code, fix
  30643   special test, cross compiler

Cut down the range even further by setting LOW_PATCH and HIGH_PATCH
within the config file to ids where the log file shows the test
passed or failed.

To run one, do

  reg-hunt 28970.config > 28970.log 2>&1

Check on its progress using

  ./reg-watch 28970.log

To run them all, do

  echo "hunt 28970" > queue
  echo "hunt 29106" >> queue
  echo "hunt 29578" >> queue
  echo "hunt 29906a" >> queue
  echo "hunt 29906b" >> queue
  echo "hunt 30643" >> queue
  ./testall queue

This allows you to add more to the queue if you're setting up lots of
hunts.
This directory contains scripts that are used for identifying the
patch that introduced a regression.  General information about such
searches is covered in http://gcc.gnu.org/bugs/reghunt.html.

  reg_search searches for a small time interval within a range of
  dates in which results for a test changed, using a binary search.
  The functionality for getting sources, building the component to
  test, and running the test are in other scripts that are run from
  here.

  reg_periodic invokes separate tools (the same scripts invoked by
  reg_search) over a range of dates at specified intervals.

  reg_test_template shows the format for the script that runs a test
  and determines whether to continue the search with a later or
  earlier date.
This directory contains the -liberty library of free software.
It is a collection of subroutines used by various GNU programs.
Current members include:

	getopt -- get options from command line
	obstack -- stacks of arbitrarily-sized objects
	strerror -- error message strings corresponding to errno
	strtol -- string-to-long conversion
	strtoul -- string-to-unsigned-long conversion

We expect many of the GNU subroutines that are floating around to
eventually arrive here.

The library must be configured from the top source directory.  Don't
try to run configure in this directory.  Follow the configuration
instructions in ../README.

Please report bugs to "gcc-bugs@gcc.gnu.org" and send fixes to
"gcc-patches@gcc.gnu.org".  Thank you.

ADDING A NEW FILE
=================

There are two sets of files:  Those that are "required" will be
included in the library for all configurations, while those
that are "optional" will be included in the library only if "needed."

To add a new required file, edit Makefile.in to add the source file
name to CFILES and the object file to REQUIRED_OFILES.

To add a new optional file, it must provide a single function, and the
name of the function must be the same as the name of the file.

    * Add the source file name to CFILES in Makefile.in and the object
      file to CONFIGURED_OFILES.

    * Add the function to name to the funcs shell variable in
      configure.ac.

    * Add the function to the AC_CHECK_FUNCS lists just after the
      setting of the funcs shell variable.  These AC_CHECK_FUNCS calls
      are never executed; they are there to make autoheader work
      better.

    * Consider the special cases of building libiberty; as of this
      writing, the special cases are newlib and VxWorks.  If a
      particular special case provides the function, you do not need
      to do anything.  If it does not provide the function, add the
      object file to LIBOBJS, and add the function name to the case
      controlling whether to define HAVE_func.

Finally, in the build directory of libiberty, configure with
"--enable-maintainer-mode", run "make maint-deps" to update
Makefile.in, and run 'make stamp-functions' to regenerate
functions.texi.

The optional file you've added (e.g. getcwd.c) should compile and work
on all hosts where it is needed.  It does not have to work or even
compile on hosts where it is not needed.

ADDING A NEW CONFIGURATION
==========================

On most hosts you should be able to use the scheme for automatically
figuring out which files are needed.  In that case, you probably
don't need a special Makefile stub for that configuration.

If the fully automatic scheme doesn't work, you may be able to get
by with defining EXTRA_OFILES in your Makefile stub.  This is
a list of object file names that should be treated as required
for this configuration - they will be included in libiberty.a,
regardless of whatever might be in the C library.
The files in this directory are part of the GNU C Library, not part of
GCC.  As described at <http://gcc.gnu.org/codingconventions.html>,
changes should be made to the GNU C Library and the changed files then
imported into GCC.
This README file is copied into the directory for GCC-only header files
when fixincludes is run by the makefile for GCC.

Many of the files in this directory were automatically edited from the
standard system header files by the fixincludes process.  They are
system-specific, and will not work on any other kind of system.  They
are also not part of GCC.  The reason we have to do this is because
GCC requires ANSI C headers and many vendors supply ANSI-incompatible
headers.

Because this is an automated process, sometimes headers get "fixed"
that do not, strictly speaking, need a fix.  As long as nothing is broken
by the process, it is just an unfortunate collateral inconvenience.
We would like to rectify it, if it is not "too inconvenient".

GCC MAINTAINER INFORMATION
==========================

If you are having some problem with a system header that is either
broken by the manufacturer, or is broken by the fixinclude process,
then you will need to alter or add information to the include fix
definitions file, ``inclhack.def''.  Please also send relevant
information to gcc-bugs@gcc.gnu.org, gcc-patches@gcc.gnu.org and,
please, to me:  bkorb@gnu.org.

To make your fix, you will need to do several things:

1.  Obtain access to the AutoGen program on some platform.  It does
    not have to be your build platform, but it is more convenient.

2.  Edit "inclhack.def" to reflect the changes you need to make.
    See below for information on how to make those changes.

3.  Run the "genfixes" shell script to produce a new copy of
    the "fixincl.x" file.

4.  Rebuild the compiler and check the header causing the issue.
    Make sure it is now properly handled.  Add tests to the
    "test_text" entry(ies) that validate your fix.  This will
    help ensure that future fixes won't negate your work.
    Do *NOT* specify test text for "wrap" or "replacement" fixes.
    There is no real possibility that these fixes will fail.
    If they do, you will surely know straight away.

    NOTE:  "test_text" is interpreted by the shell as it gets
    copied into the test header.  THEREFORE you must quote
    dollar sign characters and back quotes -- unless you mean
    for them to be interpreted by the shell.

    e.g. the math_huge_val_from_dbl_max test_text needs to
    put text into both float.h and math.h, so it includes a
    back-quoted script to add text to float.h.

5.  Go into the fixincludes build directory and type, "make check".
    You are guaranteed to have issues printed out as a result.
    Look at the diffs produced.  Make sure you have not clobbered
    the proper functioning of a different fix.  Make sure your
    fix is properly tested and it does what it is supposed to do.

6.  Now that you have the right things happening, synchronize the
    $(srcdir)/tests/base directory with the $(builddir)/tests/res
    directory.  The output of "make check" will be some diffs that
    should give you some hints about what to do.

7.  Rerun "make check" and verify that there are no issues left.


MAKING CHANGES TO INCLHACK.DEF
==============================

0.  If you are not the fixincludes maintainer, please send that
    person email about any changes you may want to make.  Thanks!

1.  Every fix must have a "hackname" that is compatible with C syntax
    for variable names and is unique without regard to alphabetic case.
    Please keep them alphabetical by this name.  :-)

2.  If the problem is known to exist only in certain files, then
    identify the files with "files = " entries.  If you use fnmatch(3C)
    wild card characters in a "files" entry, be certain that the first
    "files" entry has no such character.  Otherwise, the "make check"
    machinery will attempt to create files with those characters in the
    name.  That is inconvenient.

3.  It is relatively expensive to fire off a process to fix a source
    file, therefore write apply tests to avoid unnecessary fix
    processes.  The preferred apply tests are "select", "bypass", "mach"
    "sum", and "c-test" because they are performed internally:

    * select - Run a regex on the contents of the file being considered.
               All such regex-es must match.  Matching is done with
               extended regular expressions.

    * bypass - Run a regex on the contents of the file being considered.
               No such regex may match.

    * sum    - Select a specific version of a file that has a matching
               check sum.  The BSD version of checksum ["sum(1BSD)"]
               is used.  Each "sum" entry should contain exactly three
               space separated tokens:  the sum, some number and the
               basename of the file.  The "some number" is ignored.
               If there are multiple "sum" entries, only one needs to
               match in order to pass.  For example:

                   sum = '1234 3 foobar.h';

               specifies that the "foobar.h" header in any directory
               will match if it has the checksum 1234.

    * c_test - call a function in fixtests.c.  See that file.

    * files  - the "fnmatch" pattern of the file(s) to examine for
               the issue.  There may be several copies of this attribute.
               If the header lives in a /usr/include subdirectory, be
               sure to include that subdirectory in the name. e.g. net/if.h

    * mach   - Match the output of config.guess against a series of fnmatch
               patterns.  It must match at least one of the patterns, unless
               "not-machine" has also been specified.  In that case, the
               config.guess output must not match any of the patterns.

    The next test is relatively slow because it must be handled in a
    separate shell process.  Some platforms do not support server shells,
    so the whole process is even slower and more cumbersome there.

    * test   - These should be arguments to the program, "/bin/test".
               You may perform multiple commands, if you enclose them
               in backquotes and echo out valid test arguments.  For
               example, you might echo out '0 -eq 1' if you want a false
               result, or '0 -eq 0' for a true result.

    These tests are required to:

    1.  Be positive for all header files that require the fix.

    It is desirable to:

    2.  Be negative as often as possible whenever the fix is not
        required, avoiding the process overhead.

    It is nice if:

    3.  The expression is as simple as possible to both
        process and understand by people.  :-)

        Please take advantage of the fact AutoGen will glue
        together string fragments.  It helps.  Also take note
        that double quote strings and single quote strings have
        different formation rules.  Double quote strings are a
        tiny superset of ANSI-C string syntax.  Single quote
        strings follow shell single quote string formation
        rules, except that the backslash is processed before
        '\\', '\'' and '#' characters (using C character syntax).

    Each test must pass or the fix is not applied.  For example,
    all "select" expressions must be found and not one "bypass"
    selection may be found.

    Examples of test specifications:

      hackname = broken_assert_stdio;
      files    = assert.h;
      select   = stderr;
      bypass   = "include.*stdio.h";

    The ``broken_assert_stdio'' fix will be applied only to a file
    named "assert.h" if it contains the string "stderr" _and_ it
    does _not_ contain the expression "include.*stdio.h".

      hackname = no_double_slash;
      c_test   = "double_slash";

    The ``no_double_slash'' fix will be applied if the
    ``double_slash_test()'' function says to.  See ``fixtests.c''
    for documentation on how to include new functions into that
    module.

4.  There are currently four methods of fixing a file:

    1.  a series of sed expressions.  Each will be an individual
        "-e" argument to a single invocation of sed.  Unless you
        need to use multiple or complex sed expressions, please
        use the "replacement text" method instead.

    2.  a shell script.  These scripts are _required_ to read all
        of stdin in order to avoid pipe stalls.  They may choose to
        discard the input.

    3.  Replacement text.  If the replacement is empty, then no
        fix is applied.  Otherwise, the replacement text is
        written to the output file and no further fixes are
        applied.  If you really want a no-op file, replace the
        file with a comment.

        Replacement text "fixes" must be first in this file!!

    4.  A C language subroutine method for both tests and fixes.
        See ``fixtests.c'' for instructions on writing C-language
        applicability tests and ``fixfixes.c'' for C-language fixing.
        These files also contain tables that describe the currently
        implemented fixes and tests.

    If at all possible, you should try to use one of the C language
    fixes as it is far more efficient.  There are currently five
    such fixes, three of which are very special purpose:

    i) char_macro_def - This function repairs the definition of an
        ioctl macro that presumes CPP macro substitution within
        pairs of single quote characters.

    ii) char_macro_use - This function repairs the usage of ioctl
        macros that no longer can wrap an argument with single quotes.

    iii) machine_name - This function will look at "#if", "#ifdef",
        "#ifndef" and "#elif" directive lines and replace the first
        occurrence of a non-reserved name that is traditionally
        pre-defined by the native compiler.

    The next two are for general use:

    iv) wrap - wraps the entire file with "#ifndef", "#define" and
        "#endif" self-exclusionary text.  It also, optionally, inserts
        a prolog after the "#define" and an epilog just before the
        "#endif".  You can use this for a fix as follows:

            c_fix     = wrap;
            c_fix_arg = "/* prolog text */";
            c_fix_arg = "/* epilog text */";

        If you want an epilog without a prolog, set the first "c_fix_arg"
        to the empty string.  Both or the second "c_fix_arg"s may be
        omitted and the file will still be wrapped.

	THERE IS A SPECIAL EXCEPTION TO THIS, HOWEVER:

	If the regular expression '#if.*__need' is found, then it is
	assumed that the file needs to be read and interpreted more
	than once.  However, the prolog and epilog text (if any) will
	be inserted.

    v) format - Replaces text selected with a regular expression with
        a specialized formating string.  The formatting works as follows:
        The format text is copied to the output until a '%' character
        is found.  If the character after the '%' is another '%', then
        one '%' is output and processing continues.  If the following
        character is not a digit, then the '%' and that character are
        copied and processing continues.  Finally, if the '%' *is*
        followed by a digit, that digit is used as an index into the
        regmatch_t array to replace the two characters with the matched
        text.  i.e.: "%0" is replaced by the full matching text, "%1"
        is the first matching sub-expression, etc.

        This is used as follows:

            c_fix     = format;
            c_fix_arg = "#ifndef %1\n%0\n#endif";
            c_fix_arg = "#define[ \t]+([A-Z][A-Z0-9a-z_]*).*";

        This would wrap a one line #define inside of a "#ifndef"/"#endif"
        pair.  The second "c_fix_arg" may be omitted *IF* there is at least
        one select clause and the first one identifies the text you wish to
        reformat.  It will then be used as the second "c_fix_arg".  You may
        delete the selected text by supplying an empty string for the
        replacement format (the first "c_fix_arg").

	Note: In general, a format c_fix may be used in place of one
	sed expression.  However, it will need to be rewritten by
	hand.  For example:

	sed = 's@^#if __GNUC__ == 2 && __GNUC_MINOR__ >= 7$'
	       '@& || __GNUC__ >= 3@';

	may be rewritten using a format c_fix as:

	c_fix     = format;
	c_fix_arg = '%0 || __GNUC__ >= 3';
	c_fix_arg = '^#if __GNUC__ == 2 && __GNUC_MINOR__ >= 7$';

	Multiple sed substitution expressions probably ought to remain sed
	expressions in order to maintain clarity.  Also note that if the
	second sed expression is the same as the first select expression,
	then you may omit the second c_fix_arg.  The select expression will
	be picked up and used in its absence.

EXAMPLES OF FIXES:
==================

      hackname = AAA_ki_iface;
      replace; /* empty replacement -> no fixing the file */

    When this ``fix'' is invoked, it will prevent any fixes
    from being applied.

    ------------------

      hackname = AAB_svr4_no_varargs;
      replace  = "/* This file was generated by fixincludes.  */\n"
                 "#ifndef _SYS_VARARGS_H\n"
                 "#define _SYS_VARARGS_H\n\n"

                 "#ifdef __STDC__\n"
                 "#include <stdarg.h>\n"
                 "#else\n"
                 "#include <varargs.h>\n"
                 "#endif\n\n"

                 "#endif  /* _SYS_VARARGS_H */\n";

    When this ``fix'' is invoked, the replacement text will be
    emitted into the replacement include file.  No further fixes
    will be applied.

    ------------------

        hackname  = hpux11_fabsf;
        files     = math.h;
        select    = "^[ \t]*#[ \t]*define[ \t]+fabsf\\(.*";
        bypass    = "__cplusplus";

        c_fix     = format;
        c_fix_arg = "#ifndef __cplusplus\n%0\n#endif";

        test_text =
        "#  define fabsf(x) ((float)fabs((double)(float)(x)))\n";

    This fix will ensure that the #define for fabs is wrapped
    with C++ protection, providing the header is not already
    C++ aware.

    ------------------

5.  Testing fixes.

    The brute force method is, of course, to configure and build
    GCC.  But you can also:

        cd ${top_builddir}/gcc
        rm -rf include-fixed/ stmp-fixinc
        make stmp-fixinc

    I would really recommend, however:

        cd ${top_builddir}/fixincludes
        make check

    To do this, you *must* have autogen installed on your system.
    The "check" step will proceed to construct a shell script that
    will exercise all the fixes, using the sample test_text
    provided with each fix.  Once done, the changes made will
    be compared against the changes saved in the source directory.
    If you are changing the tests or fixes, the change will likely
    be highlighted.
Copyright (C) 2000-2015 Free Software Foundation, Inc.

This file is intended to contain a few notes about writing C code
within GCC so that it compiles without error on the full range of
compilers GCC needs to be able to compile on.

The problem is that many ISO-standard constructs are not accepted by
either old or buggy compilers, and we keep getting bitten by them.
This knowledge until now has been sparsely spread around, so I
thought I'd collect it in one useful place.  Please add and correct
any problems as you come across them.

I'm going to start from a base of the ISO C90 standard, since that is
probably what most people code to naturally.  Obviously using
constructs introduced after that is not a good idea.

For the complete coding style conventions used in GCC, please read
http://gcc.gnu.org/codingconventions.html


String literals
---------------

Irix6 "cc -n32" and OSF4 "cc" have problems with constant string
initializers with parens around it, e.g.

const char string[] = ("A string");

This is unfortunate since this is what the GNU gettext macro N_
produces.  You need to find a different way to code it.

Some compilers like MSVC++ have fairly low limits on the maximum
length of a string literal; 509 is the lowest we've come across.  You
may need to break up a long printf statement into many smaller ones.


Empty macro arguments
---------------------

ISO C (6.8.3 in the 1990 standard) specifies the following:

If (before argument substitution) any argument consists of no
preprocessing tokens, the behavior is undefined.

This was relaxed by ISO C99, but some older compilers emit an error,
so code like

#define foo(x, y) x y
foo (bar, )

needs to be coded in some other way.


Avoid unnecessary test before free
----------------------------------

Since SunOS 4 stopped being a reasonable portability target,
(which happened around 2007) there has been no need to guard
against "free (NULL)".  Thus, any guard like the following
constitutes a redundant test:

  if (P)
    free (P);

It is better to avoid the test.[*]
Instead, simply free P, regardless of whether it is NULL.

[*] However, if your profiling exposes a test like this in a
performance-critical loop, say where P is nearly always NULL, and
the cost of calling free on a NULL pointer would be prohibitively
high, consider using __builtin_expect, e.g., like this:

  if (__builtin_expect (ptr != NULL, 0))
    free (ptr);



Trigraphs
---------

You weren't going to use them anyway, but some otherwise ISO C
compliant compilers do not accept trigraphs.


Suffixes on Integer Constants
-----------------------------

You should never use a 'l' suffix on integer constants ('L' is fine),
since it can easily be confused with the number '1'.


			Common Coding Pitfalls
			======================

errno
-----

errno might be declared as a macro.


Implicit int
------------

In C, the 'int' keyword can often be omitted from type declarations.
For instance, you can write

  unsigned variable;

as shorthand for

  unsigned int variable;

There are several places where this can cause trouble.  First, suppose
'variable' is a long; then you might think

  (unsigned) variable

would convert it to unsigned long.  It does not.  It converts to
unsigned int.  This mostly causes problems on 64-bit platforms, where
long and int are not the same size.

Second, if you write a function definition with no return type at
all:

  operate (int a, int b)
  {
    ...
  }

that function is expected to return int, *not* void.  GCC will warn
about this.

Implicit function declarations always have return type int.  So if you
correct the above definition to

  void
  operate (int a, int b)
  ...

but operate() is called above its definition, you will get an error
about a "type mismatch with previous implicit declaration".  The cure
is to prototype all functions at the top of the file, or in an
appropriate header.

Char vs unsigned char vs int
----------------------------

In C, unqualified 'char' may be either signed or unsigned; it is the
implementation's choice.  When you are processing 7-bit ASCII, it does
not matter.  But when your program must handle arbitrary binary data,
or fully 8-bit character sets, you have a problem.  The most obvious
issue is if you have a look-up table indexed by characters.

For instance, the character '\341' in ISO Latin 1 is SMALL LETTER A
WITH ACUTE ACCENT.  In the proper locale, isalpha('\341') will be
true.  But if you read '\341' from a file and store it in a plain
char, isalpha(c) may look up character 225, or it may look up
character -31.  And the ctype table has no entry at offset -31, so
your program will crash.  (If you're lucky.)

It is wise to use unsigned char everywhere you possibly can.  This
avoids all these problems.  Unfortunately, the routines in <string.h>
take plain char arguments, so you have to remember to cast them back
and forth - or avoid the use of strxxx() functions, which is probably
a good idea anyway.

Another common mistake is to use either char or unsigned char to
receive the result of getc() or related stdio functions.  They may
return EOF, which is outside the range of values representable by
char.  If you use char, some legal character value may be confused
with EOF, such as '\377' (SMALL LETTER Y WITH UMLAUT, in Latin-1).
The correct choice is int.

A more subtle version of the same mistake might look like this:

  unsigned char pushback[NPUSHBACK];
  int pbidx;
  #define unget(c) (assert(pbidx < NPUSHBACK), pushback[pbidx++] = (c))
  #define get(c) (pbidx ? pushback[--pbidx] : getchar())
  ...
  unget(EOF);

which will mysteriously turn a pushed-back EOF into a SMALL LETTER Y
WITH UMLAUT.


Other common pitfalls
---------------------

o Expecting 'plain' char to be either sign or unsigned extending.

o Shifting an item by a negative amount or by greater than or equal to
  the number of bits in a type (expecting shifts by 32 to be sensible
  has caused quite a number of bugs at least in the early days).

o Expecting ints shifted right to be sign extended.

o Modifying the same value twice within one sequence point.

o Host vs. target floating point representation, including emitting NaNs
  and Infinities in a form that the assembler handles.

o qsort being an unstable sort function (unstable in the sense that
  multiple items that sort the same may be sorted in different orders
  by different qsort functions).

o Passing incorrect types to fprintf and friends.

o Adding a function declaration for a module declared in another file to
  a .c file instead of to a .h file.
This directory contains machine-specific files for the GNU C compiler.
It has a subdirectory for each basic CPU type.
The only files in this directory itself
are some .h files that pertain to particular operating systems
and are used for more than one CPU type.
Random Notes
------------

The MSP430 port does not use leading underscores.  However, the
assembler has no way of differentiating between, for example, register
R12 and symbol R12.  So, if you do "int r12;" in your C program, you
may get an assembler error, and will certainly have runtime problems.
		Arm / Thumb Interworking
		========================

The Cygnus GNU Pro Toolkit for the ARM7T processor supports function
calls between code compiled for the ARM instruction set and code
compiled for the Thumb instruction set and vice versa.  This document
describes how that interworking support operates and explains the
command line switches that should be used in order to produce working
programs.

Note:  The Cygnus GNU Pro Toolkit does not support switching between
compiling for the ARM instruction set and the Thumb instruction set
on anything other than a per file basis.  There are in fact two
completely separate compilers, one that produces ARM assembler
instructions and one that produces Thumb assembler instructions.  The
two compilers share the same assembler, linker and so on.


1. Explicit interworking support for C and C++ files
====================================================

By default if a file is compiled without any special command line
switches then the code produced will not support interworking.
Provided that a program is made up entirely from object files and
libraries produced in this way and which contain either exclusively
ARM instructions or exclusively Thumb instructions then this will not
matter and a working executable will be created.  If an attempt is
made to link together mixed ARM and Thumb object files and libraries,
then warning messages will be produced by the linker and a non-working
executable will be created.

In order to produce code which does support interworking it should be
compiled with the

	-mthumb-interwork

command line option.  Provided that a program is made up entirely from
object files and libraries built with this command line switch a
working executable will be produced, even if both ARM and Thumb
instructions are used by the various components of the program.  (No
warning messages will be produced by the linker either).

Note that specifying -mthumb-interwork does result in slightly larger,
slower code being produced.  This is why interworking support must be
specifically enabled by a switch.


2. Explicit interworking support for assembler files
====================================================

If assembler files are to be included into an interworking program
then the following rules must be obeyed:

	* Any externally visible functions must return by using the BX
	instruction.

	* Normal function calls can just use the BL instruction.  The
	linker will automatically insert code to switch between ARM
	and Thumb modes as necessary.

	* Calls via function pointers should use the BX instruction if
	the call is made in ARM mode:

		.code 32
		mov lr, pc
		bx  rX

	This code sequence will not work in Thumb mode however, since
	the mov instruction will not set the bottom bit of the lr
	register.  Instead a branch-and-link to the _call_via_rX
	functions should be used instead:

		.code 16
		bl  _call_via_rX

	where rX is replaced by the name of the register containing
	the function address.

	* All externally visible functions which should be entered in
	Thumb mode must have the .thumb_func pseudo op specified just
	before their entry point.  e.g.:

			.code 16
			.global function
			.thumb_func
		function:
			...start of function....

	* All assembler files must be assembled with the switch
	-mthumb-interwork specified on the command line.  (If the file
	is assembled by calling gcc it will automatically pass on the
	-mthumb-interwork switch to the assembler, provided that it
	was specified on the gcc command line in the first place.) 


3. Support for old, non-interworking aware code.
================================================

If it is necessary to link together code produced by an older,
non-interworking aware compiler, or code produced by the new compiler
but without the -mthumb-interwork command line switch specified, then
there are two command line switches that can be used to support this.

The switch

	-mcaller-super-interworking

will allow calls via function pointers in Thumb mode to work,
regardless of whether the function pointer points to old,
non-interworking aware code or not.  Specifying this switch does
produce slightly slower code however.

Note:  There is no switch to allow calls via function pointers in ARM
mode to be handled specially.  Calls via function pointers from
interworking aware ARM code to non-interworking aware ARM code work
without any special considerations by the compiler.  Calls via
function pointers from interworking aware ARM code to non-interworking
aware Thumb code however will not work.  (Actually under some
circumstances they may work, but there are no guarantees).  This is
because only the new compiler is able to produce Thumb code, and this
compiler already has a command line switch to produce interworking
aware code.


The switch

	-mcallee-super-interworking

will allow non-interworking aware ARM or Thumb code to call Thumb
functions, either directly or via function pointers.  Specifying this
switch does produce slightly larger, slower code however.

Note:  There is no switch to allow non-interworking aware ARM or Thumb
code to call ARM functions.  There is no need for any special handling
of calls from non-interworking aware ARM code to interworking aware
ARM functions, they just work normally.  Calls from non-interworking
aware Thumb functions to ARM code however, will not work.  There is no
option to support this, since it is always possible to recompile the
Thumb code to be interworking aware.

As an alternative to the command line switch
-mcallee-super-interworking, which affects all externally visible
functions in a file, it is possible to specify an attribute or
declspec for individual functions, indicating that that particular
function should support being called by non-interworking aware code.
The function should be defined like this:

	int __attribute__((interfacearm)) function 
	{
		... body of function ...
	}

or

	int __declspec(interfacearm) function
	{
		... body of function ...
	}



4. Interworking support in dlltool
==================================

It is possible to create DLLs containing mixed ARM and Thumb code.  It
is also possible to call Thumb code in a DLL from an ARM program and
vice versa.  It is even possible to call ARM DLLs that have been compiled
without interworking support (say by an older version of the compiler),
from Thumb programs and still have things work properly.

   A version of the `dlltool' program which supports the `--interwork'
command line switch is needed, as well as the following special
considerations when building programs and DLLs:

*Use `-mthumb-interwork'*
     When compiling files for a DLL or a program the `-mthumb-interwork'
     command line switch should be specified if calling between ARM and
     Thumb code can happen.  If a program is being compiled and the
     mode of the DLLs that it uses is not known, then it should be
     assumed that interworking might occur and the switch used.

*Use `-m thumb'*
     If the exported functions from a DLL are all Thumb encoded then the
     `-m thumb' command line switch should be given to dlltool when
     building the stubs.  This will make dlltool create Thumb encoded
     stubs, rather than its default of ARM encoded stubs.

     If the DLL consists of both exported Thumb functions and exported
     ARM functions then the `-m thumb' switch should not be used.
     Instead the Thumb functions in the DLL should be compiled with the
     `-mcallee-super-interworking' switch, or with the `interfacearm'
     attribute specified on their prototypes.  In this way they will be
     given ARM encoded prologues, which will work with the ARM encoded
     stubs produced by dlltool.

*Use `-mcaller-super-interworking'*
     If it is possible for Thumb functions in a DLL to call
     non-interworking aware code via a function pointer, then the Thumb
     code must be compiled with the `-mcaller-super-interworking'
     command line switch.  This will force the function pointer calls
     to use the _interwork_call_via_rX stub functions which will
     correctly restore Thumb mode upon return from the called function.

*Link with `libgcc.a'*
     When the dll is built it may have to be linked with the GCC
     library (`libgcc.a') in order to extract the _call_via_rX functions
     or the _interwork_call_via_rX functions.  This represents a partial
     redundancy since the same functions *may* be present in the
     application itself, but since they only take up 372 bytes this
     should not be too much of a consideration.

*Use `--support-old-code'*
     When linking a program with an old DLL which does not support
     interworking, the `--support-old-code' command line switch to the
     linker should be used.   This causes the linker to generate special
     interworking stubs which can cope with old, non-interworking aware
     ARM code, at the cost of generating bulkier code.  The linker will
     still generate a warning message along the lines of:
       "Warning: input file XXX does not support interworking, whereas YYY does."
     but this can now be ignored because the --support-old-code switch
     has been used.



5. How interworking support works
=================================

Switching between the ARM and Thumb instruction sets is accomplished
via the BX instruction which takes as an argument a register name.
Control is transferred to the address held in this register (with the
bottom bit masked out), and if the bottom bit is set, then Thumb
instruction processing is enabled, otherwise ARM instruction
processing is enabled.

When the -mthumb-interwork command line switch is specified, gcc
arranges for all functions to return to their caller by using the BX
instruction.  Thus provided that the return address has the bottom bit
correctly initialized to indicate the instruction set of the caller,
correct operation will ensue.

When a function is called explicitly (rather than via a function
pointer), the compiler generates a BL instruction to do this.  The
Thumb version of the BL instruction has the special property of
setting the bottom bit of the LR register after it has stored the
return address into it, so that a future BX instruction will correctly
return the instruction after the BL instruction, in Thumb mode.

The BL instruction does not change modes itself however, so if an ARM
function is calling a Thumb function, or vice versa, it is necessary
to generate some extra instructions to handle this.  This is done in
the linker when it is storing the address of the referenced function
into the BL instruction.  If the BL instruction is an ARM style BL
instruction, but the referenced function is a Thumb function, then the
linker automatically generates a calling stub that converts from ARM
mode to Thumb mode, puts the address of this stub into the BL
instruction, and puts the address of the referenced function into the
stub.  Similarly if the BL instruction is a Thumb BL instruction, and
the referenced function is an ARM function, the linker generates a
stub which converts from Thumb to ARM mode, puts the address of this
stub into the BL instruction, and the address of the referenced
function into the stub.

This is why it is necessary to mark Thumb functions with the
.thumb_func pseudo op when creating assembler files.  This pseudo op
allows the assembler to distinguish between ARM functions and Thumb
functions.  (The Thumb version of GCC automatically generates these
pseudo ops for any Thumb functions that it generates).

Calls via function pointers work differently.  Whenever the address of
a function is taken, the linker examines the type of the function
being referenced.  If the function is a Thumb function, then it sets
the bottom bit of the address.  Technically this makes the address
incorrect, since it is now one byte into the start of the function,
but this is never a problem because:

	a. with interworking enabled all calls via function pointer
	   are done using the BX instruction and this ignores the
	   bottom bit when computing where to go to.

	b. the linker will always set the bottom bit when the address
	   of the function is taken, so it is never possible to take
	   the address of the function in two different places and
	   then compare them and find that they are not equal.

As already mentioned any call via a function pointer will use the BX
instruction (provided that interworking is enabled).  The only problem
with this is computing the return address for the return from the
called function.  For ARM code this can easily be done by the code
sequence:

	mov	lr, pc
	bx	rX

(where rX is the name of the register containing the function
pointer).  This code does not work for the Thumb instruction set,
since the MOV instruction will not set the bottom bit of the LR
register, so that when the called function returns, it will return in
ARM mode not Thumb mode.  Instead the compiler generates this
sequence:

	bl	_call_via_rX

(again where rX is the name if the register containing the function
pointer).  The special call_via_rX functions look like this:

	.thumb_func
_call_via_r0:
	bx	r0
	nop

The BL instruction ensures that the correct return address is stored
in the LR register and then the BX instruction jumps to the address
stored in the function pointer, switch modes if necessary.


6. How caller-super-interworking support works
==============================================

When the -mcaller-super-interworking command line switch is specified
it changes the code produced by the Thumb compiler so that all calls
via function pointers (including virtual function calls) now go via a
different stub function.  The code to call via a function pointer now
looks like this:

	bl _interwork_call_via_r0

Note: The compiler does not insist that r0 be used to hold the
function address.  Any register will do, and there are a suite of stub
functions, one for each possible register.  The stub functions look
like this:

	.code 16
	.thumb_func
_interwork_call_via_r0
	bx 	pc
	nop
	
	.code 32
	tst	r0, #1
	stmeqdb	r13!, {lr}
	adreq	lr, _arm_return
	bx	r0

The stub first switches to ARM mode, since it is a lot easier to
perform the necessary operations using ARM instructions.  It then
tests the bottom bit of the register containing the address of the
function to be called.  If this bottom bit is set then the function
being called uses Thumb instructions and the BX instruction to come
will switch back into Thumb mode before calling this function.  (Note
that it does not matter how this called function chooses to return to
its caller, since the both the caller and callee are Thumb functions,
and mode switching is necessary).  If the function being called is an
ARM mode function however, the stub pushes the return address (with
its bottom bit set) onto the stack, replaces the return address with
the address of the a piece of code called '_arm_return' and then
performs a BX instruction to call the function.

The '_arm_return' code looks like this:

	.code 32
_arm_return:		
	ldmia 	r13!, {r12}
	bx 	r12
	.code 16


It simply retrieves the return address from the stack, and then
performs a BX operation to return to the caller and switch back into
Thumb mode.


7. How callee-super-interworking support works
==============================================

When -mcallee-super-interworking is specified on the command line the
Thumb compiler behaves as if every externally visible function that it
compiles has had the (interfacearm) attribute specified for it.  What
this attribute does is to put a special, ARM mode header onto the
function which forces a switch into Thumb mode:

  without __attribute__((interfacearm)):

		.code 16
		.thumb_func
	function:
		... start of function ...

  with __attribute__((interfacearm)):

		.code 32
	function:
		orr	r12, pc, #1
		bx	r12

		.code 16
                .thumb_func
        .real_start_of_function:

		... start of function ...

Note that since the function now expects to be entered in ARM mode, it
no longer has the .thumb_func pseudo op specified for its name.
Instead the pseudo op is attached to a new label .real_start_of_<name>
(where <name> is the name of the function) which indicates the start
of the Thumb code.  This does have the interesting side effect in that
if this function is now called from a Thumb mode piece of code
outside of the current file, the linker will generate a calling stub
to switch from Thumb mode into ARM mode, and then this is immediately
overridden by the function's header which switches back into Thumb
mode. 

In addition the (interfacearm) attribute also forces the function to
return by using the BX instruction, even if has not been compiled with
the -mthumb-interwork command line flag, so that the correct mode will
be restored upon exit from the function.


8. Some examples
================

   Given these two test files:

             int arm (void) { return 1 + thumb (); }

             int thumb (void) { return 2 + arm (); }

   The following pieces of assembler are produced by the ARM and Thumb
version of GCC depending upon the command line options used:

   `-O2':
             .code 32                               .code 16
             .global _arm                           .global _thumb
                                                    .thumb_func
     _arm:                                    _thumb:
             mov     ip, sp
             stmfd   sp!, {fp, ip, lr, pc}          push    {lr}
             sub     fp, ip, #4
             bl      _thumb                          bl      _arm
             add     r0, r0, #1                      add     r0, r0, #2
             ldmea   fp, {fp, sp, pc}                pop     {pc}

   Note how the functions return without using the BX instruction.  If
these files were assembled and linked together they would fail to work
because they do not change mode when returning to their caller.

   `-O2 -mthumb-interwork':

             .code 32                               .code 16
             .global _arm                           .global _thumb
                                                    .thumb_func
     _arm:                                    _thumb:
             mov     ip, sp
             stmfd   sp!, {fp, ip, lr, pc}          push    {lr}
             sub     fp, ip, #4
             bl      _thumb                         bl       _arm
             add     r0, r0, #1                     add      r0, r0, #2
             ldmea   fp, {fp, sp, lr}               pop      {r1}
             bx      lr                             bx       r1

   Now the functions use BX to return their caller.  They have grown by
4 and 2 bytes respectively, but they can now successfully be linked
together and be expect to work.  The linker will replace the
destinations of the two BL instructions with the addresses of calling
stubs which convert to the correct mode before jumping to the called
function.

   `-O2 -mcallee-super-interworking':

             .code 32                               .code 32
             .global _arm                           .global _thumb
     _arm:                                    _thumb:
                                                    orr      r12, pc, #1
                                                    bx       r12
             mov     ip, sp                         .code 16
             stmfd   sp!, {fp, ip, lr, pc}          push     {lr}
             sub     fp, ip, #4
             bl      _thumb                         bl       _arm
             add     r0, r0, #1                     add      r0, r0, #2
             ldmea   fp, {fp, sp, lr}               pop      {r1}
             bx      lr                             bx       r1

   The thumb function now has an ARM encoded prologue, and it no longer
has the `.thumb-func' pseudo op attached to it.  The linker will not
generate a calling stub for the call from arm() to thumb(), but it will
still have to generate a stub for the call from thumb() to arm().  Also
note how specifying `--mcallee-super-interworking' automatically
implies `-mthumb-interworking'.


9. Some Function Pointer Examples
=================================

   Given this test file:

     	int func (void) { return 1; }
     
     	int call (int (* ptr)(void)) { return ptr (); }

   The following varying pieces of assembler are produced by the Thumb
version of GCC depending upon the command line options used:

   `-O2':
     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.thumb_func
     	_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{pc}

   Note how the two functions have different exit sequences.  In
particular call() uses pop {pc} to return, which would not work if the
caller was in ARM mode.  func() however, uses the BX instruction, even
though `-mthumb-interwork' has not been specified, as this is the most
efficient way to exit a function when the return address is held in the
link register.

   `-O2 -mthumb-interwork':

     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.thumb_func
     	_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{r1}
     		bx	r1

   This time both functions return by using the BX instruction.  This
means that call() is now two bytes longer and several cycles slower
than the previous version.

   `-O2 -mcaller-super-interworking':
     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.thumb_func
     	_call:
     		push	{lr}
     		bl	__interwork_call_via_r0
     		pop	{pc}

   Very similar to the first (non-interworking) version, except that a
different stub is used to call via the function pointer.  This new stub
will work even if the called function is not interworking aware, and
tries to return to call() in ARM mode.  Note that the assembly code for
call() is still not interworking aware itself, and so should not be
called from ARM code.

   `-O2 -mcallee-super-interworking':

     		.code	32
     		.globl	_func
     	_func:
     		orr	r12, pc, #1
     		bx	r12
     
     		.code	16
     		.globl .real_start_of_func
     		.thumb_func
     	.real_start_of_func:
     		mov	r0, #1
     		bx	lr
     
     		.code	32
     		.globl	_call
     	_call:
     		orr	r12, pc, #1
     		bx	r12
     
     		.code	16
     		.globl .real_start_of_call
     		.thumb_func
     	.real_start_of_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{r1}
     		bx	r1

   Now both functions have an ARM coded prologue, and both functions
return by using the BX instruction.  These functions are interworking
aware therefore and can safely be called from ARM code.  The code for
the call() function is now 10 bytes longer than the original, non
interworking aware version, an increase of over 200%.

   If a prototype for call() is added to the source code, and this
prototype includes the `interfacearm' attribute:

     	int __attribute__((interfacearm)) call (int (* ptr)(void));

   then this code is produced (with only -O2 specified on the command
line):

     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.code	32
     	_call:
     		orr	r12, pc, #1
     		bx	r12
     
     		.code	16
     		.globl .real_start_of_call
     		.thumb_func
     	.real_start_of_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{r1}
     		bx	r1

   So now both call() and func() can be safely called via
non-interworking aware ARM code.  If, when such a file is assembled,
the assembler detects the fact that call() is being called by another
function in the same file, it will automatically adjust the target of
the BL instruction to point to .real_start_of_call.  In this way there
is no need for the linker to generate a Thumb-to-ARM calling stub so
that call can be entered in ARM mode.


10. How to use dlltool to build ARM/Thumb DLLs
==============================================
   Given a program (`prog.c') like this:

             extern int func_in_dll (void);
     
             int main (void) { return func_in_dll(); }

   And a DLL source file (`dll.c') like this:

             int func_in_dll (void) { return 1; }

   Here is how to build the DLL and the program for a purely ARM based
environment:

*Step One
     Build a `.def' file describing the DLL:

             ; example.def
             ; This file describes the contents of the DLL
             LIBRARY     example
             HEAPSIZE    0x40000, 0x2000
             EXPORTS
                          func_in_dll  1

*Step Two
     Compile the DLL source code:

            arm-pe-gcc -O2 -c dll.c

*Step Three
     Use `dlltool' to create an exports file and a library file:

            dlltool --def example.def --output-exp example.o --output-lib example.a

*Step Four
     Link together the complete DLL:

            arm-pe-ld dll.o example.o -o example.dll

*Step Five
     Compile the program's source code:

            arm-pe-gcc -O2 -c prog.c

*Step Six
     Link together the program and the DLL's library file:

            arm-pe-gcc prog.o example.a -o prog

   If instead this was a Thumb DLL being called from an ARM program, the
steps would look like this.  (To save space only those steps that are
different from the previous version are shown):

*Step Two
     Compile the DLL source code (using the Thumb compiler):

            thumb-pe-gcc -O2 -c dll.c -mthumb-interwork

*Step Three
     Build the exports and library files (and support interworking):

            dlltool -d example.def -z example.o -l example.a --interwork -m thumb

*Step Five
     Compile the program's source code (and support interworking):

            arm-pe-gcc -O2 -c prog.c -mthumb-interwork

   If instead, the DLL was an old, ARM DLL which does not support
interworking, and which cannot be rebuilt, then these steps would be
used.

*Step One
     Skip.  If you do not have access to the sources of a DLL, there is
     no point in building a `.def' file for it.

*Step Two
     Skip.  With no DLL sources there is nothing to compile.

*Step Three
     Skip.  Without a `.def' file you cannot use dlltool to build an
     exports file or a library file.

*Step Four
     Skip.  Without a set of DLL object files you cannot build the DLL.
     Besides it has already been built for you by somebody else.

*Step Five
     Compile the program's source code, this is the same as before:

            arm-pe-gcc -O2 -c prog.c

*Step Six
     Link together the program and the DLL's library file, passing the
     `--support-old-code' option to the linker:

            arm-pe-gcc prog.o example.a -Wl,--support-old-code -o prog

     Ignore the warning message about the input file not supporting
     interworking as the --support-old-code switch has taken care if this.


Copyright (C) 1998-2015 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
AddressSanitizer (http://code.google.com/p/address-sanitizer) and
ThreadSanitizer (http://code.google.com/p/thread-sanitizer/) are
projects initially developed by Google Inc.
Both tools consist of a compiler module and a run-time library.
The sources of the run-time library for these projects are hosted at
http://llvm.org/svn/llvm-project/compiler-rt in the following directories:
  include/sanitizer
  lib/sanitizer_common
  lib/interception
  lib/asan
  lib/tsan
  lib/lsan
  lib/ubsan

Trivial and urgent fixes (portability, build fixes, etc.) may go directly to the
GCC tree.  All non-trivial changes, functionality improvements, etc. should go
through the upstream tree first and then be merged back to the GCC tree.
The merges from upstream should be done with the aid of the merge.sh script;
it will also update the file MERGE to contain the upstream revision
we merged with.
This directory builds tools used by people working in the Go language.
The source code for these tools lives in libgo/go/cmd, where it is
copied from the master gofrontend repository.  This directory contains
only the configure/Makefile instructions required to build the tools.

This directory builds two programs for general use: go and gofmt.  It
also builds one program for internal use by the go tool: cgo.  For
more information on these tools see the doc.go files in the relevant
source code, which can also be seen hosted at golang.org:

http://golang.org/cmd/go
http://golang.org/cmd/gofmt
http://golang.org/cmd/cgo
This directory contains various files used by the gccadmin account on
gcc.gnu.org, mainly for automated tasks such as the daily update of
the date in gcc/DATESTAMP.  There isn't presently any scheme for files
checked in here to be automatically checked out and used by gccadmin,
so the files in Subversion and those used by gccadmin must be kept in
sync manually.

GNU Objective C notes
*********************

This document is to explain what has been done, and a little about how
specific features differ from other implementations.  The runtime has
been completely rewritten in gcc 2.4.  The earlier runtime had several
severe bugs and was rather incomplete.  The compiler has had several
new features added as well.

This is not documentation for Objective C, it is usable to someone
who knows Objective C from somewhere else.


Runtime API functions
=====================

The runtime is modeled after the NeXT Objective C runtime.  That is,
most functions have semantics as it is known from the NeXT.  The
names, however, have changed.  All runtime API functions have names
of lowercase letters and underscores as opposed to the
`traditional' mixed case names.  
	The runtime api functions are not documented as of now.
Someone offered to write it, and did it, but we were not allowed to
use it by his university (Very sad story).  We have started writing
the documentation over again.  This will be announced in appropriate
places when it becomes available.


Protocols
=========

Protocols are now fully supported.  The semantics is exactly as on the
NeXT.  There is a flag to specify how protocols should be typechecked
when adopted to classes.  The normal typechecker requires that all
methods in a given protocol must be implemented in the class that
adopts it -- it is not enough to inherit them.  The flag
`-Wno-protocol' causes it to allow inherited methods, while
`-Wprotocols' is the default which requires them defined.


+load
===========
This method, if defined, is called for each class and category
implementation when the class is loaded into the runtime.  This method
is not inherited, and is thus not called for a subclass that doesn't
define it itself.  Thus, each +load method is called exactly once by
the runtime.  The runtime invocation of this method is thread safe.


+initialize 
===========

This method, if defined, is called before any other instance or class
methods of that particular class.  For the GNU runtime, this method is 
not inherited, and is thus not called as initializer for a subclass that 
doesn't define it itself.  Thus, each +initialize method is called exactly 
once by the runtime (or never if no methods of that particular class is 
never called).  It is wise to guard against multiple invocations anyway 
to remain portable with the NeXT runtime.  The runtime invocation of 
this method is thread safe.


Passivation/Activation/Typedstreams
===================================

This is supported in the style of NeXT TypedStream's.  Consult the
headerfile Typedstreams.h for api functions.  I (Kresten) have
rewritten it in Objective C, but this implementation is not part of
2.4, it is available from the GNU Objective C prerelease archive. 
   There is one difference worth noting concerning objects stored with
objc_write_object_reference (aka NXWriteObjectReference).  When these
are read back in, their object is not guaranteed to be available until
the `-awake' method is called in the object that requests that object.
To objc_read_object you must pass a pointer to an id, which is valid
after exit from the function calling it (like e.g. an instance
variable).  In general, you should not use objects read in until the
-awake method is called.


Acknowledgements
================

The GNU Objective C team: Geoffrey Knauth <gsk@marble.com> (manager),
Tom Wood <wood@next.com> (compiler) and Kresten Krab Thorup
<krab@iesd.auc.dk> (runtime) would like to thank a some people for
participating in the development of the present GNU Objective C.

Paul Burchard <burchard@geom.umn.edu> and Andrew McCallum
<mccallum@cs.rochester.edu> has been very helpful debugging the
runtime.   Eric Herring <herring@iesd.auc.dk> has been very helpful
cleaning up after the documentation-copyright disaster and is now
helping with the new documentation.

Steve Naroff <snaroff@next.com> and Richard Stallman
<rms@gnu.ai.mit.edu> has been very helpful with implementation details
in the compiler.


Bug Reports
===========

Please read the section `Submitting Bugreports' of the gcc manual
before you submit any bugs.
This directory contains headers that are private to the runtime and
that are only included while the runtime is being compiled.  They are
not installed, so developers using the library can't actually even see
them.
This directory contains the public headers that are installed when
libobjc is installed.
The libbacktrace library
Initially written by Ian Lance Taylor <iant@google.com>

The libbacktrace library may be linked into a program or library and
used to produce symbolic backtraces.  Sample uses would be to print a
detailed backtrace when an error occurs or to gather detailed
profiling information.

The libbacktrace library is provided under a BSD license.  See the
source files for the exact license text.

The public functions are declared and documented in the header file
backtrace.h, which should be #include'd by a user of the library.

Building libbacktrace will generate a file backtrace-supported.h,
which a user of the library may use to determine whether backtraces
will work.  See the source file backtrace-supported.h.in for the
macros that it defines.

As of September 2012, libbacktrace only supports ELF executables with
DWARF debugging information.  The library is written to make it
straightforward to add support for other object file and debugging
formats.
$NetBSD: README.gcc53,v 1.1.1.1 2017/07/23 01:04:23 mrg Exp $

lib:
	libasan is disabled currently (haven't tried yet)

new stuff:
	cc1objcplus
	collect2 -- need to re-introduce?
	libcilkrts
	libmpx
	liboffloadmic
	libvtv
	libitm

other changes to look at:

Index: gcc/targhooks.c
	vs new binutils

+           doc/gcov-tool.1


arch/feature list.  anything not here has been switched already:

tools:		does build.sh tools work?
kernels:	does a kernel run?
libgcc:		does libgcc build?
native-gcc:	does a full mknative-gcc complete?
make release:	does build.sh release complete? 
runs:		does the system boot with a full world?
atf:		does atf run / compare well


architecture	tools	kernels	libgcc	native-gcc	make release	runs	atf
------------	-----	-------	------	----------	------------	----	---
arm		y	?	y	y		n[3]
armeb		y	?	y	y		n[3]
coldfire	y	N/A	y	y		?		N/A	N/A
earm		y	?	y	y		y[3,13]
earmeb		y	?	y	y		y[3,13]
earmhf		y	?	y	y		y[3]
earmhfeb	y	?	y	y		y[3]
earmv4		y	y	y	y		y[3]		y	y
earmv4eb	y	?	y	y		y[3]
earmv6		y	?	y	y		y[3]
earmv6eb	y	?	y	y		y[3]
earmv6hf	y	?	y	y		y[3]
earmv6hfeb	y	?	y	y		y[3]
earmv7		y	?	y	y		y[3]
earmv7eb	y	?	y	y		y[3]
earmv7hf	y	?	y	y		y[3]
earmv7hfeb	y	?	y	y		y[3]
m68000		y	?	y	y		y
m68k		y	y[16]	y	y		y[13]
mipseb		y	y	y	y		y		y
mipsel		y	y	y	y		y		y	y
mips64eb	y	y	y	y		y[8]		y	y
mips64el	y	?	y	y		y		y	y
powerpc		y	y	y	y		y		y[6]	
powerpc64	y	?	y	y		y
sh3eb		y	?	y	y		y[5]
sh3el		y	?	y	y		y
sparc		y	y	y	y		y		y[6]
sparc64		y	y	y	y		y		y[6,2]	y
vax		y	y	y	y		y		y[15]	n
--
or1k		n[10]
riscv32		n[10]
riscv64		n[10]
ia64
------------	-----	-------	------	----------	------------	----	---
architecture	tools	kernels	libgcc	native-gcc	make release	runs	atf


[2] - -O2 and -Os kernels hang, -O1 SIR reset
	-- may be a SMP issue; ultra10 works, ultra45 SMP does not
		-- try sb2000 (particularly with UP kernel.  u45 UP is shitty.)
[3] - MKCOMPAT=no enabled currently, infact OABI seems to be broken.  the docs say it was only deprecated, but the options make broken stuff.
[5] - sh3eb tries to use sh specific atomic config which doesn't work, has hand edited mknative output
[6] - crtbeginS.o builds incorrectly with GCC 5.3 and -O2.  a hack for -O1 has been added.
[8] - sgimips64 release build fails, mdsetimage'd gz'd kernels don't get built?
[10] - needs to be re-merged with GCC 5.3 versions of these not-merged-to-mainline-gcc ports.
[13] - builds tested:
       - m68k: mvm68k amiga atari
       - earm : shark evbarm*
       - mipseb: sgimips evbmips
       - mipsel: evbmips
       - mips64eb: sgimips[fail] evbmips
       - mips64el: evbmips
[14] - machines tested:
       - sparc ss20 (OK)
       - sparc64 ultra45 (FAIL), ultra10 (OK), sb2000 (OK UP, FAIL SMP.)
       - powerpc pegasosII (OK)
       - alpha UP1000 (OK)
       - arm shark (OK)
       - evbmips gxemul MALTA
		- had to implement some MIPS32 in gxemul to make this work
       - pmax gxemul (OK)
       - sgimips O2 (OK kernel -- but my O2 is not stable)
       - i386/amd64 (OK)
       - hppa (OK)
       - amiga (OK)
[15] - vax -- switched to GCC 5.3 already as it is less broken
	- ssh is broken, was broken with 4.8
	- there are too many -O0's we added because of mis-handled rtl
	- the eh_frame stuff is probably busted, but it never worked?
[16] - kernels tested:
	- m68k: amiga
This directory contains the GNU Compiler Collection (GCC).

The GNU Compiler Collection is free software.  See the files whose
names start with COPYING for copying permission.  The manuals, and
some of the runtime libraries, are under different terms; see the
individual source files for details.

The directory INSTALL contains copies of the installation information
as HTML and plain text.  The source of this information is
gcc/doc/install.texi.  The installation information includes details
of what is included in the GCC sources and what files GCC installs.

See the file gcc/doc/gcc.texi (together with other files that it
includes) for usage and porting information.  An online readable
version of the manual is in the files gcc/doc/gcc.info*.

See http://gcc.gnu.org/bugs/ for how to report bugs usefully.

Copyright years on GCC source files may be listed using range
notation, e.g., 1987-2012, indicating that every year in the range,
inclusive, is a copyrightable year that could otherwise be listed
individually.
This directory has been obsoleted for GCC snapshots and SVN access.

For releases the installation documentation is generated from
gcc/doc/install.texi and copied into this directory.

To read this documentation, please point your HTML browser to "index.html".
The latest version is always available at http://gcc.gnu.org/install/ .
GNU toolchain edition of GNU libintl 0.12.1

Most of the content of this directory is taken from gettext 0.12.1
and is owned by that project.  Patches should be directed to the
gettext developers first.  However, note the following:

* libintl.h comes from gettext, but is named libgnuintl.h.in in that
  project's source tree.

* The files COPYING.LIB-2.0 and COPYING.LIB-2.1 are redundant with the
  top-level COPYING.LIB and have therefore been removed.

* The files config.charset, ref-add.sin, ref-del.sin, os2compat.c,
  and os2compat.h are not used in this setup and have therefore been 
  removed.

* aclocal.m4 was constructed using automake's "aclocal -I ../config".

* configure.ac, config.intl.in, and Makefile.in were written for this
  directory layout, by Zack Weinberg <zack@codesourcery.com>.  Please
  direct patches for these files to gcc-patches@gcc.gnu.org.
file: libstdc++-v3/README

New users may wish to point their web browsers to the file
index.html in the 'doc/html' subdirectory.  It contains brief
building instructions and notes on how to configure the library in
interesting ways.
The HTML documentation in this folder is generated from the XML sources.

To change or edit, please edit the XML sources in the ../xml directory.
This directory contains scripts that are used by the regression
tester, <http://gcc.gnu.org/regtest/>

The primary script is 'btest-gcc.sh'.  This is the script that is run
to actually test the compiler.

'objs-gcc.sh' takes a combined tree and builds (but does not test) the
tools required for 'btest-gcc.sh'.  It is run periodically to update
the tools.  This script is followed by running 'btest-gcc.sh' using
the newly-build tools to check that they will not cause regressions.

'site.exp' is what $DEJAGNU points to when the regression tester runs
these scripts.

'GCC_Regression_Tester.wdgt' is a Dashboard widget that displays the
current state of the tester using Javascript.  You can use it without
needing Dashboard by pointing your web browser at
'GCC_Regression_Tester.wdgt/widget.html', if your browser supports
and permits it.

Note that any changes made here need to be approved by the regression
tester's maintainer (see MAINTAINERS).  The changes will be used on
the tester's next run, so `experimental' changes are very strongly
discouraged :-).
This directory contains files from examples of regression hunts, cut
down to smaller ranges to save space and time.  Try these out before
using the tools on your own tests.  First, update gcc-svn-env and
common.config for your own environment.

Each of the examples has multiple files:

  *.list files were created using gcc-svn-patchlist

  *.config files were written by hand based on earlier config files;
  the commented-out pieces are left as templates in case they're needed

  *.c, *.c++ are source files for the test, usually taken directly from
  the PR

  *.test files are tests specific to a bug when an existing gcc-test-*
  script can't be used

  *.log files are output from various scripts

Examples, where the identifier is the PR number:

  28970   wrong-code
  29106   special test, 4.1 branch
  29578   bogus-warning
  29906a  ice-on-valid-code, break
  29906b  ice-on-valid-code, fix
  30643   special test, cross compiler

Cut down the range even further by setting LOW_PATCH and HIGH_PATCH
within the config file to ids where the log file shows the test
passed or failed.

To run one, do

  reg-hunt 28970.config > 28970.log 2>&1

Check on its progress using

  ./reg-watch 28970.log

To run them all, do

  echo "hunt 28970" > queue
  echo "hunt 29106" >> queue
  echo "hunt 29578" >> queue
  echo "hunt 29906a" >> queue
  echo "hunt 29906b" >> queue
  echo "hunt 30643" >> queue
  ./testall queue

This allows you to add more to the queue if you're setting up lots of
hunts.
This directory contains scripts that are used for identifying the
patch that introduced a regression.  General information about such
searches is covered in http://gcc.gnu.org/bugs/reghunt.html.

  reg_search searches for a small time interval within a range of
  dates in which results for a test changed, using a binary search.
  The functionality for getting sources, building the component to
  test, and running the test are in other scripts that are run from
  here.

  reg_periodic invokes separate tools (the same scripts invoked by
  reg_search) over a range of dates at specified intervals.

  reg_test_template shows the format for the script that runs a test
  and determines whether to continue the search with a later or
  earlier date.
This directory contains the -liberty library of free software.
It is a collection of subroutines used by various GNU programs.
Current members include:

	getopt -- get options from command line
	obstack -- stacks of arbitrarily-sized objects
	strerror -- error message strings corresponding to errno
	strtol -- string-to-long conversion
	strtoul -- string-to-unsigned-long conversion

We expect many of the GNU subroutines that are floating around to
eventually arrive here.

The library must be configured from the top source directory.  Don't
try to run configure in this directory.  Follow the configuration
instructions in ../README.

Please report bugs to "gcc-bugs@gcc.gnu.org" and send fixes to
"gcc-patches@gcc.gnu.org".  Thank you.

ADDING A NEW FILE
=================

There are two sets of files:  Those that are "required" will be
included in the library for all configurations, while those
that are "optional" will be included in the library only if "needed."

To add a new required file, edit Makefile.in to add the source file
name to CFILES and the object file to REQUIRED_OFILES.

To add a new optional file, it must provide a single function, and the
name of the function must be the same as the name of the file.

    * Add the source file name to CFILES in Makefile.in and the object
      file to CONFIGURED_OFILES.

    * Add the function to name to the funcs shell variable in
      configure.ac.

    * Add the function to the AC_CHECK_FUNCS lists just after the
      setting of the funcs shell variable.  These AC_CHECK_FUNCS calls
      are never executed; they are there to make autoheader work
      better.

    * Consider the special cases of building libiberty; as of this
      writing, the special cases are newlib and VxWorks.  If a
      particular special case provides the function, you do not need
      to do anything.  If it does not provide the function, add the
      object file to LIBOBJS, and add the function name to the case
      controlling whether to define HAVE_func.

Finally, in the build directory of libiberty, configure with
"--enable-maintainer-mode", run "make maint-deps" to update
Makefile.in, and run 'make stamp-functions' to regenerate
functions.texi.

The optional file you've added (e.g. getcwd.c) should compile and work
on all hosts where it is needed.  It does not have to work or even
compile on hosts where it is not needed.

ADDING A NEW CONFIGURATION
==========================

On most hosts you should be able to use the scheme for automatically
figuring out which files are needed.  In that case, you probably
don't need a special Makefile stub for that configuration.

If the fully automatic scheme doesn't work, you may be able to get
by with defining EXTRA_OFILES in your Makefile stub.  This is
a list of object file names that should be treated as required
for this configuration - they will be included in libiberty.a,
regardless of whatever might be in the C library.
The files in this directory are part of the GNU C Library, not part of
GCC.  As described at <http://gcc.gnu.org/codingconventions.html>,
changes should be made to the GNU C Library and the changed files then
imported into GCC.
This README file is copied into the directory for GCC-only header files
when fixincludes is run by the makefile for GCC.

Many of the files in this directory were automatically edited from the
standard system header files by the fixincludes process.  They are
system-specific, and will not work on any other kind of system.  They
are also not part of GCC.  The reason we have to do this is because
GCC requires ANSI C headers and many vendors supply ANSI-incompatible
headers.

Because this is an automated process, sometimes headers get "fixed"
that do not, strictly speaking, need a fix.  As long as nothing is broken
by the process, it is just an unfortunate collateral inconvenience.
We would like to rectify it, if it is not "too inconvenient".

GCC MAINTAINER INFORMATION
==========================

If you are having some problem with a system header that is either
broken by the manufacturer, or is broken by the fixinclude process,
then you will need to alter or add information to the include fix
definitions file, ``inclhack.def''.  Please also send relevant
information to gcc-bugs@gcc.gnu.org, gcc-patches@gcc.gnu.org and,
please, to me:  bkorb@gnu.org.

To make your fix, you will need to do several things:

1.  Obtain access to the AutoGen program on some platform.  It does
    not have to be your build platform, but it is more convenient.

2.  Edit "inclhack.def" to reflect the changes you need to make.
    See below for information on how to make those changes.

3.  Run the "genfixes" shell script to produce a new copy of
    the "fixincl.x" file.

4.  Rebuild the compiler and check the header causing the issue.
    Make sure it is now properly handled.  Add tests to the
    "test_text" entry(ies) that validate your fix.  This will
    help ensure that future fixes won't negate your work.
    Do *NOT* specify test text for "wrap" or "replacement" fixes.
    There is no real possibility that these fixes will fail.
    If they do, you will surely know straight away.

    NOTE:  "test_text" is interpreted by the shell as it gets
    copied into the test header.  THEREFORE you must quote
    dollar sign characters and back quotes -- unless you mean
    for them to be interpreted by the shell.

    e.g. the math_huge_val_from_dbl_max test_text needs to
    put text into both float.h and math.h, so it includes a
    back-quoted script to add text to float.h.

5.  Go into the fixincludes build directory and type, "make check".
    You are guaranteed to have issues printed out as a result.
    Look at the diffs produced.  Make sure you have not clobbered
    the proper functioning of a different fix.  Make sure your
    fix is properly tested and it does what it is supposed to do.

6.  Now that you have the right things happening, synchronize the
    $(srcdir)/tests/base directory with the $(builddir)/tests/res
    directory.  The output of "make check" will be some diffs that
    should give you some hints about what to do.

7.  Rerun "make check" and verify that there are no issues left.


MAKING CHANGES TO INCLHACK.DEF
==============================

0.  If you are not the fixincludes maintainer, please send that
    person email about any changes you may want to make.  Thanks!

1.  Every fix must have a "hackname" that is compatible with C syntax
    for variable names and is unique without regard to alphabetic case.
    Please keep them alphabetical by this name.  :-)

2.  If the problem is known to exist only in certain files, then
    identify the files with "files = " entries.  If you use fnmatch(3C)
    wild card characters in a "files" entry, be certain that the first
    "files" entry has no such character.  Otherwise, the "make check"
    machinery will attempt to create files with those characters in the
    name.  That is inconvenient.

3.  It is relatively expensive to fire off a process to fix a source
    file, therefore write apply tests to avoid unnecessary fix
    processes.  The preferred apply tests are "select", "bypass", "mach"
    "sum", and "c-test" because they are performed internally:

    * select - Run a regex on the contents of the file being considered.
               All such regex-es must match.  Matching is done with
               extended regular expressions.

    * bypass - Run a regex on the contents of the file being considered.
               No such regex may match.

    * sum    - Select a specific version of a file that has a matching
               check sum.  The BSD version of checksum ["sum(1BSD)"]
               is used.  Each "sum" entry should contain exactly three
               space separated tokens:  the sum, some number and the
               basename of the file.  The "some number" is ignored.
               If there are multiple "sum" entries, only one needs to
               match in order to pass.  For example:

                   sum = '1234 3 foobar.h';

               specifies that the "foobar.h" header in any directory
               will match if it has the checksum 1234.

    * c_test - call a function in fixtests.c.  See that file.

    * files  - the "fnmatch" pattern of the file(s) to examine for
               the issue.  There may be several copies of this attribute.
               If the header lives in a /usr/include subdirectory, be
               sure to include that subdirectory in the name. e.g. net/if.h

    * mach   - Match the output of config.guess against a series of fnmatch
               patterns.  It must match at least one of the patterns, unless
               "not-machine" has also been specified.  In that case, the
               config.guess output must not match any of the patterns.

    The next test is relatively slow because it must be handled in a
    separate shell process.  Some platforms do not support server shells,
    so the whole process is even slower and more cumbersome there.

    * test   - These should be arguments to the program, "/bin/test".
               You may perform multiple commands, if you enclose them
               in backquotes and echo out valid test arguments.  For
               example, you might echo out '0 -eq 1' if you want a false
               result, or '0 -eq 0' for a true result.

    These tests are required to:

    1.  Be positive for all header files that require the fix.

    It is desirable to:

    2.  Be negative as often as possible whenever the fix is not
        required, avoiding the process overhead.

    It is nice if:

    3.  The expression is as simple as possible to both
        process and understand by people.  :-)

        Please take advantage of the fact AutoGen will glue
        together string fragments.  It helps.  Also take note
        that double quote strings and single quote strings have
        different formation rules.  Double quote strings are a
        tiny superset of ANSI-C string syntax.  Single quote
        strings follow shell single quote string formation
        rules, except that the backslash is processed before
        '\\', '\'' and '#' characters (using C character syntax).

    Each test must pass or the fix is not applied.  For example,
    all "select" expressions must be found and not one "bypass"
    selection may be found.

    Examples of test specifications:

      hackname = broken_assert_stdio;
      files    = assert.h;
      select   = stderr;
      bypass   = "include.*stdio.h";

    The ``broken_assert_stdio'' fix will be applied only to a file
    named "assert.h" if it contains the string "stderr" _and_ it
    does _not_ contain the expression "include.*stdio.h".

      hackname = no_double_slash;
      c_test   = "double_slash";

    The ``no_double_slash'' fix will be applied if the
    ``double_slash_test()'' function says to.  See ``fixtests.c''
    for documentation on how to include new functions into that
    module.

4.  There are currently four methods of fixing a file:

    1.  a series of sed expressions.  Each will be an individual
        "-e" argument to a single invocation of sed.  Unless you
        need to use multiple or complex sed expressions, please
        use the "replacement text" method instead.

    2.  a shell script.  These scripts are _required_ to read all
        of stdin in order to avoid pipe stalls.  They may choose to
        discard the input.

    3.  Replacement text.  If the replacement is empty, then no
        fix is applied.  Otherwise, the replacement text is
        written to the output file and no further fixes are
        applied.  If you really want a no-op file, replace the
        file with a comment.

        Replacement text "fixes" must be first in this file!!

    4.  A C language subroutine method for both tests and fixes.
        See ``fixtests.c'' for instructions on writing C-language
        applicability tests and ``fixfixes.c'' for C-language fixing.
        These files also contain tables that describe the currently
        implemented fixes and tests.

    If at all possible, you should try to use one of the C language
    fixes as it is far more efficient.  There are currently five
    such fixes, three of which are very special purpose:

    i) char_macro_def - This function repairs the definition of an
        ioctl macro that presumes CPP macro substitution within
        pairs of single quote characters.

    ii) char_macro_use - This function repairs the usage of ioctl
        macros that no longer can wrap an argument with single quotes.

    iii) machine_name - This function will look at "#if", "#ifdef",
        "#ifndef" and "#elif" directive lines and replace the first
        occurrence of a non-reserved name that is traditionally
        pre-defined by the native compiler.

    The next two are for general use:

    iv) wrap - wraps the entire file with "#ifndef", "#define" and
        "#endif" self-exclusionary text.  It also, optionally, inserts
        a prolog after the "#define" and an epilog just before the
        "#endif".  You can use this for a fix as follows:

            c_fix     = wrap;
            c_fix_arg = "/* prolog text */";
            c_fix_arg = "/* epilog text */";

        If you want an epilog without a prolog, set the first "c_fix_arg"
        to the empty string.  Both or the second "c_fix_arg"s may be
        omitted and the file will still be wrapped.

	THERE IS A SPECIAL EXCEPTION TO THIS, HOWEVER:

	If the regular expression '#if.*__need' is found, then it is
	assumed that the file needs to be read and interpreted more
	than once.  However, the prolog and epilog text (if any) will
	be inserted.

    v) format - Replaces text selected with a regular expression with
        a specialized formating string.  The formatting works as follows:
        The format text is copied to the output until a '%' character
        is found.  If the character after the '%' is another '%', then
        one '%' is output and processing continues.  If the following
        character is not a digit, then the '%' and that character are
        copied and processing continues.  Finally, if the '%' *is*
        followed by a digit, that digit is used as an index into the
        regmatch_t array to replace the two characters with the matched
        text.  i.e.: "%0" is replaced by the full matching text, "%1"
        is the first matching sub-expression, etc.

        This is used as follows:

            c_fix     = format;
            c_fix_arg = "#ifndef %1\n%0\n#endif";
            c_fix_arg = "#define[ \t]+([A-Z][A-Z0-9a-z_]*).*";

        This would wrap a one line #define inside of a "#ifndef"/"#endif"
        pair.  The second "c_fix_arg" may be omitted *IF* there is at least
        one select clause and the first one identifies the text you wish to
        reformat.  It will then be used as the second "c_fix_arg".  You may
        delete the selected text by supplying an empty string for the
        replacement format (the first "c_fix_arg").

	Note: In general, a format c_fix may be used in place of one
	sed expression.  However, it will need to be rewritten by
	hand.  For example:

	sed = 's@^#if __GNUC__ == 2 && __GNUC_MINOR__ >= 7$'
	       '@& || __GNUC__ >= 3@';

	may be rewritten using a format c_fix as:

	c_fix     = format;
	c_fix_arg = '%0 || __GNUC__ >= 3';
	c_fix_arg = '^#if __GNUC__ == 2 && __GNUC_MINOR__ >= 7$';

	Multiple sed substitution expressions probably ought to remain sed
	expressions in order to maintain clarity.  Also note that if the
	second sed expression is the same as the first select expression,
	then you may omit the second c_fix_arg.  The select expression will
	be picked up and used in its absence.

EXAMPLES OF FIXES:
==================

      hackname = AAA_ki_iface;
      replace; /* empty replacement -> no fixing the file */

    When this ``fix'' is invoked, it will prevent any fixes
    from being applied.

    ------------------

      hackname = AAB_svr4_no_varargs;
      replace  = "/* This file was generated by fixincludes.  */\n"
                 "#ifndef _SYS_VARARGS_H\n"
                 "#define _SYS_VARARGS_H\n\n"

                 "#ifdef __STDC__\n"
                 "#include <stdarg.h>\n"
                 "#else\n"
                 "#include <varargs.h>\n"
                 "#endif\n\n"

                 "#endif  /* _SYS_VARARGS_H */\n";

    When this ``fix'' is invoked, the replacement text will be
    emitted into the replacement include file.  No further fixes
    will be applied.

    ------------------

        hackname  = hpux11_fabsf;
        files     = math.h;
        select    = "^[ \t]*#[ \t]*define[ \t]+fabsf\\(.*";
        bypass    = "__cplusplus";

        c_fix     = format;
        c_fix_arg = "#ifndef __cplusplus\n%0\n#endif";

        test_text =
        "#  define fabsf(x) ((float)fabs((double)(float)(x)))\n";

    This fix will ensure that the #define for fabs is wrapped
    with C++ protection, providing the header is not already
    C++ aware.

    ------------------

5.  Testing fixes.

    The brute force method is, of course, to configure and build
    GCC.  But you can also:

        cd ${top_builddir}/gcc
        rm -rf include-fixed/ stmp-fixinc
        make stmp-fixinc

    I would really recommend, however:

        cd ${top_builddir}/fixincludes
        make check

    To do this, you *must* have autogen installed on your system.
    The "check" step will proceed to construct a shell script that
    will exercise all the fixes, using the sample test_text
    provided with each fix.  Once done, the changes made will
    be compared against the changes saved in the source directory.
    If you are changing the tests or fixes, the change will likely
    be highlighted.
Copyright (C) 2000-2015 Free Software Foundation, Inc.

This file is intended to contain a few notes about writing C code
within GCC so that it compiles without error on the full range of
compilers GCC needs to be able to compile on.

The problem is that many ISO-standard constructs are not accepted by
either old or buggy compilers, and we keep getting bitten by them.
This knowledge until now has been sparsely spread around, so I
thought I'd collect it in one useful place.  Please add and correct
any problems as you come across them.

I'm going to start from a base of the ISO C90 standard, since that is
probably what most people code to naturally.  Obviously using
constructs introduced after that is not a good idea.

For the complete coding style conventions used in GCC, please read
http://gcc.gnu.org/codingconventions.html


String literals
---------------

Irix6 "cc -n32" and OSF4 "cc" have problems with constant string
initializers with parens around it, e.g.

const char string[] = ("A string");

This is unfortunate since this is what the GNU gettext macro N_
produces.  You need to find a different way to code it.

Some compilers like MSVC++ have fairly low limits on the maximum
length of a string literal; 509 is the lowest we've come across.  You
may need to break up a long printf statement into many smaller ones.


Empty macro arguments
---------------------

ISO C (6.8.3 in the 1990 standard) specifies the following:

If (before argument substitution) any argument consists of no
preprocessing tokens, the behavior is undefined.

This was relaxed by ISO C99, but some older compilers emit an error,
so code like

#define foo(x, y) x y
foo (bar, )

needs to be coded in some other way.


Avoid unnecessary test before free
----------------------------------

Since SunOS 4 stopped being a reasonable portability target,
(which happened around 2007) there has been no need to guard
against "free (NULL)".  Thus, any guard like the following
constitutes a redundant test:

  if (P)
    free (P);

It is better to avoid the test.[*]
Instead, simply free P, regardless of whether it is NULL.

[*] However, if your profiling exposes a test like this in a
performance-critical loop, say where P is nearly always NULL, and
the cost of calling free on a NULL pointer would be prohibitively
high, consider using __builtin_expect, e.g., like this:

  if (__builtin_expect (ptr != NULL, 0))
    free (ptr);



Trigraphs
---------

You weren't going to use them anyway, but some otherwise ISO C
compliant compilers do not accept trigraphs.


Suffixes on Integer Constants
-----------------------------

You should never use a 'l' suffix on integer constants ('L' is fine),
since it can easily be confused with the number '1'.


			Common Coding Pitfalls
			======================

errno
-----

errno might be declared as a macro.


Implicit int
------------

In C, the 'int' keyword can often be omitted from type declarations.
For instance, you can write

  unsigned variable;

as shorthand for

  unsigned int variable;

There are several places where this can cause trouble.  First, suppose
'variable' is a long; then you might think

  (unsigned) variable

would convert it to unsigned long.  It does not.  It converts to
unsigned int.  This mostly causes problems on 64-bit platforms, where
long and int are not the same size.

Second, if you write a function definition with no return type at
all:

  operate (int a, int b)
  {
    ...
  }

that function is expected to return int, *not* void.  GCC will warn
about this.

Implicit function declarations always have return type int.  So if you
correct the above definition to

  void
  operate (int a, int b)
  ...

but operate() is called above its definition, you will get an error
about a "type mismatch with previous implicit declaration".  The cure
is to prototype all functions at the top of the file, or in an
appropriate header.

Char vs unsigned char vs int
----------------------------

In C, unqualified 'char' may be either signed or unsigned; it is the
implementation's choice.  When you are processing 7-bit ASCII, it does
not matter.  But when your program must handle arbitrary binary data,
or fully 8-bit character sets, you have a problem.  The most obvious
issue is if you have a look-up table indexed by characters.

For instance, the character '\341' in ISO Latin 1 is SMALL LETTER A
WITH ACUTE ACCENT.  In the proper locale, isalpha('\341') will be
true.  But if you read '\341' from a file and store it in a plain
char, isalpha(c) may look up character 225, or it may look up
character -31.  And the ctype table has no entry at offset -31, so
your program will crash.  (If you're lucky.)

It is wise to use unsigned char everywhere you possibly can.  This
avoids all these problems.  Unfortunately, the routines in <string.h>
take plain char arguments, so you have to remember to cast them back
and forth - or avoid the use of strxxx() functions, which is probably
a good idea anyway.

Another common mistake is to use either char or unsigned char to
receive the result of getc() or related stdio functions.  They may
return EOF, which is outside the range of values representable by
char.  If you use char, some legal character value may be confused
with EOF, such as '\377' (SMALL LETTER Y WITH UMLAUT, in Latin-1).
The correct choice is int.

A more subtle version of the same mistake might look like this:

  unsigned char pushback[NPUSHBACK];
  int pbidx;
  #define unget(c) (assert(pbidx < NPUSHBACK), pushback[pbidx++] = (c))
  #define get(c) (pbidx ? pushback[--pbidx] : getchar())
  ...
  unget(EOF);

which will mysteriously turn a pushed-back EOF into a SMALL LETTER Y
WITH UMLAUT.


Other common pitfalls
---------------------

o Expecting 'plain' char to be either sign or unsigned extending.

o Shifting an item by a negative amount or by greater than or equal to
  the number of bits in a type (expecting shifts by 32 to be sensible
  has caused quite a number of bugs at least in the early days).

o Expecting ints shifted right to be sign extended.

o Modifying the same value twice within one sequence point.

o Host vs. target floating point representation, including emitting NaNs
  and Infinities in a form that the assembler handles.

o qsort being an unstable sort function (unstable in the sense that
  multiple items that sort the same may be sorted in different orders
  by different qsort functions).

o Passing incorrect types to fprintf and friends.

o Adding a function declaration for a module declared in another file to
  a .c file instead of to a .h file.
This directory contains machine-specific files for the GNU C compiler.
It has a subdirectory for each basic CPU type.
The only files in this directory itself
are some .h files that pertain to particular operating systems
and are used for more than one CPU type.
Random Notes
------------

The MSP430 port does not use leading underscores.  However, the
assembler has no way of differentiating between, for example, register
R12 and symbol R12.  So, if you do "int r12;" in your C program, you
may get an assembler error, and will certainly have runtime problems.
		Arm / Thumb Interworking
		========================

The Cygnus GNU Pro Toolkit for the ARM7T processor supports function
calls between code compiled for the ARM instruction set and code
compiled for the Thumb instruction set and vice versa.  This document
describes how that interworking support operates and explains the
command line switches that should be used in order to produce working
programs.

Note:  The Cygnus GNU Pro Toolkit does not support switching between
compiling for the ARM instruction set and the Thumb instruction set
on anything other than a per file basis.  There are in fact two
completely separate compilers, one that produces ARM assembler
instructions and one that produces Thumb assembler instructions.  The
two compilers share the same assembler, linker and so on.


1. Explicit interworking support for C and C++ files
====================================================

By default if a file is compiled without any special command line
switches then the code produced will not support interworking.
Provided that a program is made up entirely from object files and
libraries produced in this way and which contain either exclusively
ARM instructions or exclusively Thumb instructions then this will not
matter and a working executable will be created.  If an attempt is
made to link together mixed ARM and Thumb object files and libraries,
then warning messages will be produced by the linker and a non-working
executable will be created.

In order to produce code which does support interworking it should be
compiled with the

	-mthumb-interwork

command line option.  Provided that a program is made up entirely from
object files and libraries built with this command line switch a
working executable will be produced, even if both ARM and Thumb
instructions are used by the various components of the program.  (No
warning messages will be produced by the linker either).

Note that specifying -mthumb-interwork does result in slightly larger,
slower code being produced.  This is why interworking support must be
specifically enabled by a switch.


2. Explicit interworking support for assembler files
====================================================

If assembler files are to be included into an interworking program
then the following rules must be obeyed:

	* Any externally visible functions must return by using the BX
	instruction.

	* Normal function calls can just use the BL instruction.  The
	linker will automatically insert code to switch between ARM
	and Thumb modes as necessary.

	* Calls via function pointers should use the BX instruction if
	the call is made in ARM mode:

		.code 32
		mov lr, pc
		bx  rX

	This code sequence will not work in Thumb mode however, since
	the mov instruction will not set the bottom bit of the lr
	register.  Instead a branch-and-link to the _call_via_rX
	functions should be used instead:

		.code 16
		bl  _call_via_rX

	where rX is replaced by the name of the register containing
	the function address.

	* All externally visible functions which should be entered in
	Thumb mode must have the .thumb_func pseudo op specified just
	before their entry point.  e.g.:

			.code 16
			.global function
			.thumb_func
		function:
			...start of function....

	* All assembler files must be assembled with the switch
	-mthumb-interwork specified on the command line.  (If the file
	is assembled by calling gcc it will automatically pass on the
	-mthumb-interwork switch to the assembler, provided that it
	was specified on the gcc command line in the first place.) 


3. Support for old, non-interworking aware code.
================================================

If it is necessary to link together code produced by an older,
non-interworking aware compiler, or code produced by the new compiler
but without the -mthumb-interwork command line switch specified, then
there are two command line switches that can be used to support this.

The switch

	-mcaller-super-interworking

will allow calls via function pointers in Thumb mode to work,
regardless of whether the function pointer points to old,
non-interworking aware code or not.  Specifying this switch does
produce slightly slower code however.

Note:  There is no switch to allow calls via function pointers in ARM
mode to be handled specially.  Calls via function pointers from
interworking aware ARM code to non-interworking aware ARM code work
without any special considerations by the compiler.  Calls via
function pointers from interworking aware ARM code to non-interworking
aware Thumb code however will not work.  (Actually under some
circumstances they may work, but there are no guarantees).  This is
because only the new compiler is able to produce Thumb code, and this
compiler already has a command line switch to produce interworking
aware code.


The switch

	-mcallee-super-interworking

will allow non-interworking aware ARM or Thumb code to call Thumb
functions, either directly or via function pointers.  Specifying this
switch does produce slightly larger, slower code however.

Note:  There is no switch to allow non-interworking aware ARM or Thumb
code to call ARM functions.  There is no need for any special handling
of calls from non-interworking aware ARM code to interworking aware
ARM functions, they just work normally.  Calls from non-interworking
aware Thumb functions to ARM code however, will not work.  There is no
option to support this, since it is always possible to recompile the
Thumb code to be interworking aware.

As an alternative to the command line switch
-mcallee-super-interworking, which affects all externally visible
functions in a file, it is possible to specify an attribute or
declspec for individual functions, indicating that that particular
function should support being called by non-interworking aware code.
The function should be defined like this:

	int __attribute__((interfacearm)) function 
	{
		... body of function ...
	}

or

	int __declspec(interfacearm) function
	{
		... body of function ...
	}



4. Interworking support in dlltool
==================================

It is possible to create DLLs containing mixed ARM and Thumb code.  It
is also possible to call Thumb code in a DLL from an ARM program and
vice versa.  It is even possible to call ARM DLLs that have been compiled
without interworking support (say by an older version of the compiler),
from Thumb programs and still have things work properly.

   A version of the `dlltool' program which supports the `--interwork'
command line switch is needed, as well as the following special
considerations when building programs and DLLs:

*Use `-mthumb-interwork'*
     When compiling files for a DLL or a program the `-mthumb-interwork'
     command line switch should be specified if calling between ARM and
     Thumb code can happen.  If a program is being compiled and the
     mode of the DLLs that it uses is not known, then it should be
     assumed that interworking might occur and the switch used.

*Use `-m thumb'*
     If the exported functions from a DLL are all Thumb encoded then the
     `-m thumb' command line switch should be given to dlltool when
     building the stubs.  This will make dlltool create Thumb encoded
     stubs, rather than its default of ARM encoded stubs.

     If the DLL consists of both exported Thumb functions and exported
     ARM functions then the `-m thumb' switch should not be used.
     Instead the Thumb functions in the DLL should be compiled with the
     `-mcallee-super-interworking' switch, or with the `interfacearm'
     attribute specified on their prototypes.  In this way they will be
     given ARM encoded prologues, which will work with the ARM encoded
     stubs produced by dlltool.

*Use `-mcaller-super-interworking'*
     If it is possible for Thumb functions in a DLL to call
     non-interworking aware code via a function pointer, then the Thumb
     code must be compiled with the `-mcaller-super-interworking'
     command line switch.  This will force the function pointer calls
     to use the _interwork_call_via_rX stub functions which will
     correctly restore Thumb mode upon return from the called function.

*Link with `libgcc.a'*
     When the dll is built it may have to be linked with the GCC
     library (`libgcc.a') in order to extract the _call_via_rX functions
     or the _interwork_call_via_rX functions.  This represents a partial
     redundancy since the same functions *may* be present in the
     application itself, but since they only take up 372 bytes this
     should not be too much of a consideration.

*Use `--support-old-code'*
     When linking a program with an old DLL which does not support
     interworking, the `--support-old-code' command line switch to the
     linker should be used.   This causes the linker to generate special
     interworking stubs which can cope with old, non-interworking aware
     ARM code, at the cost of generating bulkier code.  The linker will
     still generate a warning message along the lines of:
       "Warning: input file XXX does not support interworking, whereas YYY does."
     but this can now be ignored because the --support-old-code switch
     has been used.



5. How interworking support works
=================================

Switching between the ARM and Thumb instruction sets is accomplished
via the BX instruction which takes as an argument a register name.
Control is transferred to the address held in this register (with the
bottom bit masked out), and if the bottom bit is set, then Thumb
instruction processing is enabled, otherwise ARM instruction
processing is enabled.

When the -mthumb-interwork command line switch is specified, gcc
arranges for all functions to return to their caller by using the BX
instruction.  Thus provided that the return address has the bottom bit
correctly initialized to indicate the instruction set of the caller,
correct operation will ensue.

When a function is called explicitly (rather than via a function
pointer), the compiler generates a BL instruction to do this.  The
Thumb version of the BL instruction has the special property of
setting the bottom bit of the LR register after it has stored the
return address into it, so that a future BX instruction will correctly
return the instruction after the BL instruction, in Thumb mode.

The BL instruction does not change modes itself however, so if an ARM
function is calling a Thumb function, or vice versa, it is necessary
to generate some extra instructions to handle this.  This is done in
the linker when it is storing the address of the referenced function
into the BL instruction.  If the BL instruction is an ARM style BL
instruction, but the referenced function is a Thumb function, then the
linker automatically generates a calling stub that converts from ARM
mode to Thumb mode, puts the address of this stub into the BL
instruction, and puts the address of the referenced function into the
stub.  Similarly if the BL instruction is a Thumb BL instruction, and
the referenced function is an ARM function, the linker generates a
stub which converts from Thumb to ARM mode, puts the address of this
stub into the BL instruction, and the address of the referenced
function into the stub.

This is why it is necessary to mark Thumb functions with the
.thumb_func pseudo op when creating assembler files.  This pseudo op
allows the assembler to distinguish between ARM functions and Thumb
functions.  (The Thumb version of GCC automatically generates these
pseudo ops for any Thumb functions that it generates).

Calls via function pointers work differently.  Whenever the address of
a function is taken, the linker examines the type of the function
being referenced.  If the function is a Thumb function, then it sets
the bottom bit of the address.  Technically this makes the address
incorrect, since it is now one byte into the start of the function,
but this is never a problem because:

	a. with interworking enabled all calls via function pointer
	   are done using the BX instruction and this ignores the
	   bottom bit when computing where to go to.

	b. the linker will always set the bottom bit when the address
	   of the function is taken, so it is never possible to take
	   the address of the function in two different places and
	   then compare them and find that they are not equal.

As already mentioned any call via a function pointer will use the BX
instruction (provided that interworking is enabled).  The only problem
with this is computing the return address for the return from the
called function.  For ARM code this can easily be done by the code
sequence:

	mov	lr, pc
	bx	rX

(where rX is the name of the register containing the function
pointer).  This code does not work for the Thumb instruction set,
since the MOV instruction will not set the bottom bit of the LR
register, so that when the called function returns, it will return in
ARM mode not Thumb mode.  Instead the compiler generates this
sequence:

	bl	_call_via_rX

(again where rX is the name if the register containing the function
pointer).  The special call_via_rX functions look like this:

	.thumb_func
_call_via_r0:
	bx	r0
	nop

The BL instruction ensures that the correct return address is stored
in the LR register and then the BX instruction jumps to the address
stored in the function pointer, switch modes if necessary.


6. How caller-super-interworking support works
==============================================

When the -mcaller-super-interworking command line switch is specified
it changes the code produced by the Thumb compiler so that all calls
via function pointers (including virtual function calls) now go via a
different stub function.  The code to call via a function pointer now
looks like this:

	bl _interwork_call_via_r0

Note: The compiler does not insist that r0 be used to hold the
function address.  Any register will do, and there are a suite of stub
functions, one for each possible register.  The stub functions look
like this:

	.code 16
	.thumb_func
_interwork_call_via_r0
	bx 	pc
	nop
	
	.code 32
	tst	r0, #1
	stmeqdb	r13!, {lr}
	adreq	lr, _arm_return
	bx	r0

The stub first switches to ARM mode, since it is a lot easier to
perform the necessary operations using ARM instructions.  It then
tests the bottom bit of the register containing the address of the
function to be called.  If this bottom bit is set then the function
being called uses Thumb instructions and the BX instruction to come
will switch back into Thumb mode before calling this function.  (Note
that it does not matter how this called function chooses to return to
its caller, since the both the caller and callee are Thumb functions,
and mode switching is necessary).  If the function being called is an
ARM mode function however, the stub pushes the return address (with
its bottom bit set) onto the stack, replaces the return address with
the address of the a piece of code called '_arm_return' and then
performs a BX instruction to call the function.

The '_arm_return' code looks like this:

	.code 32
_arm_return:		
	ldmia 	r13!, {r12}
	bx 	r12
	.code 16


It simply retrieves the return address from the stack, and then
performs a BX operation to return to the caller and switch back into
Thumb mode.


7. How callee-super-interworking support works
==============================================

When -mcallee-super-interworking is specified on the command line the
Thumb compiler behaves as if every externally visible function that it
compiles has had the (interfacearm) attribute specified for it.  What
this attribute does is to put a special, ARM mode header onto the
function which forces a switch into Thumb mode:

  without __attribute__((interfacearm)):

		.code 16
		.thumb_func
	function:
		... start of function ...

  with __attribute__((interfacearm)):

		.code 32
	function:
		orr	r12, pc, #1
		bx	r12

		.code 16
                .thumb_func
        .real_start_of_function:

		... start of function ...

Note that since the function now expects to be entered in ARM mode, it
no longer has the .thumb_func pseudo op specified for its name.
Instead the pseudo op is attached to a new label .real_start_of_<name>
(where <name> is the name of the function) which indicates the start
of the Thumb code.  This does have the interesting side effect in that
if this function is now called from a Thumb mode piece of code
outside of the current file, the linker will generate a calling stub
to switch from Thumb mode into ARM mode, and then this is immediately
overridden by the function's header which switches back into Thumb
mode. 

In addition the (interfacearm) attribute also forces the function to
return by using the BX instruction, even if has not been compiled with
the -mthumb-interwork command line flag, so that the correct mode will
be restored upon exit from the function.


8. Some examples
================

   Given these two test files:

             int arm (void) { return 1 + thumb (); }

             int thumb (void) { return 2 + arm (); }

   The following pieces of assembler are produced by the ARM and Thumb
version of GCC depending upon the command line options used:

   `-O2':
             .code 32                               .code 16
             .global _arm                           .global _thumb
                                                    .thumb_func
     _arm:                                    _thumb:
             mov     ip, sp
             stmfd   sp!, {fp, ip, lr, pc}          push    {lr}
             sub     fp, ip, #4
             bl      _thumb                          bl      _arm
             add     r0, r0, #1                      add     r0, r0, #2
             ldmea   fp, {fp, sp, pc}                pop     {pc}

   Note how the functions return without using the BX instruction.  If
these files were assembled and linked together they would fail to work
because they do not change mode when returning to their caller.

   `-O2 -mthumb-interwork':

             .code 32                               .code 16
             .global _arm                           .global _thumb
                                                    .thumb_func
     _arm:                                    _thumb:
             mov     ip, sp
             stmfd   sp!, {fp, ip, lr, pc}          push    {lr}
             sub     fp, ip, #4
             bl      _thumb                         bl       _arm
             add     r0, r0, #1                     add      r0, r0, #2
             ldmea   fp, {fp, sp, lr}               pop      {r1}
             bx      lr                             bx       r1

   Now the functions use BX to return their caller.  They have grown by
4 and 2 bytes respectively, but they can now successfully be linked
together and be expect to work.  The linker will replace the
destinations of the two BL instructions with the addresses of calling
stubs which convert to the correct mode before jumping to the called
function.

   `-O2 -mcallee-super-interworking':

             .code 32                               .code 32
             .global _arm                           .global _thumb
     _arm:                                    _thumb:
                                                    orr      r12, pc, #1
                                                    bx       r12
             mov     ip, sp                         .code 16
             stmfd   sp!, {fp, ip, lr, pc}          push     {lr}
             sub     fp, ip, #4
             bl      _thumb                         bl       _arm
             add     r0, r0, #1                     add      r0, r0, #2
             ldmea   fp, {fp, sp, lr}               pop      {r1}
             bx      lr                             bx       r1

   The thumb function now has an ARM encoded prologue, and it no longer
has the `.thumb-func' pseudo op attached to it.  The linker will not
generate a calling stub for the call from arm() to thumb(), but it will
still have to generate a stub for the call from thumb() to arm().  Also
note how specifying `--mcallee-super-interworking' automatically
implies `-mthumb-interworking'.


9. Some Function Pointer Examples
=================================

   Given this test file:

     	int func (void) { return 1; }
     
     	int call (int (* ptr)(void)) { return ptr (); }

   The following varying pieces of assembler are produced by the Thumb
version of GCC depending upon the command line options used:

   `-O2':
     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.thumb_func
     	_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{pc}

   Note how the two functions have different exit sequences.  In
particular call() uses pop {pc} to return, which would not work if the
caller was in ARM mode.  func() however, uses the BX instruction, even
though `-mthumb-interwork' has not been specified, as this is the most
efficient way to exit a function when the return address is held in the
link register.

   `-O2 -mthumb-interwork':

     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.thumb_func
     	_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{r1}
     		bx	r1

   This time both functions return by using the BX instruction.  This
means that call() is now two bytes longer and several cycles slower
than the previous version.

   `-O2 -mcaller-super-interworking':
     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.thumb_func
     	_call:
     		push	{lr}
     		bl	__interwork_call_via_r0
     		pop	{pc}

   Very similar to the first (non-interworking) version, except that a
different stub is used to call via the function pointer.  This new stub
will work even if the called function is not interworking aware, and
tries to return to call() in ARM mode.  Note that the assembly code for
call() is still not interworking aware itself, and so should not be
called from ARM code.

   `-O2 -mcallee-super-interworking':

     		.code	32
     		.globl	_func
     	_func:
     		orr	r12, pc, #1
     		bx	r12
     
     		.code	16
     		.globl .real_start_of_func
     		.thumb_func
     	.real_start_of_func:
     		mov	r0, #1
     		bx	lr
     
     		.code	32
     		.globl	_call
     	_call:
     		orr	r12, pc, #1
     		bx	r12
     
     		.code	16
     		.globl .real_start_of_call
     		.thumb_func
     	.real_start_of_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{r1}
     		bx	r1

   Now both functions have an ARM coded prologue, and both functions
return by using the BX instruction.  These functions are interworking
aware therefore and can safely be called from ARM code.  The code for
the call() function is now 10 bytes longer than the original, non
interworking aware version, an increase of over 200%.

   If a prototype for call() is added to the source code, and this
prototype includes the `interfacearm' attribute:

     	int __attribute__((interfacearm)) call (int (* ptr)(void));

   then this code is produced (with only -O2 specified on the command
line):

     		.code	16
     		.globl	_func
     		.thumb_func
     	_func:
     		mov	r0, #1
     		bx	lr
     
     		.globl	_call
     		.code	32
     	_call:
     		orr	r12, pc, #1
     		bx	r12
     
     		.code	16
     		.globl .real_start_of_call
     		.thumb_func
     	.real_start_of_call:
     		push	{lr}
     		bl	__call_via_r0
     		pop	{r1}
     		bx	r1

   So now both call() and func() can be safely called via
non-interworking aware ARM code.  If, when such a file is assembled,
the assembler detects the fact that call() is being called by another
function in the same file, it will automatically adjust the target of
the BL instruction to point to .real_start_of_call.  In this way there
is no need for the linker to generate a Thumb-to-ARM calling stub so
that call can be entered in ARM mode.


10. How to use dlltool to build ARM/Thumb DLLs
==============================================
   Given a program (`prog.c') like this:

             extern int func_in_dll (void);
     
             int main (void) { return func_in_dll(); }

   And a DLL source file (`dll.c') like this:

             int func_in_dll (void) { return 1; }

   Here is how to build the DLL and the program for a purely ARM based
environment:

*Step One
     Build a `.def' file describing the DLL:

             ; example.def
             ; This file describes the contents of the DLL
             LIBRARY     example
             HEAPSIZE    0x40000, 0x2000
             EXPORTS
                          func_in_dll  1

*Step Two
     Compile the DLL source code:

            arm-pe-gcc -O2 -c dll.c

*Step Three
     Use `dlltool' to create an exports file and a library file:

            dlltool --def example.def --output-exp example.o --output-lib example.a

*Step Four
     Link together the complete DLL:

            arm-pe-ld dll.o example.o -o example.dll

*Step Five
     Compile the program's source code:

            arm-pe-gcc -O2 -c prog.c

*Step Six
     Link together the program and the DLL's library file:

            arm-pe-gcc prog.o example.a -o prog

   If instead this was a Thumb DLL being called from an ARM program, the
steps would look like this.  (To save space only those steps that are
different from the previous version are shown):

*Step Two
     Compile the DLL source code (using the Thumb compiler):

            thumb-pe-gcc -O2 -c dll.c -mthumb-interwork

*Step Three
     Build the exports and library files (and support interworking):

            dlltool -d example.def -z example.o -l example.a --interwork -m thumb

*Step Five
     Compile the program's source code (and support interworking):

            arm-pe-gcc -O2 -c prog.c -mthumb-interwork

   If instead, the DLL was an old, ARM DLL which does not support
interworking, and which cannot be rebuilt, then these steps would be
used.

*Step One
     Skip.  If you do not have access to the sources of a DLL, there is
     no point in building a `.def' file for it.

*Step Two
     Skip.  With no DLL sources there is nothing to compile.

*Step Three
     Skip.  Without a `.def' file you cannot use dlltool to build an
     exports file or a library file.

*Step Four
     Skip.  Without a set of DLL object files you cannot build the DLL.
     Besides it has already been built for you by somebody else.

*Step Five
     Compile the program's source code, this is the same as before:

            arm-pe-gcc -O2 -c prog.c

*Step Six
     Link together the program and the DLL's library file, passing the
     `--support-old-code' option to the linker:

            arm-pe-gcc prog.o example.a -Wl,--support-old-code -o prog

     Ignore the warning message about the input file not supporting
     interworking as the --support-old-code switch has taken care if this.


Copyright (C) 1998-2015 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
AddressSanitizer (http://code.google.com/p/address-sanitizer) and
ThreadSanitizer (http://code.google.com/p/thread-sanitizer/) are
projects initially developed by Google Inc.
Both tools consist of a compiler module and a run-time library.
The sources of the run-time library for these projects are hosted at
http://llvm.org/svn/llvm-project/compiler-rt in the following directories:
  include/sanitizer
  lib/sanitizer_common
  lib/interception
  lib/asan
  lib/tsan
  lib/lsan
  lib/ubsan

Trivial and urgent fixes (portability, build fixes, etc.) may go directly to the
GCC tree.  All non-trivial changes, functionality improvements, etc. should go
through the upstream tree first and then be merged back to the GCC tree.
The merges from upstream should be done with the aid of the merge.sh script;
it will also update the file MERGE to contain the upstream revision
we merged with.
This directory builds tools used by people working in the Go language.
The source code for these tools lives in libgo/go/cmd, where it is
copied from the master gofrontend repository.  This directory contains
only the configure/Makefile instructions required to build the tools.

This directory builds two programs for general use: go and gofmt.  It
also builds one program for internal use by the go tool: cgo.  For
more information on these tools see the doc.go files in the relevant
source code, which can also be seen hosted at golang.org:

http://golang.org/cmd/go
http://golang.org/cmd/gofmt
http://golang.org/cmd/cgo
This directory contains various files used by the gccadmin account on
gcc.gnu.org, mainly for automated tasks such as the daily update of
the date in gcc/DATESTAMP.  There isn't presently any scheme for files
checked in here to be automatically checked out and used by gccadmin,
so the files in Subversion and those used by gccadmin must be kept in
sync manually.

GNU Objective C notes
*********************

This document is to explain what has been done, and a little about how
specific features differ from other implementations.  The runtime has
been completely rewritten in gcc 2.4.  The earlier runtime had several
severe bugs and was rather incomplete.  The compiler has had several
new features added as well.

This is not documentation for Objective C, it is usable to someone
who knows Objective C from somewhere else.


Runtime API functions
=====================

The runtime is modeled after the NeXT Objective C runtime.  That is,
most functions have semantics as it is known from the NeXT.  The
names, however, have changed.  All runtime API functions have names
of lowercase letters and underscores as opposed to the
`traditional' mixed case names.  
	The runtime api functions are not documented as of now.
Someone offered to write it, and did it, but we were not allowed to
use it by his university (Very sad story).  We have started writing
the documentation over again.  This will be announced in appropriate
places when it becomes available.


Protocols
=========

Protocols are now fully supported.  The semantics is exactly as on the
NeXT.  There is a flag to specify how protocols should be typechecked
when adopted to classes.  The normal typechecker requires that all
methods in a given protocol must be implemented in the class that
adopts it -- it is not enough to inherit them.  The flag
`-Wno-protocol' causes it to allow inherited methods, while
`-Wprotocols' is the default which requires them defined.


+load
===========
This method, if defined, is called for each class and category
implementation when the class is loaded into the runtime.  This method
is not inherited, and is thus not called for a subclass that doesn't
define it itself.  Thus, each +load method is called exactly once by
the runtime.  The runtime invocation of this method is thread safe.


+initialize 
===========

This method, if defined, is called before any other instance or class
methods of that particular class.  For the GNU runtime, this method is 
not inherited, and is thus not called as initializer for a subclass that 
doesn't define it itself.  Thus, each +initialize method is called exactly 
once by the runtime (or never if no methods of that particular class is 
never called).  It is wise to guard against multiple invocations anyway 
to remain portable with the NeXT runtime.  The runtime invocation of 
this method is thread safe.


Passivation/Activation/Typedstreams
===================================

This is supported in the style of NeXT TypedStream's.  Consult the
headerfile Typedstreams.h for api functions.  I (Kresten) have
rewritten it in Objective C, but this implementation is not part of
2.4, it is available from the GNU Objective C prerelease archive. 
   There is one difference worth noting concerning objects stored with
objc_write_object_reference (aka NXWriteObjectReference).  When these
are read back in, their object is not guaranteed to be available until
the `-awake' method is called in the object that requests that object.
To objc_read_object you must pass a pointer to an id, which is valid
after exit from the function calling it (like e.g. an instance
variable).  In general, you should not use objects read in until the
-awake method is called.


Acknowledgements
================

The GNU Objective C team: Geoffrey Knauth <gsk@marble.com> (manager),
Tom Wood <wood@next.com> (compiler) and Kresten Krab Thorup
<krab@iesd.auc.dk> (runtime) would like to thank a some people for
participating in the development of the present GNU Objective C.

Paul Burchard <burchard@geom.umn.edu> and Andrew McCallum
<mccallum@cs.rochester.edu> has been very helpful debugging the
runtime.   Eric Herring <herring@iesd.auc.dk> has been very helpful
cleaning up after the documentation-copyright disaster and is now
helping with the new documentation.

Steve Naroff <snaroff@next.com> and Richard Stallman
<rms@gnu.ai.mit.edu> has been very helpful with implementation details
in the compiler.


Bug Reports
===========

Please read the section `Submitting Bugreports' of the gcc manual
before you submit any bugs.
This directory contains headers that are private to the runtime and
that are only included while the runtime is being compiled.  They are
not installed, so developers using the library can't actually even see
them.
This directory contains the public headers that are installed when
libobjc is installed.
The libbacktrace library
Initially written by Ian Lance Taylor <iant@google.com>

The libbacktrace library may be linked into a program or library and
used to produce symbolic backtraces.  Sample uses would be to print a
detailed backtrace when an error occurs or to gather detailed
profiling information.

The libbacktrace library is provided under a BSD license.  See the
source files for the exact license text.

The public functions are declared and documented in the header file
backtrace.h, which should be #include'd by a user of the library.

Building libbacktrace will generate a file backtrace-supported.h,
which a user of the library may use to determine whether backtraces
will work.  See the source file backtrace-supported.h.in for the
macros that it defines.

As of September 2012, libbacktrace only supports ELF executables with
DWARF debugging information.  The library is written to make it
straightforward to add support for other object file and debugging
formats.

                Notes on enabling maintainer mode

Note that if you configure with --enable-maintainer-mode, you will need
special versions of automake, autoconf, libtool and gettext. You will
find the sources for these in the respective upstream directories:

  ftp://ftp.gnu.org/gnu/autoconf
  ftp://ftp.gnu.org/gnu/automake
  ftp://ftp.gnu.org/gnu/libtool
  ftp://ftp.gnu.org/gnu/gettext

The required versions of the tools for this tree are
  autoconf 2.64
  automake 1.11
  libtool 2.2.6
  gettext 0.14.5

Note - "make distclean" does not work with maintainer mode enabled.
The Makefiles in the some of the po/ subdirectories depend upon the
Makefiles in their parent directories, and distclean will delete the
Makefiles in the parent directories before running the Makefiles in
the child directories.  There is no easy way around this (short of
changing the automake macros) as these dependencies need to exist in
order to correctly build the NLS files.
		   README for GNU development tools

This directory contains various GNU compilers, assemblers, linkers, 
debuggers, etc., plus their support routines, definitions, and documentation.

If you are receiving this as part of a GDB release, see the file gdb/README.
If with a binutils release, see binutils/README;  if with a libg++ release,
see libg++/README, etc.  That'll give you info about this
package -- supported targets, how to use it, how to report bugs, etc.

It is now possible to automatically configure and build a variety of
tools with one command.  To build all of the tools contained herein,
run the ``configure'' script here, e.g.:

	./configure 
	make

To install them (by default in /usr/local/bin, /usr/local/lib, etc),
then do:
	make install

(If the configure script can't determine your type of computer, give it
the name as an argument, for instance ``./configure sun4''.  You can
use the script ``config.sub'' to test whether a name is recognized; if
it is, config.sub translates it to a triplet specifying CPU, vendor,
and OS.)

If you have more than one compiler on your system, it is often best to
explicitly set CC in the environment before running configure, and to
also set CC when running make.  For example (assuming sh/bash/ksh):

	CC=gcc ./configure
	make

A similar example using csh:

	setenv CC gcc
	./configure
	make

Much of the code and documentation enclosed is copyright by
the Free Software Foundation, Inc.  See the file COPYING or
COPYING.LIB in the various directories, for a description of the
GNU General Public License terms under which you can copy the files.

REPORTING BUGS: Again, see gdb/README, binutils/README, etc., for info
on where and how to report problems.
		README for GPROF

This is the GNU profiler.  It is distributed with other "binary
utilities" which should be in ../binutils.  See ../binutils/README for
more general notes, including where to send bug reports.

This file documents the changes and new features available with this
version of GNU gprof.

* New Features

 o Long options

 o Supports generalized file format, without breaking backward compatibility:
   new file format supports basic-block execution counts and non-realtime
   histograms (see below)

 o Supports profiling at the line level: flat profiles, call-graph profiles,
   and execution-counts can all be displayed at a level that identifies
   individual lines rather than just functions

 o Test-coverage support (similar to Sun tcov program): source files
   can be annotated with the number of times a function was invoked
   or with the number of times each basic-block in a function was
   executed

 o Generalized histograms: not just execution-time, but arbitrary
   histograms are support (for example, performance counter based
   profiles)

 o Powerful mechanism to select data to be included/excluded from
   analysis and/or output

 o Support for DEC OSF/1 v3.0

 o Full cross-platform profiling support: gprof uses BFD to support
   arbitrary, non-native object file formats and non-native byte-orders
   (this feature has not been tested yet)

 o In the call-graph function index, static function names are now
   printed together with the filename in which the function was defined
   (required bfd_find_nearest_line() support and symbolic debugging
    information to be present in the executable file)

 o Major overhaul of source code (compiles cleanly with -Wall, etc.)

* Supported Platforms

The current version is known to work on:

 o DEC OSF/1 v3.0
	All features supported.

 o SunOS 4.1.x
	All features supported.

 o Solaris 2.3
	Line-level profiling unsupported because bfd_find_nearest_line()
	is not fully implemented for Elf binaries.

 o HP-UX 9.01
	Line-level profiling unsupported because bfd_find_nearest_line()
	is not fully implemented for SOM binaries.

* Detailed Description

** User Interface Changes

The command-line interface is backwards compatible with earlier
versions of GNU gprof and Berkeley gprof.  The only exception is
the option to delete arcs from the call graph.  The old syntax
was:

	-k fromname toname

while the new syntax is:

	-k fromname/toname

This change was necessary to be compatible with long-option parsing.
Also, "fromname" and "toname" can now be arbitrary symspecs rather
than just function names (see below for an explanation of symspecs).
For example, option "-k gprof.c/" suppresses all arcs due to calls out
of file "gprof.c".

*** Sym Specs

It is often necessary to apply gprof only to specific parts of a
program.  GNU gprof has a simple but powerful mechanism to achieve
this.  So called {\em symspecs\/} provide the foundation for this
mechanism.  A symspec selects the parts of a profiled program to which
an operation should be applied to.  The syntax of a symspec is
simple:

	  filename_containing_a_dot
	| funcname_not_containing_a_dot
	| linenumber
	| ( [ any_filename ] `:' ( any_funcname | linenumber ) )

Here are some examples:

	main.c			Selects everything in file "main.c"---the
				dot in the string tells gprof to interpret
				the string as a filename, rather than as
				a function name.  To select a file whose
				name does contain a dot, a trailing colon
				should be specified.  For example, "odd:" is
				interpreted as the file named "odd".

	main			Selects all functions named "main".  Notice
				that there may be multiple instances of the
				same function name because some of the
				definitions may be local (i.e., static).
				Unless a function name is unique in a program,
				you must use the colon notation explained
				below to specify a function from a specific
				source file.  Sometimes, functionnames contain
				dots.  In such cases, it is necessary to
				add a leading colon to the name.  For example,
				":.mul" selects function ".mul".

	main.c:main		Selects function "main" in file "main.c".

	main.c:134		Selects line 134 in file "main.c".

IMPLEMENTATION NOTE: The source code uses the type sym_id for symspecs.
At some point, this probably ought to be changed to "sym_spec" to make
reading the code easier.

*** Long options

GNU gprof now supports long options.  The following is a list of all
supported options.  Options that are listed without description
operate in the same manner as the corresponding option in older
versions of gprof.

Short Form:	Long Form:
-----------	----------
-l		--line
			Request profiling at the line-level rather
			than just at the function level.  Source
			lines are identified by symbols of the form:

				func (file:line)

			where "func" is the function name, "file" is the
			file name and "line" is the line-number that
			corresponds to the line.

			To work properly, the binary must contain symbolic
			debugging information.  This means that the source
			have to be translated with option "-g" specified.
			Functions for which there is no symbolic debugging
			information available are treated as if "--line"
			had not been specified.  However, the line number
			printed with such symbols is usually incorrect
			and should be ignored.

-a		--no-static
-A[symspec]	--annotated-source[=symspec]
			Request output in the form of annotated source
			files.  If "symspec" is specified, print output only
			for symbols selected by "symspec".  If the option
			is specified multiple times, annotated output is
			generated for the union of all symspecs.

			Examples:

			  -A		Prints annotated source for all
					source files.
			  -Agprof.c	Prints annotated source for file
					gprof.c.
			  -Afoobar	Prints annotated source for files
					containing a function named "foobar".
					The entire file will be printed, but
					only the function itself will be
					annotated with profile data.

-J[symspec]	--no-annotated-source[=symspec]
			Suppress annotated source output.  If specified
			without argument, annotated output is suppressed
			completely.  With an argument, annotated output
			is suppressed only for the symbols selected by
			"symspec".  If the option is specified multiple
			times, annotated output is suppressed for the
			union of all symspecs.  This option has lower
			precedence than --annotated-source

-p[symspec]	--flat-profile[=symspec]
			Request output in the form of a flat profile
			(unless any other output-style option is specified,
			 this option is turned on by default).  If
			"symspec" is specified, include only symbols
			selected by "symspec" in flat profile.  If the
			option is specified multiple times, the flat
			profile includes symbols selected by the union
			of all symspecs.

-P[symspec]	--no-flat-profile[=symspec]
			Suppress output in the flat profile.  If given
			without an argument, the flat profile is suppressed
			completely.  If "symspec" is specified, suppress
			the selected symbols in the flat profile.  If the
			option is specified multiple times, the union of
			the selected symbols is suppressed.  This option
			has lower precedence than --flat-profile.

-q[symspec]	--graph[=symspec]
			Request output in the form of a call-graph
			(unless any other output-style option is specified,
			 this option is turned on by default).  If "symspec"
			is specified, include only symbols selected by
			"symspec" in the call-graph.  If the option is
			specified multiple times, the call-graph includes
			symbols selected by the union of all symspecs.

-Q[symspec]	--no-graph[=symspec]
			Suppress output in the call-graph.  If given without
			an argument, the call-graph is suppressed completely.
			With a "symspec", suppress the selected symbols
			from the call-graph.  If the option is specified
			multiple times, the union of the selected symbols
			is suppressed.  This option has lower precedence
			than --graph.

-C[symspec]	--exec-counts[=symspec]
			Request output in the form of execution counts.
			If "symspec" is present, include only symbols
			selected by "symspec" in the execution count
			listing.  If the option is specified multiple
			times, the execution count listing includes
			symbols selected by the union of all symspecs.

-Z[symspec]	--no-exec-counts[=symspec]
			Suppress output in the execution count listing.
			If given without an argument, the listing is
			suppressed completely.  With a "symspec", suppress
			the selected symbols from the call-graph.  If the
			option is specified multiple times, the union of
			the selected symbols is suppressed.  This option
			has lower precedence than --exec-counts.

-i		--file-info
			Print information about the profile files that
			are read.  The information consists of the
			number and types of records present in the
			profile file.  Currently, a profile file can
			contain any number and any combination of histogram,
			call-graph, or basic-block count records.

-s		--sum

-x		--all-lines
			This option affects annotated source output only.
			By default, only the lines at the beginning of
			a basic-block are annotated.  If this option is
			specified, every line in a basic-block is annotated
			by repeating the annotation for the first line.
			This option is identical to tcov's "-a".

-I dirs		--directory-path=dirs
			This option affects annotated source output only.
			Specifies the list of directories to be searched
			for source files.  The argument "dirs" is a colon
			separated list of directories.  By default, gprof
			searches for source files relative to the current
			working directory only.

-z		--display-unused-functions

-m num		--min-count=num
			This option affects annotated source and execution
			count output only.  Symbols that are executed
			less than "num" times are suppressed.  For annotated
			source output, suppressed symbols are marked
			by five hash-marks (#####).  In an execution count
			output, suppressed symbols do not appear at all.

-L		--print-path
			Normally, source filenames are printed with the path
			component suppressed.  With this option, gprof
			can be forced to print the full pathname of
			source filenames.  The full pathname is determined
			from symbolic debugging information in the image file
			and is relative to the directory in which the compiler
			was invoked.

-y		--separate-files
			This option affects annotated source output only.
			Normally, gprof prints annotated source files
			to standard-output.  If this option is specified,
			annotated source for a file named "path/filename"
			is generated in the file "filename-ann".  That is,
			annotated output is {\em always\/} generated in
			gprof's current working directory.  Care has to
			be taken if a program consists of files that have
			identical filenames, but distinct paths.

-c		--static-call-graph

-t num		--table-length=num
			This option affects annotated source output only.
			After annotating a source file, gprof generates
			an execution count summary consisting of a table
			of lines with the top execution counts.  By
			default, this table is ten entries long.
			This option can be used to change the table length
			or, by specifying an argument value of 0, it can be
			suppressed completely.

-n symspec	--time=symspec
			Only symbols selected by "symspec" are considered
			in total and percentage time computations.
			However, this option does not affect percentage time
			computation for the flat profile.
			If the option is specified multiple times, the union
			of all selected symbols is used in time computations.

-N		--no-time=symspec
			Exclude the symbols selected by "symspec" from
			total and percentage time computations.
			However, this option does not affect percentage time
			computation for the flat profile.
			This option is ignored if any --time options are
			specified.

-w num		--width=num
			Sets the output line width.  Currently, this option
			affects the printing of the call-graph function index
			only.

-e		<no long form---for backwards compatibility only>
-E		<no long form---for backwards compatibility only>
-f		<no long form---for backwards compatibility only>
-F		<no long form---for backwards compatibility only>
-k		<no long form---for backwards compatibility only>
-b		--brief
-dnum		--debug[=num]

-h		--help
			Prints a usage message.

-O name		--file-format=name
			Selects the format of the profile data files.
			Recognized formats are "auto", "bsd", "magic",
			and "prof".  The last one is not yet supported.
			Format "auto" attempts to detect the file format
			automatically (this is the default behavior).
			It attempts to read the profile data files as
			"magic" files and if this fails, falls back to
			the "bsd" format.  "bsd" forces gprof to read
			the data files in the BSD format.  "magic" forces
			gprof to read the data files in the "magic" format.

-T		--traditional
-v		--version

** File Format Changes

The old BSD-derived format used for profile data does not contain a
magic cookie that allows to check whether a data file really is a
gprof file.  Furthermore, it does not provide a version number, thus
rendering changes to the file format almost impossible.  GNU gprof
uses a new file format that provides these features.  For backward
compatibility, GNU gprof continues to support the old BSD-derived
format, but not all features are supported with it.  For example,
basic-block execution counts cannot be accommodated by the old file
format.

The new file format is defined in header file \file{gmon_out.h}.  It
consists of a header containing the magic cookie and a version number,
as well as some spare bytes available for future extensions.  All data
in a profile data file is in the native format of the host on which
the profile was collected.  GNU gprof adapts automatically to the
byte-order in use.

In the new file format, the header is followed by a sequence of
records.  Currently, there are three different record types: histogram
records, call-graph arc records, and basic-block execution count
records.  Each file can contain any number of each record type.  When
reading a file, GNU gprof will ensure records of the same type are
compatible with each other and compute the union of all records.  For
example, for basic-block execution counts, the union is simply the sum
of all execution counts for each basic-block.

*** Histogram Records

Histogram records consist of a header that is followed by an array of
bins.  The header contains the text-segment range that the histogram
spans, the size of the histogram in bytes (unlike in the old BSD
format, this does not include the size of the header), the rate of the
profiling clock, and the physical dimension that the bin counts
represent after being scaled by the profiling clock rate.  The
physical dimension is specified in two parts: a long name of up to 15
characters and a single character abbreviation.  For example, a
histogram representing real-time would specify the long name as
"seconds" and the abbreviation as "s".  This feature is useful for
architectures that support performance monitor hardware (which,
fortunately, is becoming increasingly common).  For example, under DEC
OSF/1, the "uprofile" command can be used to produce a histogram of,
say, instruction cache misses.  In this case, the dimension in the
histogram header could be set to "i-cache misses" and the abbreviation
could be set to "1" (because it is simply a count, not a physical
dimension).  Also, the profiling rate would have to be set to 1 in
this case.

Histogram bins are 16-bit numbers and each bin represent an equal
amount of text-space.  For example, if the text-segment is one
thousand bytes long and if there are ten bins in the histogram, each
bin represents one hundred bytes.


*** Call-Graph Records

Call-graph records have a format that is identical to the one used in
the BSD-derived file format.  It consists of an arc in the call graph
and a count indicating the number of times the arc was traversed
during program execution.  Arcs are specified by a pair of addresses:
the first must be within caller's function and the second must be
within the callee's function.  When performing profiling at the
function level, these addresses can point anywhere within the
respective function.  However, when profiling at the line-level, it is
better if the addresses are as close to the call-site/entry-point as
possible.  This will ensure that the line-level call-graph is able to
identify exactly which line of source code performed calls to a
function.

*** Basic-Block Execution Count Records

Basic-block execution count records consist of a header followed by a
sequence of address/count pairs.  The header simply specifies the
length of the sequence.  In an address/count pair, the address
identifies a basic-block and the count specifies the number of times
that basic-block was executed.  Any address within the basic-address can
be used.

IMPLEMENTATION NOTE: gcc -a can be used to instrument a program to
record basic-block execution counts.  However, the __bb_exit_func()
that is currently present in libgcc2.c does not generate a gmon.out
file in a suitable format.  This should be fixed for future releases
of gcc.  In the meantime, contact davidm@cs.arizona.edu for a version
of __bb_exit_func() to is appropriate.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
GNU toolchain edition of GNU libintl 0.12.1

Most of the content of this directory is taken from gettext 0.12.1
and is owned by that project.  Patches should be directed to the
gettext developers first.  However, note the following:

* libintl.h comes from gettext, but is named libgnuintl.h.in in that
  project's source tree.

* The files COPYING.LIB-2.0 and COPYING.LIB-2.1 are redundant with the
  top-level COPYING.LIB and have therefore been removed.

* The files config.charset, ref-add.sin, ref-del.sin, os2compat.c,
  and os2compat.h are not used in this setup and have therefore been 
  removed.

* aclocal.m4 was constructed using automake's "aclocal -I ../config".

* configure.ac, config.intl.in, and Makefile.in were written for this
  directory layout, by Zack Weinberg <zack@codesourcery.com>.  Please
  direct patches for these files to gcc-patches@gcc.gnu.org.
BFD is an object file library.  It permits applications to use the
same routines to process object files regardless of their format.

BFD is used by the GNU debugger, assembler, linker, and the binary
utilities.

The documentation on using BFD is scanty and may be occasionally
incorrect.  Pointers to documentation problems, or an entirely
rewritten manual, would be appreciated.

There is some BFD internals documentation in doc/bfdint.texi which may
help programmers who want to modify BFD.

BFD is normally built as part of another package.  See the build
instructions for that package, probably in a README file in the
appropriate directory.

BFD supports the following configure options:

  --target=TARGET
	The default target for which to build the library.  TARGET is
	a configuration target triplet, such as sparc-sun-solaris.
  --enable-targets=TARGET,TARGET,TARGET...
	Additional targets the library should support.  To include
	support for all known targets, use --enable-targets=all.
  --enable-64-bit-bfd
	Include support for 64 bit targets.  This is automatically
	turned on if you explicitly request a 64 bit target, but not
	for --enable-targets=all.  This requires a compiler with a 64
	bit integer type, such as gcc.
  --enable-shared
	Build BFD as a shared library.
  --with-mmap
	Use mmap when accessing files.  This is faster on some hosts,
	but slower on others.  It may not work on all hosts.

Report bugs with BFD to bug-binutils@gnu.org.

Patches are encouraged.  When sending patches, always send the output
of diff -u or diff -c from the original file to the new file.  Do not
send default diff output.  Do not make the diff from the new file to
the original file.  Remember that any patch must not break other
systems.  Remember that BFD must support cross compilation from any
host to any target, so patches which use ``#ifdef HOST'' are not
acceptable.  Please also read the ``Reporting Bugs'' section of the
gcc manual.

Bug reports without patches will be remembered, but they may never get
fixed until somebody volunteers to fix them.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
elfcpp is a C++ library for reading and writing ELF information.  This
was written to support gold, the ELF linker, and may not be generally
useful.

elfcpp does not do file I/O.  It deals only with offsets and memory
data.

For efficiency, most accessors are templates with two arguments: the
ELF file class (32 or 64 bits) and the endianness.


Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
This directory contains the -liberty library of free software.
It is a collection of subroutines used by various GNU programs.
Current members include:

	getopt -- get options from command line
	obstack -- stacks of arbitrarily-sized objects
	strerror -- error message strings corresponding to errno
	strtol -- string-to-long conversion
	strtoul -- string-to-unsigned-long conversion

We expect many of the GNU subroutines that are floating around to
eventually arrive here.

The library must be configured from the top source directory.  Don't
try to run configure in this directory.  Follow the configuration
instructions in ../README.

Please report bugs to "gcc-bugs@gcc.gnu.org" and send fixes to
"gcc-patches@gcc.gnu.org".  Thank you.

ADDING A NEW FILE
=================

There are two sets of files:  Those that are "required" will be
included in the library for all configurations, while those
that are "optional" will be included in the library only if "needed."

To add a new required file, edit Makefile.in to add the source file
name to CFILES and the object file to REQUIRED_OFILES.

To add a new optional file, it must provide a single function, and the
name of the function must be the same as the name of the file.

    * Add the source file name to CFILES in Makefile.in and the object
      file to CONFIGURED_OFILES.

    * Add the function to name to the funcs shell variable in
      configure.ac.

    * Add the function to the AC_CHECK_FUNCS lists just after the
      setting of the funcs shell variable.  These AC_CHECK_FUNCS calls
      are never executed; they are there to make autoheader work
      better.

    * Consider the special cases of building libiberty; as of this
      writing, the special cases are newlib and VxWorks.  If a
      particular special case provides the function, you do not need
      to do anything.  If it does not provide the function, add the
      object file to LIBOBJS, and add the function name to the case
      controlling whether to define HAVE_func.

Finally, in the build directory of libiberty, configure with
"--enable-maintainer-mode", run "make maint-deps" to update
Makefile.in, and run 'make stamp-functions' to regenerate
functions.texi.

The optional file you've added (e.g. getcwd.c) should compile and work
on all hosts where it is needed.  It does not have to work or even
compile on hosts where it is not needed.

ADDING A NEW CONFIGURATION
==========================

On most hosts you should be able to use the scheme for automatically
figuring out which files are needed.  In that case, you probably
don't need a special Makefile stub for that configuration.

If the fully automatic scheme doesn't work, you may be able to get
by with defining EXTRA_OFILES in your Makefile stub.  This is
a list of object file names that should be treated as required
for this configuration - they will be included in libiberty.a,
regardless of whatever might be in the C library.
		README for GAS

A number of things have changed since version 1 and the wonderful
world of gas looks very different.  There's still a lot of irrelevant
garbage lying around that will be cleaned up in time.  Documentation
is scarce, as are logs of the changes made since the last gas release.
My apologies, and I'll try to get something useful.

Unpacking and Installation - Summary
====================================

See ../binutils/README.

To build just the assembler, make the target all-gas.

Documentation
=============

The GAS release includes texinfo source for its manual, which can be processed
into `info' or `dvi' forms.

The DVI form is suitable for printing or displaying; the commands for doing
this vary from system to system.  On many systems, `lpr -d' will print a DVI
file.  On others, you may need to run a program such as `dvips' to convert the
DVI file into a form your system can print.

If you wish to build the DVI file, you will need to have TeX installed on your
system.  You can rebuild it by typing:

	cd gas/doc
	make as.dvi

The Info form is viewable with the GNU Emacs `info' subsystem, or the
stand-alone `info' program, available as part of the GNU Texinfo distribution.
To build the info files, you will need the `makeinfo' program.  Type:

	cd gas/doc
	make info

Specifying names for hosts and targets
======================================

   The specifications used for hosts and targets in the `configure'
script are based on a three-part naming scheme, but some short
predefined aliases are also supported.  The full naming scheme encodes
three pieces of information in the following pattern:

     ARCHITECTURE-VENDOR-OS

   For example, you can use the alias `sun4' as a HOST argument or in a
`--target=TARGET' option.  The equivalent full name is
`sparc-sun-sunos4'.

   The `configure' script accompanying GAS does not provide any query
facility to list all supported host and target names or aliases.
`configure' calls the Bourne shell script `config.sub' to map
abbreviations to full names; you can read the script, if you wish, or
you can use it to test your guesses on abbreviations--for example:

     % sh config.sub i386v
     i386-unknown-sysv
     % sh config.sub i786v
     Invalid configuration `i786v': machine `i786v' not recognized


`configure' options
===================

   Here is a summary of the `configure' options and arguments that are
most often useful for building GAS.  `configure' also has several other
options not listed here.

     configure [--help]
               [--prefix=DIR]
               [--srcdir=PATH]
               [--host=HOST]
               [--target=TARGET]
               [--with-OPTION]
               [--enable-OPTION]

You may introduce options with a single `-' rather than `--' if you
prefer; but you may abbreviate option names if you use `--'.

`--help'
     Print a summary of the options to `configure', and exit.

`-prefix=DIR'
     Configure the source to install programs and files under directory
     `DIR'.

`--srcdir=PATH'
     Look for the package's source code in directory DIR.  Usually
     `configure' can determine that directory automatically.

`--host=HOST'
     Configure GAS to run on the specified HOST.  Normally the
     configure script can figure this out automatically.

     There is no convenient way to generate a list of all available
     hosts.

`--target=TARGET'
     Configure GAS for cross-assembling programs for the specified
     TARGET.  Without this option, GAS is configured to assemble .o files
     that run on the same machine (HOST) as GAS itself.

     There is no convenient way to generate a list of all available
     targets.

`--enable-OPTION'
     These flags tell the program or library being configured to
     configure itself differently from the default for the specified
     host/target combination.  See below for a list of `--enable'
     options recognized in the gas distribution.

`configure' accepts other options, for compatibility with configuring
other GNU tools recursively; but these are the only options that affect
GAS or its supporting libraries.

The `--enable' options recognized by software in the gas distribution are:

`--enable-targets=...'
     This causes one or more specified configurations to be added to those for
     which BFD support is compiled.  Currently gas cannot use any format other
     than its compiled-in default, so this option is not very useful.

`--enable-bfd-assembler'
     This causes the assembler to use the new code being merged into it to use
     BFD data structures internally, and use BFD for writing object files.
     For most targets, this isn't supported yet.  For most targets where it has
     been done, it's already the default.  So generally you won't need to use
     this option.

Compiler Support Hacks
======================

On a few targets, the assembler has been modified to support a feature
that is potentially useful when assembling compiler output, but which
may confuse assembly language programmers.  If assembler encounters a
.word pseudo-op of the form symbol1-symbol2 (the difference of two
symbols), and the difference of those two symbols will not fit in 16
bits, the assembler will create a branch around a long jump to
symbol1, and insert this into the output directly before the next
label: The .word will (instead of containing garbage, or giving an
error message) contain (the address of the long jump)-symbol2.  This
allows the assembler to assemble jump tables that jump to locations
very far away into code that works properly.  If the next label is
more than 32K away from the .word, you lose (silently); RMS claims
this will never happen.  If the -K option is given, you will get a
warning message when this happens.


REPORTING BUGS IN GAS
=====================

Bugs in gas should be reported to:

   bug-binutils@gnu.org.

They may be cross-posted to gcc-bugs@gnu.org if they affect the use of
gas with gcc.  They should not be reported just to gcc-bugs, since not
all of the maintainers read that list.

See ../binutils/README for what we need in a bug report.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
This directory contains the zlib package, which is not part of GCC but
shipped with GCC as convenience.

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.8 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://tools.ietf.org/html/rfc1950 (zlib format), rfc1951 (deflate format) and
rfc1952 (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  A usage example
of the library is given in the file test/example.c which also tests that
the library is working correctly.  Another example is given in the file
test/minigzip.c.  The compression library itself is composed of all source
files in the root directory.

To compile all files and run the test program, follow the instructions given at
the top of Makefile.in.  In short "./configure; make test", and if that goes
well, "make install" should work for most flavors of Unix.  For Windows, use
one of the special makefiles in win32/ or contrib/vstudio/ .  For VMS, use
make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version.  The zlib home page is
http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read the zlib FAQ http://zlib.net/zlib_faq.html before asking for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan.  1997
issue of Dr.  Dobb's Journal; a copy of the article is available at
http://marknelson.us/1997/01/01/zlib-engine/ .

The changes made in version 1.2.8 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory contrib/ .

zlib is available in Java using the java.util.zip package, documented at
http://java.sun.com/developer/technicalArticles/Programming/compression/ .

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is available
at CPAN (Comprehensive Perl Archive Network) sites, including
http://search.cpan.org/~pmqs/IO-Compress-Zlib/ .

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://docs.python.org/library/zlib.html .

zlib is built into tcl: http://wiki.tcl.tk/4610 .

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS or BEOS.

- For PalmOs, see http://palmzlib.sourceforge.net/


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz.  The deflate and
  zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
  people who reported problems and suggested various improvements in zlib; they
  are too numerous to cite here.

Copyright notice:

 (C) 1995-2013 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
This directory contains examples of the use of zlib and other relevant
programs and documentation.

enough.c
    calculation and justification of ENOUGH parameter in inftrees.h
    - calculates the maximum table space used in inflate tree
      construction over all possible Huffman codes

fitblk.c
    compress just enough input to nearly fill a requested output size
    - zlib isn't designed to do this, but fitblk does it anyway

gun.c
    uncompress a gzip file
    - illustrates the use of inflateBack() for high speed file-to-file
      decompression using call-back functions
    - is approximately twice as fast as gzip -d
    - also provides Unix uncompress functionality, again twice as fast

gzappend.c
    append to a gzip file
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of deflatePrime() to start at any bit

gzjoin.c
    join gzip files without recalculating the crc or recompressing
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of crc32_combine()

gzlog.c
gzlog.h
    efficiently and robustly maintain a message log file in gzip format
    - illustrates use of raw deflate, Z_PARTIAL_FLUSH, deflatePrime(),
      and deflateSetDictionary()
    - illustrates use of a gzip header extra field

zlib_how.html
    painfully comprehensive description of zpipe.c (see below)
    - describes in excruciating detail the use of deflate() and inflate()

zpipe.c
    reads and writes zlib streams from stdin to stdout
    - illustrates the proper use of deflate() and inflate()
    - deeply commented in zlib_how.html (see above)

zran.c
    index a zlib or gzip stream and randomly access it
    - illustrates the use of Z_BLOCK, inflatePrime(), and
      inflateSetDictionary() to provide random access
Puff -- A Simple Inflate
3 Mar 2003
Mark Adler
madler@alumni.caltech.edu

What this is --

puff.c provides the routine puff() to decompress the deflate data format.  It
does so more slowly than zlib, but the code is about one-fifth the size of the
inflate code in zlib, and written to be very easy to read.

Why I wrote this --

puff.c was written to document the deflate format unambiguously, by virtue of
being working C code.  It is meant to supplement RFC 1951, which formally
describes the deflate format.  I have received many questions on details of the
deflate format, and I hope that reading this code will answer those questions.
puff.c is heavily commented with details of the deflate format, especially
those little nooks and cranies of the format that might not be obvious from a
specification.

puff.c may also be useful in applications where code size or memory usage is a
very limited resource, and speed is not as important.

How to use it --

Well, most likely you should just be reading puff.c and using zlib for actual
applications, but if you must ...

Include puff.h in your code, which provides this prototype:

int puff(unsigned char *dest,           /* pointer to destination pointer */
         unsigned long *destlen,        /* amount of output space */
         unsigned char *source,         /* pointer to source data pointer */
         unsigned long *sourcelen);     /* amount of input available */

Then you can call puff() to decompress a deflate stream that is in memory in
its entirety at source, to a sufficiently sized block of memory for the
decompressed data at dest.  puff() is the only external symbol in puff.c  The
only C library functions that puff.c needs are setjmp() and longjmp(), which
are used to simplify error checking in the code to improve readabilty.  puff.c
does no memory allocation, and uses less than 2K bytes off of the stack.

If destlen is not enough space for the uncompressed data, then inflate will
return an error without writing more than destlen bytes.  Note that this means
that in order to decompress the deflate data successfully, you need to know
the size of the uncompressed data ahead of time.

If needed, puff() can determine the size of the uncompressed data with no
output space.  This is done by passing dest equal to (unsigned char *)0.  Then
the initial value of *destlen is ignored and *destlen is set to the length of
the uncompressed data.  So if the size of the uncompressed data is not known,
then two passes of puff() can be used--first to determine the size, and second
to do the actual inflation after allocating the appropriate memory.  Not
pretty, but it works.  (This is one of the reasons you should be using zlib.)

The deflate format is self-terminating.  If the deflate stream does not end
in *sourcelen bytes, puff() will return an error without reading at or past
endsource.

On return, *sourcelen is updated to the amount of input data consumed, and
*destlen is updated to the size of the uncompressed data.  See the comments
in puff.c for the possible return codes for puff().
These classes provide a C++ stream interface to the zlib library. It allows you
to do things like:

  gzofstream outf("blah.gz");
  outf << "These go into the gzip file " << 123 << endl;

It does this by deriving a specialized stream buffer for gzipped files, which is
the way Stroustrup would have done it. :->

The gzifstream and gzofstream classes were originally written by Kevin Ruland
and made available in the zlib contrib/iostream directory. The older version still
compiles under gcc 2.xx, but not under gcc 3.xx, which sparked the development of
this version.

The new classes are as standard-compliant as possible, closely following the
approach of the standard library's fstream classes. It compiles under gcc versions
3.2 and 3.3, but not under gcc 2.xx. This is mainly due to changes in the standard
library naming scheme. The new version of gzifstream/gzofstream/gzfilebuf differs
from the previous one in the following respects:
- added showmanyc
- added setbuf, with support for unbuffered output via setbuf(0,0)
- a few bug fixes of stream behavior
- gzipped output file opened with default compression level instead of maximum level
- setcompressionlevel()/strategy() members replaced by single setcompression()

The code is provided "as is", with the permission to use, copy, modify, distribute
and sell it for any purpose without fee.

Ludwig Schwardt
<schwardt@sun.ac.za>

DSP Lab
Electrical & Electronic Engineering Department
University of Stellenbosch
South Africa
This is a patched version of zlib, modified to use
Pentium-Pro-optimized assembly code in the deflation algorithm. The
files changed/added by this patch are:

README.686
match.S

The speedup that this patch provides varies, depending on whether the
compiler used to build the original version of zlib falls afoul of the
PPro's speed traps. My own tests show a speedup of around 10-20% at
the default compression level, and 20-30% using -9, against a version
compiled using gcc 2.7.2.3. Your mileage may vary.

Note that this code has been tailored for the PPro/PII in particular,
and will not perform particuarly well on a Pentium.

If you are using an assembler other than GNU as, you will have to
translate match.S to use your assembler's syntax. (Have fun.)

Brian Raiter
breadbox@muppetlabs.com
April, 1998


Added for zlib 1.1.3:

The patches come from
http://www.muppetlabs.com/~breadbox/software/assembly.html

To compile zlib with this asm file, copy match.S to the zlib directory
then do:

CFLAGS="-O3 -DASMV" ./configure
make OBJA=match.o


Update:

I've been ignoring these assembly routines for years, believing that
gcc's generated code had caught up with it sometime around gcc 2.95
and the major rearchitecting of the Pentium 4. However, I recently
learned that, despite what I believed, this code still has some life
in it. On the Pentium 4 and AMD64 chips, it continues to run about 8%
faster than the code produced by gcc 4.1.

In acknowledgement of its continuing usefulness, I've altered the
license to match that of the rest of zlib. Share and Enjoy!

Brian Raiter
breadbox@muppetlabs.com
April, 2007
Read blast.h for purpose and usage.

Mark Adler
madler@alumni.caltech.edu
See infback9.h for what this is and how to use it.
This directory contains files that have not been updated for zlib 1.2.x

(Volunteers are encouraged to help clean this up.  Thanks.)
This Makefile requires devkitARM (http://www.devkitpro.org/category/devkitarm/) and works inside "contrib/nds". It is based on a devkitARM template.

Eduardo Costa <eduardo.m.costa@gmail.com>
January 3, 2009

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.8 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://www.ietf.org/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate format)
and rfc1952.txt (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  Two compiled
examples are distributed in this package, example and minigzip.  The example_d
and minigzip_d flavors validate that the zlib1.dll file is working correctly.

Questions about zlib should be sent to <zlib@gzip.org>.  The zlib home page
is http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read DLL_FAQ.txt, and the the zlib FAQ http://zlib.net/zlib_faq.html
before asking for help.


Manifest:

The package zlib-1.2.8-win32-x86.zip will contain the following files:

  README-WIN32.txt This document
  ChangeLog        Changes since previous zlib packages
  DLL_FAQ.txt      Frequently asked questions about zlib1.dll
  zlib.3.pdf       Documentation of this library in Adobe Acrobat format

  example.exe      A statically-bound example (using zlib.lib, not the dll)
  example.pdb      Symbolic information for debugging example.exe

  example_d.exe    A zlib1.dll bound example (using zdll.lib)
  example_d.pdb    Symbolic information for debugging example_d.exe

  minigzip.exe     A statically-bound test program (using zlib.lib, not the dll)
  minigzip.pdb     Symbolic information for debugging minigzip.exe

  minigzip_d.exe   A zlib1.dll bound test program (using zdll.lib)
  minigzip_d.pdb   Symbolic information for debugging minigzip_d.exe

  zlib.h           Install these files into the compilers' INCLUDE path to
  zconf.h          compile programs which use zlib.lib or zdll.lib

  zdll.lib         Install these files into the compilers' LIB path if linking
  zdll.exp         a compiled program to the zlib1.dll binary

  zlib.lib         Install these files into the compilers' LIB path to link zlib
  zlib.pdb         into compiled programs, without zlib1.dll runtime dependency
                   (zlib.pdb provides debugging info to the compile time linker)

  zlib1.dll        Install this binary shared library into the system PATH, or
                   the program's runtime directory (where the .exe resides)
  zlib1.pdb        Install in the same directory as zlib1.dll, in order to debug
                   an application crash using WinDbg or similar tools.

All .pdb files above are entirely optional, but are very useful to a developer
attempting to diagnose program misbehavior or a crash.  Many additional
important files for developers can be found in the zlib127.zip source package
available from http://zlib.net/ - review that package's README file for details.


Acknowledgments:

The deflate format used by zlib was defined by Phil Katz.  The deflate and
zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
people who reported problems and suggested various improvements in zlib; they
are too numerous to cite here.


Copyright notice:

  (C) 1995-2012 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
		README for LD

This is the GNU linker.  It is distributed with other "binary
utilities" which should be in ../binutils.  See ../binutils/README for
more general notes, including where to send bug reports.

There are many features of the linker:

* The linker uses a Binary File Descriptor library (../bfd)
  that it uses to read and write object files.  This helps
  insulate the linker itself from the format of object files.

* The linker supports a number of different object file
  formats.  It can even handle multiple formats at once:
  Read two input formats and write a third.

* The linker can be configured for cross-linking.

* The linker supports a control language.

* There is a user manual (ld.texinfo), as well as the
  beginnings of an internals manual (ldint.texinfo).

Installation
============

See ../binutils/README.

If you want to make a cross-linker, you may want to specify
a different search path of -lfoo libraries than the default.
You can do this by setting the LIB_PATH variable in ./Makefile
or using the --with-lib-path configure switch.

To build just the linker, make the target all-ld from the top level
directory (one directory above this one).

Porting to a new target
=======================

See the ldint.texinfo manual.

Reporting bugs etc
===========================

See ../binutils/README.

Known problems
==============

The Solaris linker normally exports all dynamic symbols from an
executable.  The GNU linker does not do this by default.  This is
because the GNU linker tries to present the same interface for all
similar targets (in this case, all native ELF targets).  This does not
matter for normal programs, but it can make a difference for programs
which try to dlopen an executable, such as PERL or Tcl.  You can make
the GNU linker export all dynamic symbols with the -E or
--export-dynamic command line option.

HP/UX 9.01 has a shell bug that causes the linker scripts to be
generated incorrectly.  The symptom of this appears to be "fatal error
- scanner input buffer overflow" error messages.  There are various
workarounds to this:
  * Build and install bash, and build with "make SHELL=bash".
  * Update to a version of HP/UX with a working shell (e.g., 9.05).
  * Replace "(. ${srcdir}/scripttempl/${SCRIPT_NAME}.sc)" in
    genscripts.sh with "sh ${srcdir}..." (no parens) and make sure the
    emulparams script used exports any shell variables it sets.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
The files in this directory are read by genscripts.sh as shell commands.
They set parameters for the emulations.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
The files in this directory are linker script templates.
genscripts.sh sets some shell variables, then sources
EMULATION.sc, to generate EMULATION.{x,xr,xu,xn,xbn} -- the script
files for default, -r, -Ur, -n, -N.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
The files in this directory are sourced by genscripts.sh, after
setting some variables to substitute in, to produce
C source files that contain jump tables for each emulation.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
gold is an ELF linker.  It is intended to have complete support for
ELF and to run as fast as possible on modern systems.  For normal use
it is a drop-in replacement for the older GNU linker.

gold is part of the GNU binutils.  See ../binutils/README for more
general notes, including where to send bug reports.

gold was originally developed at Google, and was contributed to the
Free Software Foundation in March 2008.  At Google it was designed by
Ian Lance Taylor, with major contributions by Cary Coutant, Craig
Silverstein, and Andrew Chatham.

The existing GNU linker manual is intended to be accurate
documentation for features which gold supports.  gold supports most of
the features of the GNU linker for ELF targets.  Notable
omissions--features of the GNU linker not currently supported in
gold--are:
  * MRI compatible linker scripts
  * cross-reference reports (--cref)
  * various other minor options


Notes on the code
=================

These are some notes which may be helpful to people working on the
source code of gold itself.

gold is written in C++.  It is a GNU program, and therefore follows
the GNU formatting standards as modified for C++.  Source documents in
order of decreasing precedence:
    http://www.gnu.org/prep/standards/
    http://gcc.gnu.org/onlinedocs/libstdc++/manual/source_code_style.html
    http://www.zembu.com/eng/procs/c++style.html

The linker is intended to have complete support for cross-compilation,
while still supporting the normal case of native linking as fast as
possible.  In order to do this, many classes are actually templates
whose parameter is the ELF file class (e.g., 32 bits or 64 bits).  The
C++ code is the same, but we don't pay the execution time cost of
always using 64-bit integers if the target is 32 bits.  Many of these
class templates also have an endianness parameter: true for
big-endian, false for little-endian.

The linker is multi-threaded.  The Task class represents a single unit
of work.  Task objects are stored on a single Workqueue object.  Tasks
communicate via Task_token objects.  Task_token objects are only
manipulated while holding the master Workqueue lock.  Relatively few
mutexes are used.


Build requirements
==================

The gold source code uses templates heavily.  Building it requires a
recent version of g++.  g++ 4.0.3 and 4.1.3 are known to work.  g++
3.2, 3.4.3, and 4.1.2 are known to fail.

The linker script parser uses features which are only in newer
versions of bison.  bison 2.3 is known to work.  bison 1.26 is known
to fail.  If you are building gold from an official binutils release,
the bison output should already be included.


Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
		README for BINUTILS

These are the GNU binutils.  These are utilities of use when dealing
with binary files, either object files or executables.  These tools
consist of the linker (ld), the assembler (gas), and the profiler
(gprof) each of which have their own sub-directory named after them.
There is also a collection of other binary tools, including the
disassembler (objdump) in this directory.  These tools make use of a
pair of libraries (bfd and opcodes) and a common set of header files
(include).

There are README and NEWS files in most of the program sub-directories
which give more information about those specific programs.


Copyright Notices
=================

Copyright years on binutils source files may be listed using range
notation, e.g., 1991-2012, indicating that every year in the range,
inclusive, is a copyrightable year that could otherwise be listed
individually.


Unpacking and Installation -- quick overview
============================================

When you unpack the binutils archive file, you will get a directory
called something like `binutils-XXX', where XXX is the number of the
release.  (Probably 2.13 or higher).  This directory contains
various files and sub-directories.  Most of the files in the top
directory are for information and for configuration.  The actual
source code is in sub-directories.

To build binutils, you can just do:

	cd binutils-XXX
	./configure [options]
	make
	make install # copies the programs files into /usr/local/bin
		     # by default.

This will configure and build all the libraries as well as the
assembler, the binutils, and the linker.

If you have GNU make, we recommend building in a different directory:

	mkdir objdir
	cd objdir
	../binutils-XXX/configure [options]
	make
	make install

This relies on the VPATH feature of GNU make.

By default, the binutils will be configured to support the system on
which they are built.  When doing cross development, use the --target
configure option to specify a different target, eg:

	./configure --target=foo-elf

The --enable-targets option adds support for more binary file formats
besides the default.  List them as the argument to --enable-targets,
separated by commas.  For example:

	./configure --enable-targets=sun3,rs6000-aix,decstation

The name 'all' compiles in support for all valid BFD targets:

	./configure --enable-targets=all

On 32-bit hosts though, this support will be restricted to 32-bit
target unless the --enable-64-bit-bfd option is also used:

	./configure --enable-64-bit-bfd --enable-targets=all

You can also specify the --enable-shared option when you run
configure.  This will build the BFD and opcodes libraries as shared
libraries.  You can use arguments with the --enable-shared option to
indicate that only certain libraries should be built shared; for
example, --enable-shared=bfd.  The only potential shared libraries in
a binutils release are bfd and opcodes.

The binutils will be linked against the shared libraries.  The build
step will attempt to place the correct library in the run-time search
path for the binaries.  However, in some cases, after you install the
binaries, you may have to set an environment variable, normally
LD_LIBRARY_PATH, so that the system can find the installed libbfd
shared library.

On hosts that support shared system libraries the binutils will be
linked against them.  If you have static versions of the system
libraries installed as well and you wish to create static binaries
instead then use the LDFLAGS environment variable,  like this:

  ../binutils-XXX/configure LDFLAGS="--static" [more options]

Note: the two dashes are important.  The binutils make use of the
libtool script which has a special interpretation of "-static" when it
is in the LDFLAGS environment variable.

To build under openVMS/AXP, see the file makefile.vms in the top level
directory.


Native Language Support
=======================

By default Native Language Support will be enabled for binutils.  On
some systems however this support is not present and can lead to error
messages such as "undefined reference to `libintl_gettext'" when
building there tools.  If that happens the NLS support can be disabled
by adding the --disable-nls switch to the configure line like this:

	../binutils-XXX/configure --disable-nls


If you don't have ar
====================

If your system does not already have an 'ar' program, the normal
binutils build process will not work.  In this case, run configure as
usual.  Before running make, run this script:

#!/bin/sh
MAKE_PROG="${MAKE-make}"
MAKE="${MAKE_PROG} AR=true LINK=true"
export MAKE
${MAKE} $* all-libiberty
${MAKE} $* all-intl
${MAKE} $* all-bfd
cd binutils
MAKE="${MAKE_PROG}"
export MAKE
${MAKE} $* ar_DEPENDENCIES= ar_LDADD='../bfd/*.o ../libiberty/*.o `if test -f ../intl/gettext.o; then echo '../intl/*.o'; fi`' ar

This script will build an ar program in binutils/ar.  Move binutils/ar
into a directory on your PATH.  After doing this, you can run make as
usual to build the complete binutils distribution.  You do not need
the ranlib program in order to build the distribution.

Porting
=======

Binutils-2.13 supports many different architectures, but there
are many more not supported, including some that were supported
by earlier versions.  We are hoping for volunteers to improve this
situation.

The major effort in porting binutils to a new host and/or target
architecture involves the BFD library.  There is some documentation
in ../bfd/doc.  The file ../gdb/doc/gdbint.texinfo (distributed
with gdb-5.x) may also be of help.

Reporting bugs
==============

Send bug reports and patches to:

   bug-binutils@gnu.org.

Please include the following in bug reports:

- A description of exactly what went wrong, and exactly what should have
  happened instead.

- The configuration name(s) given to the "configure" script.  The
  "config.status" file should have this information.  This is assuming
  you built binutils yourself.  If you didn't build binutils youself,
  then we need information regarding your machine and operating system,
  and it may be more appropriate to report bugs to wherever you obtained
  binutils.

- The options given to the tool (gas, objcopy, ld etc.) at run time.

- The actual input file that caused the problem.

Always mention the version number you are running; this is printed by
running any of the binutils with the --version option.  We appreciate
reports about bugs, but we do not promise to fix them, particularly so
when the bug report is against an old version.  If you are able, please
consider building the latest tools from git to check that your bug has
not already been fixed.

When reporting problems about gas and ld, it's useful to provide a
testcase that triggers the problem.  In the case of a gas problem, we
want input files to gas and command line switches used.  The inputs to
gas are _NOT_ .c or .i files, but rather .s files.  If your original
source was a C program, you can generate the .s file and see the command
line options by passing -v -save-temps to gcc in addition to all the
usual options you use.  The reason we don't want C files is that we
might not have a C compiler around for the target you use.  While it
might be possible to build a compiler, that takes considerable time and
disk space, and we might not end up with exactly the same compiler you
use.

In the case of a ld problem, the input files are .o, .a and .so files,
and possibly a linker script specified with -T.  Again, when using gcc
to link, you can see these files by adding options to the gcc command
line.  Use -v -save-temps -Wl,-t, except that on targets that use gcc's
collect2, you would add -v -save-temps -Wl,-t,-debug.  The -t option
tells ld to print all files and libraries used, so that, for example,
you can associate -lc on the ld command line with the actual libc used.
Note that your simple two line C program to trigger a problem typically
expands into several megabytes of objects by the time you include
libraries.

It is antisocial to post megabyte sized attachments to mailing lists, so
please put large testcases somewhere on an ftp or web site so that only
interested developers need to download them, or offer to email them on
request.  Better still, try to reduce the testcase, for example, try to
develop a ld testcase that doesn't use system libraries.  However,
please be sure it is a complete testcase and that it really does
demonstrate the problem.  Also, don't bother paring it down if that will
cause large delays in filing the bug report.

If you expect to be contributing a large number of test cases, it would
be helpful if you would look at the test suite included in the release
(based on the Deja Gnu testing framework, available from the usual ftp
sites) and write test cases to fit into that framework.  This is
certainly not required.

VMS
===

This section was written by Klaus K"ampf <kkaempf@rmi.de>.  It
describes how to build and install the binutils on openVMS (Alpha and
Vax).  (The BFD library only supports reading Vax object files.)

Compiling the release:

To compile the gnu binary utilities and the gnu assembler, you'll
need DEC C or GNU C for openVMS/Alpha. You'll need *both* compilers
on openVMS/Vax.

Compiling with either DEC C or GNU C works on openVMS/Alpha only. Some
of the opcodes and binutils files trap a bug in the DEC C optimizer,
so these files must be compiled with /noopt.

Compiling on openVMS/Vax is a bit complicated, as the bfd library traps
a bug in GNU C and the gnu assembler a bug in (my version of) DEC C.

I never tried compiling with VAX C.


You further need GNU Make Version 3.76 or later. This is available
at ftp.progis.de or any GNU archive site. The makefiles assume that
gmake starts gnu make as a foreign command.

If you're compiling with DEC C or VAX C, you must run

  $ @setup

before starting gnu-make. This isn't needed with GNU C.

On the Alpha you can choose the compiler by editing the toplevel
makefile.vms. Either select CC=cc (for DEC C) or CC=gcc (for GNU C)


Installing the release

Provided that your directory setup conforms to the GNU on openVMS
standard, you already have a concealed device named 'GNU_ROOT'.
In this case, a simple

 $ gmake install

suffices to copy all programs and libraries to the proper directories.

Define the programs as foreign commands by adding these lines to your
login.com:

  $ gas :== $GNU_ROOT:[bin]as.exe
  $ size :== $GNU_ROOT:[bin]size.exe
  $ nm :== $GNU_ROOT:[bin]nm.exe
  $ objdump :== $GNU_ROOT:[bin]objdump.exe
  $ strings :== $GNU_ROOT:[bin]strings.exe

If you have a different directory setup, copy the binary utilities
([.binutils]size.exe, [.binutils]nm.exe, [.binutils]objdump.exe,
and [.binutils]strings.exe) and the gnu assembler and preprocessor
([.gas]as.exe and [.gas]gasp.exe]) to a directory of your choice
and define all programs as foreign commands.


If you're satisfied with the compilation, you may want to remove
unneeded objects and libraries:

  $ gmake clean


If you have any problems or questions about the binutils on VMS, feel
free to mail me at kkaempf@rmi.de.

Copyright (C) 2012-2016 Free Software Foundation, Inc.

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.
Copyright 2000-2016 Free Software Foundation, Inc.
Contributed by the AriC and Caramba projects, INRIA.

This file is part of the GNU MPFR Library.

The GNU MPFR Library is free software; you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation; either version 3 of the License, or (at your
option) any later version.

The GNU MPFR Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
License for more details.

You should have received a copy of the GNU Lesser General Public License
along with the GNU MPFR Library; see the file COPYING.LESSER.  If not, see
http://www.gnu.org/licenses/ or write to the Free Software Foundation, Inc.,
51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA.

##############################################################################

The GNU MPFR distribution contains the following files:
(This does not apply to code retrieved by Subversion.)

AUTHORS         - the authors of the library
BUGS            - bugs in MPFR - please read this file!
COPYING         - the GNU General Public License, version 3
COPYING.LESSER  - the GNU Lesser General Public License, version 3
ChangeLog       - the log of changes
INSTALL         - how to install MPFR (see also mpfr.texi)
Makefile*       - files for building the library
NEWS            - new features with respect to previous versions
PATCHES         - empty file (until patches are applied)
README          - this file
TODO            - what remains to do (any help is welcome!)
VERSION         - version of MPFR (next release version if taken by Subversion)
ac*.m4          - automatic configuration files
ar-lib          - auxiliary installation file
compile         - auxiliary installation file
config.*        - auxiliary installation files
configure*      - configuration files
depcomp         - auxiliary installation file
doc/            - directory containing the documentation (manual, FAQ)
examples/       - directory containing examples
install-sh      - installation file
ltmain.sh       - auxiliary installation file
m4/             - directory containing additional configuration files
missing         - auxiliary installation file
src/            - directory containing the MPFR source
test-driver     - auxiliary installation file
tests/          - directory containing the testsuite (for "make check")
tools/          - directory containing various tools
tune/           - directory containing files for tuning MPFR

According to the special exception to the GNU General Public License,
the autotools files compile, config.sub, config.guess, ltmain.sh,
m4/libtool.m4 and missing are distributed under the same licence of
GNU MPFR.

For any copyright year range specified as YYYY-ZZZZ in this package,
note that the range specifies every single year in that closed interval.


You can get the latest source code by Subversion at InriaForge:

  svn checkout svn://scm.gforge.inria.fr/svn/mpfr/trunk mpfr

or

  svn checkout https://scm.gforge.inria.fr/svn/mpfr/trunk mpfr

(the last argument can be any directory name). You can use

  svn ls svn://scm.gforge.inria.fr/svn/mpfr/branches
  svn ls svn://scm.gforge.inria.fr/svn/mpfr/tags

to get the list of branches or tags (releases), then checkout a
particular branch or tag instead of the trunk. Alternatively, you
can now use the "https:" scheme (a.k.a. DAV) instead of "svn:".
For more information about Subversion, please see:

  * http://svnbook.red-bean.com/ (the official Subversion book);
  * https://gcc.gnu.org/wiki/SvnHelp (written for GCC developers,
    but interesting general information can be found there);
  * http://subversion.apache.org/faq.html (the Subversion FAQ).

Subversion users should read the file "doc/README.dev" (provided via
SVN only).
$NetBSD: README,v 1.7 2017/08/17 23:00:38 mrg Exp $

GMP in NetBSD.  We need GMP for GCC >= 4.2.


Building GMP without configure - how to port GMP build to a new platform.

The GMP build doesn't map very well to normal make.  The ./configure phase
creates a bunch of symlinks and weeds out the sources lists, and there are
files with the same name in different subdirectories linked into the same
final product.  All of these issues need to be dealt with.

There are a few steps to this:

	- run ./configure, save the output.  you can use the makefile
	  "Makefile.netbsd-gmp" in this directory to run this with the
	  right options, etc.  run it with nbmake-$MACHINE.

	- create src/external/gpl3/gmp/lib/libgmp/arch/${MACHINE_ARCH} dir,
	  and copy these files into it:
		config.h
		config.m4
		gmp-mparam.h
		gmp.h

	  some of these files might have src/obj references.  in particular
	  fix GMP_MPARAM_H_SUGGEST to start from ./mpn/... and make sure
	  we #define __GMP_CC to "gcc -std=gnu99", and make sure that
	  CONFIG_TOP_SRCDIR is not defined in config.m4

	  XXX  make this automatic


	- parse the ./configure output and note all created symlinks
	  for mpn.  these need to be converted into a new Makefile.inc.
	  there is a script in this subdir build-gmp-Makefile.inc.awk
	  that can be used to do this.  it should just work to generate
	  the first section of Makefile.inc if fed the entire configure
	  output.

	  assembler files generally want -DOPERATION_${foo} defined for
	  each way they are compiled or pre-processed.  the pre-processor
	  used is m4 to parse, and we and create .s files from the .asm
	  files that we then we feed into $CC.

The amd64 port is a good reference to compare.

This mips64* ports need some minor hacks to the generated gmp*.h
files to fix their library builds for compat issues.  See these
files in:
	http://mail-index.netbsd.org/source-changes/2011/07/10/msg024467.html


This is still a work in progress and methods used to build may be
changed at any time.


mrg@netbsd.org
- 2011/06/22
Copyright 1991, 1996, 1999, 2000, 2007 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.






			THE GNU MP LIBRARY


GNU MP is a library for arbitrary precision arithmetic, operating on signed
integers, rational numbers, and floating point numbers.  It has a rich set of
functions, and the functions have a regular interface.

GNU MP is designed to be as fast as possible, both for small operands and huge
operands.  The speed is achieved by using fullwords as the basic arithmetic
type, by using fast algorithms, with carefully optimized assembly code for the
most common inner loops for lots of CPUs, and by a general emphasis on speed
(instead of simplicity or elegance).

GNU MP is believed to be faster than any other similar library.  Its advantage
increases with operand sizes for certain operations, since GNU MP in many
cases has asymptotically faster algorithms.

GNU MP is free software and may be freely copied on the terms contained in the
files COPYING* (see the manual for information on which license(s) applies to
which components of GNU MP).



			OVERVIEW OF GNU MP

There are four classes of functions in GNU MP.

 1. Signed integer arithmetic functions (mpz).  These functions are intended
    to be easy to use, with their regular interface.  The associated type is
    `mpz_t'.

 2. Rational arithmetic functions (mpq).  For now, just a small set of
    functions necessary for basic rational arithmetics.  The associated type
    is `mpq_t'.

 3. Floating-point arithmetic functions (mpf).  If the C type `double'
    doesn't give enough precision for your application, declare your
    variables as `mpf_t' instead, set the precision to any number desired,
    and call the functions in the mpf class for the arithmetic operations.

 4. Positive-integer, hard-to-use, very low overhead functions are in the
    mpn class.  No memory management is performed.  The caller must ensure
    enough space is available for the results.  The set of functions is not
    regular, nor is the calling interface.  These functions accept input
    arguments in the form of pairs consisting of a pointer to the least
    significant word, and an integral size telling how many limbs (= words)
    the pointer points to.

    Almost all calculations, in the entire package, are made by calling these
    low-level functions.

For more information on how to use GNU MP, please refer to the documentation.
It is composed from the file doc/gmp.texi, and can be displayed on the screen
or printed.  How to do that, as well how to build the library, is described in
the INSTALL file in this directory.



			REPORTING BUGS

If you find a bug in the library, please make sure to tell us about it!

You should first check the GNU MP web pages at https://gmplib.org/, under
"Status of the current release".  There will be patches for all known serious
bugs there.

Report bugs to gmp-bugs@gmplib.org.  What information is needed in a useful bug
report is described in the manual.  The same address can be used for suggesting
modifications and enhancements.




----------------
Local variables:
mode: text
fill-column: 78
End:
Copyright 2000-2002, 2004 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





               GMP SPEED MEASURING AND PARAMETER TUNING


The programs in this directory are for knowledgeable users who want to
measure GMP routines on their machine, and perhaps tweak some settings or
identify things that can be improved.

The programs here are tools, not ready to run solutions.  Nothing is built
in a normal "make all", but various Makefile targets described below exist.

Relatively few systems and CPUs have been tested, so be sure to verify that
results are sensible before relying on them.




MISCELLANEOUS NOTES

--enable-assert

    Don't configure with --enable-assert, since the extra code added by
    assertion checking may influence measurements.

Direct mapped caches

    Some effort has been made to accommodate CPUs with direct mapped caches,
    by putting data blocks more or less contiguously on the stack.  But this
    will depend on TMP_ALLOC using alloca, and even then it may or may not
    be enough.

FreeBSD 4.2 i486 getrusage

    This getrusage seems to be a bit doubtful, it looks like it's
    microsecond accurate, but sometimes ru_utime remains unchanged after a
    time of many microseconds has elapsed.  It'd be good to detect this in
    the time.c initializations, but for now the suggestion is to pretend it
    doesn't exist.

        ./configure ac_cv_func_getrusage=no

NetBSD 1.4.1 m68k macintosh time base

    On this system it's been found getrusage often goes backwards, making it
    unusable (time.c getrusage_backwards_p detects this).  gettimeofday
    sometimes doesn't update atomically when it crosses a 1 second boundary.
    Not sure what to do about this.  Expect possible intermittent failures.

SCO OpenUNIX 8 /etc/hw

    /etc/hw takes about a second to return the cpu frequency, which suggests
    perhaps it's measuring each time it runs.  If this is annoying when
    running the speed program repeatedly then set a GMP_CPU_FREQUENCY
    environment variable (see TIME BASE section below).

Timing on GNU/Linux

    On Linux, timing currently uses the cycle counter. This is unreliable,
    since the counter is not saved and restored at context switches (unlike
    FreeBSD and Solaris where the cycle counter is "virtualized").

    Using the clock_gettime method with CLOCK_PROCESS_CPUTIME_ID (posix) or
    CLOCK_VIRTUAL (BSD) should be more reliable. To get clock_gettime
    with glibc, one has to link with -lrt (which also drags in the pthreads
    threading library). configure.in must be hacked to detect this and
    arrange proper linking. Something like

      old_LIBS="$LIBS"
      AC_SEARCH_LIBS(clock_gettime, rt, [AC_DEFINE(HAVE_CLOCK_GETTIME)])
      TUNE_LIBS="$LIBS"
      LIBS="$old_LIBS"

      AC_SUBST(TUNE_LIBS)

    might work.

Low resolution timebase

    Parameter tuning can be very time consuming if the only timebase
    available is a 10 millisecond clock tick, to the point of being
    unusable.  This is currently the case on VAX and ARM systems.




PARAMETER TUNING

The "tuneup" program runs some tests designed to find the best settings for
various thresholds, like MUL_TOOM22_THRESHOLD.  Its output can be put
into gmp-mparam.h.  The program is built and run with

        make tune

If the thresholds indicated are grossly different from the values in the
selected gmp-mparam.h then there may be a performance boost in applicable
size ranges by changing gmp-mparam.h accordingly.

Be sure to do a full reconfigure and rebuild to get any newly set thresholds
to take effect.  A partial rebuild is enough sometimes, but a fresh
configure and make is certain to be correct.

If a CPU has specific tuned parameters coming from a gmp-mparam.h in one of
the mpn subdirectories then the values from "make tune" should be similar.
But check that the configured CPU is right and there are no machine specific
effects causing a difference.

It's hoped the compiler and options used won't have too much effect on
thresholds, since for most CPUs they ultimately come down to comparisons
between assembler subroutines.  Missing out on the longlong.h macros by not
using gcc will probably have an effect.

Some thresholds produced by the tune program are merely single values chosen
from what's a range of sizes where two algorithms are pretty much the same
speed.  When this happens the program is likely to give somewhat different
values on successive runs.  This is noticeable on the toom3 thresholds for
instance.




SPEED PROGRAM

The "speed" program can be used for measuring and comparing various
routines, and producing tables of data or gnuplot graphs.  Compile it with

	make speed

(Or on DOS systems "make speed.exe".)

Here are some examples of how to use it.  Check the code for all the
options.

Draw a graph of mpn_mul_n, stepping through sizes by 10 or a factor of 1.05
(whichever is greater).

        ./speed -s 10-5000 -t 10 -f 1.05 -P foo mpn_mul_n
	gnuplot foo.gnuplot

Compare mpn_add_n and an mpn_lshift by 1, showing times in cycles and
showing under mpn_lshift the difference between it and mpn_add_n.

	./speed -s 1-40 -c -d mpn_add_n mpn_lshift.1

Using option -c for times in cycles is interesting but normally only
necessary when looking carefully at assembler subroutines.  You might think
it would always give an integer value, but this doesn't happen in practice,
probably due to overheads in the time measurements.

In the free-form output the "#" symbol against a measurement means the
corresponding routine is fastest at that size.  This is a convenient visual
cue when comparing different routines.  The graph data files <name>.data
don't get this since it would upset gnuplot or other data viewers.




TIME BASE

The time measuring method is determined in time.c, based on what the
configured host has available.  A cycle counter is preferred, possibly
supplemented by another method if the counter has a limited range.  A
microsecond accurate getrusage() or gettimeofday() will work quite well too.

The cycle counters (except possibly on alpha) and gettimeofday() will depend
on the machine being otherwise idle, or rather on other jobs not stealing
CPU time from the measuring program.  Short routines (those that complete
within a timeslice) should work even on a busy machine.

Some trouble is taken by speed_measure() in common.c to avoid ill effects
from sporadic interrupts, or other intermittent things (like cron waking up
every minute).  But generally an idle machine will be necessary to be
certain of consistent results.

The CPU frequency is needed to convert between cycles and seconds, or for
when a cycle counter is supplemented by getrusage() etc.  The speed program
will convert as necessary according to the output format requested.  The
tune program will work with either cycles or seconds.

freq.c knows how to get the frequency on some systems, or can measure a
cycle counter against gettimeofday() or getrusage(), but when that fails, or
needs to be overridden, an environment variable GMP_CPU_FREQUENCY can be
used (in Hertz).  For example in "bash" on a 650 MHz machine,

	export GMP_CPU_FREQUENCY=650e6

A high precision time base makes it possible to get accurate measurements in
a shorter time.




EXAMPLE COMPARISONS - VARIOUS

Here are some ideas for things that can be done with the speed program.

There's always going to be a certain amount of overhead in the time
measurements, due to reading the time base, and in the loop that runs a
routine enough times to get a reading of the desired precision.  Noop
functions taking various arguments are available to measure this.  The
"overhead" printed by the speed program each time in its intro is the "noop"
routine, but note that this is just for information, it isn't deducted from
the times printed or anything.

	./speed -s 1 noop noop_wxs noop_wxys

To see how many cycles per limb a routine is taking, look at the time
increase when the size increments, using option -D.  This avoids fixed
overheads in the measuring.  Also, remember many of the assembler routines
have unrolled loops, so it might be necessary to compare times at, say, 16,
32, 48, 64 etc to see what the unrolled part is taking, as opposed to any
finishing off.

        ./speed -s 16-64 -t 16 -C -D mpn_add_n

The -C option on its own gives cycles per limb, but is really only useful at
big sizes where fixed overheads are small compared to the code doing the
real work.  Remember of course memory caching and/or page swapping will
affect results at large sizes.

        ./speed -s 500000 -C mpn_add_n

Once a calculation stops fitting in the CPU data cache, it's going to start
taking longer.  Exactly where this happens depends on the cache priming in
the measuring routines, and on what sort of "least recently used" the
hardware does.  Here's an example for a CPU with a 16kbyte L1 data cache and
32-bit limb, showing a suddenly steeper curve for mpn_add_n at about 2000
limbs.

        ./speed -s 1-4000 -t 5 -f 1.02 -P foo mpn_add_n
	gnuplot foo.gnuplot

When a routine has an unrolled loop for, say, multiples of 8 limbs and then
an ordinary loop for the remainder, it can happen that it's actually faster
to do an operation on, say, 8 limbs than it is on 7 limbs.  The following
draws a graph of mpn_sub_n, to see whether times smoothly increase with
size.

        ./speed -s 1-100 -c -P foo mpn_sub_n
	gnuplot foo.gnuplot

If mpn_lshift and mpn_rshift have special case code for shifts by 1, it
ought to be faster (or at least not slower) than shifting by, say, 2 bits.

        ./speed -s 1-200 -c mpn_rshift.1 mpn_rshift.2

An mpn_lshift by 1 can be done by mpn_add_n adding a number to itself, and
if the lshift isn't faster there's an obvious improvement that's possible.

        ./speed -s 1-200 -c mpn_lshift.1 mpn_add_n_self

On some CPUs (AMD K6 for example) an "in-place" mpn_add_n where the
destination is one of the sources is faster than a separate destination.
Here's an example to see this.  ".1" selects dst==src1 for mpn_add_n (and
mpn_sub_n), for other values see speed.h SPEED_ROUTINE_MPN_BINARY_N_CALL.

        ./speed -s 1-200 -c mpn_add_n mpn_add_n.1

The gmp manual points out that divisions by powers of two should be done
using a right shift because it'll be significantly faster than an actual
division.  The following shows by what factor mpn_rshift is faster than
mpn_divrem_1, using division by 32 as an example.

        ./speed -s 10-20 -r mpn_rshift.5 mpn_divrem_1.32




EXAMPLE COMPARISONS - MULTIPLICATION

mul_basecase takes a ".<r>" parameter. If positive, it gives the second
(smaller) operand size.  For example to show speeds for 3x3 up to 20x3 in
cycles,

        ./speed -s 3-20 -c mpn_mul_basecase.3

A negative ".<-r>" parameter fixes the size of the product to the absolute
value r.  For example to show speeds for 10x10 up to 19x1 in cycles,

        ./speed -s 10-19 -c mpn_mul_basecase.-20

mul_basecase with no parameter does an NxN multiply, so for example to show
speeds in cycles for 1x1, 2x2, 3x3, etc, up to 20x20, in cycles,

        ./speed -s 1-20 -c mpn_mul_basecase

sqr_basecase is implemented by a "triangular" method on most CPUs, making it
up to twice as fast as mul_basecase.  In practice loop overheads and the
products on the diagonal mean it falls short of this.  Here's an example
running the two and showing by what factor an NxN mul_basecase is slower
than an NxN sqr_basecase.  (Some versions of sqr_basecase only allow sizes
below SQR_TOOM2_THRESHOLD, so if it crashes at that point don't worry.)

        ./speed -s 1-20 -r mpn_sqr_basecase mpn_mul_basecase

The technique described above with -CD for showing the time difference in
cycles per limb between two size operations can be done on an NxN
mul_basecase using -E to change the basis for the size increment to N*N.
For instance a 20x20 operation is taken to be doing 400 limbs, and a 16x16
doing 256 limbs.  The following therefore shows the per crossproduct speed
of mul_basecase and sqr_basecase at around 20x20 limbs.

        ./speed -s 16-20 -t 4 -CDE mpn_mul_basecase mpn_sqr_basecase

Of course sqr_basecase isn't really doing NxN crossproducts, but it can be
interesting to compare it to mul_basecase as if it was.  For sqr_basecase
the -F option can be used to base the deltas on N*(N+1)/2 operations, which
is the triangular products sqr_basecase does.  For example,

        ./speed -s 16-20 -t 4 -CDF mpn_sqr_basecase

Both -E and -F are preliminary and might change.  A consistent approach to
using them when claiming certain per crossproduct or per triangularproduct
speeds hasn't really been established, but the increment between speeds in
the range karatsuba will call seems sensible, that being k to k/2.  For
instance, if the karatsuba threshold was 20 for the multiply and 30 for the
square,

        ./speed -s 10-20 -t 10 -CDE mpn_mul_basecase
        ./speed -s 15-30 -t 15 -CDF mpn_sqr_basecase



EXAMPLE COMPARISONS - MALLOC

The gmp manual recommends application programs avoid excessive initializing
and clearing of mpz_t variables (and mpq_t and mpf_t too).  Every new
variable will at a minimum go through an init, a realloc for its first
store, and finally a clear.  Quite how long that takes depends on the C
library.  The following compares an mpz_init/realloc/clear to a 10 limb
mpz_add.  Don't be surprised if the mallocing is quite slow.

        ./speed -s 10 -c mpz_init_realloc_clear mpz_add

On some systems malloc and free are much slower when dynamic linked.  The
speed-dynamic program can be used to see this.  For example the following
measures malloc/free, first static then dynamic.

        ./speed -s 10 -c malloc_free
        ./speed-dynamic -s 10 -c malloc_free

Of course a real world program has big problems if it's doing so many
mallocs and frees that it gets slowed down by a dynamic linked malloc.





EXAMPLE COMPARISONS - STRING CONVERSIONS

mpn_get_str does a binary to string conversion.  The base is specified with
a ".<r>" parameter, or decimal by default.  Power of 2 bases are much faster
than general bases.  The following compares decimal and hex for instance.

        ./speed -s 1-20 -c mpn_get_str mpn_get_str.16

Smaller bases need more divisions to split a given size number, and so are
slower.  The following compares base 3 and base 9.  On small operands 9 will
be nearly twice as fast, though at bigger sizes this reduces since in the
current implementation both divide repeatedly by 3^20 (or 3^40 for 64 bit
limbs) and those divisions come to dominate.

        ./speed -s 1-20 -cr mpn_get_str.3 mpn_get_str.9

mpn_set_str does a string to binary conversion.  The base is specified with
a ".<r>" parameter, or decimal by default.  Power of 2 bases are faster than
general bases on large conversions.

	./speed -s 1-512 -f 2 -c mpn_set_str.8 mpn_set_str.10

mpn_set_str also has some special case code for decimal which is a bit
faster than the general case, basically by giving the compiler a chance to
optimize some multiplications by 10.

	./speed -s 20-40 -c mpn_set_str.9 mpn_set_str.10 mpn_set_str.11




EXAMPLE COMPARISONS - GCDs

mpn_gcd_1 has a threshold for when to reduce using an initial x%y when both
x and y are single limbs.  This isn't tuned currently, but a value can be
established by a measurement like

	./speed -s 10-32 mpn_gcd_1.10

This runs src[0] from 10 to 32 bits, and y fixed at 10 bits.  If the div
threshold is high, say 31 so it's effectively disabled then a 32x10 bit gcd
is done by nibbling away at the 32-bit operands bit-by-bit.  When the
threshold is small, say 1 bit, then an initial x%y is done to reduce it to a
10x10 bit operation.

The threshold in mpn/generic/gcd_1.c or the various assembler
implementations can be tweaked up or down until there's no more speedups on
interesting combinations of sizes.  Note that this affects only a 1x1 limb
operation and so isn't very important.  (An Nx1 limb operation always does
an initial modular reduction, using mpn_mod_1 or mpn_modexact_1_odd.)




SPEED PROGRAM EXTENSIONS

Potentially lots of things could be made available in the program, but it's
been left at only the things that have actually been wanted and are likely
to be reasonably useful in the future.

Extensions should be fairly easy to make though.  speed-ext.c is an example,
in a style that should suit one-off tests, or new code fragments under
development.

many.pl is a script for generating a new speed program supplemented with
alternate versions of the standard routines.  It can be used for measuring
experimental code, or for comparing different implementations that exist
within a CPU family.




THRESHOLD EXAMINING

The speed program can be used to examine the speeds of different algorithms
to check the tune program has done the right thing.  For example to examine
the karatsuba multiply threshold,

	./speed -s 5-40 mpn_mul_basecase mpn_kara_mul_n

When examining the toom3 threshold, remember it depends on the karatsuba
threshold, so the right karatsuba threshold needs to be compiled into the
library first.  The tune program uses specially recompiled versions of
mpn/mul_n.c etc for this reason, but the speed program simply uses the
normal libgmp.la.

Note further that the various routines may recurse into themselves on sizes
far enough above applicable thresholds.  For example, mpn_kara_mul_n will
recurse into itself on sizes greater than twice the compiled-in
MUL_TOOM22_THRESHOLD.

When doing the above comparison between mul_basecase and kara_mul_n what's
probably of interest is mul_basecase versus a kara_mul_n that does one level
of Karatsuba then calls to mul_basecase, but this only happens on sizes less
than twice the compiled MUL_TOOM22_THRESHOLD.  A larger value for that
setting can be compiled-in to avoid the problem if necessary.  The same
applies to toom3 and DC, though in a trickier fashion.

There are some upper limits on some of the thresholds, arising from arrays
dimensioned according to a threshold (mpn_mul_n), or asm code with certain
sized displacements (some x86 versions of sqr_basecase).  So putting huge
values for the thresholds, even just for testing, may fail.




FUTURE

Make a program to check the time base is working properly, for small and
large measurements.  Make it able to test each available method, including
perhaps the apparent resolution of each.

Make a general mechanism for specifying operand overlap, and a syntax like
maybe "mpn_add_n.dst=src2" to select it.  Some measuring routines do this
sort of thing with the "r" parameter currently.



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 1996, 1999 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.




This directory contains all code for the mpn layer of GMP.

Most subdirectories contain machine-dependent code, written in assembly or C.
The `generic' subdirectory contains default code, used when there is no
machine-dependent replacement for a particular machine.

There is one subdirectory for each ISA family.  Note that e.g., 32-bit SPARC
and 64-bit SPARC are very different ISA's, and thus cannot share any code.

A particular compile will only use code from one subdirectory, and the
`generic' subdirectory.  The ISA-specific subdirectories contain hierachies of
directories for various architecture variants and implementations; the
top-most level contains code that runs correctly on all variants.
Copyright 1997, 1999-2002 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





This directory contains mpn functions for 64-bit V9 SPARC

RELEVANT OPTIMIZATION ISSUES

Notation:
  IANY = shift/add/sub/logical/sethi
  IADDLOG = add/sub/logical/sethi
  MEM = ld*/st*
  FA = fadd*/fsub*/f*to*/fmov*
  FM = fmul*

UltraSPARC can issue four instructions per cycle, with these restrictions:
* Two IANY instructions, but only one of these may be a shift.  If there is a
  shift and an IANY instruction, the shift must precede the IANY instruction.
* One FA.
* One FM.
* One branch.
* One MEM.
* IANY/IADDLOG/MEM must be insn 1, 2, or 3 in an issue bundle.  Taken branches
  should not be in slot 4, since that makes the delay insn come from separate
  bundle.
* If two IANY/IADDLOG instructions are to be executed in the same cycle and one
  of these is setting the condition codes, that instruction must be the second
  one.

To summarize, ignoring branches, these are the bundles that can reach the peak
execution speed:

insn1	iany	iany	mem	iany	iany	mem	iany	iany	mem
insn2	iaddlog	mem	iany	mem	iaddlog	iany	mem	iaddlog	iany
insn3	mem	iaddlog	iaddlog	fa	fa	fa	fm	fm	fm
insn4	fa/fm	fa/fm	fa/fm	fm	fm	fm	fa	fa	fa

The 64-bit integer multiply instruction mulx takes from 5 cycles to 35 cycles,
depending on the position of the most significant bit of the first source
operand.  When used for 32x32->64 multiplication, it needs 20 cycles.
Furthermore, it stalls the processor while executing.  We stay away from that
instruction, and instead use floating-point operations.

Floating-point add and multiply units are fully pipelined.  The latency for
UltraSPARC-1/2 is 3 cycles and for UltraSPARC-3 it is 4 cycles.

Integer conditional move instructions cannot dual-issue with other integer
instructions.  No conditional move can issue 1-5 cycles after a load.  (This
might have been fixed for UltraSPARC-3.)

The UltraSPARC-3 pipeline is very simular to the one of UltraSPARC-1/2 , but is
somewhat slower.  Branches execute slower, and there may be other new stalls.
But integer multiply doesn't stall the entire CPU and also has a much lower
latency.  But it's still not pipelined, and thus useless for our needs.

STATUS

* mpn_lshift, mpn_rshift: The current code runs at 2.0 cycles/limb on
  UltraSPARC-1/2 and 2.65 on UltraSPARC-3.  For UltraSPARC-1/2, the IEU0
  functional unit is saturated with shifts.

* mpn_add_n, mpn_sub_n: The current code runs at 4 cycles/limb on
  UltraSPARC-1/2 and 4.5 cycles/limb on UltraSPARC-3.  The 4 instruction
  recurrency is the speed limiter.

* mpn_addmul_1: The current code runs at 14 cycles/limb asymptotically on
  UltraSPARC-1/2 and 17.5 cycles/limb on UltraSPARC-3.  On UltraSPARC-1/2, the
  code sustains 4 instructions/cycle.  It might be possible to invent a better
  way of summing the intermediate 49-bit operands, but it is unlikely that it
  will save enough instructions to save an entire cycle.

  The load-use of the u operand is not enough scheduled for good L2 cache
  performance.  The UltraSPARC-1/2 L1 cache is direct mapped, and since we use
  temporary stack slots that will conflict with the u and r operands, we miss
  to L2 very often.  The load-use of the std/ldx pairs via the stack are
  perhaps over-scheduled.

  It would be possible to save two instructions: (1) The mov could be avoided
  if the std/ldx were less scheduled.  (2) The ldx of the r operand could be
  split into two ld instructions, saving the shifts/masks.

  It should be possible to reach 14 cycles/limb for UltraSPARC-3 if the fp
  operations where rescheduled for this processor's 4-cycle latency.

* mpn_mul_1: The current code is a straightforward edit of the mpn_addmul_1
  code.  It would be possible to shave one or two cycles from it, with some
  labour.

* mpn_submul_1: Simpleminded code just calling mpn_mul_1 + mpn_sub_n.  This
  means that it runs at 18 cycles/limb on UltraSPARC-1/2 and 23 cycles/limb on
  UltraSPARC-3.  It would be possible to either match the mpn_addmul_1
  performance, or in the worst case use one more instruction group.

* US1/US2 cache conflict resolving.  The direct mapped L1 date cache of US1/US2
  is a problem for mul_1, addmul_1 (and a prospective submul_1).  We should
  allocate a larger cache area, and put the stack temp area in a place that
  doesn't cause cache conflicts.
Copyright 2002, 2005 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





                    POWERPC 32-BIT MPN SUBROUTINES


This directory contains mpn functions for various 32-bit PowerPC chips.


CODE ORGANIZATION

	directory	  used for
	================================================
	powerpc           generic, 604, 604e, 744x, 745x
	powerpc/750       740, 750, 7400, 7410


The top-level powerpc directory is currently mostly aimed at 604/604e but
should be reasonable on all powerpcs.



STATUS

The code is quite well optimized for the 604e, other chips have had less
attention.

Altivec SIMD available in 74xx might hold some promise, but unfortunately
GMP only guarantees 32-bit data alignment, so there's lots of fiddling
around with partial operations at the start and end of limb vectors.  A
128-bit limb would be a novel idea, but is unlikely to be practical, since
it would have to work with ordinary +, -, * etc in the C code.

Also, Altivec isn't very well suited for the GMP multiplication needs.
Using floating-point based multiplication has much better better performance
potential for all current powerpcs, both the ones with slow integer multiply
units (603, 740, 750, 7400, 7410) and those with fast (604, 604e, 744x,
745x).  This is because all powerpcs do some level of pipelining in the FPU:

603 and 750 can sustain one fmadd every 2nd cycle.
604 and 604e can sustain one fmadd per cycle.
7400 and 7410 can sustain 3 fmadd in 4 cycles.
744x and 745x can sustain 4 fmadd in 5 cycles.



REGISTER NAMES

The normal powerpc convention is to give registers as plain numbers, like
"mtctr 6", but on Apple MacOS X (powerpc*-*-rhapsody* and
powerpc*-*-darwin*) the assembler demands an "r" like "mtctr r6".  Note
however when register 0 in an instruction means a literal zero the "r" is
omitted, for instance "lwzx r6,0,r7".

The GMP code uses the "r" forms, powerpc-defs.m4 transforms them to plain
numbers according to what GMP_ASM_POWERPC_R_REGISTERS finds is needed.
(Note that this style isn't fully general, as the identifier r4 and the
register r4 will not be distinguishable on some systems.  However, this is
not a problem for the limited GMP assembly usage.)



GLOBAL REFERENCES

Linux non-PIC
	lis	9, __gmp_binvert_limb_table@ha
	rlwinm	11, 5, 31, 25, 31
	la	9, __gmp_binvert_limb_table@l(9)
	lbzx	11, 9, 11

Linux PIC (FIXME)
.LCL0:
	.long .LCTOC1-.LCF0
	bcl	20, 31, .LCF0
.LCF0:
	mflr	30
	lwz	7, .LCL0-.LCF0(30)
	add	30, 7, 30
	lwz	11, .LC0-.LCTOC1(30)
	rlwinm	3, 5, 31, 25, 31
	lbzx	7, 11, 3

AIX (always PIC)
LC..0:
	.tc __gmp_binvert_limb_table[TC],__gmp_binvert_limb_table[RW]
	lwz	9, LC..0(2)
	rlwinm	0, 5, 31, 25, 31
	lbzx	0, 9, 0

Darwin (non-PIC)
	lis	r2, ha16(___gmp_binvert_limb_table)
	rlwinm	r9, r5, 31, 25, 31
	la	r2, lo16(___gmp_binvert_limb_table)(r2)
	lbzx	r0, r2, r9
Darwin (PIC)
	mflr	r0
	bcl	20, 31, L0001$pb
L0001$pb:
	mflr	r7
	mtlr	r0
	addis	r2, r7, ha16(L___gmp_binvert_limb_table$non_lazy_ptr-L0001$pb)
	rlwinm	r9, r5, 31, 25, 31
	lwz	r2, lo16(L___gmp_binvert_limb_table$non_lazy_ptr-L0001$pb)(r2)
	lbzx	r0, r2, r9
------
	.non_lazy_symbol_pointer
L___gmp_binvert_limb_table$non_lazy_ptr:
	.indirect_symbol ___gmp_binvert_limb_table
	.long	0
	.subsections_via_symbols


For GNU/Linux and Darwin, we might want to duplicate __gmp_binvert_limb_table
into the text section in this file.  We should thus be able to reach it like
this:

	blr	L0
L0:	mflr	r2
	rlwinm	r9, r5, 31, 25, 31
	addi	r9, r9, lo16(local_binvert_table-L0)
	lbzx	r0, r2, r9



REFERENCES

PowerPC Microprocessor Family: The Programming Environments for 32-bit
Microprocessors, IBM document G522-0290-01, 2000.

PowerPC 604e RISC Microprocessor User's Manual with Supplement for PowerPC
604 Microprocessor, IBM document G552-0330-00, Freescale document
MPC604EUM/AD, 3/1998.

MPC7410/MPC7400 RISC Microprocessor User's Manual, Freescale document
MPC7400UM/D, rev 1, 11/2002.

MPC7450 RISC Microprocessor Family Reference Manual, Freescale document
MPC7450UM, rev 5, 1/2005.

The above are available online from

	http://www.ibm.com/chips/techlib/techlib.nsf/productfamilies/PowerPC
	http://www.freescale.com/PowerPC



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2002, 2012, 2015 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





This directory contains mpn functions for ARM processors.  It has been
optimised mainly for Cortex-A9 and Cortex-A15, but the code in the top-level
directory should run on all ARM processors at architecture level v4 or later.
This directory contains Neon code which runs and is efficient on all
ARM CPUs which support Neon.
Copyright 1996, 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





This directory contains mpn functions for various SPARC chips.  Code that
runs only on version 8 SPARC implementations, is in the v8 subdirectory.

RELEVANT OPTIMIZATION ISSUES

  Load and Store timing

On most early SPARC implementations, the ST instructions takes multiple
cycles, while a STD takes just a single cycle more than an ST.  For the CPUs
in SPARCstation I and II, the times are 3 and 4 cycles, respectively.
Therefore, combining two ST instructions into a STD when possible is a
significant optimization.

Later SPARC implementations have single cycle ST.

For SuperSPARC, we can perform just one memory instruction per cycle, even
if up to two integer instructions can be executed in its pipeline.  For
programs that perform so many memory operations that there are not enough
non-memory operations to issue in parallel with all memory operations, using
LDD and STD when possible helps.

UltraSPARC-1/2 has very slow integer multiplication.  In the v9 subdirectory,
we therefore use floating-point multiplication.

STATUS

1. On a SuperSPARC, mpn_lshift and mpn_rshift run at 3 cycles/limb, or 2.5
   cycles/limb asymptotically.  We could optimize speed for special counts
   by using ADDXCC.

2. On a SuperSPARC, mpn_add_n and mpn_sub_n runs at 2.5 cycles/limb, or 2
   cycles/limb asymptotically.

3. mpn_mul_1 runs at what is believed to be optimal speed.

4. On SuperSPARC, mpn_addmul_1 and mpn_submul_1 could both be improved by a
   cycle by avoiding one of the add instructions.  See a29k/addmul_1.

The speed of the code for other SPARC implementations is uncertain.
Code for SPARC processors implementing version 9 of the SPARC architecture.
This code is for systems that doesn't preserve the full 64-bit contents of
integer register at context switch.  For other systems (such as Solaris 7 or
later) use the code in ../../sparc64.
Copyright 2000-2002 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.






The code in this directory works for Cray vector systems such as C90,
J90, T90 (both the CFP variant and the IEEE variant) and SV1.  (For
the T3E and T3D systems, see the `alpha' subdirectory at the same
level as the directory containing this file.)

The cfp subdirectory is for systems utilizing the traditional Cray
floating-point format, and the ieee subdirectory is for the newer
systems that use the IEEE floating-point format.

There are several issues that reduces speed on Cray systems.  For
systems with cfp floating point, the main obstacle is the forming of
128-bit products.  For IEEE systems, adding, and in particular
computing carry is the main issue.  There are no vectorizing
unsigned-less-than instructions, and the sequence that implement that
operation is very long.

Shifting is the only operation that is simple to make fast.  All Cray
systems have a bitblt instructions (Vi Vj,Vj<Ak and Vi Vj,Vj>Ak) that
should be really useful.

For best speed for cfp systems, we need a mul_basecase, since that
reduces the need for carry propagation to a minimum.  Depending on the
size (vn) of the smaller of the two operands (V), we should split U and V
in different chunk sizes:

U split in 2 32-bit parts
V split according to the table:
parts			4	5	6	7	8
bits/part		16	13	11	10	8
max allowed vn		1	8	32	64	256
number of multiplies	8	10	12	14	16
peak cycles/limb	4	5	6	7	8

U split in 3 22-bit parts
V split according to the table:
parts			3	4	5
bits/part		22	16	13
max allowed vn		16	1024	8192
number of multiplies	9	12	15
peak cycles/limb	4.5	6	7.5

U split in 4 16-bit parts
V split according to the table:
parts			4
bits/part		16
max allowed vn		65536
number of multiplies	16
peak cycles/limb	8

(A T90 CPU can accumulate two products per cycle.)

IDEA:
* Rewrite mpn_add_n:
    short cy[n + 1];
    #pragma _CRI ivdep
      for (i = 0; i < n; i++)
	{ s = up[i] + vp[i];
	  rp[i] = s;
	  cy[i + 1] = s < up[i]; }
      more_carries = 0;
    #pragma _CRI ivdep
      for (i = 1; i < n; i++)
	{ s = rp[i] + cy[i];
	  rp[i] = s;
	  more_carries += s < cy[i]; }
      cys = 0;
      if (more_carries)
	{
	  cys = rp[1] < cy[1];
	  for (i = 2; i < n; i++)
	    { rp[i] += cys;
	      cys = rp[i] < cys; }
	}
      return cys + cy[n];

* Write mpn_add3_n for adding three operands.  First add operands 1
  and 2, and generate cy[].  Then add operand 3 to the partial result,
  and accumulate carry into cy[].  Finally propagate carry just like
  in the new mpn_add_n.

IDEA:

Store fewer bits, perhaps 62, per limb.  That brings mpn_add_n time
down to 2.5 cycles/limb and mpn_addmul_1 times to 4 cycles/limb.  By
storing even fewer bits per limb, perhaps 56, it would be possible to
write a mul_mul_basecase that would run at effectively 1 cycle/limb.
(Use VM here to better handle the romb-shaped multiply area, perhaps
rounding operand sizes up to the next power of 2.)
Copyright 2003 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





                      M88K MPN SUBROUTINES

This directory contains mpn functions for various m88k family chips.

CODE ORGANIZATION

	m88k             m88000, m88100
	m88k/mc88110     m88110

STATUS

The code herein is old and poorly maintained.

* The .s files assume the system uses a "_" underscore prefix, which
  should be controlled by configure.

* The mc88110/*.S files are using the defunct "sysdep.h" configuration
  scheme and won't compile.

Conversion to the current m4 .asm style wouldn't be difficult.




----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2001, 2003, 2004 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





                      M68K MPN SUBROUTINES


This directory contains mpn functions for various m68k family chips.


CODE ORGANIZATION

	m68k             m68000, m68010, m68060
	m68k/mc68020     m68020, m68030, m68040, and CPU32


The m5200 "coldfire", which is m68000 less a few instructions, currently has
no assembler code support.


STATUS

The code herein is old and poorly maintained.  If somebody really cared, it
could be optimized substantially.  For example,

* mpn_add_n and mpn_sub_n could, with more unrolling be improved from 6 to
  close to 4 c/l (on m68040).

* The multiplication loops could be sped up by using the FPU.

* mpn_lshift by 31 should use the special-case mpn_rshift by 1 code, and
  vice versa mpn_rshift by 31 use the special lshift by 1, when operand
  overlap permits.

* On 68000, mpn_mul_1, mpn_addmul_1 and mpn_submul_1 could check for a
  16-bit multiplier and use two multiplies per limb, not four.

  Similarly various other _1 operations like mpn_mod_1, mpn_divrem_1,
  mpn_divexact_1, mpn_modexact_1c_odd.

* On 68000, mpn_lshift and mpn_rshift could use a roll and mask instead of
  lsrl and lsll.  This promises to be a speedup, effectively trading a 6+2*n
  shift for one or two 4 cycle masks.  Suggested by Jean-Charles Meyrignac.

* config.guess detects 68000, 68010, CPU32 and 68020 by running some code,
  but relies on system information for 030, 040 and 060.  Can they be
  identified by running some code?  Currently this only makes a difference
  to the compiler options selected, since we have no specific asm code for
  those chips.

One novel idea for 68000 would be to use a 16-bit limb instead of 32-bits.
This would suit the native 16x16 multiply, but might make it difficult to
get full value from the native 32x32 add/sub/etc.  This would be an ABI
option, and would select "__GMP_SHORT_LIMB" in gmp.h.

Naturally an entirely new set of asm subroutines would be needed for a
16-bit limb.  Also there's various places in the C code assuming limb>=long,
which would need to be updated, eg. mpz_set_ui.  Some of the nails changes
may have helped cover some of this.


ASM FILES

The .asm files are put through m4 for macro processing, and with the help of
configure give either MIT or Motorola syntax.  The generic mpn/asm-defs.m4
is used, together with mpn/m68k/m68k-defs.m4.  See comments in those files.

Not all possible syntax variations are covered.  GCC config/m68k for
instance has things like $ for immediates on CRDS or reversed cmp order for
AT&T SGS.  These could probably be handled if anyone really needs it.


CALLING CONVENTIONS

The SVR4 standard has an int of 32 bits, and all parameters 32-bit aligned
on the stack.

PalmOS and perhaps various embedded systems intended for 68000 however use
an int of 16 bits and parameters only 16-bit aligned on the stack.  This is
generated by "gcc -mshort" (and is the default for the PalmOS gcc port, we
believe).

The asm files adapt to these two ABIs by checking sizeof(unsigned), coming
through config.m4 as SIZEOF_UNSIGNED.  Only mpn_lshift and mpn_rshift are
affected, all other routines take longs and pointers, which are 32-bits in
both cases.

Strictly speaking the size of an int doesn't determine the stack padding
convention.  But if int is 16 bits then we can definitely say the host
system is not SVR4, and therefore may as well assume we're in 16-bit stack
alignment.


REFERENCES

"Motorola M68000 Family Programmer's Reference Manual", available online,

	http://e-www.motorola.com/brdata/PDFDB/docs/M68000PM.pdf

"System V Application Binary Interface: Motorola 68000 Processor Family
Supplement", AT&T, 1990, ISBN 0-13-877553-6.  Has details of calling
conventions and ELF style PIC coding.



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2011 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.



There are 5 generations of 64-but s390 processors, z900, z990, z9,
z10, and z196.  The current GMP code was optimised for the two oldest,
z900 and z990.


mpn_copyi

This code makes use of a loop around MVC.  It almost surely runs very
close to optimally.  A small improvement could be done by using one
MVC for size 256 bytes, now we use two (we use an extra MVC when
copying any multiple of 256 bytes).


mpn_copyd

We have tried several feed-in variants here, branch tree, jump table
and computed goto.  The fastest (on z990) turned out to be computed
goto.

An approach not tried is EX of LMG and STMG, modifying the register set
on-the-fly.  Using that trick, we could completely avoid using
separate feed-in paths.


mpn_lshift, mpn_rshift

The current code runs at pipeline decode bandwidth on z990.


mpn_add_n, mpn_sub_n

The current code is 4-way unrolled.  It should be unrolled more, at
least 8x, in order to reach 2.5 c/l.


mpn_mul_1, mpn_addmul_1, mpn_submul_1

The current code is very naive, but due to the non-pipelined nature of
MLGR on z900 and z990, more sophisticated code would not gain much.

On z10 one would need to cluster at least 4 MLGR together, in order to
reduce stalling.

On z196, one surely want to use unrolling and pipelining, to perhaps
reach around 12 c/l.  A major issue here and on z10 is ALCGR's 3 cycle
stalling.


mpn_mul_2, mpn_addmul_2

At least for older machines (z900, z990) with very slow MLGR, we
should use Karatsuba's algorithm on 2-limb units, making mul_2 and
addmul_2 the main multiplication primitives.  The newer machines might
benefit less from this approach, perhaps in particular z10, where MLGR
clustering is more important.

With Karatsuba, one could hope for around 16 cycles per accumulated
128 cross product, on z990.
All current (2001) S/390 and z/Architecture machines are single-issue,
but some newer machines have a deep pipeline.  Software-pipelining is
therefore beneficial.

* mpn_add_n, mpn_sub_n: Use code along the lines below.  Two-way unrolling
  would be adequate.

  mp_limb_t
  mpn_add_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
  {
    mp_limb_t a, b, r, cy;
    mp_size_t i;
    mp_limb_t mm = -1;

    cy = 0;
    up += n;
    vp += n;
    rp += n;
    i = -n;
    do
      {
	a = up[i];
	b = vp[i];
	r = a + b + cy;
	rp[i] = r;
	cy = (((a & b) | ((a | b) & (r ^ mm)))) >> 31;
	i++;
      }
    while (i < 0);
    return cy;
  }

* mpn_lshift, mpn_rshift: Use SLDL/SRDL, and two-way unrolling.

* mpn_mul_1, mpn_addmul_1, mpn_submul_1: For machines with just signed
  multiply (MR), use two loops, similar to the corresponding VAX or
  POWER functions.  Handle carry like for mpn_add_n.
Copyright 2003, 2004, 2006, 2008 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





			AMD64 MPN SUBROUTINES


This directory contains mpn functions for AMD64 chips.  It is also useful
for 64-bit Pentiums, and "Core 2".


		     RELEVANT OPTIMIZATION ISSUES

The Opteron and Athlon64 can sustain up to 3 instructions per cycle, but in
practice that is only possible for integer instructions.  But almost any
three integer instructions can issue simultaneously, including any 3 ALU
operations, including shifts.  Up to two memory operations can issue each
cycle.

Scheduling typically requires that load-use instructions are split into
separate load and use instructions.  That requires more decode resources,
and it is rarely a win.  Opteron/Athlon64 have deep out-of-order core.


Optimizing for 64-bit Pentium4 is probably a waste of time, as the most
critical instructions are very poorly implemented here.  Perhaps we could
save a cycle or two, but the most common loops now run at between 10 and 22
cycles, so a saved cycle isn't too exciting.


The new spin of the venerable P6 core, the "Core 2" is much better than the
Pentium4 for the GMP loops.  Its integer pipeline is somewhat similar to to
the Opteron/Athlon64 pipeline, except that the GMP favourites ADC/SBB and
MUL are slower.  Furthermore, an INC/DEC followed by ADC/SBB incur a
pipeline stall of around 10 cycles.  The default mpn_add_n and mpn_sub_n
code suffers badly from the stall.  The code in the core2 subdirectory uses
the almost forgotten instruction JRCXZ for loop control, and updates the
induction variable using LEA.



REFERENCES

"System V Application Binary Interface AMD64 Architecture Processor
Supplement", draft version 0.99, December 2007.
http://www.x86-64.org/documentation/abi.pdf
This directory contains code for AMD bulldozer including its piledriver update.

We currently make limited use of SIMD instructions, both via the MPN_PATH and
via inclusion of x86_64/fastsse files.

The bd1 cores share one SIMD/FPU pipeline for two integer units.  This probably
means that an all-core GMP load (such as a HPC load) might run slower if there
is significant SIMD dependency.

We should perhaps allow a special 'bd1nosimd' pseudo cpu-name excluding any
SIMD code.
This directory contains code for x86-64 processors with fast
implementations of SSE operations, hence the name "fastsse".

Current processors that might benefit from this code are:

  AMD K10
  AMD Bulldozer/Piledriver/Steamroller/Excavator
  Intel Nocona
  Intel Nehalem/Westmere
  Intel Sandybridge/Ivybridge
  Intel Haswell/Broadwell
  VIA Nano

Current processors that do not benefit from this code are:

  AMD K8
  AMD Bobcat
  Intel Atom

Intel Conroe/Penryn is a border case; its handling of non-aligned
128-bit memory operands is poor.  VIA Nano also have poor handling of
non-aligned operands.
Copyright 2000-2005 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.



                      IA-64 MPN SUBROUTINES


This directory contains mpn functions for the IA-64 architecture.


CODE ORGANIZATION

	mpn/ia64          itanium-2, and generic ia64

The code here has been optimized primarily for Itanium 2.  Very few Itanium 1
chips were ever sold, and Itanium 2 is more powerful, so the latter is what
we concentrate on.



CHIP NOTES

The IA-64 ISA keeps instructions three and three in 128 bit bundles.
Programmers/compilers need to put explicit breaks `;;' when there are WAW or
RAW dependencies, with some notable exceptions.  Such "breaks" are typically
at the end of a bundle, but can be put between operations within some bundle
types too.

The Itanium 1 and Itanium 2 implementations can under ideal conditions
execute two bundles per cycle.  The Itanium 1 allows 4 of these instructions
to do integer operations, while the Itanium 2 allows all 6 to be integer
operations.

Taken cloop branches seem to insert a bubble into the pipeline most of the
time on Itanium 1.

Loads to the fp registers bypass the L1 cache and thus get extremely long
latencies, 9 cycles on the Itanium 1 and 6 cycles on the Itanium 2.

The software pipeline stuff using br.ctop instruction causes delays, since
many issue slots are taken up by instructions with zero predicates, and
since many extra instructions are needed to set things up.  These features
are clearly designed for code density, not speed.

Misc pipeline limitations (Itanium 1):
* The getf.sig instruction can only execute in M0.
* At most four integer instructions/cycle.
* Nops take up resources like any plain instructions.

Misc pipeline limitations (Itanium 2):
* The getf.sig instruction can only execute in M0.
* Nops take up resources like any plain instructions.


ASSEMBLY SYNTAX

.align pads with nops in a text segment, but gas 2.14 and earlier
incorrectly byte-swaps its nop bundle in big endian mode (eg. hpux), making
it come out as break instructions.  We use the ALIGN() macro in
mpn/ia64/ia64-defs.m4 when it might be executed across.  That macro
suppresses any .align if the problem is detected by configure.  Lack of
alignment might hurt performance but will at least be correct.

foo:: to create a global symbol is not accepted by gas.  Use separate
".global foo" and "foo:" instead.

.global is the standard global directive.  gas accepts .globl, but hpux "as"
doesn't.

.proc / .endp generates the appropriate .type and .size information for ELF,
so the latter directives don't need to be given explicitly.

.pred.rel "mutex"... is standard for annotating predicate register
relationships.  gas also accepts .pred.rel.mutex, but hpux "as" doesn't.

.pred directives can't be put on a line with a label, like
".Lfoo: .pred ...", the HP assembler on HP-UX 11.23 rejects that.
gas is happy with it, and past versions of HP had seemed ok.

// is the standard comment sequence, but we prefer "C" since it inhibits m4
macro expansion.  See comments in ia64-defs.m4.


REGISTER USAGE

Special:
   r0: constant 0
   r1: global pointer (gp)
   r8: return value
   r12: stack pointer (sp)
   r13: thread pointer (tp)
Caller-saves: r8-r11 r14-r31 f6-f15 f32-f127
Caller-saves but rotating: r32-


================================================================
mpn_add_n, mpn_sub_n:

The current code runs at 1.25 c/l on Itanium 2.

================================================================
mpn_mul_1:

The current code runs at 2 c/l on Itanium 2.

Using a blocked approach, working off of 4 separate places in the operands,
one could make use of the xma accumulation, and approach 1 c/l.

	ldf8 [up]
	xma.l
	xma.hu
	stf8  [wrp]

================================================================
mpn_addmul_1:

The current code runs at 2 c/l on Itanium 2.

It seems possible to use a blocked approach, as with mpn_mul_1.  We should
read rp[] to integer registers, allowing for just one getf.sig per cycle.

	ld8  [rp]
	ldf8 [up]
	xma.l
	xma.hu
	getf.sig
	add+add+cmp+cmp
	st8  [wrp]

These 10 instructions can be scheduled to approach 1.667 cycles, and with
the 4 cycle latency of xma, this means we need at least 3 blocks.  Using
ldfp8 we could approach 1.583 c/l.

================================================================
mpn_submul_1:

The current code runs at 2.25 c/l on Itanium 2.  Getting to 2 c/l requires
ldfp8 with all alignment headache that implies.

================================================================
mpn_addmul_N

For best speed, we need to give up using mpn_addmul_2 as the main multiply
building block, and instead take multiple v limbs per loop.  For the Itanium
1, we need to take about 8 limbs at a time for full speed.  For the Itanium
2, something like mpn_addmul_4 should be enough.

The add+cmp+cmp+add we use on the other codes is optimal for shortening
recurrencies (1 cycle) but the sequence takes up 4 execution slots.  When
recurrency depth is not critical, a more standard 3-cycle add+cmp+add is
better.

/* First load the 8 values from v */
	ldfp8		v0, v1 = [r35], 16;;
	ldfp8		v2, v3 = [r35], 16;;
	ldfp8		v4, v5 = [r35], 16;;
	ldfp8		v6, v7 = [r35], 16;;

/* In the inner loop, get a new U limb and store a result limb. */
	mov		lc = un
Loop:	ldf8		u0 = [r33], 8
	ld8		r0 = [r32]
	xma.l		lp0 = v0, u0, hp0
	xma.hu		hp0 = v0, u0, hp0
	xma.l		lp1 = v1, u0, hp1
	xma.hu		hp1 = v1, u0, hp1
	xma.l		lp2 = v2, u0, hp2
	xma.hu		hp2 = v2, u0, hp2
	xma.l		lp3 = v3, u0, hp3
	xma.hu		hp3 = v3, u0, hp3
	xma.l		lp4 = v4, u0, hp4
	xma.hu		hp4 = v4, u0, hp4
	xma.l		lp5 = v5, u0, hp5
	xma.hu		hp5 = v5, u0, hp5
	xma.l		lp6 = v6, u0, hp6
	xma.hu		hp6 = v6, u0, hp6
	xma.l		lp7 = v7, u0, hp7
	xma.hu		hp7 = v7, u0, hp7
	getf.sig	l0 = lp0
	getf.sig	l1 = lp1
	getf.sig	l2 = lp2
	getf.sig	l3 = lp3
	getf.sig	l4 = lp4
	getf.sig	l5 = lp5
	getf.sig	l6 = lp6
	add+cmp+add	xx, l0, r0
	add+cmp+add	acc0, acc1, l1
	add+cmp+add	acc1, acc2, l2
	add+cmp+add	acc2, acc3, l3
	add+cmp+add	acc3, acc4, l4
	add+cmp+add	acc4, acc5, l5
	add+cmp+add	acc5, acc6, l6
	getf.sig	acc6 = lp7
	st8		[r32] = xx, 8
	br.cloop Loop

	49 insn at max 6 insn/cycle:		8.167 cycles/limb8
	11 memops at max 2 memops/cycle:	5.5 cycles/limb8
	16 fpops at max 2 fpops/cycle:		8 cycles/limb8
	21 intops at max 4 intops/cycle:	5.25 cycles/limb8
	11+21 memops+intops at max 4/cycle	8 cycles/limb8

================================================================
mpn_lshift, mpn_rshift

The current code runs at 1 cycle/limb on Itanium 2.

Using 63 separate loops, we could use the double-word shrp instruction.
That instruction has a plain single-cycle latency.  We need 63 loops since
this instruction only accept immediate count.  That would lead to a somewhat
silly code size, but the speed would be 0.75 c/l on Itanium 2 (by using shrp
each cycle plus shl/shr going down I1 for a further limb every second
cycle).

================================================================
mpn_copyi, mpn_copyd

The current code runs at 0.5 c/l on Itanium 2.  But that is just for L1
cache hit.  The 4-way unrolled loop takes just 2 cycles, and thus load-use
scheduling isn't great.  It might be best to actually use modulo scheduled
loops, since that will allow us to do better load-use scheduling without too
much unrolling.

Depending on size or operand alignment, we get 1 c/l or 0.5 c/l on Itanium
2, according to tune/speed.  Cache bank conflicts?



REFERENCES

Intel Itanium Architecture Software Developer's Manual, volumes 1 to 3,
Intel document 245317-004, 245318-004, 245319-004 October 2002.  Volume 1
includes an Itanium optimization guide.

Intel Itanium Processor-specific Application Binary Interface (ABI), Intel
document 245370-003, May 2001.  Describes C type sizes, dynamic linking,
etc.

Intel Itanium Architecture Assembly Language Reference Guide, Intel document
248801-004, 2000-2002.  Describes assembly instruction syntax and other
directives.

Itanium Software Conventions and Runtime Architecture Guide, Intel document
245358-003, May 2001.  Describes calling conventions, including stack
unwinding requirements.

Intel Itanium Processor Reference Manual for Software Optimization, Intel
document 245473-003, November 2001.

Intel Itanium-2 Processor Reference Manual for Software Development and
Optimization, Intel document 251110-003, May 2004.

All the above documents can be found online at

    http://developer.intel.com/design/itanium/manuals.htm
Copyright 1999-2002 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





                      X86 MPN SUBROUTINES


This directory contains mpn functions for various 80x86 chips.


CODE ORGANIZATION

	x86               i386, generic
	x86/i486          i486
	x86/pentium       Intel Pentium (P5, P54)
	x86/pentium/mmx   Intel Pentium with MMX (P55)
	x86/p6            Intel Pentium Pro
	x86/p6/mmx        Intel Pentium II, III
	x86/p6/p3mmx      Intel Pentium III
	x86/k6            \ AMD K6
	x86/k6/mmx        /
	x86/k6/k62mmx     AMD K6-2
	x86/k7            \ AMD Athlon
	x86/k7/mmx        /
	x86/pentium4      \
	x86/pentium4/mmx  | Intel Pentium 4
	x86/pentium4/sse2 /


The top-level x86 directory contains blended style code, meant to be
reasonable on all x86s.



STATUS

The code is well-optimized for AMD and Intel chips, but there's nothing
specific for Cyrix chips, nor for actual 80386 and 80486 chips.



ASM FILES

The x86 .asm files are BSD style assembler code, first put through m4 for
macro processing.  The generic mpn/asm-defs.m4 is used, together with
mpn/x86/x86-defs.m4.  See comments in those files.

The code is meant for use with GNU "gas" or a system "as".  There's no
support for assemblers that demand Intel style code.



STACK FRAME

m4 macros are used to define the parameters passed on the stack, and these
act like comments on what the stack frame looks like too.  For example,
mpn_mul_1() has the following.

        defframe(PARAM_MULTIPLIER, 16)
        defframe(PARAM_SIZE,       12)
        defframe(PARAM_SRC,         8)
        defframe(PARAM_DST,         4)

PARAM_MULTIPLIER becomes `FRAME+16(%esp)', and the others similarly.  The
return address is at offset 0, but there's not normally any need to access
that.

FRAME is redefined as necessary through the code so it's the number of bytes
pushed on the stack, and hence the offsets in the parameter macros stay
correct.  At the start of a routine FRAME should be zero.

        deflit(`FRAME',0)
	...
	deflit(`FRAME',4)
	...
	deflit(`FRAME',8)
	...

Helper macros FRAME_pushl(), FRAME_popl(), FRAME_addl_esp() and
FRAME_subl_esp() exist to adjust FRAME for the effect of those instructions,
and can be used instead of explicit definitions if preferred.
defframe_pushl() is a combination FRAME_pushl() and defframe().

There's generally some slackness in redefining FRAME.  If new values aren't
going to get used then the redefinitions are omitted to keep from cluttering
up the code.  This happens for instance at the end of a routine, where there
might be just four pops and then a ret, so FRAME isn't getting used.

Local variables and saved registers can be similarly defined, with negative
offsets representing stack space below the initial stack pointer.  For
example,

	defframe(SAVE_ESI,   -4)
	defframe(SAVE_EDI,   -8)
	defframe(VAR_COUNTER,-12)

	deflit(STACK_SPACE, 12)

Here STACK_SPACE gets used in a "subl $STACK_SPACE, %esp" to allocate the
space, and that instruction must be followed by a redefinition of FRAME
(setting it equal to STACK_SPACE) to reflect the change in %esp.

Definitions for pushed registers are only put in when they're going to be
used.  If registers are just saved and restored with pushes and pops then
definitions aren't made.



ASSEMBLER EXPRESSIONS

Only addition and subtraction seem to be universally available, certainly
that's all the Solaris 8 "as" seems to accept.  If expressions are wanted
then m4 eval() should be used.

In particular note that a "/" anywhere in a line starts a comment in Solaris
"as", and in some configurations of gas too.

	addl	$32/2, %eax           <-- wrong

	addl	$eval(32/2), %eax     <-- right

Binutils gas/config/tc-i386.c has a choice between "/" being a comment
anywhere in a line, or only at the start.  FreeBSD patches 2.9.1 to select
the latter, and from 2.9.5 it's the default for GNU/Linux too.



ASSEMBLER COMMENTS

Solaris "as" doesn't support "#" commenting, using /* */ instead.  For that
reason "C" commenting is used (see asm-defs.m4) and the intermediate ".s"
files have no comments.

Any comments before include(`../config.m4') must use m4 "dnl", since it's
only after the include that "C" is available.  By convention "dnl" is also
used for comments about m4 macros.



TEMPORARY LABELS

Temporary numbered labels like "1:" used as "1f" or "1b" are available in
"gas" and Solaris "as", but not in SCO "as".  Normal L() labels should be
used instead, possibly with a counter to make them unique, see jadcl0() in
x86-defs.m4 for instance.  A separate counter for each macro makes it
possible to nest them, for instance movl_text_address() can be used within
an ASSERT().

"1:" etc must be avoided in gcc __asm__ blocks too.  "%=" for generating a
unique number looks like a good alternative, but is that actually a
documented feature?  In any case this problem doesn't currently arise.



ZERO DISPLACEMENTS

In a couple of places addressing modes like 0(%ebx) with a byte-sized zero
displacement are wanted, rather than (%ebx) with no displacement.  These are
either for computed jumps or to get desirable code alignment.  Explicit
.byte sequences are used to ensure the assembler doesn't turn 0(%ebx) into
(%ebx).  The Zdisp() macro in x86-defs.m4 is used for this.

Current gas 2.9.5 or recent 2.9.1 leave 0(%ebx) as written, but old gas
1.92.3 changes it.  In general changing would be the sort of "optimization"
an assembler might perform, hence explicit ".byte"s are used where
necessary.



SHLD/SHRD INSTRUCTIONS

The %cl count forms of double shift instructions like "shldl %cl,%eax,%ebx"
must be written "shldl %eax,%ebx" for some assemblers.  gas takes either,
Solaris "as" doesn't allow %cl, gcc generates %cl for gas and NeXT (which is
gas), and omits %cl elsewhere.

For GMP an autoconf test GMP_ASM_X86_SHLDL_CL is used to determine whether
%cl should be used, and the macros shldl, shrdl, shldw and shrdw in
mpn/x86/x86-defs.m4 pass through or omit %cl as necessary.  See the comments
with those macros for usage.



IMUL INSTRUCTION

GCC config/i386/i386.md (cvs rev 1.187, 21 Oct 00) under *mulsi3_1 notes
that the following two forms produce identical object code

	imul	$12, %eax
	imul	$12, %eax, %eax

but that the former isn't accepted by some assemblers, in particular the SCO
OSR5 COFF assembler.  GMP follows GCC and uses only the latter form.

(This applies only to immediate operands, the three operand form is only
valid with an immediate.)



DIRECTION FLAG

The x86 calling conventions say that the direction flag should be clear at
function entry and exit.  (See iBCS2 and SVR4 ABI books, references below.)
Although this has been so since the year dot, it's not absolutely clear
whether it's universally respected.  Since it's better to be safe than
sorry, GMP follows glibc and does a "cld" if it depends on the direction
flag being clear.  This happens only in a few places.



POSITION INDEPENDENT CODE

  Coding Style

    Defining the symbol PIC in m4 processing selects SVR4 / ELF style
    position independent code.  This is necessary for shared libraries
    because they can be mapped into different processes at different virtual
    addresses.  Actually, relocations are allowed but text pages with
    relocations aren't shared, defeating the purpose of a shared library.

    The GOT is used to access global data, and the PLT is used for
    functions.  The use of the PLT adds a fixed cost to every function call,
    and the GOT adds a cost to any function accessing global variables.
    These are small but might be noticeable when working with small
    operands.

  Scope

    It's intended, as a matter of policy, that references within libgmp are
    resolved within libgmp.  Certainly there's no need for an application to
    replace any internals, and we take the view that there's no value in an
    application subverting anything documented either.

    Resolving references within libgmp in theory means calls can be made with a
    plain PC-relative call instruction, which is faster and smaller than going
    through the PLT, and data references can be similarly PC-relative, saving a
    GOT entry and fetch from there.  Unfortunately the normal linker behaviour
    doesn't allow us to do this.

    By default an R_386_PC32 PC-relative reference, either for a call or for
    data, is left in libgmp.so by the linker so that it can be resolved at
    runtime to a location in the application or another shared library.  This
    means a text segment relocation which we don't want.

  -Bsymbolic

    Under the "-Bsymbolic" option, the linker resolves references to symbols
    within libgmp.so.  This gives us the desired effect for R_386_PC32,
    ie. it's resolved at link time.  It also resolves R_386_PLT32 calls
    directly to their target without creating a PLT entry (though if this is
    done to normal compiler-generated code it still leaves a setup of %ebx
    to _GLOBAL_OFFSET_TABLE_ which may then be unnecessary).

    Unfortunately -Bsymbolic does bad things to global variables defined in
    a shared library but accessed by non-PIC code from the mainline (or a
    static library).

    The problem is that the mainline needs a fixed data address to avoid
    text segment relocations, so space is allocated in its data segment and
    the value from the variable is copied from the shared library's data
    segment when the library is loaded.  Under -Bsymbolic, however,
    references in the shared library are then resolved still to the shared
    library data area.  Not surprisingly it bombs badly to have mainline
    code and library code accessing different locations for what should be
    one variable.

    Note that this -Bsymbolic effect for the shared library is not just for
    R_386_PC32 offsets which might have been cooked up in assembler, but is
    done also for the contents of GOT entries.  -Bsymbolic simply applies a
    general rule that symbols are resolved first from the local module.

  Visibility Attributes

    GCC __attribute__ ((visibility ("protected"))), which is available in
    recent versions, eg. 3.3, is probably what we'd like to use.  It makes
    gcc generate plain PC-relative calls to indicated functions, and directs
    the linker to resolve references to the given function within the link
    module.

    Unfortunately, as of debian binutils 2.13.90.0.16 at least, the
    resulting libgmp.so comes out with text segment relocations, references
    are not resolved at link time.  If the gcc description is to be believed
    this is this not how it should work.  If a symbol cannot be overridden
    by another module then surely references within that module can be
    resolved immediately (ie. at link time).

  Present

    In any case, all this means that we have no optimizations we can
    usefully make to function or variable usages, neither for assembler nor
    C code.  Perhaps in the future the visibility attribute will work as
    we'd like.




GLOBAL OFFSET TABLE

The magic _GLOBAL_OFFSET_TABLE_ used by code establishing the address of the
GOT sometimes requires an extra underscore prefix.  SVR4 systems and NetBSD
don't need a prefix, OpenBSD does need one.  Note that NetBSD and OpenBSD
are both a.out underscore systems, so the prefix for _GLOBAL_OFFSET_TABLE_
is not simply the same as the prefix for ordinary globals.

In any case in the asm code we write _GLOBAL_OFFSET_TABLE_ and let a macro
in x86-defs.m4 add an extra underscore if required (according to a configure
test).

Old gas 1.92.3 which comes with FreeBSD 2.2.8 gets a segmentation fault when
asked to assemble the following,

        L1:
            addl  $_GLOBAL_OFFSET_TABLE_+[.-L1], %ebx

It seems that using the label in the same instruction it refers to is the
problem, since a nop in between works.  But the simplest workaround is to
follow gcc and omit the +[.-L1] since it does nothing,

            addl  $_GLOBAL_OFFSET_TABLE_, %ebx

Current gas 2.10 generates incorrect object code when %eax is used in such a
construction (with or without +[.-L1]),

            addl  $_GLOBAL_OFFSET_TABLE_, %eax

The R_386_GOTPC gets a displacement of 2 rather than the 1 appropriate for
the 1 byte opcode of "addl $n,%eax".  The best workaround is just to use any
other register, since then it's a two byte opcode+mod/rm.  GCC for example
always uses %ebx (which is needed for calls through the PLT).

A similar problem occurs in an leal (again with or without a +[.-L1]),

            leal  _GLOBAL_OFFSET_TABLE_(%edi), %ebx

This time the R_386_GOTPC gets a displacement of 0 rather than the 2
appropriate for the opcode and mod/rm, making this form unusable.




SIMPLE LOOPS

The overheads in setting up for an unrolled loop can mean that at small
sizes a simple loop is faster.  Making small sizes go fast is important,
even if it adds a cycle or two to bigger sizes.  To this end various
routines choose between a simple loop and an unrolled loop according to
operand size.  The path to the simple loop, or to special case code for
small sizes, is always as fast as possible.

Adding a simple loop requires a conditional jump to choose between the
simple and unrolled code.  The size of a branch misprediction penalty
affects whether a simple loop is worthwhile.

The convention is for an m4 definition UNROLL_THRESHOLD to set the crossover
point, with sizes < UNROLL_THRESHOLD using the simple loop, sizes >=
UNROLL_THRESHOLD using the unrolled loop.  If position independent code adds
a couple of cycles to an unrolled loop setup, the threshold will vary with
PIC or non-PIC.  Something like the following is typical.

	deflit(UNROLL_THRESHOLD, ifdef(`PIC',10,8))

There's no automated way to determine the threshold.  Setting it to a small
value and then to a big value makes it possible to measure the simple and
unrolled loops each over a range of sizes, from which the crossover point
can be determined.  Alternately, just adjust the threshold up or down until
there's no more speedups.



UNROLLED LOOP CODING

The x86 addressing modes allow a byte displacement of -128 to +127, making
it possible to access 256 bytes, which is 64 limbs, without adjusting
pointer registers within the loop.  Dword sized displacements can be used
too, but they increase code size, and unrolling to 64 ought to be enough.

When unrolling to the full 64 limbs/loop, the limb at the top of the loop
will have a displacement of -128, so pointers have to have a corresponding
+128 added before entering the loop.  When unrolling to 32 limbs/loop
displacements 0 to 127 can be used with 0 at the top of the loop and no
adjustment needed to the pointers.

Where 64 limbs/loop is supported, the +128 adjustment is done only when 64
limbs/loop is selected.  Usually the gain in speed using 64 instead of 32 or
16 is small, so support for 64 limbs/loop is generally only for comparison.



COMPUTED JUMPS

When working from least significant limb to most significant limb (most
routines) the computed jump and pointer calculations in preparation for an
unrolled loop are as follows.

	S = operand size in limbs
	N = number of limbs per loop (UNROLL_COUNT)
	L = log2 of unrolling (UNROLL_LOG2)
	M = mask for unrolling (UNROLL_MASK)
	C = code bytes per limb in the loop
	B = bytes per limb (4 for x86)

	computed jump            (-S & M) * C + entrypoint
	subtract from pointers   (-S & M) * B
	initial loop counter     (S-1) >> L
	displacements            0 to B*(N-1)

The loop counter is decremented at the end of each loop, and the looping
stops when the decrement takes the counter to -1.  The displacements are for
the addressing accessing each limb, eg. a load with "movl disp(%ebx), %eax".

Usually the multiply by "C" can be handled without an imul, using instead an
leal, or a shift and subtract.

When working from most significant to least significant limb (eg. mpn_lshift
and mpn_copyd), the calculations change as follows.

	add to pointers          (-S & M) * B
	displacements            0 to -B*(N-1)



OLD GAS 1.92.3

This version comes with FreeBSD 2.2.8 and has a couple of gremlins that
affect GMP code.

Firstly, an expression involving two forward references to labels comes out
as zero.  For example,

		addl	$bar-foo, %eax
	foo:
		nop
	bar:

This should lead to "addl $1, %eax", but it comes out as "addl $0, %eax".
When only one forward reference is involved, it works correctly, as for
example,

	foo:
		addl	$bar-foo, %eax
		nop
	bar:

Secondly, an expression involving two labels can't be used as the
displacement for an leal.  For example,

	foo:
		nop
	bar:
		leal	bar-foo(%eax,%ebx,8), %ecx

A slightly cryptic error is given, "Unimplemented segment type 0 in
parse_operand".  When only one label is used it's ok, and the label can be a
forward reference too, as for example,

		leal	foo(%eax,%ebx,8), %ecx
		nop
	foo:

These problems only affect PIC computed jump calculations.  The workarounds
are just to do an leal without a displacement and then an addl, and to make
sure the code is placed so that there's at most one forward reference in the
addl.



REFERENCES

"Intel Architecture Software Developer's Manual", volumes 1, 2a, 2b, 3a, 3b,
2006, order numbers 253665 through 253669.  Available on-line,

	ftp://download.intel.com/design/Pentium4/manuals/25366518.pdf
	ftp://download.intel.com/design/Pentium4/manuals/25366618.pdf
	ftp://download.intel.com/design/Pentium4/manuals/25366718.pdf
	ftp://download.intel.com/design/Pentium4/manuals/25366818.pdf
	ftp://download.intel.com/design/Pentium4/manuals/25366918.pdf


"System V Application Binary Interface", Unix System Laboratories Inc, 1992,
published by Prentice Hall, ISBN 0-13-880410-9.  And the "Intel386 Processor
Supplement", AT&T, 1991, ISBN 0-13-877689-X.  These have details of calling
conventions and ELF shared library PIC coding.  Versions of both available
on-line,

	http://www.sco.com/developer/devspecs

"Intel386 Family Binary Compatibility Specification 2", Intel Corporation,
published by McGraw-Hill, 1991, ISBN 0-07-031219-2.  (Same as the above 386
ABI supplement.)



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.




                   INTEL PENTIUM-4 MPN SUBROUTINES


This directory contains mpn functions optimized for Intel Pentium-4.

The mmx subdirectory has routines using MMX instructions, the sse2
subdirectory has routines using SSE2 instructions.  All P4s have these, the
separate directories are just so configure can omit that code if the
assembler doesn't support it.


STATUS

                                cycles/limb

	mpn_add_n/sub_n            4 normal, 6 in-place

	mpn_mul_1                  4 normal, 6 in-place
	mpn_addmul_1               6
	mpn_submul_1               7

	mpn_mul_basecase           6 cycles/crossproduct (approx)

	mpn_sqr_basecase           3.5 cycles/crossproduct (approx)
                                   or 7.0 cycles/triangleproduct (approx)

	mpn_l/rshift               1.75



The shifts ought to be able to go at 1.5 c/l, but not much effort has been
applied to them yet.

In-place operations, and all addmul, submul, mul_basecase and sqr_basecase
calls, suffer from pipeline anomalies associated with write combining and
movd reads and writes to the same or nearby locations.  The movq
instructions do not trigger the same hardware problems.  Unfortunately,
using movq and splitting/combining seems to require too many extra
instructions to help.  Perhaps future chip steppings will be better.



NOTES

The Pentium-4 pipeline "Netburst", provides for quite a number of surprises.
Many traditional x86 instructions run very slowly, requiring use of
alterative instructions for acceptable performance.

adcl and sbbl are quite slow at 8 cycles for reg->reg.  paddq of 32-bits
within a 64-bit mmx register seems better, though the combination
paddq/psrlq when propagating a carry is still a 4 cycle latency.

incl and decl should be avoided, instead use add $1 and sub $1.  Apparently
the carry flag is not separately renamed, so incl and decl depend on all
previous flags-setting instructions.

shll and shrl have a 4 cycle latency, or 8 times the latency of the fastest
integer instructions (addl, subl, orl, andl, and some more).  shldl and
shrdl seem to have 13 and 15 cycles latency, respectively.  Bizarre.

movq mmx -> mmx does have 6 cycle latency, as noted in the documentation.
pxor/por or similar combination at 2 cycles latency can be used instead.
The movq however executes in the float unit, thereby saving MMX execution
resources.  With the right juggling, data moves shouldn't be on a dependent
chain.

L1 is write-through, but the write-combining sounds like it does enough to
not require explicit destination prefetching.

xmm registers so far haven't found a use, but not much effort has been
expended.  A configure test for whether the operating system knows
fxsave/fxrestor will be needed if they're used.



REFERENCES

Intel Pentium-4 processor manuals,

	http://developer.intel.com/design/pentium4/manuals

"Intel Pentium 4 Processor Optimization Reference Manual", Intel, 2001,
order number 248966.  Available on-line:

	http://developer.intel.com/design/pentium4/manuals/248966.htm



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2000, 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.




                      AMD K7 MPN SUBROUTINES


This directory contains code optimized for the AMD Athlon CPU.

The mmx subdirectory has routines using MMX instructions.  All Athlons have
MMX, the separate directory is just so that configure can omit it if the
assembler doesn't support MMX.



STATUS

Times for the loops, with all code and data in L1 cache.

                               cycles/limb
	mpn_add/sub_n             1.6

	mpn_copyi                 0.75 or 1.0   \ varying with data alignment
	mpn_copyd                 0.75 or 1.0   /

	mpn_divrem_1             17.0 integer part, 15.0 fractional part
	mpn_mod_1                17.0
	mpn_divexact_by3          8.0

	mpn_l/rshift              1.2

	mpn_mul_1                 3.4
	mpn_addmul/submul_1       3.9

	mpn_mul_basecase          4.42 cycles/crossproduct (approx)
        mpn_sqr_basecase          2.3 cycles/crossproduct (approx)
				  or 4.55 cycles/triangleproduct (approx)

Prefetching of sources hasn't yet been tried.



NOTES

cmov, MMX, 3DNow and some extensions to MMX and 3DNow are available.

Write-allocate L1 data cache means prefetching of destinations is unnecessary.

Floating point multiplications can be done in parallel with integer
multiplications, but there doesn't seem to be any way to make use of this.

Unsigned "mul"s can be issued every 3 cycles.  This suggests 3 is a limit on
the speed of the multiplication routines.  The documentation shows mul
executing in IEU0 (or maybe in IEU0 and IEU1 together), so it might be that,
to get near 3 cycles code has to be arranged so that nothing else is issued
to IEU0.  A busy IEU0 could explain why some code takes 4 cycles and other
apparently equivalent code takes 5.



OPTIMIZATIONS

Unrolled loops are used to reduce looping overhead.  The unrolling is
configurable up to 32 limbs/loop for most routines and up to 64 for some.
The K7 has 64k L1 code cache so quite big unrolling is allowable.

Computed jumps into the unrolling are used to handle sizes not a multiple of
the unrolling.  An attractive feature of this is that times increase
smoothly with operand size, but it may be that some routines should just
have simple loops to finish up, especially when PIC adds between 2 and 16
cycles to get %eip.

Position independent code is implemented using a call to get %eip for the
computed jumps and a ret is always done, rather than an addl $4,%esp or a
popl, so the CPU return address branch prediction stack stays synchronised
with the actual stack in memory.

Branch prediction, in absence of any history, will guess forward jumps are
not taken and backward jumps are taken.  Where possible it's arranged that
the less likely or less important case is under a taken forward jump.



CODING

Instructions in general code have been shown grouped if they can execute
together, which means up to three direct-path instructions which have no
successive dependencies.  K7 always decodes three and has out-of-order
execution, but the groupings show what slots might be available and what
dependency chains exist.

When there's vector-path instructions an effort is made to get triplets of
direct-path instructions in between them, even if there's dependencies,
since this maximizes decoding throughput and might save a cycle or two if
decoding is the limiting factor.



INSTRUCTIONS

adcl       direct
divl       39 cycles back-to-back
lodsl,etc  vector
loop       1 cycle vector (decl/jnz opens up one decode slot)
movd reg   vector
movd mem   direct
mull       issue every 3 cycles, latency 4 cycles low word, 6 cycles high word
popl	   vector (use movl for more than one pop)
pushl	   direct, will pair with a load
shrdl %cl  vector, 3 cycles, seems to be 3 decode too
xorl r,r   false read dependency recognised



REFERENCES

"AMD Athlon Processor X86 Code Optimization Guide", AMD publication number
22007, revision K, February 2002.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/22007.pdf

"3DNow Technology Manual", AMD publication number 21928G/0-March 2000.
This describes the femms and prefetch instructions.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/21928.pdf

"AMD Extensions to the 3DNow and MMX Instruction Sets Manual", AMD
publication number 22466, revision D, March 2000.  This describes
instructions added in the Athlon processor, such as pswapd and the extra
prefetch forms.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/22466.pdf

"3DNow Instruction Porting Guide", AMD publication number 22621, revision B,
August 1999.  This has some notes on general Athlon optimizations as well as
3DNow.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/22621.pdf




----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 1996, 1999-2001, 2003 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





                   INTEL PENTIUM P5 MPN SUBROUTINES


This directory contains mpn functions optimized for Intel Pentium (P5,P54)
processors.  The mmx subdirectory has additional code for Pentium with MMX
(P55).


STATUS

                                cycles/limb

	mpn_add_n/sub_n            2.375

	mpn_mul_1                 12.0
	mpn_add/submul_1          14.0

	mpn_mul_basecase          14.2 cycles/crossproduct (approx)

	mpn_sqr_basecase           8 cycles/crossproduct (approx)
                                   or 15.5 cycles/triangleproduct (approx)

	mpn_l/rshift               5.375 normal (6.0 on P54)
				   1.875 special shift by 1 bit

	mpn_divrem_1              44.0
	mpn_mod_1                 28.0
	mpn_divexact_by3          15.0

	mpn_copyi/copyd            1.0

Pentium MMX gets the following improvements

	mpn_l/rshift               1.75

	mpn_mul_1                 12.0 normal, 7.0 for 16-bit multiplier


mpn_add_n and mpn_sub_n run at asymptotically 2 cycles/limb.  Due to loop
overhead and other delays (cache refill?), they run at or near 2.5
cycles/limb.

mpn_mul_1, mpn_addmul_1, mpn_submul_1 all run 1 cycle faster than they
should.  Intel documentation says a mul instruction is 10 cycles, but it
measures 9 and the routines using it run as 9.



P55 MMX AND X87

The cost of switching between MMX and x87 floating point on P55 is about 100
cycles (fld1/por/emms for instance).  In order to avoid that the two aren't
mixed and currently that means using MMX and not x87.

MMX offers a big speedup for lshift and rshift, and a nice speedup for
16-bit multipliers in mpn_mul_1.  If fast code using x87 is found then
perhaps the preference for MMX will be reversed.




P54 SHLDL

mpn_lshift and mpn_rshift run at about 6 cycles/limb on P5 and P54, but the
documentation indicates that they should take only 43/8 = 5.375 cycles/limb,
or 5 cycles/limb asymptotically.  The P55 runs them at the expected speed.

It seems that on P54 a shldl or shrdl allows pairing in one following cycle,
but not two.  For example, back to back repetitions of the following

	shldl(	%cl, %eax, %ebx)
	xorl	%edx, %edx
	xorl	%esi, %esi

run at 5 cycles, as expected, but repetitions of the following run at 7
cycles, whereas 6 would be expected (and is achieved on P55),

	shldl(	%cl, %eax, %ebx)
	xorl	%edx, %edx
	xorl	%esi, %esi
	xorl	%edi, %edi
	xorl	%ebp, %ebp

Three xorls run at 7 cycles too, so it doesn't seem to be just that pairing
inhibited is only in the second following cycle (or something like that).

Avoiding this problem would bring P54 shifts down from 6.0 c/l to 5.5 with a
pattern of shift, 2 loads, shift, 2 stores, shift, etc.  A start has been
made on something like that, but it's not yet complete.




OTHER NOTES

Prefetching Destinations

    Pentium doesn't allocate cache lines on writes, unlike most other modern
    processors.  Since the functions in the mpn class do array writes, we
    have to handle allocating the destination cache lines by reading a word
    from it in the loops, to achieve the best performance.

Prefetching Sources

    Prefetching of sources is pointless since there's no out-of-order loads.
    Any load instruction blocks until the line is brought to L1, so it may
    as well be the load that wants the data which blocks.

Data Cache Bank Clashes

    Pairing of memory operations requires that the two issued operations
    refer to different cache banks (ie. different addresses modulo 32
    bytes).  The simplest way to ensure this is to read/write two words from
    the same object.  If we make operations on different objects, they might
    or might not be to the same cache bank.

PIC %eip Fetching

    A simple call $+5 and popl can be used to get %eip, there's no need to
    balance calls and returns since P5 doesn't have any return stack branch
    prediction.

Float Multiplies

    fmul is pairable and can be issued every 2 cycles (with a 4 cycle
    latency for data ready to use).  This is a lot better than integer mull
    or imull at 9 cycles non-pairing.  Unfortunately the advantage is
    quickly eaten away by needing to throw data through memory back to the
    integer registers to adjust for fild and fist being signed, and to do
    things like propagating carry bits.





REFERENCES

"Intel Architecture Optimization Manual", 1997, order number 242816.  This
is mostly about P5, the parts about P6 aren't relevant.  Available on-line:

        http://download.intel.com/design/PentiumII/manuals/242816.htm



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2000, 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





                      INTEL P6 MPN SUBROUTINES



This directory contains code optimized for Intel P6 class CPUs, meaning
PentiumPro, Pentium II and Pentium III.  The mmx and p3mmx subdirectories
have routines using MMX instructions.



STATUS

Times for the loops, with all code and data in L1 cache, are as follows.
Some of these might be able to be improved.

                               cycles/limb

	mpn_add_n/sub_n           3.7

	mpn_copyi                 0.75
	mpn_copyd                 1.75 (or 0.75 if no overlap)

	mpn_divrem_1             39.0
	mpn_mod_1                21.5
	mpn_divexact_by3          8.5

	mpn_mul_1                 5.5
	mpn_addmul/submul_1       6.35

	mpn_l/rshift              2.5

	mpn_mul_basecase          8.2 cycles/crossproduct (approx)
	mpn_sqr_basecase          4.0 cycles/crossproduct (approx)
				  or 7.75 cycles/triangleproduct (approx)

Pentium II and III have MMX and get the following improvements.

	mpn_divrem_1             25.0 integer part, 17.5 fractional part

	mpn_l/rshift              1.75




NOTES

Write-allocate L1 data cache means prefetching of destinations is unnecessary.

Mispredicted branches have a penalty of between 9 and 15 cycles, and even up
to 26 cycles depending how far speculative execution has gone.  The 9 cycle
minimum penalty comes from the issue pipeline being 9 stages.

A copy with rep movs seems to copy 16 bytes at a time, since speeds for 4,
5, 6 or 7 limb operations are all the same.  The 0.75 cycles/limb would be 3
cycles per 16 byte block.




CODING

Instructions in general code have been shown grouped if they can execute
together, which means up to three instructions with no successive
dependencies, and with only the first being a multiple micro-op.

P6 has out-of-order execution, so the groupings are really only showing
dependent paths where some shuffling might allow some latencies to be
hidden.




REFERENCES

"Intel Architecture Optimization Reference Manual", 1999, revision 001 dated
02/99, order number 245127 (order number 730795-001 is in the document too).
Available on-line:

	http://download.intel.com/design/PentiumII/manuals/245127.htm

"Intel Architecture Optimization Manual", 1997, order number 242816.  This
is an older document mostly about P5 and not as good as the above.
Available on-line:

	http://download.intel.com/design/PentiumII/manuals/242816.htm



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2000, 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.




			AMD K6 MPN SUBROUTINES



This directory contains code optimized for AMD K6 CPUs, meaning K6, K6-2 and
K6-3.

The mmx subdirectory has MMX code suiting plain K6, the k62mmx subdirectory
has MMX code suiting K6-2 and K6-3.  All chips in the K6 family have MMX,
the separate directories are just so that ./configure can omit them if the
assembler doesn't support MMX.




STATUS

Times for the loops, with all code and data in L1 cache, are as follows.

                                 cycles/limb

	mpn_add_n/sub_n            3.25 normal, 2.75 in-place

	mpn_mul_1                  6.25
	mpn_add/submul_1           7.65-8.4  (varying with data values)

	mpn_mul_basecase           9.25 cycles/crossproduct (approx)
	mpn_sqr_basecase           4.7  cycles/crossproduct (approx)
                                   or 9.2 cycles/triangleproduct (approx)

	mpn_l/rshift               3.0

	mpn_divrem_1              20.0
	mpn_mod_1                 20.0
	mpn_divexact_by3          11.0

	mpn_copyi                  1.0
	mpn_copyd                  1.0


K6-2 and K6-3 have dual-issue MMX and get the following improvements.

	mpn_l/rshift               1.75


Prefetching of sources hasn't yet given any joy.  With the 3DNow "prefetch"
instruction, code seems to run slower, and with just "mov" loads it doesn't
seem faster.  Results so far are inconsistent.  The K6 does a hardware
prefetch of the second cache line in a sector, so the penalty for not
prefetching in software is reduced.




NOTES

All K6 family chips have MMX, but only K6-2 and K6-3 have 3DNow.

Plain K6 executes MMX instructions only in the X pipe, but K6-2 and K6-3 can
execute them in both X and Y (and in both together).

Branch misprediction penalty is 1 to 4 cycles (Optimization Manual
chapter 6 table 12).

Write-allocate L1 data cache means prefetching of destinations is unnecessary.
Store queue is 7 entries of 64 bits each.

Floating point multiplications can be done in parallel with integer
multiplications, but there doesn't seem to be any way to make use of this.



OPTIMIZATIONS

Unrolled loops are used to reduce looping overhead.  The unrolling is
configurable up to 32 limbs/loop for most routines, up to 64 for some.

Sometimes computed jumps into the unrolling are used to handle sizes not a
multiple of the unrolling.  An attractive feature of this is that times
smoothly increase with operand size, but an indirect jump is about 6 cycles
and the setups about another 6, so it depends on how much the unrolled code
is faster than a simple loop as to whether a computed jump ought to be used.

Position independent code is implemented using a call to get eip for
computed jumps and a ret is always done, rather than an addl $4,%esp or a
popl, so the CPU return address branch prediction stack stays synchronised
with the actual stack in memory.  Such a call however still costs 4 to 7
cycles.

Branch prediction, in absence of any history, will guess forward jumps are
not taken and backward jumps are taken.  Where possible it's arranged that
the less likely or less important case is under a taken forward jump.



MMX

Putting emms or femms as late as possible in a routine seems to be fastest.
Perhaps an emms or femms stalls until all outstanding MMX instructions have
completed, so putting it later gives them a chance to complete on their own,
in parallel with other operations (like register popping).

The Optimization Manual chapter 5 recommends using a femms on K6-2 and K6-3
at the start of a routine, in case it's been preceded by x87 floating point
operations.  This isn't done because in gmp programs it's expected that x87
floating point won't be much used and that chances are an mpn routine won't
have been preceded by any x87 code.



CODING

Instructions in general code are shown paired if they can decode and execute
together, meaning two short decode instructions with the second not
depending on the first, only the first using the shifter, no more than one
load, and no more than one store.

K6 does some out of order execution so the pairings aren't essential, they
just show what slots might be available.  When decoding is the limiting
factor things can be scheduled that might not execute until later.



NOTES

Code alignment

- if an opcode/modrm or 0Fh/opcode/modrm crosses a cache line boundary,
  short decode is inhibited.  The cross.pl script detects this.

- loops and branch targets should be aligned to 16 bytes, or ensure at least
  2 instructions before a 32 byte boundary.  This makes use of the 16 byte
  cache in the BTB.

Addressing modes

- (%esi) degrades decoding from short to vector.  0(%esi) doesn't have this
  problem, and can be used as an equivalent, or easier is just to use a
  different register, like %ebx.

- K6 and pre-CXT core K6-2 have the following problem.  (K6-2 CXT and K6-3
  have it fixed, these being cpuid function 1 signatures 0x588 to 0x58F).

  If more than 3 bytes are needed to determine instruction length then
  decoding degrades from direct to long, or from long to vector.  This
  happens with forms like "0F opcode mod/rm" with mod/rm=00-xxx-100 since
  with mod=00 the sib determines whether there's a displacement.

  This affects all MMX and 3DNow instructions, and others with an 0F prefix,
  like movzbl.  The modes affected are anything with an index and no
  displacement, or an index but no base, and this includes (%esp) which is
  really (,%esp,1).

  The cross.pl script detects problem cases.  The workaround is to always
  use a displacement, and to do this with Zdisp if it's zero so the
  assembler doesn't discard it.

  See Optimization Manual rev D page 67 and 3DNow Porting Guide rev B pages
  13-14 and 36-37.

Calls

- indirect jumps and calls are not branch predicted, they measure about 6
  cycles.

Various

- adcl      2 cycles of decode, maybe 2 cycles executing in the X pipe
- bsf       12-27 cycles
- emms      5 cycles
- femms     3 cycles
- jecxz     2 cycles taken, 13 not taken (optimization manual says 7 not taken)
- divl      20 cycles back-to-back
- imull     2 decode, 3 execute
- mull      2 decode, 3 execute (optimization manual decoding sample)
- prefetch  2 cycles
- rcll/rcrl implicit by one bit: 2 cycles
            immediate or %cl count: 11 + 2 per bit for dword
                                    13 + 4 per bit for byte
- setCC	    2 cycles
- xchgl	%eax,reg  1.5 cycles, back-to-back (strange)
        reg,reg   2 cycles, back-to-back




REFERENCES

"AMD-K6 Processor Code Optimization Application Note", AMD publication
number 21924, revision D amendment 0, January 2000.  This describes K6-2 and
K6-3.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/21924.pdf

"AMD-K6 MMX Enhanced Processor x86 Code Optimization Application Note", AMD
publication number 21828, revision A amendment 0, August 1997.  This is an
older edition of the above document, describing plain K6.  Available
on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/21828.pdf

"3DNow Technology Manual", AMD publication number 21928G/0-March 2000.
This describes the femms and prefetch instructions, but nothing else from
3DNow has been used.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/21928.pdf

"3DNow Instruction Porting Guide", AMD publication number 22621, revision B,
August 1999.  This has some notes on general K6 optimizations as well as
3DNow.  Available on-line,

http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/22621.pdf



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 1996, 1997, 1999-2005 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





This directory contains mpn functions optimized for DEC Alpha processors.

ALPHA ASSEMBLY RULES AND REGULATIONS

The `.prologue N' pseudo op marks the end of instruction that needs special
handling by unwinding.  It also says whether $27 is really needed for computing
the gp.  The `.mask M' pseudo op says which registers are saved on the stack,
and at what offset in the frame.

Cray T3 code is very very different...

"$6" / "$f6" etc is the usual syntax for registers, but on Unicos instead "r6"
/ "f6" is required.  We use the "r6" / "f6" forms, and have m4 defines expand
them to "$6" or "$f6" where necessary.

"0x" introduces a hex constant in gas and DEC as, but on Unicos "^X" is
required.  The X() macro accommodates this difference.

"cvttqc" is required by DEC as, "cvttq/c" is required by Unicos, and gas will
accept either.  We use cvttqc and have an m4 define expand to cvttq/c where
necessary.

"not" as an alias for "ornot r31, ..." is available in gas and DEC as, but not
the Unicos assembler.  The full "ornot" must be used.

"unop" is not available in Unicos.  We make an m4 define to the usual "ldq_u
r31,0(r30)", and in fact use that define on all systems since it comes out the
same.

"!literal!123" etc explicit relocations as per Tru64 4.0 are apparently not
available in older alpha assemblers (including gas prior to 2.12), according to
the GCC manual, so the assembler macro forms must be used (eg. ldgp).



RELEVANT OPTIMIZATION ISSUES

EV4

1. This chip has very limited store bandwidth.  The on-chip L1 cache is write-
   through, and a cache line is transferred from the store buffer to the off-
   chip L2 in as much 15 cycles on most systems.  This delay hurts mpn_add_n,
   mpn_sub_n, mpn_lshift, and mpn_rshift.

2. Pairing is possible between memory instructions and integer arithmetic
   instructions.

3. mulq and umulh are documented to have a latency of 23 cycles, but 2 of these
   cycles are pipelined.  Thus, multiply instructions can be issued at a rate
   of one each 21st cycle.

EV5

1. The memory bandwidth of this chip is good, both for loads and stores.  The
   L1 cache can handle two loads or one store per cycle, but two cycles after a
   store, no ld can issue.

2. mulq has a latency of 12 cycles and an issue rate of 1 each 8th cycle.
   umulh has a latency of 14 cycles and an issue rate of 1 each 10th cycle.
   (Note that published documentation gets these numbers slightly wrong.)

3. mpn_add_n.  With 4-fold unrolling, we need 37 instructions, whereof 12
   are memory operations.  This will take at least
	ceil(37/2) [dual issue] + 1 [taken branch] = 19 cycles
   We have 12 memory cycles, plus 4 after-store conflict cycles, or 16 data
   cache cycles, which should be completely hidden in the 19 issue cycles.
   The computation is inherently serial, with these dependencies:

	       ldq  ldq
		 \  /\
	  (or)   addq |
	   |\   /   \ |
	   | addq  cmpult
	    \  |     |
	     cmpult  |
		 \  /
		  or

   I.e., 3 operations are needed between carry-in and carry-out, making 12
   cycles the absolute minimum for the 4 limbs.  We could replace the `or' with
   a cmoveq/cmovne, which could issue one cycle earlier that the `or', but that
   might waste a cycle on EV4.  The total depth remain unaffected, since cmov
   has a latency of 2 cycles.

     addq
     /   \
   addq  cmpult
     |      \
   cmpult -> cmovne

  Montgomery has a slightly different way of computing carry that requires one
  less instruction, but has depth 4 (instead of the current 3).  Since the code
  is currently instruction issue bound, Montgomery's idea should save us 1/2
  cycle per limb, or bring us down to a total of 17 cycles or 4.25 cycles/limb.
  Unfortunately, this method will not be good for the EV6.

4. addmul_1 and friends: We previously had a scheme for splitting the single-
   limb operand in 21-bits chunks and the multi-limb operand in 32-bit chunks,
   and then use FP operations for every 2nd multiply, and integer operations
   for every 2nd multiply.

   But it seems much better to split the single-limb operand in 16-bit chunks,
   since we save many integer shifts and adds that way.  See powerpc64/README
   for some more details.

EV6

Here we have a really parallel pipeline, capable of issuing up to 4 integer
instructions per cycle.  In actual practice, it is never possible to sustain
more than 3.5 integer insns/cycle due to rename register shortage.  One integer
multiply instruction can issue each cycle.  To get optimal speed, we need to
pretend we are vectorizing the code, i.e., minimize the depth of recurrences.

There are two dependencies to watch out for.  1) Address arithmetic
dependencies, and 2) carry propagation dependencies.

We can avoid serializing due to address arithmetic by unrolling loops, so that
addresses don't depend heavily on an index variable.  Avoiding serializing
because of carry propagation is trickier; the ultimate performance of the code
will be determined of the number of latency cycles it takes from accepting
carry-in to a vector point until we can generate carry-out.

Most integer instructions can execute in either the L0, U0, L1, or U1
pipelines.  Shifts only execute in U0 and U1, and multiply only in U1.

CMOV instructions split into two internal instructions, CMOV1 and CMOV2.  CMOV
split the mapping process (see pg 2-26 in cmpwrgd.pdf), suggesting the CMOV
should always be placed as the last instruction of an aligned 4 instruction
block, or perhaps simply avoided.

Perhaps the most important issue is the latency between the L0/U0 and L1/U1
clusters; a result obtained on either cluster has an extra cycle of latency for
consumers in the opposite cluster.  Because of the dynamic nature of the
implementation, it is hard to predict where an instruction will execute.



REFERENCES

"Alpha Architecture Handbook", version 4, Compaq, October 1998, order number
EC-QD2KC-TE.

"Alpha 21164 Microprocessor Hardware Reference Manual", Compaq, December 1998,
order number EC-QP99C-TE.

"Alpha 21264/EV67 Microprocessor Hardware Reference Manual", revision 1.4,
Compaq, September 2000, order number DS-0028B-TE.

"Compiler Writer's Guide for the Alpha 21264", Compaq, June 1999, order number
EC-RJ66A-TE.

All of the above are available online from

  http://ftp.digital.com/pub/Digital/info/semiconductor/literature/dsc-library.html
  ftp://ftp.compaq.com/pub/products/alphaCPUdocs

"Tru64 Unix Assembly Language Programmer's Guide", Compaq, March 1996, part
number AA-PS31D-TE.

"Digital UNIX Calling Standard for Alpha Systems", Digital Equipment Corp,
March 1996, part number AA-PY8AC-TE.

The above are available online,

  http://h30097.www3.hp.com/docs/pub_page/V40F_DOCS.HTM

(Dunno what h30097 means in this URL, but if it moves try searching for "tru64
online documentation" from the main www.hp.com page.)



----------------
Local variables:
mode: text
fill-column: 79
End:
Copyright 2002, 2005 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





This directory contains assembly code for nails-enabled 21264.  The code is not
very well optimized.

For addmul_N, as N grows larger, we could make multiple loads together, then do
about 3.3 i/c.  10 cycles after the last load, we can increase to 4 i/c.  This
would surely allow addmul_4 to run at 2 c/l, but the same should be possible
also for addmul_3 and perhaps even addmul_2.


		current		fair		best
Routine		c/l  unroll	c/l  unroll	c/l  i/c
mul_1		3.25		2.75		2.75 3.273
addmul_1	4.0	4	3.5	4 14	3.25 3.385
addmul_2	4.0	1	2.5	2 10	2.25 3.333
addmul_3	3.0	1	2.33	2 14	2    3.333
addmul_4	2.5	1	2.125	2 17	2    3.135

addmul_5			2	1 10
addmul_6			2	1 12
addmul_7			2	1 14

(The "best" column doesn't account for bookkeeping instructions and
thereby assumes infinite unrolling.)

Basecase usages:

1	 addmul_1
2	 addmul_2
3	 addmul_3
4	 addmul_4
5	 addmul_3 + addmul_2	2.3998
6	 addmul_4 + addmul_2
7	 addmul_4 + addmul_3
Copyright 1999-2001, 2003-2005 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.



                    POWERPC-64 MPN SUBROUTINES


This directory contains mpn functions for 64-bit PowerPC chips.


CODE ORGANIZATION

	mpn/powerpc64          mode-neutral code
	mpn/powerpc64/mode32   code for mode32
	mpn/powerpc64/mode64   code for mode64


The mode32 and mode64 sub-directories contain code which is for use in the
respective chip mode, 32 or 64.  The top-level directory is code that's
unaffected by the mode.

The "adde" instruction is the main difference between mode32 and mode64.  It
operates on either on a 32-bit or 64-bit quantity according to the chip mode.
Other instructions have an operand size in their opcode and hence don't vary.



POWER3/PPC630 pipeline information:

Decoding is 4-way + branch and issue is 8-way with some out-of-order
capability.

Functional units:
LS1  - ld/st unit 1
LS2  - ld/st unit 2
FXU1 - integer unit 1, handles any simple integer instruction
FXU2 - integer unit 2, handles any simple integer instruction
FXU3 - integer unit 3, handles integer multiply and divide
FPU1 - floating-point unit 1
FPU2 - floating-point unit 2

Memory:		  Any two memory operations can issue, but memory subsystem
		  can sustain just one store per cycle.  No need for data
		  prefetch; the hardware has very sophisticated prefetch logic.
Simple integer:	  2 operations (such as add, rl*)
Integer multiply: 1 operation every 9th cycle worst case; exact timing depends
		  on 2nd operand's most significant bit position (10 bits per
		  cycle).  Multiply unit is not pipelined, only one multiply
		  operation in progress is allowed.
Integer divide:	  ?
Floating-point:	  Any plain 2 arithmetic instructions (such as fmul, fadd, and
		  fmadd), latency 4 cycles.
Floating-point divide:
		  ?
Floating-point square root:
		  ?

POWER3/PPC630 best possible times for the main loops:
shift:	      1.5 cycles limited by integer unit contention.
	      With 63 special loops, one for each shift count, we could
	      reduce the needed integer instructions to 2, which would
	      reduce the best possible time to 1 cycle.
add/sub:      1.5 cycles, limited by ld/st unit contention.
mul:	      18 cycles (average) unless floating-point operations are used,
	      but that would only help for multiplies of perhaps 10 and more
	      limbs.
addmul/submul:Same situation as for mul.


POWER4/PPC970 and POWER5 pipeline information:

This is a very odd pipeline, it is basically a VLIW masquerading as a plain
architecture.  Its issue rules are not made public, and since it is so weird,
it is very hard to figure out any useful information from experimentation.
An example:

  A well-aligned loop with nop's take 3, 4, 6, 7, ... cycles.
    3 cycles for  0,  1,  2,  3,  4,  5,  6,  7 nop's
    4 cycles for  8,  9, 10, 11, 12, 13, 14, 15 nop's
    6 cycles for 16, 17, 18, 19, 20, 21, 22, 23 nop's
    7 cycles for 24, 25, 26, 27 nop's
    8 cycles for 28, 29, 30, 31 nop's
    ... continues regularly


Functional units:
LS1  - ld/st unit 1
LS2  - ld/st unit 2
FXU1 - integer unit 1, handles any integer instruction
FXU2 - integer unit 2, handles any integer instruction
FPU1 - floating-point unit 1
FPU2 - floating-point unit 2

While this is one integer unit less than POWER3/PPC630, the remaining units
are more powerful; here they handle multiply and divide.

Memory:		  2 ld/st.  Stores go to the L2 cache, which can sustain just
		  one store per cycle.
		  L1 load latency: to gregs 3-4 cycles, to fregs 5-6 cycles.
		  Operations that modify the address register might be split
		  to use also an integer issue slot.
Simple integer:	  2 operations every cycle, latency 2.
Integer multiply: 2 operations every 6th cycle, latency 7 cycles.
Integer divide:	  ?
Floating-point:	  Any plain 2 arithmetic instructions (such as fmul, fadd, and
		  fmadd), latency 6 cycles.
Floating-point divide:
		  ?
Floating-point square root:
		  ?


IDEAS

*mul_1: Handling one limb using mulld/mulhdu and two limbs using floating-
point operations should give performance of about 20 cycles for 3 limbs, or 7
cycles/limb.

We should probably split the single-limb operand in 32-bit chunks, and the
multi-limb operand in 16-bit chunks, allowing us to accumulate well in fp
registers.

Problem is to get 32-bit or 16-bit words to the fp registers.  Only 64-bit fp
memops copies bits without fiddling with them.  We might therefore need to
load to integer registers with zero extension, store as 64 bits into temp
space, and then load to fp regs.  Alternatively, load directly to fp space
and add well-chosen constants to get cancellation.  (Other part after given by
subsequent subtraction.)

Possible code mix for load-via-intregs variant:

lwz,std,lfd
fmadd,fmadd,fmul,fmul
fctidz,stfd,ld,fctidz,stfd,ld
add,adde
lwz,std,lfd
fmadd,fmadd,fmul,fmul
fctidz,stfd,ld,fctidz,stfd,ld
add,adde
srd,sld,add,adde,add,adde
Copyright 1999, 2001, 2002, 2004 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.




This directory contains mpn functions for 64-bit PA-RISC 2.0.

PIPELINE SUMMARY

The PA8x00 processors have an orthogonal 4-way out-of-order pipeline.  Each
cycle two ALU operations and two MEM operations can issue, but just one of the
MEM operations may be a store.  The two ALU operations can be almost any
combination of non-memory operations.  Unlike every other processor, integer
and fp operations are completely equal here; they both count as just ALU
operations.

Unfortunately, some operations cause hickups in the pipeline.  Combining
carry-consuming operations like ADD,DC with operations that does not set carry
like ADD,L cause long delays.  Skip operations also seem to cause hickups.  If
several ADD,DC are issued consecutively, or if plain carry-generating ADD feed
ADD,DC, stalling does not occur.  We can effectively issue two ADD,DC
operations/cycle.

Latency scheduling is not as important as making sure to have a mix of ALU and
MEM operations, but for full pipeline utilization, it is still a good idea to
do some amount of latency scheduling.

Like for all other processors, RAW memory scheduling is critically important.
Since integer multiplication takes place in the floating-point unit, the GMP
code needs to handle this problem frequently.

STATUS

* mpn_lshift and mpn_rshift run at 1.5 cycles/limb on PA8000 and at 1.0
  cycles/limb on PA8500.  With latency scheduling, the numbers could
  probably be improved to 1.0 cycles/limb for all PA8x00 chips.

* mpn_add_n and mpn_sub_n run at 2.0 cycles/limb on PA8000 and at about
  1.6875 cycles/limb on PA8500.  With latency scheduling, this could
  probably be improved to get close to 1.5 cycles/limb.  A problem is the
  stalling of carry-inputting instructions after instructions that do not
  write to carry.

* mpn_mul_1, mpn_addmul_1, and mpn_submul_1 run at between 5.625 and 6.375
  on PA8500 and later, and about a cycle/limb slower on older chips.  The
  code uses ADD,DC for adjacent limbs, and relies heavily on reordering.


REFERENCES

Hewlett Packard, "64-Bit Runtime Architecture for PA-RISC 2.0", version 3.3,
October 1997.
Copyright 1996, 1999, 2001, 2002, 2004 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.






This directory contains mpn functions for various HP PA-RISC chips.  Code
that runs faster on the PA7100 and later implementations, is in the pa7100
directory.

RELEVANT OPTIMIZATION ISSUES

  Load and Store timing

On the PA7000 no memory instructions can issue the two cycles after a store.
For the PA7100, this is reduced to one cycle.

The PA7100 has a lookup-free cache, so it helps to schedule loads and the
dependent instruction really far from each other.

STATUS

1. mpn_mul_1 could be improved to 6.5 cycles/limb on the PA7100, using the
   instructions below (but some sw pipelining is needed to avoid the
   xmpyu-fstds delay):

	fldds	s1_ptr

	xmpyu
	fstds	N(%r30)
	xmpyu
	fstds	N(%r30)

	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)

	addc
	stws	res_ptr
	addc
	stws	res_ptr

	addib	Loop

2. mpn_addmul_1 could be improved from the current 10 to 7.5 cycles/limb
   (asymptotically) on the PA7100, using the instructions below.  With proper
   sw pipelining and the unrolling level below, the speed becomes 8
   cycles/limb.

	fldds	s1_ptr
	fldds	s1_ptr

	xmpyu
	fstds	N(%r30)
	xmpyu
	fstds	N(%r30)
	xmpyu
	fstds	N(%r30)
	xmpyu
	fstds	N(%r30)

	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	ldws	N(%r30)
	addc
	addc
	addc
	addc
	addc	%r0,%r0,cy-limb

	ldws	res_ptr
	ldws	res_ptr
	ldws	res_ptr
	ldws	res_ptr
	add
	stws	res_ptr
	addc
	stws	res_ptr
	addc
	stws	res_ptr
	addc
	stws	res_ptr

	addib

3. For the PA8000 we have to stick to using 32-bit limbs before compiler
   support emerges.  But we want to use 64-bit operations whenever possible,
   in particular for loads and stores.  It is possible to handle mpn_add_n
   efficiently by rotating (when s1/s2 are aligned), masking+bit field
   inserting when (they are not).  The speed should double compared to the
   code used today.




LABEL SYNTAX

The HP-UX assembler takes labels starting in column 0 with no colon,

	L$loop  ldws,mb -4(0,%r25),%r22

Gas on hppa GNU/Linux however requires a colon,

	L$loop: ldws,mb -4(0,%r25),%r22

This is covered by using LDEF() from asm-defs.m4.  An alternative would be
to use ".label" which is accepted by both,

		.label  L$loop
		ldws,mb -4(0,%r25),%r22

but that's not as nice to look at, not if you're used to assembler code
having labels in column 0.




REFERENCES

Hewlett Packard, "HP Assembler Reference Manual", 9th edition, June 1998,
part number 92432-90012.



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 1996 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.





This directory contains mpn functions optimized for MIPS3.  Example of
processors that implement MIPS3 are R4000, R4400, R4600, R4700, and R8000.

RELEVANT OPTIMIZATION ISSUES

1. On the R4000 and R4400, branches, both the plain and the "likely" ones,
   take 3 cycles to execute.  (The fastest possible loop will take 4 cycles,
   because of the delay insn.)

   On the R4600, branches takes a single cycle

   On the R8000, branches often take no noticeable cycles, as they are
   executed in a separate function unit..

2. The R4000 and R4400 have a load latency of 4 cycles.

3. On the R4000 and R4400, multiplies take a data-dependent number of
   cycles, contrary to the SGI documentation.  There seem to be 3 or 4
   possible latencies.

4. The R1x000 processors can issue one floating-point operation, two integer
   operations, and one memory operation per cycle.  The FPU has very short
   latencies, while the integer multiply unit is non-pipelined.  We should
   therefore write fp based mpn_Xmul_1.

STATUS

Good...
Copyright 2000, 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library test suite.

The GNU MP Library test suite is free software; you can redistribute it
and/or modify it under the terms of the GNU General Public License as
published by the Free Software Foundation; either version 3 of the License,
or (at your option) any later version.

The GNU MP Library test suite is distributed in the hope that it will be
useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
Public License for more details.

You should have received a copy of the GNU General Public License along with
the GNU MP Library test suite.  If not, see https://www.gnu.org/licenses/.




                       DEVELOPMENT TEST PROGRAMS


This directory contains various programs used during development.  Casual
GMP users are unlikely to find anything of interest.

Nothing here is built or installed, nor even run in a "make check", but
there's Makefile rules to build each program, or "allprogs" to build
everything.



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2011-2013 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.


This is "mini-gmp", a small implementation of a subset of GMP's mpn
and mpz interfaces.

It is intended for applications which need arithmetic on numbers
larger than a machine word, but which don't need to handle very large
numbers very efficiently. Those applications can include a copy of
mini-gmp to get a GMP-compatible interface with small footprint. One
can also arrange for optional linking with the real GMP library, using
mini-gmp as a fallback when for some reason GMP is not available, or
not desired as a dependency.

The supported GMP subset is declared in mini-gmp.h. The implemented
functions are fully compatible with the corresponding GMP functions,
as specified in the GMP manual, with a few exceptions:

  mpz_set_str, mpz_init_set_str, mpz_get_str, mpz_out_str and
  mpz_sizeinbase support only |base| <= 36;
  mpz_export and mpz_import support only NAILS = 0.

  The REALLOC_FUNC and FREE_FUNC registered with
  mp_set_memory_functions does not get the correct size of the
  allocated block in the corresponding argument. mini-gmp always
  passes zero for these rarely used arguments.

The implementation is a single file, mini-gmp.c.

The performance target for mini-gmp is to be at most 10 times slower
than the real GMP library, for numbers of size up to a few hundred
bits. No asymptotically fast algorithms are included in mini-gmp, so
it will be many orders of magnitude slower than GMP for very large
numbers.

You should never "install" mini-gmp. Applications can either just
#include mini-gmp.c (but then, beware that it defines several macros
and functions outside of the advertised interface). Or compile
mini-gmp.c as a separate compilation unit, and use the declarations in
mini-gmp.h.

The tests subdirectory contains a testsuite. To use it, you need GMP
and GNU make. Just run make check in the tests directory. If the
hard-coded compiler settings are not right, you have to either edit the
Makefile or pass overriding values on the make command line (e.g.,
make CC=cc check). Testing is not (yet) as thorough as for the real
GMP.

The current version was put together by Niels Möller
<nisse@lysator.liu.se>, with a fair amount of copy-and-paste from the
GMP sources.
Copyright 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 3 of the License, or (at your option) any later
version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
this program.  If not, see https://www.gnu.org/licenses/.




                   DEMONSTRATION CALCULATOR PROGRAM


This is a simple program, meant only to show one way to use GMP with yacc
and lex to make a calculator.  Usage and comments on the implementation can
be found in calc.y.

Within a GMP build tree, the generated Makefile can be used to build the
program,

	make calc

(or on a DOS system, "make calc.exe").

Elsewhere, once GMP has been installed, the program can be compiled with for
instance

	gcc calc.c calclex.c -lgmp -o calc

Or if GNU readline is used then

	gcc calc.c calclex.c calcread.c -lgmp -lreadline -o calc

(again, on a DOS system "-o calc.exe").

Readline support can be enabled or disabled in calc-config.h.  That file is
created by the GMP ./configure based on the --with-readline option.  The
default is --with-readline=detect, which means to use readline if available.
"yes" can be used to force it to be used, or "no" to not use it.

The supplied calc.c was generated by GNU bison, but a standard yacc should
work too.

The supplied calclex.c was generated by GNU flex, but a standard lex should
work too.  The readline support may or may not work with a standard lex (see
comments with input() in calcread.c).  Note also that a standard lex will
require its library "-ll" on the compile command line.  "./configure" sets
this up in the GMP build tree Makefile.



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright 2001, 2004 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.






                    GMP EXPRESSION EVALUATION
                    -------------------------



THIS CODE IS PRELIMINARY AND MAY BE SUBJECT TO INCOMPATIBLE CHANGES IN
FUTURE VERSIONS OF GMP.



The files in this directory implement a simple scheme of string based
expression parsing and evaluation, supporting mpz, mpq and mpf.

This will be slower than direct GMP library calls, but may be convenient in
various circumstances, such as while prototyping, or for letting a user
enter values in symbolic form.  "2**5723-7" for example is a lot easier to
enter or maintain than the equivalent written out in decimal.



BUILDING

Nothing in this directory is a normal part of libgmp, and nothing is built
or installed, but various Makefile rules are available to compile
everything.

All the functions are available through a little library (there's no shared
library since upward binary compatibility is not guaranteed).

	make libexpr.a

In a program, prototypes are available using

	#include "expr.h"

run-expr.c is a sample program doing evaluations from the command line.

	make run-expr
	./run-expr '1+2*3'

t-expr.c is self-test program, it prints nothing if successful.

	make t-expr
	./t-expr

The expr*.c sources don't depend on gmp-impl.h and can be compiled with just
a standard installed GMP.  This isn't true of t-expr though, since it uses
some of the internal tests/libtests.la.



SIMPLE USAGE

int mpz_expr (mpz_t res, int base, const char *e, ...);
int mpq_expr (mpq_t res, int base, const char *e, ...);
int mpf_expr (mpf_t res, int base, const char *e, ...);

These functions evaluate simple arithmetic expressions.  For example,

	mpz_expr (result, 0, "123+456", NULL);

Numbers are parsed by mpz_expr and mpq_expr the same as mpz_set_str with the
given base.  mpf_expr follows mpf_set_str, but supporting an "0x" prefix for
hex when base==0.

	mpz_expr (result, 0, "0xAAAA * 0x5555", NULL);

White space, as indicated by <ctype.h> isspace(), is ignored except for the
purpose of separating tokens.

Variables can be included in expressions by putting them in the stdarg list
after the string.  "a", "b", "c" etc in the expression string designate
those values.  For example,

        mpq_t  foo, bar;
        ...
	mpq_expr (q, 10, "2/3 + 1/a + b/2", foo, bar, NULL);

Here "a" will be the value from foo and "b" from bar.  Up to 26 variables
can be included this way.  The NULL must be present to indicate the end of
the list.

Variables can also be written "$a", "$b" etc.  This is necessary when using
bases greater than 10 since plain "a", "b" etc will otherwise be interpreted
as numbers.  For example,

        mpf_t  quux;
        mpf_expr (f, 16, "F00F@-6 * $a", quux, NULL);

All the standard C operators are available, with the usual precedences, plus
"**" for exponentiation at the highest precedence (and right associative).

        Operators      Precedence
         **              220
         ~ ! - (unary)   210
         * / %           200
         + -             190
         << >>           180
         <= < >= >       170
         == !=           160
         &               150
         ^               140
         |               130
         &&              120
         ||              110
         ? :             100/101

Currently only mpz_expr has the bitwise ~ % & ^ and | operators.  The
precedence numbers are of interest in the advanced usage described below.

Various functions are available too.  For example,

        mpz_expr (res, 10, "gcd(123,456,789) * abs(a)", var, NULL);

The following is the full set of functions,

        mpz_expr
            abs bin clrbit cmp cmpabs congruent_p divisible_p even_p fib fac
            gcd hamdist invert jacobi kronecker lcm lucnum max min nextprime
            odd_p perfect_power_p perfect_square_p popcount powm
            probab_prime_p root scan0 scan1 setbit sgn sqrt

        mpq_expr
            abs, cmp, den, max, min, num, sgn

        mpf_expr
            abs, ceil, cmp, eq, floor, integer_p, max, min, reldiff, sgn,
            sqrt, trunc

All these are the same as the GMP library functions, except that min and max
don't exist in the library.  Note also that min, max, gcd and lcm take any
number of arguments, not just two.

mpf_expr does all calculations to the precision of the destination variable.


Expression parsing can succeed or fail.  The return value indicates this,
and will be one of the following

	MPEXPR_RESULT_OK
	MPEXPR_RESULT_BAD_VARIABLE
	MPEXPR_RESULT_BAD_TABLE
	MPEXPR_RESULT_PARSE_ERROR
	MPEXPR_RESULT_NOT_UI

BAD_VARIABLE is when a variable is referenced that hasn't been provided.
For example if "c" is used when only two parameters have been passed.
BAD_TABLE is applicable to the advanced usage described below.

PARSE_ERROR is a general syntax error, returned for any mal-formed input
string.

NOT_UI is returned when an attempt is made to use an operand that's bigger
than an "unsigned long" with a function that's restricted to that range.
For example "fib" is mpz_fib_ui and only accepts an "unsigned long".




ADVANCED USAGE

int mpz_expr_a (const struct mpexpr_operator_t *table,
                mpz_ptr res, int base, const char *e, size_t elen,
                mpz_srcptr var[26])
int mpq_expr_a (const struct mpexpr_operator_t *table,
                mpq_ptr res, int base, const char *e, size_t elen,
                mpq_srcptr var[26])
int mpf_expr_a (const struct mpexpr_operator_t *table,
                mpf_ptr res, int base, unsigned long prec,
                const char *e, size_t elen,
                mpf_srcptr var[26])

These functions are an advanced interface to expression parsing.

The string is taken as pointer and length.  This makes it possible to parse
an expression in the middle of somewhere without copying and null
terminating it.

Variables are an array of 26 pointers to the appropriate operands, or NULL
for variables that are not available.  Any combination of variables can be
given, for example just "x" and "y" (var[23] and var[24]) could be set.

Operators and functions are specified with a table.  This makes it possible
to provide additional operators or functions, or to completely change the
syntax.  The standard tables used by the simple functions above are
available as

	const struct mpexpr_operator_t * const mpz_expr_standard_table;
	const struct mpexpr_operator_t * const mpq_expr_standard_table;
	const struct mpexpr_operator_t * const mpf_expr_standard_table;

struct mpexpr_operator_t is the following

	struct mpexpr_operator_t {
	  const char    *name;
	  mpexpr_fun_t  fun;
	  int           type;
	  int           precedence;
	};

        typedef void (*mpexpr_fun_t) (void);

As an example, the standard mpz_expr table entry for multiplication is as
follows.  See the source code for the full set of standard entries.

	{ "*", (mpexpr_fun_t) mpz_mul, MPEXPR_TYPE_BINARY, 200 },

"name" is the string to parse, "fun" is the function to call for it, "type"
indicates what parameters the function takes (among other things), and
"precedence" sets its operator precedence.

A NULL for "name" indicates the end of the table, so for example an mpf
table with nothing but addition could be

        struct mpexpr_operator_t  table[] = {
          { "+", (mpexpr_fun_t) mpf_add, MPEXPR_TYPE_BINARY, 190 },
          { NULL }
        };

A special type MPEXPR_TYPE_NEW_TABLE makes it possible to chain from one
table to another.  For example the following would add a "mod" operator to
the standard mpz table,

        struct mpexpr_operator_t  table[] = {
        { "mod", (mpexpr_fun_t) mpz_fdiv_r, MPEXPR_TYPE_BINARY, 125 },
        { (const char *) mpz_expr_standard_table, NULL, MPEXPR_TYPE_NEW_TABLE }
        };

Notice the low precedence on "mod", so that for instance "45+26 mod 7"
parses as "(45+26)mod7".


Functions are designated by a precedence of 0.  They always occur as
"foo(expr)" and so have no need for a precedence level.  mpq_abs in the
standard mpq table is

	{ "abs", (mpexpr_fun_t) mpq_abs, MPEXPR_TYPE_UNARY },

Functions expecting no arguments as in "foo()" can be given with
MPEXPR_TYPE_0ARY, or actual constants to be parsed as just "foo" are
MPEXPR_TYPE_CONSTANT.  For example if a "void mpf_const_pi(mpf_t f)"
function existed (which it doesn't) it could be,

	{ "pi", (mpexpr_fun_t) mpf_const_pi, MPEXPR_TYPE_CONSTANT },


Parsing of operator names is done by seeking the table entry with the
longest matching name.  So for instance operators "<" and "<=" exist, and
when presented with "x <= y" the parser matches "<=" because it's longer.

Parsing of function names, on the other hand, is done by requiring a whole
alphanumeric word to match.  For example presented with "fib2zz(5)" the
parser will attempt to find a function called "fib2zz".  A function "fib"
wouldn't be used because it doesn't match the whole word.

The flag MPEXPR_TYPE_WHOLEWORD can be ORed into an operator type to override
the default parsing style.  Similarly MPEXPR_TYPE_OPERATOR into a function.


Binary operators are left associative by default, meaning they're evaluated
from left to right, so for example "1+2+3" is treated as "(1+2)+3".
MPEXPR_TYPE_RIGHTASSOC can be ORed into the operator type to work from right
to left as in "1+(2+3)".  This is generally what's wanted for
exponentiation, and for example the standard mpz table has

        { "**", (mpexpr_fun_t) mpz_pow_ui,
          MPEXPR_TYPE_BINARY_UI | MPEXPR_TYPE_RIGHTASSOC, 220 }

Unary operators are postfix by default.  For example a factorial to be used
as "123!" might be

	{ "!", (mpexpr_fun_t) mpz_fac_ui, MPEXPR_TYPE_UNARY_UI, 215 }

MPEXPR_TYPE_PREFIX can be ORed into the type to get a prefix operator.  For
instance negation (unary minus) in the standard mpf table is

	{ "-", (mpexpr_fun_t) mpf_neg,
          MPEXPR_TYPE_UNARY | MPEXPR_TYPE_PREFIX, 210 },


The same operator can exist as a prefix unary and a binary, or as a prefix
and postfix unary, simply by putting two entries in the table.  While
parsing the context determines which style is sought.  But note that the
same operator can't be both a postfix unary and a binary, since the parser
doesn't try to look ahead to decide which ought to be used.

When there's two entries for an operator, both prefix or both postfix (or
binary), then the first in the table will be used.  This makes it possible
to override an entry in a standard table, for example to change the function
it calls, or perhaps its precedence level.  The following would change mpz
division from tdiv to cdiv,

        struct mpexpr_operator_t  table[] = {
          { "/", (mpexpr_fun_t) mpz_cdiv_q, MPEXPR_TYPE_BINARY, 200 },
          { "%", (mpexpr_fun_t) mpz_cdiv_r, MPEXPR_TYPE_BINARY, 200 },
          { (char *) mpz_expr_standard_table, NULL, MPEXPR_TYPE_NEW_TABLE }
        };


The type field indicates what parameters the given function expects.  The
following styles of functions are supported.  mpz_t is shown, but of course
this is mpq_t for mpq_expr_a, mpf_t for mpf_expr_a, etc.

    MPEXPR_TYPE_CONSTANT     void func (mpz_t result);

    MPEXPR_TYPE_0ARY         void func (mpz_t result);
    MPEXPR_TYPE_I_0ARY       int func (void);

    MPEXPR_TYPE_UNARY        void func (mpz_t result, mpz_t op);
    MPEXPR_TYPE_UNARY_UI     void func (mpz_t result, unsigned long op);
    MPEXPR_TYPE_I_UNARY      int func (mpz_t op);
    MPEXPR_TYPE_I_UNARY_UI   int func (unsigned long op);

    MPEXPR_TYPE_BINARY       void func (mpz_t result, mpz_t op1, mpz_t op2);
    MPEXPR_TYPE_BINARY_UI    void func (mpz_t result,
                                        mpz_t op1, unsigned long op2);
    MPEXPR_TYPE_I_BINARY     int func (mpz_t op1, mpz_t op2);
    MPEXPR_TYPE_I_BINARY_UI  int func (mpz_t op1, unsigned long op2);

    MPEXPR_TYPE_TERNARY      void func (mpz_t result,
                                        mpz_t op1, mpz_t op2, mpz_t op3);
    MPEXPR_TYPE_TERNARY_UI   void func (mpz_t result, mpz_t op1, mpz_t op2,
                                        unsigned long op3);
    MPEXPR_TYPE_I_TERNARY    int func (mpz_t op1, mpz_t op2, mpz_t op3);
    MPEXPR_TYPE_I_TERNARY_UI int func (mpz_t op1, mpz_t op2,
                                       unsigned long op3);

Notice the pattern of "UI" for the last parameter as an unsigned long, or
"I" for the result as an "int" return value.

It's important that the declared type for an operator or function matches
the function pointer given.  Any mismatch will have unpredictable results.

For binary functions, a further type attribute is MPEXPR_TYPE_PAIRWISE which
indicates that any number of arguments should be accepted, and evaluated by
applying the given binary function to them pairwise.  This is used by gcd,
lcm, min and max.  For example the standard mpz gcd is

	{ "gcd", (mpexpr_fun_t) mpz_gcd,
	  MPEXPR_TYPE_BINARY | MPEXPR_TYPE_PAIRWISE },

Some special types exist for comparison operators (or functions).
MPEXPR_TYPE_CMP_LT through MPEXPR_TYPE_CMP_GE expect an MPEXPR_TYPE_I_BINARY
function, returning positive, negative or zero like mpz_cmp and similar.
For example the standard mpf "!=" operator is

	{ "!=", (mpexpr_fun_t) mpf_cmp, MPEXPR_TYPE_CMP_NE, 160 },

But there's no obligation to use these types, for instance the standard mpq
table just uses a plain MPEXPR_TYPE_I_BINARY and mpq_equal for "==".

Further special types MPEXPR_TYPE_MIN and MPEXPR_TYPE_MAX exist to implement
the min and max functions, and they take a function like mpf_cmp similarly.
The standard mpf max function is

	{ "max",  (mpexpr_fun_t) mpf_cmp,
          MPEXPR_TYPE_MAX | MPEXPR_TYPE_PAIRWISE },

These can be used as operators too, for instance the following would be the
>? operator which is a feature of GNU C++,

	{ ">?", (mpexpr_fun_t) mpf_cmp, MPEXPR_TYPE_MAX, 175 },

Other special types are used to define "(" ")" parentheses, "," function
argument separator, "!" through "||" logical booleans, ternary "?"  ":", and
the "$" which introduces variables.  See the sources for how they should be
used.


User definable operator tables will have various uses.  For example,

  - a subset of the C operators, to be rid of infrequently used things
  - a more mathematical syntax like "." for multiply, "^" for powering,
    and "!" for factorial
  - a boolean evaluator with "^" for AND, "v" for OR
  - variables introduced with "%" instead of "$"
  - brackets as "[" and "]" instead of "(" and ")"

The only fixed parts of the parsing are the treatment of numbers, whitespace
and the two styles of operator/function name recognition.

As a final example, the following would be a complete mpz table implementing
some operators with a more mathematical syntax.  Notice there's no need to
preserve the standard precedence values, anything can be used so long as
they're in the desired relation to each other.  There's also no need to have
entries in precedence order, but it's convenient to do so to show what comes
where.

        static const struct mpexpr_operator_t  table[] = {
	  { "^",   (mpexpr_fun_t) mpz_pow_ui,
            MPEXPR_TYPE_BINARY_UI | MPEXPR_TYPE_RIGHTASSOC,           9 },

          { "!",   (mpexpr_fun_t) mpz_fac_ui, MPEXPR_TYPE_UNARY_UI,   8 },
          { "-",   (mpexpr_fun_t) mpz_neg,
            MPEXPR_TYPE_UNARY | MPEXPR_TYPE_PREFIX,                   7 },

          { "*",   (mpexpr_fun_t) mpz_mul,    MPEXPR_TYPE_BINARY,     6 },
          { "/",   (mpexpr_fun_t) mpz_fdiv_q, MPEXPR_TYPE_BINARY,     6 },

          { "+",   (mpexpr_fun_t) mpz_add,    MPEXPR_TYPE_BINARY,     5 },
          { "-",   (mpexpr_fun_t) mpz_sub,    MPEXPR_TYPE_BINARY,     5 },

          { "mod", (mpexpr_fun_t) mpz_mod,    MPEXPR_TYPE_BINARY,     6 },

          { ")",   NULL,                      MPEXPR_TYPE_CLOSEPAREN, 4 },
          { "(",   NULL,                      MPEXPR_TYPE_OPENPAREN,  3 },
          { ",",   NULL,                      MPEXPR_TYPE_ARGSEP,     2 },

          { "$",   NULL,                      MPEXPR_TYPE_VARIABLE,   1 },
          { NULL }
        };




INTERNALS

Operator precedence is implemented using a control and data stack, there's
no C recursion.  When an expression like 1+2*3 is read the "+" is held on
the control stack and 1 on the data stack until "*" has been parsed and
applied to 2 and 3.  This happens any time a higher precedence operator
follows a lower one, or when a right-associative operator like "**" is
repeated.

Parentheses are handled by making "(" a special prefix unary with a low
precedence so a whole following expression is read.  The special operator
")" knows to discard the pending "(".  Function arguments are handled
similarly, with the function pretending to be a low precedence prefix unary
operator, and with "," allowed within functions.  The same special ")"
operator recognises a pending function and will invoke it appropriately.

The ternary "? :" operator is also handled using precedences.  ":" is one
level higher than "?", so when a valid a?b:c is parsed the ":" finds a "?"
on the control stack.  It's a parse error for ":" to find anything else.



FUTURE

The ternary "?:" operator evaluates the "false" side of its pair, which is
wasteful, though it ought to be harmless.  It'd be better if it could
evaluate only the "true" side.  Similarly for the logical booleans "&&" and
"||" if they know their result already.

Functions like MPEXPR_TYPE_BINARY could return a status indicating operand
out of range or whatever, to get an error back through mpz_expr etc.  That
would want to be just an option, since plain mpz_add etc have no such
return.

Could have assignments like "a = b*c" modifying the input variables.
Assignment could be an operator attribute, making it expect an lvalue.
There would want to be a standard table without assignments available
though, so user input could be safely parsed.

The closing parenthesis table entry could specify the type of open paren it
expects, so that "(" and ")" could match and "[" and "]" match but not a
mixture of the two.  Currently "[" and "]" can be added, but there's no
error on writing a mixed expression like "2*(3+4]".  Maybe also there could
be a way to say that functions can only be written with one or the other
style of parens.



----------------
Local variables:
mode: text
fill-column: 76
End:
Copyright (C) INRIA 2003, 2005, 2008, 2009, 2011

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved. This file is offered as-is,
without any warranty.


GNU MPC is a complex floating-point library with exact rounding.
It is based on the GNU MPFR floating-point library (http://www.mpfr.org/),
which is itself based on the GNU MP library (http://gmplib.org/).
README for the tz distribution

"What time is it?" -- Richard Deacon as The King
"Any time you want it to be." -- Frank Baxter as The Scientist
					(from the Bell System film "About Time")

The Time Zone Database (often called tz or zoneinfo) contains code and
data that represent the history of local time for many representative
locations around the globe.  It is updated periodically to reflect
changes made by political bodies to time zone boundaries, UTC offsets,
and daylight-saving rules.

See <https://www.iana.org/time-zones/repository/tz-link.html> or the
file tz-link.htm for how to acquire the code and data.  Once acquired,
read the comments in the file 'Makefile' and make any changes needed
to make things right for your system, especially if you are using some
platform other than GNU/Linux.  Then run the following commands,
substituting your desired installation directory for "$HOME/tzdir":

	make TOPDIR=$HOME/tzdir install
	$HOME/tzdir/etc/zdump -v America/Los_Angeles

Historical local time information has been included here to:

*	provide a compendium of data about the history of civil time
	that is useful even if not 100% accurate;

*	give an idea of the variety of local time rules that have
	existed in the past and thus an idea of the variety that may be
	expected in the future;

*	provide a test of the generality of the local time rule description
	system.

The information in the time zone data files is by no means authoritative;
fixes and enhancements are welcome.  Please see the file CONTRIBUTING
for details.

Thanks to these Time Zone Caballeros who've made major contributions to the
time conversion package: Keith Bostic; Bob Devine; Paul Eggert; Robert Elz;
Guy Harris; Mark Horton; John Mackin; and Bradley White.  Thanks also to
Michael Bloom, Art Neilson, Stephen Prince, John Sovereign, and Frank Wales
for testing work, and to Gwillim Law for checking local mean time data.
Thanks in particular to Arthur David Olson, the project's founder and first
maintainer, to whom the time zone community owes the greatest debt of all.
None of them are responsible for remaining errors.

-----

This file is in the public domain, so clarified as of 2009-05-17 by
Arthur David Olson.  The other files in this distribution are either
public domain or BSD licensed; see the file LICENSE for details.

XZ Utils
========

    0. Overview
    1. Documentation
       1.1. Overall documentation
       1.2. Documentation for command-line tools
       1.3. Documentation for liblzma
    2. Version numbering
    3. Reporting bugs
    4. Translating the xz tool
    5. Other implementations of the .xz format
    6. Contact information


0. Overview
-----------

    XZ Utils provide a general-purpose data-compression library plus
    command-line tools. The native file format is the .xz format, but
    also the legacy .lzma format is supported. The .xz format supports
    multiple compression algorithms, which are called "filters" in the
    context of XZ Utils. The primary filter is currently LZMA2. With
    typical files, XZ Utils create about 30 % smaller files than gzip.

    To ease adapting support for the .xz format into existing applications
    and scripts, the API of liblzma is somewhat similar to the API of the
    popular zlib library. For the same reason, the command-line tool xz
    has a command-line syntax similar to that of gzip.

    When aiming for the highest compression ratio, the LZMA2 encoder uses
    a lot of CPU time and may use, depending on the settings, even
    hundreds of megabytes of RAM. However, in fast modes, the LZMA2 encoder
    competes with bzip2 in compression speed, RAM usage, and compression
    ratio.

    LZMA2 is reasonably fast to decompress. It is a little slower than
    gzip, but a lot faster than bzip2. Being fast to decompress means
    that the .xz format is especially nice when the same file will be
    decompressed very many times (usually on different computers), which
    is the case e.g. when distributing software packages. In such
    situations, it's not too bad if the compression takes some time,
    since that needs to be done only once to benefit many people.

    With some file types, combining (or "chaining") LZMA2 with an
    additional filter can improve the compression ratio. A filter chain may
    contain up to four filters, although usually only one or two are used.
    For example, putting a BCJ (Branch/Call/Jump) filter before LZMA2
    in the filter chain can improve compression ratio of executable files.

    Since the .xz format allows adding new filter IDs, it is possible that
    some day there will be a filter that is, for example, much faster to
    compress than LZMA2 (but probably with worse compression ratio).
    Similarly, it is possible that some day there is a filter that will
    compress better than LZMA2.

    XZ Utils doesn't support multithreaded compression or decompression
    yet. It has been planned though and taken into account when designing
    the .xz file format.


1. Documentation
----------------

1.1. Overall documentation

    README              This file

    INSTALL.generic     Generic install instructions for those not familiar
                        with packages using GNU Autotools
    INSTALL             Installation instructions specific to XZ Utils
    PACKAGERS           Information to packagers of XZ Utils

    COPYING             XZ Utils copyright and license information
    COPYING.GPLv2       GNU General Public License version 2
    COPYING.GPLv3       GNU General Public License version 3
    COPYING.LGPLv2.1    GNU Lesser General Public License version 2.1

    AUTHORS             The main authors of XZ Utils
    THANKS              Incomplete list of people who have helped making
                        this software
    NEWS                User-visible changes between XZ Utils releases
    ChangeLog           Detailed list of changes (commit log)
    TODO                Known bugs and some sort of to-do list

    Note that only some of the above files are included in binary
    packages.


1.2. Documentation for command-line tools

    The command-line tools are documented as man pages. In source code
    releases (and possibly also in some binary packages), the man pages
    are also provided in plain text (ASCII only) and PDF formats in the
    directory "doc/man" to make the man pages more accessible to those
    whose operating system doesn't provide an easy way to view man pages.


1.3. Documentation for liblzma

    The liblzma API headers include short docs about each function
    and data type as Doxygen tags. These docs should be quite OK as
    a quick reference.

    I have planned to write a bunch of very well documented example
    programs, which (due to comments) should work as a tutorial to
    various features of liblzma. No such example programs have been
    written yet.

    For now, if you have never used liblzma, libbzip2, or zlib, I
    recommend learning the *basics* of the zlib API. Once you know that,
    it should be easier to learn liblzma.

        http://zlib.net/manual.html
        http://zlib.net/zlib_how.html


2. Version numbering
--------------------

    The version number format of XZ Utils is X.Y.ZS:

      - X is the major version. When this is incremented, the library
        API and ABI break.

      - Y is the minor version. It is incremented when new features
        are added without breaking the existing API or ABI. An even Y
        indicates a stable release and an odd Y indicates unstable
        (alpha or beta version).

      - Z is the revision. This has a different meaning for stable and
        unstable releases:

          * Stable: Z is incremented when bugs get fixed without adding
            any new features. This is intended to be convenient for
            downstream distributors that want bug fixes but don't want
            any new features to minimize the risk of introducing new bugs.

          * Unstable: Z is just a counter. API or ABI of features added
            in earlier unstable releases having the same X.Y may break.

      - S indicates stability of the release. It is missing from the
        stable releases, where Y is an even number. When Y is odd, S
        is either "alpha" or "beta" to make it very clear that such
        versions are not stable releases. The same X.Y.Z combination is
        not used for more than one stability level, i.e. after X.Y.Zalpha,
        the next version can be X.Y.(Z+1)beta but not X.Y.Zbeta.


3. Reporting bugs
-----------------

    Naturally it is easiest for me if you already know what causes the
    unexpected behavior. Even better if you have a patch to propose.
    However, quite often the reason for unexpected behavior is unknown,
    so here are a few things to do before sending a bug report:

      1. Try to create a small example how to reproduce the issue.

      2. Compile XZ Utils with debugging code using configure switches
         --enable-debug and, if possible, --disable-shared. If you are
         using GCC, use CFLAGS='-O0 -ggdb3'. Don't strip the resulting
         binaries.

      3. Turn on core dumps. The exact command depends on your shell;
         for example in GNU bash it is done with "ulimit -c unlimited",
         and in tcsh with "limit coredumpsize unlimited".

      4. Try to reproduce the suspected bug. If you get "assertion failed"
         message, be sure to include the complete message in your bug
         report. If the application leaves a coredump, get a backtrace
         using gdb:
           $ gdb /path/to/app-binary   # Load the app to the debugger.
           (gdb) core core   # Open the coredump.
           (gdb) bt   # Print the backtrace. Copy & paste to bug report.
           (gdb) quit   # Quit gdb.

    Report your bug via email or IRC (see Contact information below).
    Don't send core dump files or any executables. If you have a small
    example file(s) (total size less than 256 KiB), please include
    it/them as an attachment. If you have bigger test files, put them
    online somewhere and include a URL to the file(s) in the bug report.

    Always include the exact version number of XZ Utils in the bug report.
    If you are using a snapshot from the git repository, use "git describe"
    to get the exact snapshot version. If you are using XZ Utils shipped
    in an operating system distribution, mention the distribution name,
    distribution version, and exact xz package version; if you cannot
    repeat the bug with the code compiled from unpatched source code,
    you probably need to report a bug to your distribution's bug tracking
    system.


4. Translating the xz tool
--------------------------

    The messages from the xz tool have been translated into a few
    languages. Before starting to translate into a new language, ask
    the author whether someone else hasn't already started working on it.

    Test your translation. Testing includes comparing the translated
    output to the original English version by running the same commands
    in both your target locale and with LC_ALL=C. Ask someone to
    proof-read and test the translation.

    Testing can be done e.g. by installing xz into a temporary directory:

        ./configure --disable-shared --prefix=/tmp/xz-test
        # <Edit the .po file in the po directory.>
        make -C po update-po
        make install
        bash debug/translation.bash | less
        bash debug/translation.bash | less -S  # For --list outputs

    Repeat the above as needed (no need to re-run configure though).

    Note especially the following:

      - The output of --help and --long-help must look nice on
        an 80-column terminal. It's OK to add extra lines if needed.

      - In contrast, don't add extra lines to error messages and such.
        They are often preceded with e.g. a filename on the same line,
        so you have no way to predict where to put a \n. Let the terminal
        do the wrapping even if it looks ugly. Adding new lines will be
        even uglier in the generic case even if it looks nice in a few
        limited examples.

      - Be careful with column alignment in tables and table-like output
        (--list, --list --verbose --verbose, --info-memory, --help, and
        --long-help):

          * All descriptions of options in --help should start in the
            same column (but it doesn't need to be the same column as
            in the English messages; just be consistent if you change it).
            Check that both --help and --long-help look OK, since they
            share several strings.

          * --list --verbose and --info-memory print lines that have
            the format "Description:   %s". If you need a longer
            description, you can put extra space between the colon
            and %s. Then you may need to add extra space to other
            strings too so that the result as a whole looks good (all
            values start at the same column).

          * The columns of the actual tables in --list --verbose --verbose
            should be aligned properly. Abbreviate if necessary. It might
            be good to keep at least 2 or 3 spaces between column headings
            and avoid spaces in the headings so that the columns stand out
            better, but this is a matter of opinion. Do what you think
            looks best.

      - Be careful to put a period at the end of a sentence when the
        original version has it, and don't put it when the original
        doesn't have it. Similarly, be careful with \n characters
        at the beginning and end of the strings.

      - Read the TRANSLATORS comments that have been extracted from the
        source code and included in xz.pot. If they suggest testing the
        translation with some type of command, do it. If testing needs
        input files, use e.g. tests/files/good-*.xz.

      - When updating the translation, read the fuzzy (modified) strings
        carefully, and don't mark them as updated before you actually
        have updated them. Reading through the unchanged messages can be
        good too; sometimes you may find a better wording for them.

      - If you find language problems in the original English strings,
        feel free to suggest improvements. Ask if something is unclear.

      - The translated messages should be understandable (sometimes this
        may be a problem with the original English messages too). Don't
        make a direct word-by-word translation from English especially if
        the result doesn't sound good in your language.

    In short, take your time and pay attention to the details. Making
    a good translation is not a quick and trivial thing to do. The
    translated xz should look as polished as the English version.


5. Other implementations of the .xz format
------------------------------------------

    7-Zip and the p7zip port of 7-Zip support the .xz format starting
    from the version 9.00alpha.

        http://7-zip.org/
        http://p7zip.sourceforge.net/

    XZ Embedded is a limited implementation written for use in the Linux
    kernel, but it is also suitable for other embedded use.

        http://tukaani.org/xz/embedded.html


6. Contact information
----------------------

    If you have questions, bug reports, patches etc. related to XZ Utils,
    contact Lasse Collin <lasse.collin@tukaani.org> (in Finnish or English).
    I'm sometimes slow at replying. If you haven't got a reply within two
    weeks, assume that your email has got lost and resend it or use IRC.

    You can find me also from #tukaani on Freenode; my nick is Larhzu.
    The channel tends to be pretty quiet, so just ask your question and
    someone may wake up.


.xz Test Files
----------------

0. Introduction

    This directory contains bunch of files to test handling of .xz files
    in .xz decoder implementations. Many of the files have been created
    by hand with a hex editor, thus there is no better "source code" than
    the files themselves. All the test files (*.xz) and this README have
    been put into the public domain.


1. File Types

    Good files (good-*.xz) must decode successfully without requiring
    a lot of CPU time or RAM.

    Unsupported files (unsupported-*.xz) are good files, but headers
    indicate features not supported by the current file format
    specification.

    Bad files (bad-*.xz) must cause the decoder to give an error. Like
    with the good files, these files must not require a lot of CPU time
    or RAM before they get detected to be broken.


2. Descriptions of Individual Files

2.1. Good Files

    good-0-empty.xz has one Stream with no Blocks.

    good-0pad-empty.xz has one Stream with no Blocks followed by
    four-byte Stream Padding.

    good-0cat-empty.xz has two zero-Block Streams concatenated without
    Stream Padding.

    good-0catpad-empty.xz has two zero-Block Streams concatenated with
    four-byte Stream Padding between the Streams.

    good-1-check-none.xz has one Stream with one Block with two
    uncompressed LZMA2 chunks and no integrity check.

    good-1-check-crc32.xz has one Stream with one Block with two
    uncompressed LZMA2 chunks and CRC32 check.

    good-1-check-crc64.xz is like good-1-check-crc32.xz but with CRC64.

    good-1-check-sha256.xz is like good-1-check-crc32.xz but with
    SHA256.

    good-2-lzma2.xz has one Stream with two Blocks with one uncompressed
    LZMA2 chunk in each Block.

    good-1-block_header-1.xz has both Compressed Size and Uncompressed
    Size in the Block Header. This has also four extra bytes of Header
    Padding.

    good-1-block_header-2.xz has known Compressed Size.

    good-1-block_header-3.xz has known Uncompressed Size.

    good-1-delta-lzma2.tiff.xz is an image file that compresses
    better with Delta+LZMA2 than with plain LZMA2.

    good-1-x86-lzma2.xz uses the x86 filter (BCJ) and LZMA2. The
    uncompressed file is compress_prepared_bcj_x86 found from the tests
    directory.

    good-1-sparc-lzma2.xz uses the SPARC filter and LZMA. The
    uncompressed file is compress_prepared_bcj_sparc found from the tests
    directory.

    good-1-lzma2-1.xz has two LZMA2 chunks, of which the second sets
    new properties.

    good-1-lzma2-2.xz has two LZMA2 chunks, of which the second resets
    the state without specifying new properties.

    good-1-lzma2-3.xz has two LZMA2 chunks, of which the first is
    uncompressed and the second is LZMA. The first chunk resets dictionary
    and the second sets new properties.

    good-1-lzma2-4.xz has three LZMA2 chunks: First is LZMA, second is
    uncompressed with dictionary reset, and third is LZMA with new
    properties but without dictionary reset.

    good-1-lzma2-5.xz has an empty LZMA2 stream with only the end of
    payload marker. XZ Utils 5.0.1 and older incorrectly see this file
    as corrupt.

    good-1-3delta-lzma2.xz has three Delta filters and LZMA2.


2.2. Unsupported Files

    unsupported-check.xz uses Check ID 0x02 which isn't supported by
    the current version of the file format. It is implementation-defined
    how this file handled (it may reject it, or decode it possibly with
    a warning).

    unsupported-block_header.xz has a non-null byte in Header Padding,
    which may indicate presence of a new unsupported field.

    unsupported-filter_flags-1.xz has unsupported Filter ID 0x7F.

    unsupported-filter_flags-2.xz specifies only Delta filter in the
    List of Filter Flags, but Delta isn't allowed as the last filter in
    the chain. It could be a little more correct to detect this file as
    corrupt instead of unsupported, but saying it is unsupported is
    simpler in case of liblzma.

    unsupported-filter_flags-3.xz specifies two LZMA2 filters in the
    List of Filter Flags. LZMA2 is allowed only as the last filter in the
    chain. It could be a little more correct to detect this file as
    corrupt instead of unsupported, but saying it is unsupported is
    simpler in case of liblzma.


2.3. Bad Files

    bad-0pad-empty.xz has one Stream with no Blocks followed by
    five-byte Stream Padding. Stream Padding must be a multiple of four
    bytes, thus this file is corrupt.

    bad-0catpad-empty.xz has two zero-Block Streams concatenated with
    five-byte Stream Padding between the Streams.

    bad-0cat-alone.xz is good-0-empty.xz concatenated with an empty
    LZMA_Alone file.

    bad-0cat-header_magic.xz is good-0cat-empty.xz but with one byte
    wrong in the Header Magic Bytes field of the second Stream. liblzma
    gives LZMA_DATA_ERROR for this. (LZMA_FORMAT_ERROR is used only if
    the first Stream of a file has invalid Header Magic Bytes.)

    bad-0-header_magic.xz is good-0-empty.xz but with one byte wrong
    in the Header Magic Bytes field. liblzma gives LZMA_FORMAT_ERROR for
    this.

    bad-0-footer_magic.xz is good-0-empty.xz but with one byte wrong
    in the Footer Magic Bytes field. liblzma gives LZMA_DATA_ERROR for
    this.

    bad-0-empty-truncated.xz is good-0-empty.xz without the last byte
    of the file.

    bad-0-nonempty_index.xz has no Blocks but Index claims that there is
    one Block.

    bad-0-backward_size.xz has wrong Backward Size in Stream Footer.

    bad-1-stream_flags-1.xz has different Stream Flags in Stream Header
    and Stream Footer.

    bad-1-stream_flags-2.xz has wrong CRC32 in Stream Header.

    bad-1-stream_flags-3.xz has wrong CRC32 in Stream Footer.

    bad-1-vli-1.xz has two-byte variable-length integer in the
    Uncompressed Size field in Block Header while one-byte would be enough
    for that value. It's important that the file gets rejected due to too
    big integer encoding instead of due to Uncompressed Size not matching
    the value stored in the Block Header. That is, the decoder must not
    try to decode the Compressed Data field.

    bad-1-vli-2.xz has ten-byte variable-length integer as Uncompressed
    Size in Block Header. It's important that the file gets rejected due
    to too big integer encoding instead of due to Uncompressed Size not
    matching the value stored in the Block Header. That is, the decoder
    must not try to decode the Compressed Data field.

    bad-1-block_header-1.xz has Block Header that ends in the middle of
    the Filter Flags field.

    bad-1-block_header-2.xz has Block Header that has Compressed Size and
    Uncompressed Size but no List of Filter Flags field.

    bad-1-block_header-3.xz has wrong CRC32 in Block Header.

    bad-1-block_header-4.xz has too big Compressed Size in Block Header
    (2^63 - 1 bytes while maximum is a little less, because the whole
    Block must stay smaller than 2^63). It's important that the file
    gets rejected due to invalid Compressed Size value; the decoder
    must not try decoding the Compressed Data field.

    bad-1-block_header-5.xz has zero as Compressed Size in Block Header.

    bad-1-block_header-6.xz has corrupt Block Header which may crash
    xz -lvv in XZ Utils 5.0.3 and earlier. It was fixed in the commit
    c0297445064951807803457dca1611b3c47e7f0f.

    bad-2-index-1.xz has wrong Unpadded Sizes in Index.

    bad-2-index-2.xz has wrong Uncompressed Sizes in Index.

    bad-2-index-3.xz has non-null byte in Index Padding.

    bad-2-index-4.xz wrong CRC32 in Index.

    bad-2-index-5.xz has zero as Unpadded Size. It is important that the
    file gets rejected specifically due to Unpadded Size having an invalid
    value.

    bad-2-compressed_data_padding.xz has non-null byte in the padding of
    the Compressed Data field of the first Block.

    bad-1-check-crc32.xz has wrong Check (CRC32).

    bad-1-check-crc64.xz has wrong Check (CRC64).

    bad-1-check-sha256.xz has wrong Check (SHA-256).

    bad-1-lzma2-1.xz has LZMA2 stream whose first chunk (uncompressed)
    doesn't reset the dictionary.

    bad-1-lzma2-2.xz has two LZMA2 chunks, of which the second chunk
    indicates dictionary reset, but the LZMA compressed data tries to
    repeat data from the previous chunk.

    bad-1-lzma2-3.xz sets new invalid properties (lc=8, lp=0, pb=0) in
    the middle of Block.

    bad-1-lzma2-4.xz has two LZMA2 chunks, of which the first is
    uncompressed and the second is LZMA. The first chunk resets dictionary
    as it should, but the second chunk tries to reset state without
    specifying properties for LZMA.

    bad-1-lzma2-5.xz is like bad-1-lzma2-4.xz but doesn't try to reset
    anything in the header of the second chunk.

    bad-1-lzma2-6.xz has reserved LZMA2 control byte value (0x03).

    bad-1-lzma2-7.xz has EOPM at LZMA level.

    bad-1-lzma2-8.xz is like good-1-lzma2-4.xz but doesn't set new
    properties in the third LZMA2 chunk.

# $NetBSD: README,v 1.1 2016/03/30 21:31:44 christos Exp $
To regenerate the man pages, build and run sqlite2mdoc < /usr/include/sqlite3.h
## Synopsis

This utility accepts an [SQLite](https://www.sqlite.org) header file
`sqlite.h` and produces a set of decently well-formed
[mdoc(7)](http://man.openbsd.org/OpenBSD-current/man7/mdoc.7) files
documenting the C API.
These will be roughly equivalent to the [C-language Interface
Specification for SQLite](https://www.sqlite.org/c3ref/intro.html).

You can also use it for any file(s) using the documentation standards of
SQLite.
See the [sqlite2mdoc.1](sqlite2mdoc.1) manpage for syntax details.

This [GitHub](https://www.github.com) repository is a read-only mirror
of the project's CVS repository.

## Installation

Simply run `make`: this utility isn't meant for installation, but for
integration into your SQLite deployment phase.
You can run `make install`, however, if you plan on using it for other
documentation.
There are no compile-time or run-time dependencies unless you're on
Linux, in which case you'll need
[libbsd](https://libbsd.freedesktop.org).
You'll also need to uncomment the `LDADD` line in the
[Makefile](Makefile), in this case.


This software has been used on OpenBSD, Mac OS X, and Linux machines.

## License

All sources use the ISC (like OpenBSD) license.
See the [LICENSE.md](LICENSE.md) file for details.
OpenLDAP 2.4 README
    For a description of what this distribution contains, see the
    ANNOUNCEMENT file in this directory.  For a description of
    changes from previous releases, see the CHANGES file in this
    directory.

    This is 2.4 release, it includes significant changes from prior
    releases.

REQUIRED SOFTWARE
    Building OpenLDAP Software requires a number of software packages
    to be preinstalled.  Additional information regarding prerequisite
    software can be found in the OpenLDAP Administrator's Guide.

    Base system (libraries and tools):
        Standard C compiler (required)
        Cyrus SASL 2.1.21+ (recommended)
        OpenSSL 0.9.7+ (recommended)
        Reentrant POSIX REGEX software (required)

    SLAPD:
        BDB and HDB backends require Oracle Berkeley DB 4.4 - 4.8,
	or 5.0 - 5.1.  It is highly recommended to apply the
        patches from Oracle for a given release.

    CLIENTS/CONTRIB ware:
        Depends on package.  See per package README.


MAKING AND INSTALLING THE DISTRIBUTION
    Please see the INSTALL file for basic instructions.  More
    detailed instructions can be found in the OpenLDAP Admnistrator's
    Guide (see DOCUMENTATION section).


DOCUMENTATION
    The OpenLDAP Administrator's Guide is available in the
    guide.html file in the doc/guide/admin directory.  The
    guide and a number of other documents are available at
    <http://www.openldap.org/doc/admin/guide.html>.

    The distribution also includes manual pages for most programs
    and library APIs.  See ldap(3) for details.

    The OpenLDAP website is available and contains the latest LDAP
    news, releases announcements, pointers to other LDAP resources,
    etc..  It is located at <http://www.OpenLDAP.org/>.

    The OpenLDAP Software FAQ is available at
    <http://www.openldap.org/faq/>.


SUPPORT / FEEDBACK / PROBLEM REPORTS / DISCUSSIONS
    OpenLDAP Software is user supported.  If you have problems, please
    review the OpenLDAP FAQ <http://www.openldap.org/faq/> and
    archives of the OpenLDAP-software and OpenLDAP-bugs mailing lists
    <http://www.openldap.org/lists/>.  If you cannot find the answer,
    please enquire on the OpenLDAP-software list.

    Issues, such as bug reports, should be reported using our
    Issue Tracking System <http://www.OpenLDAP.org/its/>.  Do not
    use this system for software enquiries.  Please direct these
    to an appropriate mailing list.


CONTRIBUTING
    See <http://www.openldap.org/devel/contributing.html> for
    information regarding how to contribute code or documentation
    to the OpenLDAP Project for inclusion in OpenLDAP Software.
    While you are encouraged to coordinate and discuss the development
    activities on the <openldap-devel@openldap.org> mailing list
    prior to submission, it is noted that contributions must be
    submitted using the Issue Tracking System
    <http://www.openldap.org/its/> to be considered.

---
$OpenLDAP$

This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 1998-2016 The OpenLDAP Foundation.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

OpenLDAP is a registered trademark of the OpenLDAP Foundation.
#
# Id: README,v 1.3 1999/09/21 15:47:43 mleisher Exp 
#
# Copyright 1997, 1998, 1999 Computing Research Labs,
# New Mexico State University
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
# THE COMPUTING RESEARCH LAB OR NEW MEXICO STATE UNIVERSITY BE LIABLE FOR ANY
# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT
# OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR
# THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#


                       Unicode and Regular Expressions
                                 Version 0.5

This is a simple regular expression package for matching against Unicode text
in UCS2 form.  The implementation of this URE package is a variation on the
RE->DFA algorithm done by Mark Hopkins (markh@csd4.csd.uwm.edu).  Mark
Hopkins' algorithm had the virtue of being very simple, so it was used as a
model.

---------------------------------------------------------------------------

Assumptions:

  o  Regular expression and text already normalized.

  o  Conversion to lower case assumes a 1-1 mapping.

Definitions:

  Separator - any one of U+2028, U+2029, '\n', '\r'.

Operators:
  .   - match any character.
  *   - match zero or more of the last subexpression.
  +   - match one or more of the last subexpression.
  ?   - match zero or one of the last subexpression.
  ()  - subexpression grouping.

  Notes:

    o  The "." operator normally does not match separators, but a flag is
       available for the ure_exec() function that will allow this operator to
       match a separator.

Literals and Constants:

  c       - literal UCS2 character.
  \x....  - hexadecimal number of up to 4 digits.
  \X....  - hexadecimal number of up to 4 digits.
  \u....  - hexadecimal number of up to 4 digits.
  \U....  - hexadecimal number of up to 4 digits.

Character classes:

  [...]           - Character class.
  [^...]          - Negated character class.
  \pN1,N2,...,Nn  - Character properties class.
  \PN1,N2,...,Nn  - Negated character properties class.

  POSIX character classes recognized:

    :alnum:
    :alpha:
    :cntrl:
    :digit:
    :graph:
    :lower:
    :print:
    :punct:
    :space:
    :upper:
    :xdigit:

  Notes:

    o  Character property classes are \p or \P followed by a comma separated
       list of integers between 1 and 32.  These integers are references to
       the following character properties:

        N	Character Property
        --------------------------
        1	_URE_NONSPACING
        2	_URE_COMBINING
        3	_URE_NUMDIGIT
        4	_URE_NUMOTHER
        5	_URE_SPACESEP
        6	_URE_LINESEP
        7	_URE_PARASEP
        8	_URE_CNTRL
        9	_URE_PUA
        10	_URE_UPPER
        11	_URE_LOWER
        12	_URE_TITLE
        13	_URE_MODIFIER
        14	_URE_OTHERLETTER
        15	_URE_DASHPUNCT
        16	_URE_OPENPUNCT
        17	_URE_CLOSEPUNCT
        18	_URE_OTHERPUNCT
        19	_URE_MATHSYM
        20	_URE_CURRENCYSYM
        21	_URE_OTHERSYM
        22	_URE_LTR
        23	_URE_RTL
        24	_URE_EURONUM
        25	_URE_EURONUMSEP
        26	_URE_EURONUMTERM
        27	_URE_ARABNUM
        28	_URE_COMMONSEP
        29	_URE_BLOCKSEP
        30	_URE_SEGMENTSEP
        31	_URE_WHITESPACE
        32	_URE_OTHERNEUT

    o  Character classes can contain literals, constants, and character
       property classes. Example:

       [abc\U10A\p1,3,4]

---------------------------------------------------------------------------

Before using URE
----------------
Before URE is used, two functions need to be created.  One to check if a
character matches a set of URE character properties, and one to convert a
character to lower case.

Stubs for these function are located in the urestubs.c file.

Using URE
---------

Sample pseudo-code fragment.

  ure_buffer_t rebuf;
  ure_dfa_t dfa;
  ucs2_t *re, *text;
  unsigned long relen, textlen;
  unsigned long match_start, match_end;

  /*
   * Allocate the dynamic storage needed to compile regular expressions.
   */
  rebuf = ure_buffer_create();

  for each regular expression in a list {
      re = next regular expression;
      relen = length(re);

      /*
       * Compile the regular expression with the case insensitive flag
       * turned on.
       */
      dfa = ure_compile(re, relen, 1, rebuf);

      /*
       * Look for the first match in some text.  The matching will be done
       * in a case insensitive manner because the expression was compiled
       * with the case insensitive flag on.
       */
      if (ure_exec(dfa, 0, text, textlen, &match_start, &match_end))
        printf("MATCH: %ld %ld\n", match_start, match_end);

      /*
       * Look for the first match in some text, ignoring non-spacing
       * characters.
       */
      if (ure_exec(dfa, URE_IGNORE_NONSPACING, text, textlen,
                   &match_start, &match_end))
        printf("MATCH: %ld %ld\n", match_start, match_end);

      /*
       * Free the DFA.
       */
      ure_free_dfa(dfa);
  }

  /*
   * Free the dynamic storage used for compiling the expressions.
   */
  ure_free_buffer(rebuf);

---------------------------------------------------------------------------

Mark Leisher <mleisher@crl.nmsu.edu>
29 March 1997

===========================================================================

CHANGES
-------

Version: 0.5
Date   : 21 September 1999
==========================
  1. Added copyright stuff and put in CVS.
#
# Id: README,v 1.33 2001/01/02 18:46:19 mleisher Exp 
#

                           MUTT UCData Package 2.5
                           -----------------------

This is a package that supports ctype-like operations for Unicode UCS-2 text
(and surrogates), case mapping, decomposition lookup, and provides a
bidirectional reordering algorithm.  To use it, you will need to get the
latest "UnicodeData-*.txt" (or later) file from the Unicode Web or FTP site.

The character information portion of the package consists of three parts:

  1. A program called "ucgendat" which generates five data files from the
     UnicodeData-*.txt file.  The files are:

     A. case.dat   - the case mappings.
     B. ctype.dat  - the character property tables.
     C. comp.dat   - the character composition pairs.
     D. decomp.dat - the character decompositions.
     E. cmbcl.dat  - the non-zero combining classes.
     F. num.dat    - the codes representing numbers.

  2. The "ucdata.[ch]" files which implement the functions needed to
     check to see if a character matches groups of properties, to map between
     upper, lower, and title case, to look up the decomposition of a
     character, look up the combining class of a character, and get the number
     value of a character.

  3. The UCData.java class which provides the same API (with minor changes for
     the numbers) and loads the same binary data files as the C code.

A short reference to the functions available is in the "api.txt" file.

Techie Details
==============

The "ucgendat" program parses files from the command line which are all in the
Unicode Character Database (UCDB) format.  An additional properties file,
"MUTTUCData.txt", provides some extra properties for some characters.

The program looks for the two character properties fields (2 and 4), the
combining class field (3), the decomposition field (5), the numeric value
field (8), and the case mapping fields (12, 13, and 14).  The decompositions
are recursively expanded before being written out.

The decomposition table contains all the canonical decompositions.  This means
all decompositions that do not have tags such as "<compat>" or "<font>".

The data is almost all stored as unsigned longs (32-bits assumed) and the
routines that load the data take care of endian swaps when necessary.  This
also means that supplementary characters (>= 0x10000) can be placed in the
data files the "ucgendat" program parses.

The data is written as external files and broken into six parts so it can be
selectively updated at runtime if necessary.

The data files currently generated from the "ucgendat" program total about 56K
in size all together.

The format of the binary data files is documented in the "format.txt" file.

==========================================================================

                       The "Pretty Good Bidi Algorithm"
                       --------------------------------

This routine provides an alternative to the Unicode Bidi algorithm.  The
difference is that this version of the PGBA does not handle the explicit
directional codes (LRE, RLE, LRO, RLO, PDF).  It should now produce the same
results as the Unicode BiDi algorithm for implicit reordering.  Included are
functions for doing cursor motion in both logical and visual order.

This implementation is provided to demonstrate an effective alternate method
for implicit reordering.  To make this useful for an application, it probably
needs some changes to the memory allocation and deallocation, as well as data
structure additions for rendering.

Mark Leisher <mleisher@crl.nmsu.edu>
19 November 1999

-----------------------------------------------------------------------------

CHANGES
=======
Version 2.5
-----------
1. Changed the number lookup to set the denominator to 1 in cases of digits.
   This restores functional compatibility with John Cowan's UCType package.

2. Added support for the AL property.

3. Modified load and reload functions to return error codes.

Version 2.4
-----------
1. Improved some bidi algorithm documentation in the code.

2. Fixed a code mixup that produced a non-working version.

Version 2.3
-----------
1. Fixed a misspelling in the ucpgba.h header file.

2. Fixed a bug which caused trailing weak non-digit sequences to be left out of
   the reordered string in the bidi algorithm.

3. Fixed a problem with weak sequences containing non-spacing marks in the
   bidi algorithm.

4. Fixed a problem with text runs of the opposite direction of the string
   surrounding a weak + neutral text run appearing in the wrong order in the
   bidi algorithm.

5. Added a default overall direction parameter to the reordering function for
   cases of strings with no strong directional characters in the bidi
   algorithm.

6. The bidi API documentation was improved.

7. Added a man page for the bidi API.

Version 2.2
-----------
1. Fixed a problem with the bidi algorithm locating directional section
   boundaries.

2. Fixed a problem with the bidi algorithm starting the reordering correctly.

3. Fixed a problem with the bidi algorithm determining end boundaries for LTR
   segments.

4. Fixed a problem with the bidi algorithm reordering weak (digits and number
   separators) segments.

5. Added automatic switching of symmetrically paired characters when
   reversing RTL segments.

6. Added a missing symmetric character to the extra character properties in
   MUTTUCData.txt.

7. Added support for doing logical and visual cursor traversal.

Version 2.1
-----------
1. Updated the ucgendat program to handle the Unicode 3.0 character database
   properties.  The AL and BM bidi properties gets marked as strong RTL and
   Other Neutral, the NSM, LRE, RLE, PDF, LRO, and RLO controls all get marked
   as Other Neutral.

2. Fixed some problems with testing against signed values in the UCData.java
   code and some minor cleanup.

3. Added the "Pretty Good Bidi Algorithm."

Version 2.0
-----------
1. Removed the old Java stuff for a new class that loads directly from the
   same data files as the C code does.

2. Fixed a problem with choosing the correct field when mapping case.

3. Adjust some search routines to start their search in the correct position.

4. Moved the copyright year to 1999.

Version 1.9
-----------
1. Fixed a problem with an incorrect amount of storage being allocated for the
   combining class nodes.

2. Fixed an invalid initialization in the number code.

3. Changed the Java template file formatting a bit.

4. Added tables and function for getting decompositions in the Java class.

Version 1.8
-----------
1. Fixed a problem with adding certain ranges.

2. Added two more macros for testing for identifiers.

3. Tested with the UnicodeData-2.1.5.txt file.

Version 1.7
-----------
1. Fixed a problem with looking up decompositions in "ucgendat."

Version 1.6
-----------
1. Added two new properties introduced with UnicodeData-2.1.4.txt.

2. Changed the "ucgendat.c" program a little to automatically align the
   property data on a 4-byte boundary when new properties are added.

3. Changed the "ucgendat.c" programs to only generate canonical
   decompositions.

4. Added two new macros ucisinitialpunct() and ucisfinalpunct() to check for
   initial and final punctuation characters.

5. Minor additions and changes to the documentation.

Version 1.5
-----------
1. Changed all file open calls to include binary mode with "b" for DOS/WIN
   platforms.

2. Wrapped the unistd.h include so it won't be included when compiled under
   Win32.

3. Fixed a bad range check for hex digits in ucgendat.c.

4. Fixed a bad endian swap for combining classes.

5. Added code to make a number table and associated lookup functions.
   Functions added are ucnumber(), ucdigit(), and ucgetnumber().  The last
   function is to maintain compatibility with John Cowan's "uctype" package.

Version 1.4
-----------
1. Fixed a bug with adding a range.

2. Fixed a bug with inserting a range in order.

3. Fixed incorrectly specified ucisdefined() and ucisundefined() macros.

4. Added the missing unload for the combining class data.

5. Fixed a bad macro placement in ucisweak().

Version 1.3
-----------
1. Bug with case mapping calculations fixed.

2. Bug with empty character property entries fixed.

3. Bug with incorrect type in the combining class lookup fixed.

4. Some corrections done to api.txt.

5. Bug in certain character property lookups fixed.

6. Added a character property table that records the defined characters.

7. Replaced ucisunknown() with ucisdefined() and ucisundefined().

Version 1.2
-----------
1. Added code to ucgendat to generate a combining class table.

2. Fixed an endian problem with the byte count of decompositions.

3. Fixed some minor problems in the "format.txt" file.

4. Removed some bogus "Ss" values from MUTTUCData.txt file.

5. Added API function to get combining class.

6. Changed the open mode to "rb" so binary data files will be opened correctly
   on DOS/WIN as well as other platforms.

7. Added the "api.txt" file.

Version 1.1
-----------
1. Added ucisxdigit() which I overlooked.

2. Added UC_LT to the ucisalpha() macro which I overlooked.

3. Change uciscntrl() to include UC_CF.

4. Added ucisocntrl() and ucfntcntrl() macros.

5. Added a ucisblank() which I overlooked.

6. Added missing properties to ucissymbol() and ucisnumber().

7. Added ucisgraph() and ucisprint().

8. Changed the "Mr" property to "Sy" to mark this subset of mirroring
   characters as symmetric to avoid trampling the Unicode/ISO10646 sense of
   mirroring.

9. Added another property called "Ss" which includes control characters
   traditionally seen as spaces in the isspace() macro.

10. Added a bunch of macros to be API compatible with John Cowan's package.

ACKNOWLEDGEMENTS
================

Thanks go to John Cowan <cowan@locke.ccil.org> for pointing out lots of
missing things and giving me stuff, particularly a bunch of new macros.

Thanks go to Bob Verbrugge <bob_verbrugge@nl.compuware.com> for pointing out
various bugs.

Thanks go to Christophe Pierret <cpierret@businessobjects.com> for pointing
out that file modes need to have "b" for DOS/WIN machines, pointing out
unistd.h is not a Win 32 header, and pointing out a problem with ucisalnum().

Thanks go to Kent Johnson <kent@pondview.mv.com> for finding a bug that caused
incomplete decompositions to be generated by the "ucgendat" program.

Thanks go to Valeriy E. Ushakov <uwe@ptc.spbu.ru> for spotting an allocation
error and an initialization error.

Thanks go to Stig Venaas <Stig.Venaas@uninett.no> for providing a patch to
support return types on load and reload, and for major updates to handle
canonical composition and decomposition.
#
# Id: README,v 1.1 1999/09/21 15:45:17 mleisher Exp 
#
# Copyright 1997, 1998, 1999 Computing Research Labs,
# New Mexico State University
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
# THE COMPUTING RESEARCH LAB OR NEW MEXICO STATE UNIVERSITY BE LIABLE FOR ANY
# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT
# OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR
# THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#

                       Unicode and Boyer-Moore Searching
                                 Version 0.2

UTBM (Unicode Tuned Boyer-Moore) is a simple package that provides tuned
Boyer-Moore searches on Unicode UCS2 text (handles high and low surrogates).

---------------------------------------------------------------------------

Assumptions:

  o  Search pattern and text already normalized in some fasion.

  o  Upper, lower, and title case conversions are one-to-one.

  o  For conversions between upper, lower, and title case, UCS2 characters
     always convert to other UCS2 characters, and UTF-16 characters always
     convert to other UTF-16 characters.

Flags:

  UTBM provides three processing flags:

  o  UTBM_CASEFOLD          - search in a case-insensitive manner.

  o  UTBM_IGNORE_NONSPACING - ignore non-spacing characters in the pattern and
                              the text.

  o  UTBM_SPACE_COMPRESS    - view as a *single space*, sequential groups of
                              U+2028, U+2029, '\n', '\r', '\t', and any
                              character identified as a space by the Unicode
                              support on the platform.

                              This flag also causes all characters identified
                              as control by the Unicode support on the
                              platform to be ignored (except for '\n', '\r',
                              and '\t').

---------------------------------------------------------------------------

Before using UTBM
-----------------
Before UTBM is used, some functions need to be created.  The "utbmstub.c" file
contains stubs that need to be rewritten so they work with the Unicode support
on the platform on which this package is being used.

Using UTBM
----------

Sample pseudo-code fragment.

  utbm_pattern_t pat;
  ucs2_t *pattern, *text;
  unsigned long patternlen, textlen;
  unsigned long flags, match_start, match_end;

  /*
   * Allocate the dynamic storage needed for a search pattern.
   */
  pat = utbm_create_pattern();

  /*
   * Set the search flags desired.
   */
  flags = UTBM_CASEFOLD|UTBM_IGNORE_NONSPACING;

  /*
   * Compile the search pattern.
   */
  utbm_compile(pattern, patternlen, flags, pat);

  /*
   * Find the first occurance of the search pattern in the text.
   */
  if (utbm_exec(pat, text, textlen, &match_start, &match_end))
    printf("MATCH: %ld %ld\n", match_start, match_end);

  /*
   * Free the dynamic storage used for the search pattern.
   */
  ure_free_pattern(pat);

---------------------------------------------------------------------------

Mark Leisher <mleisher@crl.nmsu.edu>
2 May 1997

===========================================================================

CHANGES
-------

Version: 0.2
Date   : 21 September 1999
==========================
  1. Added copyright stuff and put in CVS.

Author: Pierangelo Masarati <ando@OpenLDAP.org>

Back-sql can be tested with sql-test000-read; it requires a bit of work 
to get everything up and running appropriately.

This document briefly describes the steps that are required to prepare
a quick'n'dirty installation of back-sql and of the related RDBMS
and ODBC; Examples are provided, but by no means they pretent
to represent an exaustive source of info about how to setup the ODBC;
refer to the docs for any problem or detail.

Currently, the system has been tested with IBM db2, PostgreSQL and MySQL;
basic support and test data for other RDBMSes is in place, but as of
today (November 2004) it's totally untested.  If you succeed in running
any of the other RDBMSes, please provide feedback about any required
change either in the code or in the test scripts by means of OpenLDAP's
Issue Tracking System (http://www.openldap.org/its/).

1) slapd must be compiled with back-sql support, i.e. configure 
with --enable-sql switch.  This requires an implementation of the ODBC
to be installed.

2) The ODBC must be set up appropriately, by editing the odbc.ini file
in /etc/ (or wherever your installation puts it) and, if appropriate,
the odbcinst.ini file.  Note: you can also use custom odbc.ini and
odbcinst.ini files, provided you export in ODBCINI the full path to the
odbc.ini file, and in ODBCSYSINI the directory where the odbcinst.ini
file resides.
Relevant info for our test setup is highlighted with '<===' on the right.

2.1) PostgreSQL

2.1.1) Add to the odbc.ini file a block of the form

[example]                        <===
Description         = Example for OpenLDAP's back-sql
Driver              = PostgreSQL
Trace               = No
Database            = example    <===
Servername          = localhost
UserName            = manager    <===
Password            = secret     <===
Port                = 5432
;Protocol            = 6.4
ReadOnly            = No
RowVersioning       = No
ShowSystemTables    = No
ShowOidColumn       = No
FakeOidIndex        = No
ConnSettings        =

2.1.2) Add to the odbcinst.ini file a block of the form

[PostgreSQL]
Description     = ODBC for PostgreSQL
Driver          = /usr/lib/libodbcpsql.so
Setup           = /usr/lib/libodbcpsqlS.so
FileUsage       = 1

2.2) MySQL

2.2.1) Add to the odbc.ini file a block of the form

[example]                        <===
Description         = Example for OpenLDAP's back-sql
Driver              = MySQL
Trace               = No
Database            = example    <===
Servername          = localhost
UserName            = manager    <===
Password            = secret     <===
ReadOnly            = No
RowVersioning       = No
ShowSystemTables    = No
ShowOidColumn       = No
FakeOidIndex        = No
ConnSettings        =
SOCKET              = /var/lib/mysql/mysql.sock

2.2.2) Add to the odbcinst.ini file a block of the form

[MySQL]
Description     = ODBC for MySQL
Driver          = /usr/lib/libmyodbc.so
FileUsage       = 1

2.3) IBM db2
[n.a.]

3) The RDBMS must be setup; examples are provided for my installations 
of PostgreSQL and MySQL, but details may change; other RDBMSes should
be configured in a similar manner, you need to find out the details by
reading their documentation.

3.1) PostgreSQL

3.1.1) Start the server
on RedHat:
[root@localhost]# service postgresql start
on other systems: read the docs...

3.1.2) Create the database:
[root@localhost]# su - postgres
[postgres@localhost]$ createdb example

3.1.3) Create the user:
[root@localhost]# su - postgres
[postgres@localhost]$ psql example
example=> create user manager with password 'secret';
example=> <control-D>

3.1.4) Populate the database:
[root@localhost]# cd $SOURCES/servers/slapd/back-sql/rdbms_depend/pgsql/
[root@localhost]# psql -U manager -W example
example=> <control-D>
[root@localhost]# psql -U manager example < backsql_create.sql
[root@localhost]# psql -U manager example < testdb_create.sql
[root@localhost]# psql -U manager example < testdb_data.sql
[root@localhost]# psql -U manager example < testdb_metadata.sql

3.1.5) Run the test:
[root@localhost]# cd $SOURCES/tests
[root@localhost]# SLAPD_USE_SQL=pgsql ./run sql-test000

3.2) MySQL

3.2.1) Start the server
on RedHat:
[root@localhost]# service mysqld start
on other systems: read the docs...

3.2.2) Create the database:
[root@localhost]# mysqladmin -u root -p create example
(hit <return> for the empty password).

3.2.3) Create the user:
[root@localhost]# mysql -u root -p example
(hit <return> for the empty password)
mysql> grant all privileges on *.* \
  to 'manager'@'localhost' identified by 'secret' with grant option;
mysql> exit;

3.2.4) Populate the database:
[root@localhost]# cd $SOURCES/servers/slapd/back-sql/rdbms_depend/mysql/
[root@localhost]# mysql -u manager -p example < backsql_create.sql
[root@localhost]# mysql -u manager -p example < testdb_create.sql
[root@localhost]# mysql -u manager -p example < testdb_data.sql
[root@localhost]# mysql -u manager -p example < testdb_metadata.sql

3.2.5) Run the test:
[root@localhost]# cd $SOURCES/tests
[root@localhost]# SLAPD_USE_SQL=mysql ./run sql-test000

3.3) IBM db2
[n.a.]

3.3.1) Start the server:

3.3.2) Create the database:

3.3.3) Create the user:

3.3.4) Populate the database:
connect to the database as user manager, and execute the test files
in auto-commit mode (-c)
[root@localhost]# su - manager
[manager@localhost]$ db2 "connect to example user manager using secret"
[manager@localhost]$ db2 -ctvf backsql_create.sql
[manager@localhost]$ db2 -ctvf testdb_create.sql
[manager@localhost]$ db2 -ctvf testdb_data.sql
[manager@localhost]$ db2 -ctvf testdb_metadata.sql
[manager@localhost]$ db2 "connect reset"

3.3.5) Run the test:
[root@localhost]# cd $SOURCES/tests
[root@localhost]# SLAPD_USE_SQL=ibmdb2 ./run sql-test000

4) Cleanup:
The test is basically readonly; this can be performed by all RDBMSes 
(listed above).

There is another test, sql-test900-write, which is currently enabled
only for PostgreSQL and IBM db2.  Note that after a successful run 
of the write test, the database is no longer in the correct state 
to restart either of the tests, and step 3.X.4 needs to be re-run first.

More tests are to come; PostgreSQL is known to allow a full reload 
of the test database starting from an empty database.

The Null Backend is described in the slapd-null(5) manual page.
This directory contains user application schema definitions for use
with slapd(8).

File                    Description
----                    -----------
collective.schema       Collective attributes (experimental)
corba.schema            Corba Object
core.schema             OpenLDAP "core"
cosine.schema           COSINE Pilot
duaconf.schema          Client Configuration (work in progress)
dyngroup.schema			Dynamic Group (experimental)
inetorgperson.schema    InetOrgPerson
java.schema             Java Object
misc.schema             Miscellaneous Schema (experimental)
nadf.schema             North American Directory Forum (obsolete)
nis.schema              Network Information Service (experimental)
openldap.schema         OpenLDAP Project (FYI)
ppolicy.schema          Password Policy Schema (work in progress)

Additional "generally useful" schema definitions can be submitted
using the OpenLDAP Issue Tracking System <http://www.openldap.org/its/>.
Submissions should include a stable reference to a mature, open
technical specification (e.g., an RFC) for the schema.

The core.ldif and openldap.ldif files are equivalent to their
corresponding .schema files. They have been provided as examples
for use with the dynamic configuration backend. These example files
are not actually necessary since slapd will automatically convert any
included *.schema files into LDIF when converting a slapd.conf file
to a configuration database, but they serve as a model of how to
convert schema files in general.

---

This notice applies to all files in this directory.

Copyright 1998-2016 The OpenLDAP Foundation, Redwood City, California, USA
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.  A copy of this license is available at
http://www.OpenLDAP.org/license.html or in file LICENSE in the
top-level directory of the distribution.

---

This notice applies to all schema in this directory which are derived
from RFCs and other IETF documents.

Portions Copyright 1991-2004, The Internet Society.  All Rights Reserved.

This document and translations of it may be copied and furnished
to others, and derivative works that comment on or otherwise explain
it or assist in its implementation may be prepared, copied, published
and distributed, in whole or in part, without restriction of any
kind, provided that the above copyright notice and this paragraph  
are included on all such copies and derivative works.  However,
this document itself may not be modified in any way, such as by      
removing the copyright notice or references to the Internet Society
or other Internet organizations, except as needed for the  purpose
of developing Internet standards in which case the procedures for
copyrights defined in the Internet Standards process must be
followed, or as required to translate it into languages other than
English.

The limited permissions granted above are perpetual and will not
be revoked by the Internet Society or its successors or assigns.

This document and the information contained herein is provided on
an "AS IS" basis and THE AUTHORS, THE INTERNET SOCIETY, AND THE
INTERNET ENGINEERING TASK FORCE DISCLAIMS ALL WARRANTIES, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTY THAT THE USE
OF THE INFORMATION HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY       
IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR
PURPOSE.


---
$OpenLDAP$
MONITOR BACKEND

	NAME: 	back-monitor

	Backend for monitoring the server's activity.



COMPILE AND CONFIGURATION OPTIONS

It must be explicitly enabled by configuring with 

	--enable-monitor

set; then it must be activated by placing in slapd.conf the database
configure directive

	database	monitor

The suffix "cn=Monitor" is implicitly activated (it cannot be given
as a suffix of the database as usually done for conventional backends).
Note that the "cn=Monitor" naming context appears in the rootDSE
in the attribute monitorContext

A bind operation is provided; at present it allows to bind as the
backend rootdn.  As a result, the backend supports the rootdn/rootpw 
directives (only simple bind at present).



NAMING CONTEXT AND TREE STRUCTURE

The backend naming context is "cn=Monitor"; the first level entries 
represent the monitored subsystems.  It is implemented in a modular way,
to ease the addition of new subsystems.



SCHEMA

All the subsystems get a default "cn" attribute, represented by the
subsystem's name, and they all have "top", "monitor" and "extensibleObject"
objectclasses.
"extensibleObject" is used, and the "description" attribute 
is used to hold the monitor information of each entry.



FUNCTIONALITY

Most of the sybsystems contain an additional depth level, represented
by detailed item monitoring.
All the entries undergo an update operation, if a related method is
defined, prior to being returned.  Moreover, there's a mechanism to
allow volatile entries to be defined, and generated on the fly when
requested.  As an instance, the connection statistics are updated
at each request, while each active connection data is created on the
fly.

One nice feature of this solution is that granular ACLs can be applied 
to each entry.



OPERATIONS

The backend currently supports:

	bind
	compare
	modify
	search



SUBSYSTEMS

Currently some subsystems are partially supported.  "Partially"
means their entries are correctly generated, but sometimes only 
partially useful information is provided.

The subsystems are:

	Backends
	Connections
	Databases
	Listener
	Log
	Operations
	Overlays
	SASL
	Statistics
	Threads
	Time
	TLS
	Read/Write Waiters



BACKENDS SUBSYSTEMS

The main entry contains the type of backends enabled at compile time;
the subentries, for each backend, contain the type of the backend.
It should also contain the modules that have been loaded if dynamic 
backends are enabled.



CONNECTIONS

The main entry is empty; it should contain some statistics on the number 
of connections.
Dynamic subentries are created for each open connection, with stats on
the activity on that connection (the format will be detailed later).
There are two special subentries that show the number of total and
current connections respectively.



DATABASES SUBSYSTEM

The main entry contains the naming context of each configured database; 
the subentries contain, for each database, the type and the naming
context.



LISTENER SUBSYSTEM

It contains the description of the devices the server is currently 
listening on



LOG SUBSYSTEM

It contains the currently active log items.  The "Log" subsystem allows 
user modify operations on the "description" attribute, whose values MUST 
be in the list of admittable log switches:

	Trace
	Packets
	Args
	Conns
	BER
	Filter
	Config		(useless)
	ACL
	Stats
	Stats2
	Shell
	Parse
	Cache		(deprecated)
	Index

These values can be added, replaced or deleted; they affect what 
messages are sent to the syslog device.



OPERATIONS SUBSYSTEM

It shows some statistics on the operations performed by the server:

	Initiated
	Completed

and for each operation type, i.e.:

	Bind
        Unbind
        Add
        Delete
        Modrdn
        Modify
        Compare
        Search
        Abandon
        Extended



OVERLAYS SUBSYSTEM

The main entry contains the type of overlays available at run-time;
the subentries, for each overlay, contain the type of the overlay.
It should also contain the modules that have been loaded if dynamic 
overlays are enabled.



SASL

Currently empty.



STATISTICS SUBSYSTEM

It shows some statistics on the data sent by the server:

	Bytes
	PDU
	Entries
	Referrals



THREADS SUBSYSTEM

It contains the maximum number of threads enabled at startup and the 
current backload.



TIME SUBSYSTEM

It contains two subentries with the start time and the current time 
of the server.



TLS

Currently empty.



READ/WRITE WAITERS SUBSYSTEM

It contains the number of current read waiters.



NOTES

This document is in a very early stage of maturity and will 
probably be rewritten many times before the monitor backend is released.



AUTHOR:	Pierangelo Masarati <ando@OpenLDAP.org>

Relay backend sets up a relay virtual database that allows
to access other databases in the same instance of slapd
through different naming contexts and remapping attribute 
values.

The DN rewrite, filter rewrite and attributeType/objectClass
mapping is done by means of the rewrite-remap overlay.

The database containing the real naming context can be
explicitly selected by means of the "relay" directive,
which must contain the naming context of the target 
database.  This also causes the rewrite-remap overlay 
to be automatically instantiated.  If the optional keyword 
"massage" is present, the rewrite-remap overlay is 
automatically configured to map the virtual to the real 
naming context and vice-versa.

Otherwise, the rewrite-remap overlay must be explicitly
instantiated, by using the "overlay" directive, as 
illustrated below.  This allows much more freedom in target 
database selection and DN rewriting.

If the "relay" directive is not present, the backend is
not bound to a single target database; on the contrary,
the target database is selected on a per-operation basis.

This allows, for instance, to relay one database for 
authentication and anotheir for search/modify, or allows
to use one target for persons and another for groups
and so on.

To summarize: the "relay" directive:
- explicitly bounds the database to a single database 
  holding the real naming context;
- automatically instantiates the rewrite-remap overlay;
- automatically configures the naming context massaging
  if the optional "massage" keyword is added

If the "relay" directive is not used, the rewrite-remap
overlay must be explicitly instantiated and the massaging
must be configured, either by using the "suffixmassage"
directive, or by issuing more sophisticate rewrite 
instructions.

AttributeType/objectClass mapping must be explicitly
required.

Note that the rewrite-remap overlay is not complete nor 
production- ready yet.
Examples are given of all the suggested usages. 

# automatically massage from virtual to real naming context
database	relay
suffix		"dc=virtual,dc=naming,dc=context"
relay		"dc=real,dc=naming,dc=context" massage

# explicitly massage (same as above)
database	relay
suffix		"dc=virtual,dc=naming,dc=context"
relay		"dc=real,dc=naming,dc=context"
suffixmassage	"dc=virtual,dc=naming,dc=context" \
			"dc=real,dc=naming,dc=context"

# explicitly massage (same as above, but dynamic backend resolution)
database	relay
suffix		"dc=virtual,dc=naming,dc=context"
overlay		rewrite-remap
suffixmassage	"dc=virtual,dc=naming,dc=context" \
			"dc=real,dc=naming,dc=context"

# old fashioned suffixalias, applied also to DN-valued attributes
# from virtual to real naming context, but not the reverse...
database	relay
suffix		"dc=virtual,dc=naming,dc=context"
relay		"dc=real,dc=naming,dc=context"
rewriteContext	default
rewriteRule	"(.*)dc=virtual,dc=naming,dc=context$" \
			"$1dc=real,dc=naming,dc=context"
rewriteContext	searchFilter
rewriteContext	searchResult
rewriteContext	searchResultAttrDN
rewriteContext	matchedDN

This directory contains a number of SLAPD overlays, some
project-maintained, some not.  Some are generally usable,
others are purely experimental.  Additional overlays can
be found in the contrib/slapd-modules directory.

Differences from 2.0 Perl API:

- Perl 5.6 is supported

- backend methods return actual LDAP result codes, not
  true/false; this gives the Perl module finer control
  of the error returned to the client

- a filterSearchResults configuration file directive was
  added to tell the backend glue that the results returned
  from the Perl module are candidates only

- the "init" method is called after the backend has been
  initialized - this lets you do some initialization after
  *all* configuration file directives have been read

- the interface for the search method is improved to
  pass the scope, deferencing policy, size limit, etc.
  See SampleLDAP.pm for details.

These changes were sponsored by myinternet Limited.

Luke Howard <lukeh@padl.com>

OpenLDAP Contributed Software README

OpenLDAP Project provides a number of freely-distributable LDAP
software packages.  While distributed as part of OpenLDAP Software,
they are not necessarily supported by the OpenLDAP Project.  Some
packages may be out of date.  Each package in this directory has its
own use and may have different redistribution restrictions than typical
for OpenLDAP Software.

Current contributions:
	ldapc++
		LDAP C++ API
		Contributed by SuSE Gmbh.

	ldaptcl
		LDAP TCL API
		Contributed by NeoSoft

	slapd-modules
		Native-API modules

	slapd-tools
		Tools to use with slapd

	slapi-plugins
		SLAPI plugins


OpenLDAP Contributing Guidelines are available at:
  <http://www.openldap.org/devel/contributing.html>.

$OpenLDAP$
This directory contains a SLAPI plugin, addrdnvalues, which will add to
an entry any attribute values that appear in the entry's RDN but not in
the entry. This is necessary for compliance with some "broken" clients.

To use the plugin, add:

plugin preoperation libaddrdnvalues-plugin.so addrdnvalues_preop_init

to your slapd configuration file.

No Makefile is provided. Use a command line similar to:

gcc -shared -I../../../include -Wall -g -o libaddrdnvalues-plugin.so addrdnvalues.c

to compile this plugin.

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 2003-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

Directory contents:

statslog
	Program to output selected parts of slapd's statslog output
	(LDAP request/response log), grouping log lines by LDAP
	connection.  Useful to search and inspect the server log.

---
Copyright 2004-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

Copyright 2008-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

This directory contains native-API slapd modules (overlays etc):

acl (plugins)
	Plugins implementing access rules.  Currently one plugin
	which implements access control based on posixGroup membership.

addpartial (overlay)
	Treat Add requests as Modify requests if the entry exists.

allop (overlay)
	Return operational attributes for root DSE even when not
	requested, since some clients expect this.

autogroup (overlay)
	Automated updates of group memberships.

cloak (overlay)
	Hide specific attributes unless explicitely requested

comp_match (plugin)
	Component Matching rules (RFC 3687).

denyop (overlay)
	Deny selected operations, returning unwillingToPerform.

dsaschema (plugin)
	Permit loading DSA-specific schema, including operational attrs.

lastbind (overlay)
	Record the last successful authentication on an entry.

lastmod (overlay)
	Track the time of the last write operation to a database.

nops (overlay)
	Remove null operations, e.g. changing a value to same as before.

nssov (listener overlay)
	Handle NSS lookup requests through a local Unix Domain socket.

passwd (plugins)
	Support additional password mechanisms.
	Currently Kerberos, Netscape MTA-MD5 and RADIUS.

proxyOld (plugin)
	Proxy Authorization compatibility with obsolete internet-draft.

smbk5pwd (overlay)
	Make the PasswordModify Extended Operation update Kerberos
	keys and Samba password hashes as well as userPassword.

trace (overlay)
	Trace overlay invocation.

$OpenLDAP$
This directory contains a slapd overlay, allop.
The intended usage is as a global overlay for use with those clients
that do not make use of the RFC3673 allOp ("+") in the requested 
attribute list, but expect all operational attributes to be returned.
Usage: add to slapd.conf(5)

moduleload	path/to/allop.so

overlay		allop
allop-URI	<ldapURI>

if the allop-URI is not given, the rootDSE, i.e. "ldap:///??base",
is assumed.

Use Makefile to compile this plugin or use a command line similar to:

gcc -shared -I../../../include -I../../../servers/slapd -Wall -g \
	-o allop.so allop.c

---
Copyright 2004-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

Copyright 2004-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

This directory contains a native slapd plugin, dsaschema, that permits the
loading of DSA-specific schema from configuration files (including operational
attributes).

To use the plugin, add:

moduleload dsaschema.so
	/etc/openldap/schema/foo1.schema
	...etc...
	/etc/openldap/schema/fooN.schema

to your slapd configuration file.

Use Makefile to compile this plugin or use a command line similar to:

gcc -shared -I../../../include -Wall -g -o dsaschema.so dsaschema.c

This directory contains a slapd overlay, "allowed".

    --- o --- o --- o ---

It adds to entries returned by search operations the value of attributes

"allowedAttributes"
	<http://msdn.microsoft.com/en-us/library/ms675217(VS.85).aspx>

"allowedAttributesEffective"
	<http://msdn.microsoft.com/en-us/library/ms675218(VS.85).aspx>

"allowedChildClasses"
	<http://msdn.microsoft.com/en-us/library/ms675219(VS.85).aspx>

"allowedChildClassesEffective"
	<http://msdn.microsoft.com/en-us/library/ms675220(VS.85).aspx>

No other use is made of those attributes: they cannot be compared,
they cannot be used in search filters, they cannot be used in ACLs, ...

    --- o --- o --- o ---

Usage: add to slapd.conf(5)


moduleload	path/to/allowed.so
overlay		allowed

or add

dn: olcOverlay={0}allowed,olcDatabase={1}bdb,cn=config
objectClass: olcOverlayConfig
olcOverlay: {0}allowed

as a child of the database that's intended to support this feature
(replace "olcDatabase={1}bdb,cn=config" with the appropriate parent);
or use

dn: olcOverlay={0}allowed,olcDatabase={-1}frontend,cn=config
objectClass: olcOverlayConfig
olcOverlay: {0}allowed

if it's supposed to be global.

    --- o --- o --- o ---

Use Makefile to compile this plugin or use a command line similar to:

gcc -shared -I../../../include -I../../../servers/slapd -Wall -g \
	-o allowed.so allowed.c

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 2006-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

ACKNOWLEDGEMENTS:
This work was initially developed by Pierangelo Masarati for inclusion in
OpenLDAP Software.

This directory contains a slapd overlay, nssov, that handles
NSS lookup requests through a local Unix Domain socket. It uses the
same IPC protocol as Arthur de Jong's nss-ldapd, and a complete
copy of the nss-ldapd source is included here. It also handles
PAM requests.

To use this code, you will need the client-side stuf library from
nss-pam-ldapd.  You can get it from:
http://arthurdejong.org/nss-pam-ldapd
You will not need the nslcd daemon; this overlay replaces that part.
To disable building of the nslcd daemon in nss-pam-ldapd, add the
--disable-nslcd option to the nss-pam-ldapd configure script. You
should already be familiar with the RFC2307 and RFC2307bis schema
to use this overlay.  See the nss-pam-ldapd README for more information
on the schema and which features are supported.

To use the overlay, add:

	include <path to>nis.schema

	moduleload <path to>nssov.so
	...

	database hdb
	...
	overlay nssov

to your slapd configuration file. (The nis.schema file contains
the original RFC2307 schema. Some modifications will be needed to
use RFC2307bis.)

The overlay may be configured with Service Search Descriptors (SSDs)
for each NSS service that will be used. SSDs are configured using

	nssov-ssd <service> <url>

where the <service> may be one of
	aliases
	ethers
	group
	hosts
	netgroup
	networks
	passwd
	protocols
	rpc
	services
	shadow

and the <url> must be of the form
	ldap:///[<basedn>][??[<scope>][?<filter>]]

The <basedn> will default to the first suffix of the current database.
The <scope> defaults to "subtree". The default <filter> depends on which
service is being used.

If the local database is actually a proxy to a foreign LDAP server, some
mapping of schema may be needed. Some simple attribute substitutions may
be performed using

	nssov-map <service> <orig> <new>

See the nss-ldapd/README for the original attribute names used in this code.

The overlay also supports dynamic configuration in cn=config. The layout
of the config entry is

	dn: olcOverlay={0}nssov,ocDatabase={1}hdb,cn=config
	objectClass: olcOverlayConfig
	objectClass: olcNssOvConfig
	olcOverlay: {0}nssov
	olcNssSsd: passwd ldap:///ou=users,dc=example,dc=com??one
	olcNssMap: passwd uid accountName

which enables the passwd service, and uses the accountName attribute to
fetch what is usually retrieved from the uid attribute.

PAM authentication, account management, session management, and password
management are supported.

Authentication is performed using Simple Binds. Since all operations occur
inside the slapd overlay, "fake" connections are used and they are
inherently secure. Two methods of mapping the PAM username to an LDAP DN
are provided:
  the mapping can be accomplished using slapd's authz-regexp facility. In
this case, a DN of the form
	cn=<service>+uid=<user>,cn=<hostname>,cn=pam,cn=auth
is fed into the regexp matcher. If a match is produced, the resulting DN
is used.
  otherwise, the NSS passwd map is invoked (which means it must already
be configured).

If no DN is found, the overlay returns PAM_USER_UNKNOWN. If the DN is
found, and Password Policy is supported, then the Bind will use the
Password Policy control and return expiration information to PAM.

Account management also uses two methods. These methods depend on the
ldapns.schema included with the nssov source.
  The first is identical to the method used in PADL's pam_ldap module:
host and authorizedService attributes may be looked up in the user's entry,
and checked to determine access. Also a check may be performed to see if
the user is a member of a particular group. This method is pretty
inflexible and doesn't scale well to large networks of users, hosts,
and services.
  The second uses slapd's ACL engine to check if the user has "compare"
privilege on an ipHost object whose name matches the current hostname, and
whose authorizedService attribute matches the current service name. This
method is preferred, since it allows authorization to be centralized in
the ipHost entries instead of scattered across the entire user population.
The ipHost entries must have an authorizedService attribute (e.g. by way
of the authorizedServiceObject auxiliary class) to use this method.

Session management: the overlay may optionally add a "logged in" attribute
to a user's entry for successful logins, and delete the corresponding
value upon logout. The attribute value is of the form
	<generalizedTime> <host> <service> <tty> (<ruser@rhost>)

Password management: the overlay will perform a PasswordModify exop
in the server for the given user.

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 1998-2016 The OpenLDAP Foundation.
Portions Copyright 2008-2009 Howard Chu, Symas Corp. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

These files were pulled from the nss-pam-ldapd project version 0.9.4.
Copyright notices are in the individual files.

This is not the full distribution of nss-pam-ldapd, and does not
include the client-side stub libraries.  Get the latest release of
nss-pam-ldapd from http://arthurdejong.org/nss-pam-ldapd/ to use
this overlay.

If your system already has the nss-pam-ldapd stub libraries
installed, make sure the versions match the version number
shown above. Otherwise, there may be incompatible differences in
the protocols being used. Currently nssov requires at least
version 0.9.0. If your system's version is older, you will need
to install the client-side stubs from source.

This directory contains the "kinit" slapd module. It is a simple plugin to
have slapd request a Kerberos TGT and keep it renewed as long as slapd is
running.

The current implementation has only been tested against the MIT variant of
the Kerberos libraries. (Heimdal support might come later)

To use the overlay just load it into the slapd process:

    moduleload </path/to>/kinit.so <principal> </path/to/key.tab>

The module accepts two arguments. The first one being the principal for which
to request the TGT (it defaults to "ldap/<your hostname>@<DEFAULTREALM>")
and the second one is the path to the keytab file to use for
authentication, defaulting to whatever your system wide kerberos settings
default to).

Use Makefile or the following commands should work to
build it from inside the unpacked slapd sources, provided the required KRB5
header files and libaries are installed on your system:

    gcc -fPIC -c -I ../../../include/ -I ../../../servers/slapd kinit.c
    gcc -shared -o kinit.so kinit.o -lkrb5

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 2010-2016 The OpenLDAP Foundation.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.
This directory contains native slapd plugins that implement access rules.

posixgroup.c contains a simple example that implements access control
based on posixGroup membership, loosely inspired by ITS#3849.  It should
be made clear that this access control policy does not reflect any
standard track model of handling access control, and should be 
essentially viewed as an illustration of the use of the dynamic 
extension of access control within slapd.

To use the acl-posixgroup plugin, add:

moduleload acl-posixgroup.so

to your slapd configuration file; it requires "nis.schema" to be loaded.
It is configured using

access to <what>
	by dynacl/posixGroup[.{exact,expand}]=<dnpat> {<level>|<priv(s)}

The default is "exact"; in case of "expand", "<dnpat>" results from
the expansion of submatches in the "<what>" portion.  "<level>|<priv(s)>"
describe the level of privilege this rule can assume.

Use Makefile to compile this plugin or use a command line similar to:

gcc -shared -I../../../include -I../../../servers/slapd -Wall -g \
	-o acl-posixgroup.so posixgroup.c

---
Copyright 2005-2016 The OpenLDAP Foundation. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

This directory contains native slapd plugins that implement access rules.

gssacl.c contains a simple example that implements access control
based on GSS naming extensions attributes.

To use the acl-gssacl plugin, add:

moduleload acl-gssacl.so

to your slapd configuration file.
It is configured using

access to <what>
        by dynacl/gss/<attribute>.[.{base,regex,expand}]=<valpat> {<level>|<priv(s)>}

The default is "exact"; in case of "expand", "<valpat>" results from
the expansion of submatches in the "<what>" portion.  "<level>|<priv(s)>"
describe the level of privilege this rule can assume.

Use Makefile to compile this plugin or use a command line similar to:

gcc -shared -I../../../include -I../../../servers/slapd -Wall -g \
	-o acl-gssacl.so gssacl.c


---
Copyright 2011 PADL Software Pty Ltd. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

# $OpenLDAP$

This directory contains slapd overlays specific to samba4 LDAP backend:

	- pguid (not used)
	- rdnval (under evaluation)
	- vernum (under evaluation)


  - PGUID

This overlay maintains the operational attribute "parentUUID".  It contains
the entryUUID of the parent entry.  This overlay is not being considered
right now.


  - RDNVAL

This overlay maintains the operational attribute "rdnValue".  It contains
the value of the entry's RDN.  This attribute is defined by the overlay
itself as

	( 1.3.6.1.4.1.4203.666.1.58
		NAME 'rdnValue'
		DESC 'the value of the naming attributes'
		SYNTAX '1.3.6.1.4.1.1466.115.121.1.15'
		EQUALITY caseIgnoreMatch
		USAGE dSAOperation
		NO-USER-MODIFICATION )

under OpenLDAP's development OID arc.  This OID is temporary.

To use the overlay, add:

	moduleload <path to>rdnval.so
	...

	database <whatever>
	...
	overlay rdnval

to your slapd configuration file.  An instance is required for each database
that needs to maintain this attribute.


  - VERNUM

This overlay increments a counter any time an attribute is modified.
It is intended to increment the counter 'msDS-KeyVersionNumber' when
the attribute 'unicodePwd' is modified.
 

These overlays are only set up to be built as a dynamically loaded modules.
On most platforms, in order for the modules to be usable, all of the 
library dependencies must also be available as shared libraries.

If you need to build the overlays statically, you will have to move them
into the slapd/overlays directory and edit the Makefile and overlays.c
to reference them. 

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.
Copyright 2009-2016 The OpenLDAP Foundation.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

This directory contains a slapd overlay, smbk5pwd, that extends the
PasswordModify Extended Operation to update Kerberos keys and Samba
password hashes for an LDAP user.

The Kerberos support is written for Heimdal using its hdb-ldap backend.
If a PasswordModify is performed on an entry that has the krb5KDCEntry
objectclass, then the krb5Key and krb5KeyVersionNumber will be updated
using the new password in the PasswordModify request. Additionally, a
new "{K5KEY}" password hash mechanism is provided. For krb5KDCEntries that
have this hash specifier in their userPassword attribute, Simple Binds
will be checked against the Kerberos keys of the Entry. No data is
needed after the "{K5KEY}" hash specifier in the userPassword, it is
looked up from the Entry directly.

The Samba support is written using the Samba 3.0 LDAP schema. If a
PasswordModify is performed on an entry that has the sambaSamAccount
objectclass, then the sambaLMPassword, sambaNTPassword, and sambaPwdLastSet
attributes will be updated accordingly.

To use the overlay, add:

	include <path to>/krb5-kdc.schema
	include <path to>/samba.schema

	moduleload <path to>smbk5pwd.so
	...

	database bdb
	...
	overlay smbk5pwd

to your slapd configuration file. (You should obtain the necessary schema
files from the Heimdal and/or Samba distributions. At this time, there
are several known errors in these schema files that you will have to
correct before they will load in slapd.  As of Samba 3.0 the schema looks
fine as shipped.)

All modules compiled in (i.e. krb5 and samba) are enabled; the statement

	smbk5pwd-enable		<module>

can be used to enable only the desired one(s); legal values for <module>
are "krb5", "samba" and "shadow", if they are respectively enabled by defining
DO_KRB5, DO_SAMBA and DO_SHADOW.

The samba module also supports the

	smbk5pwd-must-change	<seconds>

which sets the "sambaPwdMustChange" attribute accordingly to force passwd
expiry.  A value of 0 disables this feature.

The overlay now supports table-driven configuration, and thus can be run-time
loaded and configured via back-config.  The layout of the entry is

	# {0}smbk5pwd, {1}bdb, config
	dn: olcOverlay={0}smbk5pwd,olcDatabase={1}bdb,cn=config
	objectClass: olcOverlayConfig
	objectClass: olcSmbK5PwdConfig
	olcOverlay: {0}smbk5pwd
	olcSmbK5PwdEnable: krb5
	olcSmbK5PwdEnable: samba
	olcSmbK5PwdMustChange: 2592000

which enables both krb5 and samba modules with a password expiry time
of 30 days.

The provided Makefile builds both Kerberos and Samba support by default.
You must edit the Makefile to insure that the correct include and library
paths are used. You can change the DEFS macro if you only want one or the
other of Kerberos or Samba support.

This overlay is only set up to be built as a dynamically loaded module.
On most platforms, in order for the module to be usable, all of the 
library dependencies must also be available as shared libraries.

If you need to build the overlay statically, you will have to move it into the
slapd/overlays directory and edit the Makefile and overlays.c to reference
it. You will also have to define SLAPD_OVER_SMBK5PWD to SLAPD_MOD_STATIC,
and add the relevant libraries to the main slapd link command.

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.
Copyright 2004-2016 The OpenLDAP Foundation.
Portions Copyright 2004-2005 Howard Chu, Symas Corp. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

Copyright 2004 Sang Seok Lim, IBM . All rights reserved.

Redistribution and use in source and binary forms, with
or without modification, are permitted only as authorized
by the OpenLDAP Public License.

A copy of this license is available in the file LICENSE in
the top-level directory of the distribution or, alternatively,
at <http://www.OpenLDAP.org/license.html>.

This directory contains a Component Matching module and
a X.509 Certificate example.  In order to understand Component
Matching, see RFC 3687 and
http://www.openldap.org/conf/odd-sandiego-2004/Sangseok.pdf

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A) Brief introduction about files in this directory
%%%%%%%%%%55%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1) init.c
module_init() and functions which are dynamically linked
into the main slapd codes.

2) componentlib.c and componentlib.h
GSER and BER decoder library of each primitive ASN.1 type.
They use component representation to store ASN.1 values.

3) certificate.c/.h authorityKeyIdentifier.c/.h
eSNACC generated BER and GSER decoder routines of the X.509
certificate specification and one of its extensions,
authorityKeyIdentifier.

4) asn_to_syn_mr.c asn.h
An mapping table from ASN.1 types to corresponding Syntaxes,
matching rules, and component description in slapd.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
B) How to use Component Matching on X.509 certificates
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1) be sure to configure slapd with enable-modules on.
2) install the GSER-support eSNACC compiler. You can find
only in www.openldap.org. At least, you need the library
(libcasn1.a) and header files for compiling this module.
3) modify Makefile accordingly. then run make.
you will get compmatch.la and other necessary files in ./libs
4) modify slapd.conf to include the following module command
	moduleload <path to>compmatch.la
5) run slapd and perform search operations against
the attribute, userCertificate. You need to read through
RFC 3687 in order to understand how to compose component
filters.
Ex) component search filter examples
"(userCertificate:componentFilterMatch:=item:{ component
\"toBeSigned.serialNumber\", rule integerMatch, value 2 })"
You can find more examples in "test031-component-filter"
in the OpenLDAP source directory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
C) How to add a new ASN.1 syntax
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1) download and install the eSNACC compiler supporting
Component Matching. You can find the compiler only in
www.openldap.org.  Before compiling, be sure to define
the "LDAP_COMPONENT" macro to obtain component
supported version of C library and back-ends of eSNACC.
Otherwise compiled library will fail to be linked to
the module.
2) using eSNACC, compile your ASN.1 specifications and
copy the generated .c and .h files to this directory
Ex)
$ esnacc -E BER_COMP -E GSER -t -d -f example.asn 
For Component Matching, set BOTH BER_COMP and GSER on.
After compiling, you will get example.c and example.h
3) modify example.c accordingly, seeing certificate.c
and certificate.asn as a reference.
- add init_module_xxx() located in generated .c file
into init_module() in init.c.
- modify the arguments of InstallOidDecoderMapping(...)
accordingly
- in the generated .c file, you need to write
"DecComponentxxxTop(...)" function for yourself.
You can copy BDecComponentCertificateTop in the 
generated .c file and modify it accordingly.
4) register a new attribute syntax with a new OID
in a schema file
5) then goto 3) of B) section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
D) How to configure Component Indexing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
You can generate indices on each component of
a given attribute whose values are in either GSER or
BER. Currently primitive ASN.1 types, DN, and RDN
can be indexed for equality matching in BDB.
In order to generate indices, put following line
in the slapd configuration file, slapd.conf.

index [attribute name].[component reference] eq

Ex)
index userCertificate eq
index userCertificate.toBeSigned.issuer.rdnSequence eq
index userCertificate.toBeSigned.serialNumber eq
index userCertificate.toBeSigned.version eq

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
D) How to configure Attribute Alias
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
If your client is unable to use component filters,
attribute aliasing can be used instead. Attribute
Alias maps a virtual attribute type to an attribute
component and a component matching rule.
You can create your own aliases by following steps.

1) register aliasing attributes in the schema file.
Sample aliasing attributes are in test.schema.
2) compose component filters for aliasing attributes
and put them in "preprocessed_comp_filter" array
in "init.c".
3) add "add_aa_entry" function calls in
"init_attribute_aliasing_table()" in "init.c"
4) perform searching against the aliasing attribute
Ex)
"(x509CertificateIssuer:distinguishedNameMatch=
cn=ray,L=yorktown,o=ibm,c=us)"
addpartial Overlay README

DESCRIPTION
    This package contains an OpenLDAP overlay called "addpartial" that 
    intercepts add requests, determines if the entry exists, determines what 
    attributes, if any, have changed, and modifies those attributes.  If the
    entry does not exist, the add request falls through and proceeds normally.
    If the entry exists but no changes have been detected, the client receives
    LDAP_SUCCESS (I suppose it is debatable what to do in this case, but this is
    the most clean for my use.  The LDAP_SUCCESS lets me know that the entry I
    sent slapd == the entry already in my slapd DB.  Perhaps this behavior
    should be configurable in the future).

    When a change is found, the addpartial overlay will replace all values for
    the attribute (if an attribute does not exist in the new entry but exists
    in the entry in the slapd DB, a replace will be done with an empty list of
    values).

    Once a modify takes place, the syncprov overlay will properly process the
    change, provided that addpartial is the first overlay to run.  Please see
    the CAVEATS for more specifics about this.

    The addpartial overlay makes it easy to replicate full entries to a slapd 
    instance without worrying about the differences between entries or even if
    the entry exists.  Using ldapadd to add entries, the addpartial overlay can
    compare about 500 records per second.  The intent of the addpartial overlay
    is to make it easy to replicate records from a source that is not an LDAP
    instance, such as a database.  The overlay is also useful in places where it
    is easier to create full entries rather than comparing an entry with an
    entry that must be retrieved (with ldapsearch or similar) from an existing
    slapd DB to find changes. 

    The addpartial overlay has been used in production since August 2004 and has
    processed millions of records without incident.

BUILDING
    A Makefile is included, please set your LDAP_SRC directory properly.

INSTALLATION
    After compiling the addpartial overlay, add the following to your 
    slapd.conf:

    ### slapd.conf
    ...
    moduleload addpartial.so
    ...
    # after database directive...
    # this overlay should be the last overlay in the config file to ensure that
    # it properly intercepts the add request
    overlay addpartial
    ...
    ### end slapd.conf

CAVEATS
    - In order to ensure that addpartial does what it needs to do, it should be
      the last overlay configured so it will run before the other overlays.
      This is especially important if you are using syncrepl, as the modify that
      addpartial does will muck with the locking that takes place in the
      syncprov overlay.

---
Copyright 2004-2016 The OpenLDAP Foundation.
Portions Copyright (C) Virginia Tech, David Hawes.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in file LICENSE in the
top-level directory of the distribution or, alternatively, at
http://www.OpenLDAP.org/license.html.
This directory contains a slapd module proxyOld that provides support
for the obsolete draft-weltman-ldapb3-proxy-05 revision of the LDAP
Proxy Authorization control. It is merely intended to provide compatibility
in environments where other servers only recognize this old control.
New installations should not use this code.

To use the module, add:

	moduleload <path to>proxyOld.so
	...

to your slapd configuration file. Since this is an obsolete feature,
the control is registered with the SLAP_CTRL_HIDE flag so that it will
not be advertised in the rootDSE's supportedControls attribute.

This code only works as a dynamically loaded module.

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 1998-2016 The OpenLDAP Foundation.
Portions Copyright 2005 Howard Chu, Symas Corp. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

This directory contains native slapd plugins for password mechanisms that
are not actively supported by the project. Currently this includes the
Kerberos, Netscape MTA-MD5 and RADIUS password mechanisms. The Apache
APR1 MD5 and BSD/Paul Henning Kamp MD5 mechanisms are also included.

To use the Kerberos plugin, add:

moduleload pw-kerberos.so

to your slapd configuration file.

To use the Netscape plugin, add:

moduleload pw-netscape.so

to your slapd configuration file.

To use the APR1/BSD/MD5 plugin, add:

moduleload pw-apr1.so

to your slapd configuration file.

To use the RADIUS plugin, add:

moduleload pw-radius.so

to your slapd configuration file; optionally, the path to a configuration
file can be appended in the form

moduleload pw-radius.so config="/etc/radius.conf"

Use Makefile to compile this plugin or use a command line similar to:

gcc -shared -I../../../include -Wall -g -DHAVE_KRB5 -o pw-kerberos.so kerberos.c

Replace HAVE_KRB5 with HAVE_KRB4 if you want to use Kerberos IV.
If your Kerberos header files are not in the C compiler's
default path, you will need to add a "-I" directive for that as well.

The corresponding command for the Netscape plugin would be:

gcc -shared -I../../../include -Wall -g -o pw-netscape.so netscape.c

The corresponding command for the RADIUS plugin would be:

gcc -shared -I../../../include -Wall -g -o pw-radius.so radius.c -lradius

(Actually, you might want to statically link the RADIUS client library
libradius.a into the module).

The corresponding command for the APR1 plugin would be:

gcc -shared -I../../../include -Wall -g -o pw-apr1.so apr1.c

---
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 2004-2016 The OpenLDAP Foundation.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

PBKDF2 for OpenLDAP
=======================

pw-pbkdf2.c provides PBKDF2 key derivation functions in OpenLDAP.

Schemes:

 * {PBKDF2} -  alias to {PBKDF2-SHA1}
 * {PBKDF2-SHA1}
 * {PBKDF2-SHA256}
 * {PBKDF2-SHA512}

# Requirements

  * OpenSSL 1.0.0 or later

# Installations

First, You need to configure and build OpenLDAP.

    $ cd <OPENLDAP_BUILD_DIR>/contrib/slapd-modules/passwd/
    $ git clone https://github.com/hamano/openldap-pbkdf2.git
    $ cd openldap-pbkdf2/
    $ make
    # make install

# Configration

In slapd.conf:

    moduleload pw-pbkdf2.so

You can also tell OpenLDAP to use the schemes when processing LDAP
Password Modify Extended Operations, thanks to the password-hash
option in slapd.conf. For example:

    password-hash {PBKDF2}
or
    password-hash {PBKDF2-SHA256}
or
    password-hash {PBKDF2-SHA512}

# Testing

You can get hash to use slappasswd.

    $ slappasswd -o module-load=pw-pbkdf2.la -h {PBKDF2} -s secret
    {PBKDF2}60000$Y6ZHtTTbeUgpIbIW0QDmDA$j/aU7jFKUSbH4UobNQDm9OEIwuw

A quick way to test whether it's working is to customize the rootdn and
rootpw in slapd.conf, eg:

    rootdn "cn=Manager,dc=example,dc=com"
    rootpw {PBKDF2}60000$Y6ZHtTTbeUgpIbIW0QDmDA$j/aU7jFKUSbH4UobNQDm9OEIwuw

Then to test, run something like:

    $ ldapsearch -x -b "dc=example,dc=com" -D "cn=Manager,dc=example,dc=com" -w secret

# Debugging
You can specify -DSLAPD_PBKDF2_DEBUG flag for debugging.

# Message Format

    {PBKDF2}<Iteration>$<Adapted Base64 Salt>$<Adapted Base64 DK>

# References

* [RFC 2898 Password-Based Cryptography][^1]
[^1]: http://tools.ietf.org/html/rfc2898

* [PKCS #5 PBKDF2 Test Vectors][^2]
[^2]: http://tools.ietf.org/html/draft-josefsson-pbkdf2-test-vectors-06

* [RFC 2307 Using LDAP as a Network Information Service][^3]
[^3]: http://tools.ietf.org/html/rfc2307

* [Python Passlib][^4]
[^4]: http://pythonhosted.org/passlib/

* [Adapted Base64 Encoding][^5]
[^5]: http://pythonhosted.org/passlib/lib/passlib.utils.html#passlib.utils.ab64_encode

# License
This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 2009-2016 The OpenLDAP Foundation.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

# ACKNOWLEDGEMENT
This work was initially developed by HAMANO Tsukasa <hamano@osstech.co.jp>
SHA-2 OpenLDAP support
----------------------

slapd-sha2.c provides support for SSHA-512, SSHA-384, SSHA-256,
SHA-512, SHA-384 and SHA-256 hashed passwords in OpenLDAP. For
instance, one could have the LDAP attribute:

userPassword: {SHA512}vSsar3708Jvp9Szi2NWZZ02Bqp1qRCFpbcTZPdBhnWgs5WtNZKnvCXdhztmeD2cmW192CF5bDufKRpayrW/isg==

or:

userPassword: {SHA384}WKd1ukESvjAFrkQHznV9iP2nHUBJe7gCbsrFTU4//HIyzo3jq1rLMK45dg/ufFPt

or:

userPassword: {SHA256}K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=

all of which encode the password 'secret'.


Building
--------

1) Customize the OPENLDAP variable in Makefile to point to the OpenLDAP
source root.

For initial testing you might also want to edit DEFS to define
SLAPD_SHA2_DEBUG, which enables logging to stderr (don't leave this on
in production, as it prints passwords in cleartext).

2) Run 'make' to produce slapd-sha2.so

3) Copy slapd-sha2.so somewhere permanent.

4) Edit your slapd.conf (eg. /etc/ldap/slapd.conf), and add:

moduleload ...path/to/slapd-sha2.so

5) Restart slapd.


Configuring
-----------

The {SSHA256}, {SSHA384}, {SSHA512}, {SSHA256}, {SHA384} and {SHA512}
password schemes should now be recognised.

You can also tell OpenLDAP to use one of these new schemes when processing LDAP
Password Modify Extended Operations, thanks to the password-hash option in
slapd.conf. For example:

password-hash	{SSHA512}


Testing
-------

A quick way to test whether it's working is to customize the rootdn and
rootpw in slapd.conf, eg:

rootdn          "cn=admin,dc=example,dc=com"
# This encrypts the string 'secret'

rootpw  {SHA256}K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=

Then to test, run something like:

ldapsearch -b "dc=example,dc=com" -D "cn=admin,dc=example,dc=com" -x -w secret


-- Test hashes:

Test hashes can be generated with openssl:

$ echo -n "secret" | openssl dgst -sha256 -binary | openssl enc -base64
K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=
$ echo -n "secret" | openssl dgst -sha384 -binary | openssl enc -base64
WKd1ukESvjAFrkQHznV9iP2nHUBJe7gCbsrFTU4//HIyzo3jq1rLMK45dg/ufFPt
$ echo -n "secret" | openssl dgst -sha512 -binary | openssl enc -base64
vSsar3708Jvp9Szi2NWZZ02Bqp1qRCFpbcTZPdBhnWgs5WtNZKnvCXdhztmeD2cm
W192CF5bDufKRpayrW/isg==

(join those lines up to form the full hash)



Alternatively we could modify an existing user's password with
ldappasswd, and then test binding as that user:

$ ldappasswd -D "cn=admin,dc=example,dc=com" -x -W -S uid=jturner,ou=People,dc=example,dc=com
New password: secret
Re-enter new password: secret
Enter LDAP Password: <cn=admin's password>

$ ldapsearch -b "dc=example,dc=com" -D "uid=jturner,ou=People,dc=example,dc=com" -x -w secret


Debugging (SHA-512, SHA-384 and SHA-256 only)
---------------------------------------------

To see what's going on, recompile with SLAPD_SHA2_DEBUG (use the
commented-out DEFS in Makefile), and then run slapd from the console
to see stderr:

$ sudo /etc/init.d/slapd stop
Stopping OpenLDAP: slapd.
$ sudo /usr/sbin/slapd -f /etc/ldap/slapd.conf -h ldap://localhost:389 -d stats
@(#) $OpenLDAP$
        buildd@palmer:/build/buildd/openldap2.3-2.4.9/debian/build/servers/slapd
slapd starting
...
Validating password
  Hash scheme:		{SHA256}
  Password to validate:	secret
  Password hash:	K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=
  Stored password hash: K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=
  Result:		match
conn=0 op=0 BIND dn="cn=admin,dc=example,dc=com" mech=SIMPLE ssf=0
conn=0 op=0 RESULT tag=97 err=0 text=
conn=0 op=1 SRCH base="dc=example,dc=com" scope=2 deref=0 filter="(objectClass=*)"
conn=0 fd=12 closed (connection lost)

---

This work is part of OpenLDAP Software <http://www.openldap.org/>.

Copyright 2009-2016 The OpenLDAP Foundation.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in the file LICENSE in the
top-level directory of the distribution or, alternatively, at
<http://www.OpenLDAP.org/license.html>.

---

ACKNOWLEDGEMENT:
This work was initially developed by Jeff Turner for inclusion in
OpenLDAP Software, based upon the SHA-2 implementation independently
developed by Aaron Gifford.

autogroup overlay Readme

DESCRIPTION
    The autogroup overlay allows automated updates of group memberships which
    meet the requirements of any filter contained in the group definition.
    The filters are built from LDAP URI-valued attributes. Any time an object
    is added/deleted/updated, it is tested for compliance with the filters,
    and its membership is accordingly updated. For searches and compares
    it behaves like a static group.
    If the attribute part of the URI is filled, the group entry is populated
    by the values of this attribute in the entries resulting from the search.

BUILDING
    A Makefile is included.

CONFIGURATION
    # dyngroup.schema:
        The dyngroup schema must be modified, adding the 'member' attribute
        to the MAY clause of the groupOfURLs object class, i.e.:

        objectClass ( NetscapeLDAPobjectClass:33
        NAME 'groupOfURLs'
        SUP top STRUCTURAL
        MUST cn
        MAY ( memberURL $ businessCategory $ description $ o $ ou $
                owner $ seeAlso $ member) )


    # slapd.conf:

    moduleload /path/to/autogroup.so
        Loads the overlay (OpenLDAP must be built with --enable-modules).

    overlay autogroup
        This directive adds the autogroup overlay to the current database.

    autogroup-attrset <group-oc> <URL-ad> <member-ad>
        This configuration option is defined for the autogroup overlay.
        It may have multiple occurrences, and it must appear after the
        overlay directive.

        The value <group-oc> is the name of the objectClass that represents 
        the group.

        The value <URL-ad> is the name of the attributeDescription that 
        contains the URI that is converted to the filters. If no URI is 
        present, there will be no members in that group. It must be a subtype
        of labeledURI.

        The value <member-ad> is the name of the attributeDescription that
        specifies the member attribute. User modification of this attribute 
        is disabled for consistency.

    autogroup-memberof-ad <memberof-ad>
        This configuration option is defined for the autogroup overlay.

        It defines the attribute that is used by the memberOf overlay
        to store the names of groups that an entry is member of; it must be
        DN-valued. It should be set to the same value as
        memberof-memberof-ad. It defaults to 'memberOf'.


EXAMPLE
    ### slapd.conf
    include /path/to/dyngroup.schema
    # ...
    moduleload /path/to/autogroup.so
    # ...

    database <database>
    # ...

    overlay autogroup
    autogroup-attrset groupOfURLs memberURL member
    ### end slapd.conf

    ### slapd.conf
    include /path/to/dyngroup.schema
    # ...
    moduleload /path/to/autogroup.so
    moduleload /path/to/memberof.so
    # ...

    database <database>
    #...

    overlay memberof
    memberof-memberof-ad foo

    overlay autogroup
    autogroup-attrset groupOfURLs memberURL member
    autogroup-memberof-ad foo
    ### end slapd.conf

CAVEATS
    As with static groups, update operations on groups with a large number
    of members may be slow.
    If the attribute part of the URI is specified, modify and delete operations
    are more difficult to handle. In these cases the overlay will try to detect
    if groups have been modified and then simply refresh them. This can cause
    performance hits if the search specified by the URI deals with a significant
    number of entries.

ACKNOWLEDGEMENTS
    This module was originally written in 2007 by Michał Szulczyński. Further
	enhancements were contributed by Howard Chu, Raphael Ouazana,
	Norbert Pueschel, and Christian Manal.

---
Copyright 1998-2016 The OpenLDAP Foundation.
Portions Copyright (C) 2007 Michał Szulczyński.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted only as authorized by the OpenLDAP
Public License.

A copy of this license is available in file LICENSE in the
top-level directory of the distribution or, alternatively, at
http://www.OpenLDAP.org/license.html.
Copyright (c) 1998-1999 NeoSoft, Inc.

For licensing information, see the file neoXldap.c and/or the COPYRIGHT
file contained in the directory you found this file.

This directory contains an extension to Tcl to interface with an
LDAP server.  While this software is being released to the OpenLDAP
community, it is the authors' intention that support continue (and
be added) for other client libraries as well.  As time goes on, it
is expected that code will converge rather than diverge.

Support is provided for University of Michigan LDAP version 3.3,
OpenLDAP, and Netscape.  The default configuration supports
OpenLDAP 1.2.4 and above.

OpenLDAP 2.x is supported, but there is not yet any support for
using SASL or TLS.  There may be interface changes in the LDAP API
which the author is unaware of (a leak was recently fixed for the
return values of ldap_first/next_attribute() calls).

It uses GNU autoconf.  It builds and installs without requiring
parallel directories, but it does require that Tcl and Extended Tcl
are installed in the directory pointed to by --prefix (/usr/local
by default).

For further info, try "./configure --help".

For example, I run:

    ./configure  --prefix=/opt/neotcl --enable-shared \
	--with-ldap=/usr/local/ldap

Remember that --prefix must be the same prefix used when building
and installint Tcl.

Netscape configuration has not been well tested, and you may have to
play with the resulting Makefile to get it to work.  In particular,
you will probably need to modify the LDAP_LIBFLAGS.  However, the
C code itself is reasonably well tested with Netscape.

This module will install a regular shell (ldaptclsh) a windowing
shell (ldapwish) a library, a pkgIndex.tcl, and a manpage (ldap.n).

If your Tcl installation has been configured with --enable-shared,
then you must also use --enable-shared here.

Shared libraries and Tcl packages.

If Tcl is built with --enable-shared, AND OpenLDAP (or another version
for that matter) has been build to create -llber and -lldap as shared
libaries, AND you build ldaptcl with --enable-shared, it should be
possible to run a plain Tcl interpreter (eg. tclsh8.0) and do

		package require Ldaptcl

which will install the "ldap" command into the interpreter.

You may need to set the LD_LIBRARY_PATH environment variable appropriately,
or use -R or -W,-rpath ld command options to resolve the search for ldap
and lber libraries.

This package was test built on a Alpha OSF4.0e with the native C
compiler.

Please email comments or bug fixes to openldap-devel@OpenLDAP.org,
or to kunkee@OpenLDAP.org.  I would also like to know if you are
using this interface, so I invite you to drop me an email if you do.
This is an unstable development release of a LDAPv3 C++ Class Library.
It was created as the diploma thesis (final project) of my computer 
science studies.
It is based upon the OpenLDAP C-API and so it needs the C-library and
Headerfiles installed.

Installation:
=============
Just run the "configure" script with the apropriate options. Especially
these two options can be imported, if you didn't install the OpenLDAP-
libraries in the default place:

--with-libldap=<path to libldap> : To tell configure where the OpenLDAP
        C-libraries are located.
--with-ldap-includes=<path to ldap include files> : To tell configure
        where the OpenLDAP include files are located.
--enable-debug to enable compliation with debugging symbols and stderr
        output
(run "configure --help" to see all possible command line options)

If configure finishes without problems. You can simply call "make" to 
build the library and "make install" to install it.

Documentation:
==============
Docs are very incomplete. You can either look in the source files for 
the documentation comment of generate documentation  using "doxygen"
or any other javadoc compatible documentation generator.

Bugreports and other feedback:
==============================
If you find bugs please feel free to send me a detailed report. All 
other kinds of feedback are welcomed as well.


	Ralf Haferkamp <rhafer@suse.de>

This directory contains a series of test scripts which are used to
verify basic functionality of the LDAP libraries and slapd.

	To run all of the tests, type "make test".
	To run BDB tests, type "make bdb".
	To run HDB tests, type "make hdb".
	To run MDB tests, type "make mdb".
	To run SQL tests, define SLAPD_USE_SQL=<rdbms> and type
		"make sql"; define SLAPD_USE_SQLWRITE=yes
		to enable write tests as well.
	To run regression tests, type "make regressions"

The test scripts depends on a number of tools commonly available on
Unix (and Unix-like) systems.  While attempts have been made to make
these scripts reasonably portable, they may not run properly on your
system.  You may have to adjust your path so that compatible versions
of tools used are available to the scripts and/or you may have to
install replacement tools.  Platform specific hints may be found at:
	http://www.openldap.org/faq/index.cgi?file=9

To modify the debug level the tests run slapd with, set the SLAPD_DEBUG
environment variable.
	env SLAPD_DEBUG=1 make test

This directory contains test related to regression tracking that require
a specific setup and a complete test.  Each regression test must be 
contained in a test directory whose name is "its<number>", where <number>
is the ITS number, and it must be entirely executed by a script, contained
in that directory and with the same name of the directory.  It can exploit
all the helpers provided for common tests (variables in scripts/defines.sh,
data files in data/, ...), but it should simultaneously be as self contained
and as general as possible.  Warning: occasionally, data files and
shell variables may change, so limit their use to real needs.

For example, if an issue only appears with a certain database type, the
test itself should only run when invoked for that database type; 
otherwise, if the issue appears whatever backend is used, the test should
be parametric, so that it is run with the backend selected at run-time
via the "-b" switch of the "run" script.

Regression tests are prepared on a voluntary basis, so don't expect all 
bugs to have a test any soon.  When the issue reporter provides a simple,
yet complete means to reproduce the bug she's reporting, this may speed up 
the process.  In case, only put neutral data in bug exploitation reports.
ITS#4184: fixed in 2.3.14
Internet-Drafts (I-Ds) are working documents of the Internet
Engineering Task Force (IETF), its areas, its working groups, and
individual contributors.

I-Ds are only valid for a maximum of six months and may be updated,
replaced, or obsoleted by other documents at any time.  It is
inappropriate to use I-Ds as reference material or to cite them
other than as "work in progress."

The OpenLDAP Project maintains copies of I-D for which we find
interesting.  Existance here does not necessarily imply any support
nor any plans to support for the I-D.  The I-Ds found in this
directory may be stale, expired, or otherwise out of date.

Please go to <http://www.ietf.org/> for latest revisions (and
status).

$OpenLDAP$
The OpenLDAP Developer's FAQ is available at:
	http://www.openldap.org/faq/index.cgi?file=4

Additional developer pages are at:
	http://www.openldap.org/devel/

---
$OpenLDAP$
This module contains OpenLDAP guides in Simple Document Format (SDF).

SDF is a freely available documentation system.  Based on a
simple, readable markup language, SDF generates high quality
output in multiple formats.

    cd admin                # OpenLDAP Administrator's Guide
    sdf -2topics index.sdf  # generate HTML for WWW publishing
    sdf -2html guide.sdf    # generate HTML for release
    sdf -2txt guide.sdf     # generate TXT for release

More information about STF can be obtained from the CPAN at:
	http://search.cpan.org/src/IANC/sdf-2.001/doc/catalog.html

SDF itself can be obtained at:
	http://search.cpan.org/~ianc/sdf-2.001/
# $OpenLDAP$
# Copyright 2007-2016 The OpenLDAP Foundation, All Rights Reserved.
# COPYING RESTRICTIONS APPLY, see COPYRIGHT.
#
# README.fonts 
#

In dia we use:

sans Normal 1.00 #000000
# $OpenLDAP$
# Copyright 2007-2016 The OpenLDAP Foundation, All Rights Reserved.
# COPYING RESTRICTIONS APPLY, see COPYRIGHT.
#
# README.spellcheck 
#

aspell.en.pws
	We use aspell to spell check the Admin Guide and Man Pages.
	
	Please move aspell.en.pws to ~/.aspell.en.pws and run:
	
	aspell --lang=en_US -c <filename>
	
	If you add additional words and terms, please add
	them or copy them to aspell.en.pws and commit.
The OpenLDAP build environment relies on non-standard versions of
configuration tools:
	Autoconf 2.13.1
	Automake 1.4a
	Libtool 1.4.3

The autoconf/automake releases used are available at:
	ftp://ftp.openldap.org/pub/tools/

The libtool release used is available from:
	ftp://ftp.gnu.org/

but with ltmain.sh replaced with versions found in this directory.
This is a development version of nvi.
Use at your own risk.

Please do not contact the original authors about bugs you
find in this version. Contact skimo-vi@kotnet.org instead.

There is no guarantee that anything in this version will be
available in upcoming stable releases.

New versions will be made available on 
http://www.kotnet.org/~skimo/nvi

As always this software comes with absolutely NO WARRANTY.

Now read the original README file.

Sven Verdoolaege
This version of vi requires DB3.1 or better, which can be found
on http://www.sleepycat.com/ .

Note that there is a small problem with DB 3.2 in that it will
not let nvi read in a final line that doesn't end in a newline.
This should be fixed in DB 3.3

If your system library (such as glibc prior to version 2.2) uses a
previous version of db (e.g. DB2) internally, you must configure with
--enable-dynamic-loading to avoid symbols in the internally used db from
being resolved against the newer db.

If, on top of that, the vi binary is explicitly linked against that
previous version of db (such as might happen if you enable the perl
embedding), you should compile the 3.x version with all symbols internally
resolved.  In case you use the Gnu linker (ld), this can be accomplished
by passing it the -Bsymbolic option. You can do this by setting CC
to e.g. "gcc -Wl,-Bsymbolic" prior to configuring db.
See docs/ref/build_unix/flags.html in the db distribution for more
information.

skimo@kotnet.org
#	Id: README,v 8.153 2001/04/30 09:31:12 skimo Exp  (Berkeley) Date: 2001/04/30 09:31:12 

This is version 1.80 (%H%) of nex/nvi, a reimplementation of the ex/vi
text editors originally distributed as part of the Fourth Berkeley
Software Distribution (4BSD), by the University of California, Berkeley.

The directory layout is as follows:

    LICENSE ....... Copyright, use and redistribution information.
    README ........ This file.
    build.unix .... UNIX build directory.
    catalog ....... Message catalogs; see catalog/README.
    cl ............ Vi interface to the curses(3) library.
    clib .......... C library replacement source code.
    common ........ Code shared by ex and vi.
    db ............ A stripped-down, replacement db(3) library.
    dist .......... Various files used to build the vi distribution.
    docs .......... Ex/vi documentation, both current and historic.
    docs/README ... Documentation overview.
    docs/edit ..... Edit: A tutorial.
    docs/exref .... Ex Reference Manual -- Version 3.7.
    docs/vi.man ... UNIX manual page for nex/nvi.
    docs/vi.ref ... Nex/nvi reference manual.
    docs/vitut .... An Introduction to Display Editing with Vi.
    ex ............ Ex source code.
    gtk ........... Vi gtk application.
    include ....... Replacement include files.
    ip ............ Library interface to vi: vi side.
    ipc ........... Library interface to vi: application side.
    motif ......... Vi motif application.
    motif_l ....... Motif library interface to vi.
    perl_api ...... Perl scripting language support.
    perl_scripts .. Perl scripts.
    regex ......... POSIX 1003.2 regular expression library.
    tcl_api ....... Tcl scripting language support.
    tcl_scripts ... Tcl scripts.
    vi ............ Vi source code.

To build DB for a UNIX platform:

    cd build.unix
    ../dist/configure
    make

To build multiple UNIX versions of DB in the same source tree, create
a new directory then configure and build.

    mkdir build.bsdos3.0
    cd build.bsdos3.0
    ../dist/configure
    make

For additional information about building DB for UNIX platforms, the
description of possible configuration options and other information
on DB configuration and build issues, see the file build.unix/README.
	
Bug fixes and updated versions of this software will periodically be made
available.  For more information, as well as a list of Frequently Asked
Questions, see:

	http://www.bostic.com/vi

To ask questions about vi, report vi problems, request notification of
future releases and/or bug fixes, or to contact the authors for any
reason, please send email to:

	bostic@bostic.com

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
o This software is several years old and is the product of many folks' work.  

	This software was originally derived from software contributed to
	the University of California, Berkeley by Steve Kirkendall, the
	author of the vi clone elvis.  Without his work, this work would
	have been far more difficult.

	IEEE POSIX 1003.2 style regular expression support is courtesy of
	Henry Spencer, for which I am *very* grateful.

	Elan Amir did the original 4BSD curses work that made it possible
	to support a full-screen editor using curses.

	George Neville-Neil added the Tcl interpreter, and the initial
	interpreter design was his.

	Sven Verdoolaege added the Perl interpreter.

	Rob Mayoff provided the original Cscope support.

o Many, many people suggested enhancements, and provided bug reports and
  testing, far too many to individually thank.

o From the original vi acknowledgements, by William Joy and Mark Horton:

	Bruce Englar encouraged the early development of this display
	editor.  Peter Kessler helped bring sanity to version 2's
	command layout.  Bill Joy wrote versions 1 and 2.0 through 2.7,
	and created the framework that users see in the present editor.
	Mark Horton added macros and other features and made the editor
	work on a large number of terminals and Unix systems.

o And...
	The financial support of UUNET Communications Services is gratefully
	acknowledged.
#	Id: README,v 8.4 1994/11/22 09:52:04 bostic Exp  (Berkeley) Date: 1994/11/22 09:52:04 

Generally, all non-system error and informational messages in nvi are
catalog messages, i.e. they can be tailored to a specific langauge.
Command strings, usage strings, system errors and other "known text"
are not.  It would certainly be possible to internationalize all the
text strings in nvi, but it's unclear that it's the right thing to do.

First, there's no portable way to do message catalogs.  The System V
scheme is a reasonable choice, but none of the 4BSD derived systems
support it.  So, catalogs are completely implemented within nvi, and
don't require any library support.

Message catalogs in nvi are fairly simple.  Every catalog message
consists of two parts -- an initial number followed by a pipe (`|')
character, followed by the English text for the message.  For example:

	msgq(sp, M_ERR, "001|This is an error message");

would be a typical message.

When the msgq() routine is called, if the user has specified a message
catalog and the format string (the third argument) has a leading number,
then it is converted to a record number, and that record is retrieved
from the message catalog and used as a replacement format string.  If
the record can't be retrieved for any reason, the English text is displayed
instead.

Each message format string MUST map into the English format string, i.e.
it can't display more or different arguments than the English one.

For example:

	msgq(sp, M_ERR, "002|Error: %d %x", arg1, arg2);

is a format string that displays two arguments.  It is possible, however,
to reorder the arguments or to not display all of them.  The convention
nvi uses is the System V printf(3) convention, i.e. "%[0-9]*$" is the name
of a specific, numbered argument.  For example:

	msgq(sp, M_ERR, "002|Error: %2$d %1$x", arg1, arg2);

displays the arguments in reverse order.

If the system supports this convention in its library printf routines
(as specified by the test #define NL_ARGMAX), nvi uses those routines.
Otherwise, there is some serious magic going on in common/msg.c to make
this all work.

Arguments to the msgq function are required to contain ONLY printable
characters.  No further translation is done by the msgq routine before
displaying the message on the screen.  For example, in the msgq call:

	msgq(sp, M_ERR, "003|File: %s", file_name);

"file_name" must contain only printable characters.  The routine
msg_print() returns a printable version of a string in allocated
memory.  For example:

	char *p;

	p = msg_print(sp, file_name);
	msgq(sp, M_ERR, M("003", "File: %s"), p);
	FREE_SPACE(sp, p, 0);

makes sure that "file_name" is printable before calling the msgq
routine.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

The message catalogs themselves are maintained in two files.  The first
is the "base file" which contains two fields, a record number and the
message itself.  All base files are named using the convention
"vi_<language>.base", e.g. the English one is "vi_english.base".  For
example:

	002 "Unable to create temporary file"
	003 "Warning: %s is not a regular file"
	004 "%s already locked, session is read-only"
	005 "%s: remove"
	006 "%s: close"
	007 "%s: remove"
	008 "%s: remove"
	009 "Read-only file, not written; use ! to override"
	010 "Read-only file, not written"

are the first few lines of the current vi_english.base file.  Note that
message #1 is missing -- the first message of each catalog is a special
one, so that nvi can recognize message catalog files.  It's added by the
Makefile script that creates the second version of the message catalog.

The second file is the file used by nvi to access messages, and is a list
of the messages, one per line:

	VI_MESSAGE_CATALOG
	Unable to create temporary fileX
	Warning: %s is not a regular fileX
	%s already locked, session is read-onlyX
	%s: removeX
	%s: closeX
	%s: removeX
	%s: removeX
	Read-only file, not written; use ! to overrideX
	Read-only file, not writtenX

Note that all messages have had a trailing 'X' character appended.  This
is to provide nvi a place to store a trailing nul for the message so that
C library routines that expect one won't be disappointed.

These files are named for their language, e.g. "vi_english".  The second
files are automatically created from the first files.

To create a new catalog for nvi:

Copy the file vi_english.base to a file that you can modify , e.g.  "cp
vi_english.base vi_german.base".  For each of the messages in the file,
replace the message with the string that you want to use.  To find out
what the arguments to a message are, I'm afraid you'll have to search
the source code for the message number.  You can find them fairly quickly
by doing:

	cd ..; egrep '123\|' */*.[chys]

I'm sorry that there's not an easier way, but I couldn't think of
anything that wasn't a lot of work.

If, for some reason, you don't have the file vi_english.base, or you
have new sources for which you want to create a new base catalog, you
can create it by running the command "make english" in the catalog
directory.

Once you've translated all of the strings, then add your catalog to the
"CAT=" line of the Makefile, and run the command "make catalog".  This
will create the second (and corresponding) file for each file named
<language>.base.

Don't worry about missing line numbers, i.e. base files that look like:

	005	Message number 5.
	007	Message number 7.

This simply means that a message was deleted during the course of nvi's
development.  It will be taken care of automatically when you create
the second form of the file.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
If you add new messages to the nvi sources, you can check your work by
doing "make english; make check".  The "make check" target lists unused
message numbers, duplicate message numbers, and duplicate messages.
Unused message numbers are only useful if you are condensing messages.
Duplicate message numbers are a serious problem and have to be fixed.
Duplicate messages are only interesting if a message appears often enough
that it's worth creating a routine so that the string is only need in
a single place.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
To select a catalog when running nvi, set the "msgcat" option.  If the
value of this option ends with a '/', it is treated as the name of a
directory that contains a message catalog "vi_XXXX", where XXXX is the
value of the LANG environmental variable, if it's set, or the value of
the LC_MESSAGES environmental variable if it's not.  If neither of those
environmental variables are set, or if the option doesn't end in a '/',
the option is treated as the full path name of the message catalog to use.

If any messages are missing from the catalog, the backup text (English)
is used instead.
#	Id: README.signal,v 10.1 1995/06/23 10:28:17 bostic Exp  (Berkeley) Date: 1995/06/23 10:28:17 

There are six (normally) asynchronous actions about which vi cares:
SIGHUP, SIGINT, SIGQUIT, SIGTERM, SIGTSTP and SIGWINCH.

The assumptions:
	1: The DB routines are not reentrant.
	2: The curses routines may not be reentrant.
	3: Neither DB nor curses will restart system calls.

XXX
Note, most C library functions don't restart system calls.  So, we should
*probably* start blocking around any imported function that we don't know
doesn't make a system call.  This is going to be a genuine annoyance...

SIGHUP, SIGTERM
	Used for file recovery.  The DB routines can't be reentered, nor
	can they handle interrupted system calls, so the vi routines that
	call DB block signals.  This means that DB routines could be
	called at interrupt time, if necessary.

SIGQUIT
	Disabled by the signal initialization routines.  Historically, ^\
	switched vi into ex mode, and we continue that practice.

SIGWINCH:
	The interrupt routine sets a global bit which is checked by the
 	key-read routine, so there are no reentrancy issues.  This means
	that the screen will not resize until vi runs out of keys, but
	that doesn't seem like a problem.

SIGINT and SIGTSTP are a much more difficult issue to resolve.  Vi has
to permit the user to interrupt long-running operations.  Generally, a
search, substitution or read/write is done on a large file, or, the user
creates a key mapping with an infinite loop.  This problem will become
worse as more complex semantics are added to vi, especially things like
making it a pure text widget.  There are four major solutions on the table,
each of which have minor permutations.

1:	Run in raw mode.

	The up side is that there's no asynchronous behavior to worry about,
	and obviously no reentrancy problems.  The down side is that it's easy
	to misinterpret characters (e.g. :w big_file^Mi^V^C is going to look
	like an interrupt) and it's easy to get into places where we won't see
	interrupt characters (e.g. ":map a ixx^[hxxaXXX" infinitely loops in
	historic implementations of vi).  Periodically reading the terminal
	input buffer might solve the latter problem, but it's not going to be
	pretty.

	Also, we're going to be checking for ^C's and ^Z's both, all over
	the place -- I hate to litter the source code with that.  For example,
	the historic version of vi didn't permit you to suspend the screen if
	you were on the colon command line.  This isn't right.  ^Z isn't a vi
	command, it's a terminal event.  (Dammit.)

2:	Run in cbreak mode.  There are two problems in this area.  First, the
	current curses implementations (both System V and Berkeley) don't give
	you clean cbreak modes. For example, the IEXTEN bit is left on, turning
	on DISCARD and LNEXT.  To clarify, what vi WANTS is 8-bit clean, with
	the exception that flow control and signals are turned on, and curses
	cbreak mode doesn't give you this.

	We can either set raw mode and twiddle the tty, or cbreak mode and
	twiddle the tty.  I chose to use raw mode, on the grounds that raw
	mode is better defined and I'm less likely to be surprised by a curses
	implementation down the road.  The twiddling consists of setting ISIG,
	IXON/IXOFF, and disabling some of the interrupt characters (see the
	comments in cl_init.c).  This is all found in historic System V (SVID
	3) and POSIX 1003.1-1992, so it should be fairly portable.

	The second problem is that vi permits you to enter literal signal
	characters, e.g. ^V^C.  There are two possible solutions.  First, you
	can turn off signals when you get a ^V, but that means that a network
	packet containing ^V and ^C will lose, since the ^C may take effect
	before vi reads the ^V.  (This is particularly problematic if you're
	talking over a protocol that recognizes signals locally and sends OOB
	packets when it sees them.)  Second, you can turn the ^C into a literal
	character in vi, but that means that there's a race between entering
	^V<character>^C, i.e. the sequence may end up being ^V^C<character>.
	Also, the second solution doesn't work for flow control characters, as
	they aren't delivered to the program as signals.

	Generally, this is what historic vi did.  (It didn't have the curses
	problems because it didn't use curses.)  It entered signals following
	^V characters into the input stream, (which is why there's no way to
	enter a literal flow control character).

3:	Run in mostly raw mode; turn signals on when doing an operation the
	user might want to interrupt, but leave them off most of the time.

	This works well for things like file reads and writes.  This doesn't
	work well for trying to detect infinite maps.  The problem is that
	you can write the code so that you don't have to turn on interrupts
	per keystroke, but the code isn't pretty and it's hard to make sure
	that an optimization doesn't cover up an infinite loop.  This also
	requires interaction or state between the vi parser and the key
	reading routines, as an infinite loop may still be returning keys
	to the parser.

	Also, if the user inserts an interrupt into the tty queue while the
	interrupts are turned off, the key won't be treated as an interrupt,
	and requiring the user to pound the keyboard to catch an interrupt
	window is nasty.

4:	Run in mostly raw mode, leaving signals on all of the time.  Done
	by setting raw mode, and twiddling the tty's termios ISIG bit.

	This works well for the interrupt cases, because the code only has
	to check to see if the interrupt flag has been set, and can otherwise
	ignore signals.  It's also less likely that we'll miss a case, and we
	don't have to worry about synchronizing between the vi parser and the
	key read routines.

	The down side is that we have to turn signals off if the user wants
	to enter a literal character (e.g. ^V^C).  If the user enters the
	combination fast enough, or as part of a single network packet,
	the text input routines will treat it as a signal instead of as a
	literal character.  To some extent, we have this problem already,
	since we turn off flow control so that the user can enter literal
	XON/XOFF characters.

	This is probably the easiest to code, and provides the smoothest
	programming interface.

There are a couple of other problems to consider.

First, System V's curses doesn't handle SIGTSTP correctly.  If you use the
newterm() interface, the TSTP signal will leave you in raw mode, and the
final endwin() will leave you in the correct shell mode.  If you use the
initscr() interface, the TSTP signal will return you to the correct shell
mode, but the final endwin() will leave you in raw mode.  There you have
it: proof that drug testing is not making any significant headway in the
computer industry.  The 4BSD curses is deficient in that it does not have
an interface to the terminal keypad.  So, regardless, we have to do our
own SIGTSTP handling.

The problem with this is that if we do our own SIGTSTP handling, in either
models #3 or #4, we're going to have to call curses routines at interrupt
time, which means that we might be reentering curses, which is something we
don't want to do.

Second, SIGTSTP has its own little problems.  It's broadcast to the entire
process group, not sent to a single process.  The scenario goes something
like this: the shell execs the mail program, which execs vi.  The user hits
^Z, and all three programs get the signal, in some random order.  The mail
program goes to sleep immediately (since it probably didn't have a SIGTSTP
handler in place).  The shell gets a SIGCHLD, does a wait, and finds out
that the only child in its foreground process group (of which it's aware)
is asleep.  It then optionally resets the terminal (because the modes aren't
how it left them), and starts prompting the user for input.  The problem is
that somewhere in the middle of all of this, vi is resetting the terminal,
and getting ready to send a SIGTSTP to the process group in order to put
itself to sleep.  There's a solution to all of this: when vi starts, it puts
itself into its own process group, and then only it (and possible child
processes) receive the SIGTSTP.  This permits it to clean up the terminal
and switch back to the original process group, where it sends that process
group a SIGTSTP, putting everyone to sleep and waking the shell.

Third, handing SIGTSTP asynchronously is further complicated by the child
processes vi may fork off.  If vi calls ex, ex resets the terminal and
starts running some filter, and SIGTSTP stops them both, vi has to know
when it restarts that it can't repaint the screen until ex's child has
finished running.  This is solveable, but it's annoying.

Well, somebody had to make a decision, and this is the way it's going to be
(unless I get talked out of it).  SIGINT is handled asynchronously, so
that we can pretty much guarantee that the user can interrupt any operation
at any time.  SIGTSTP is handled synchronously, so that we don't have to
reenter curses and so that we don't have to play the process group games.
^Z is recognized in the standard text input and command modes.  (^Z should
also be recognized during operations that may potentially take a long time.
The simplest solution is probably to twiddle the tty, install a handler for
SIGTSTP, and then restore normal tty modes when the operation is complete.)
# Id: README,v 8.1 1997/04/13 10:38:45 bostic Exp  (Berkeley) Date: 1997/04/13 10:38:45 

[USD stands for "User's Supplementary Documents".]

edit/   Roff source for "Edit: A tutorial".  This document was USD:14 in
	the 4.3BSD manuals, but was not distributed with 4.4BSD.

exref/  Roff source for "Ex Reference Manual -- Version 3.7".  This
	document was USD:16 in the 4.3BSD manuals, and USD tabbed 12 in
	the 4.4BSD manuals.

vi.man/ Roff source for a UNIX manual page for nex/nvi.  An updated version
	of the 4.4BSD manual page.

vi.ref/ Roff source for the nex/nvi reference document.  An updated version
	of the 4.4BSD document, USD tabbed 13.

vitut/  Roff source for "An Introduction to Display Editing with Vi".  This
	document was USD:15 in the 4.3BSD manuals, but was not distributed
	with 4.4BSD.  It includes the historic "Vi Quick Reference" card.

PostScript preformatted versions of the nex/nvi reference manual and
manual page are in the files named with a ".ps" suffix, in their
respective source directories.  Flat text preformatted versions of the
nex/nvi reference manual and manual page are in the files named with a
".txt" suffix, in their respective source directories.
README.LynxOS
=============

Written by Ronald F. Guilmette <rfg@monkeys.com>

Last modified Wed Aug 14 23:10:07 PDT 1996
------------------------------------------

0. Introduction
---------------

This file describes how to build and install the Berkeley nvi editor for
the LynxOS 2.4.0 operating system.

LynxOS 2.4.0 is available for a variety of different hardware platforms, in
particular, x86, m680x0, Sparc, and PowerPC.  I have successfully built nvi
on all four of these flavors of LynxOS by following the procedures given in
this file.

Note that these procedures may not work on versions of LynxOS prior to 2.4.0.
(As I understand it, a good deal of work went into making the 2.4.0 release
more POSIX-compliant, and I have no idea what build glitches, if any, you
might encounter if you try to build nvi on a pre-2.4.0 version of LynxOS.)

There are basically four steps to configuring, building, and installing nvi
on LynxOS, namely:

	1.  Get setup to use the proper C compiler.
	2.  Replace your installed `tr' program.
	3.  Fix your system include files.
	4.  Do a normal configure, build, and install of nvi.

These steps are described in separate sections below.

1.  Get Setup to Use the Proper C Compiler
------------------------------------------

The first step when building nvi on LynxOS is to set your $PATH environment
variable properly so that the gcc 2.x compiler appears first on your path,
prior to the older (and less robust) gcc 1.xx compiler (typically installed
as /bin/gcc) and/or the old Lynx proprietary C compiler (typically installed
as /bin/cc), both of which may also be present on your system.

Note that for most operating systems, the configure script for nvi tries
to use whatever compiler you have installed (and in your $PATH) as "cc",
however in the special case of LynxOS, the configure script will auto-
matically try to find a "gcc" program on your $PATH in preference to a
compiler called "cc".  If the nvi configure script only find a compiler
called "cc", that's OK.  It will still try to see if that is really just
the GNU C compiler installed under the name "cc".

Regardless of the name however (be it "gcc" or "cc") the first C compiler
in your $PATH should be some _recent_ (i.e. 2.0 or later) version of the
GNU C compiler... and the nvi configure script now checks that this is the
case, and fails if it isn't.

Oddly enough, LynxOS 2.4.0 (and some prior versions) shipped with as many
as three different C compilers installed, so it is important to set your
$PATH environment variable carfully in order to get the proper C compiler
to appear first in your $PATH.  You want to avoid having either the /bin/gcc
compiler or the /bin/cc compiler be the first C compiler in your $PATH.

To make sure that the GNU C version 2.x compiler which was shipped with your
LynxOS system appears first on your path, you will need to either set your
$PATH variable (for sh/bash/ksh users) or your $path variable (for csh/tcsh
users).  You can, of course, just do this at the shell command prompt, but
it is probably better to actually edit this change into your .profile file
(for sh/bash/ksh users) or into your .cshrc file (for csh/tcsh users).

The pathname of the directory that contains the GNU C version 2.x compiler
is (unfortunately) dependent upon the exact type of LynxOS system you have.

For LynxOS 2.4.0 on x86 systems, gcc 2.x is located in:

	/cygnus/94q4-lynxos-x86/bin

For LynxOS 2.4.0 on m680x0 systems, gcc 2.x is located in:

	/cygnus/94q4-lynxos-68k/bin

For LynxOS 2.4.0 on Sparc systems, gcc 2.x is located in:

	/cygnus/94q4-lynxos-usparc/bin

For LynxOS 2.4.0 on PowerPC systems, gcc 2.x is located in:

	/cygnus/95q2-lynxos-ppc/bin

(Note also that these locations may change in LynxOS 2.5.x and beyond.)

Anyway, it is imperative that you setup your $PATH environment variable
(*before* you do the configure step for nvi) so that the GNU C version 2.x
compiler appears in your $PATH before either the /bin/cc or /bin/gcc
compilers (if present).  If you fail to do this, the configure step for
nvi will fail, because the compiler script actually checks (now) that the
compiler you are using (if your are on a LynxOS system) is gcc 2.0 or
later.

To make absolutely sure that you will be configuring and building nvi with
the proper C compiler (i.e. the GNU C version 2.x compiler on your system)
you should add the directory name listed above for your specific system type
to your $PATH setting in your $HOME/.profile file.  (For csh/tcsh users, you
will instead want to add the relevant directory name to the setting of your
$path variable in your ~/.cshrc file.)  Once you have added the proper direc-
tory name (from the list given above) to your $HOME/.profile file (or to your
~/.cshrc file, if you are using csh or tcsh) you should log out completely
and then log back into the system just to make sure your new $PATH/$path
setting takes effect properly.

When you finish making this adjustment to your $PATH (or $path), the most
up-to-date version of gcc on your system should be available to you as the
first `gcc' program on your $PATH.  You should verify that this is indeed the
case simply by typing `gcc -v' and then checking the version number reported
by the compiler.  It should say either "2.6-94q4" or (on PowerPC systems) it
should say "2.6-95q2".  If you don't get these results, try again to set your
$PATH (or $path) until you do.  You won't be able to build nvi until you are
properly setup to use gcc version 2.0 or later.

Performing the steps shown above will insure that your subsequent configura-
tion and build steps for nvi will make use of the most up-to-date version of
gcc that was shipped with your Lynx operating system.  (Note that the versions
of gcc which are currently shipping with LynxOS 2.4.0 are also somewhat out-
of-date themselves, but they are still quite a bit newer and more bug-free
and ANSI conformant that those other two C compilers, /bin/cc and /bin/gcc,
which also ship with LynxOS 2.4.0.)

(Note:  At present, LynxOS version 2.4.0 is the latest officially released
version of LynxOS, and all of the above information is accurate and correct
for LynxOS 2.4.0 as of the time of this writing.  However it is rumored that
future releases of LynxOS may provide a still newer version of gcc, and that
it may be located in the /usr/bin directory.  Thus, if you are building nvi
for some LynxOS version later than 2.4.0, you may wish to check and see if
your system has a program called /usr/bin/gcc, and use that version of gcc,
if available, rather than the one suggested above.)

2.  Replace Your Installed `tr' Program
---------------------------------------

The `tr' program which comes bundled with LynxOS 2.4.0 (as /bin/tr) has a
somewhat obscure bug which just happens to be tickled by almost all GNU
`autoconf' generated `configure' scripts (including the one that nowadays
comes bundled with nvi).  Using the stock /bin/tr program on LynxOS when
executing such `configure' scripts _will_ cause these scripts to malfunction
in various ways.  It is therefore imperative that you replace your LynxOS
/bin/tr program with a properly working version of the `tr' command _before_
you even try to configure nvi.  (You can tell if your `tr' program has the
bug by executng the command "echo ab- | tr ab- ABC".  If this yields the
string "Ab-" then you have the bug.  If it yields "ABC" then you don't.)

You can obtain sources for a working version of the `tr' command as part of
the GNU `textutils' package (the latest version of which, at the time of this
writing, is 1.19).  The GNU textutils package is available for downloading
from prep.ai.mit.edu in the pub/gnu directory.  Look for the file named
textutils-1.19.tar.gz, or an even more recent version of textutils, if one
is available.  Fetch it, gunzip it, untar it, and follow the directions in
the INSTALL file included in the tar file to build and install the entire
textutils set of utility programs (which includes a working `tr' program).
Then just make sure that the GNU version of `tr' appears on your $PATH
_before_ the LynxOS version of `tr' (i.e. /bin/tr).  Be sure to do this
step _before_ you start to configure nvi.

When building the textutils set of programs, I suggest that you use the most
up-to-date C compiler available on your system (as described above).  Also,
note that it will be important for you to AVOID using the -O (optimize)
compiler option when building the GNU textutils package, even if you are
using the most up-to-date version of gcc which shipped with your system.
If you try to use -O when building the textutils package on an x86 with
the Cygnus 94q4 C compiler, you will end up with a `tr' program which will
malfunction even worse than the one you are trying to replace!  If you use
-O when building the textutils package on LynxOS on the PowerPC (using the
Cygnus 95q2 C compiler) you will just get yourself a compiler crash.  So
just don't use -O when building textutils.  You can avoid using -O by in-
voking make in the textutils directory as follows:

	make CFLAGS="-g"

(Note:  At present, LynxOS version 2.4.0 is the latest officially released
version of LynxOS, and all of the above information is accurate and correct
for LynxOS 2.4.0 as of the time of this writing.  However it is rumored that
the bug in the /bin/tr program will be fixed in future releases of LynxOS,
so if you have a version of LynxOS later than 2.4.0, you may wish to check
and see if your /bin/tr program even has the problematic bug before bothering
with all of this.)


3.  Fix Your System Include Files
---------------------------------

If you are building nvi on a PowerPC system, it is also important that you
apply the patches given at the end of this file to your /usr/include files.
(Note that you will have to be root in order to do this.)  Two of the patches
included below fix a pair of serious bugs in the /usr/include/stdarg.h file
on the PowerPC, and you really _do_ want to have these bugs fixed anyway,
because without these fixes, anything that you compile which uses <stdarg.h>
will very likely malfunction at run-time.

Regardless of which LynxOS platform you are using (i.e. x86, PowerPC, Sparc,
or m680x0) you may want to apply all of the system include files patches that
are included below anyway.  Doing so will clean up a few minor problems with
the relevant system include files (i.e. <stdarg.h>, <ioctl.h>, and <wait.h>)
and this step will also prevent a few warnings which you would otherwise get
during the build of nvi.

You can apply all of the patches given at the end of this file simply by
doing the following:

	su root
	cd /usr/include
	/bin/patch < this-file

Where `this-file' is the actual full pathname of the file you are now reading,
wherever it may reside on your own system.

(Note:  At present, LynxOS version 2.4.0 is the latest officially released
version of LynxOS, and all of the above information is accurate and correct
for LynxOS 2.4.0 as of the time of this writing.  However it is rumored that
future releases of LynxOS may incorporate some or all of the important system
include file fixes provided below.  Thus, if you are building nvi for some
LynxOS version later than 2.4.0, you should probably go ahead and try to
apply the patches given below to your system include files, and then just
don't worry about it if these patches seem to have already been applied.)


4.  A Brief Note about Sendmail
-------------------------------

I should mention also that LynxOS does not normally ship with the `sendmail'
mail transfer program installed, either under /usr/lib/ or anywhere else for
that matter.  This isn't really a big problem, but nvi normally wants and
expects to have a sendmail program available so that it can send users notifi-
cations (by mail) whenever a partially edited file is preserved by the editor
in response to a sudden system crash, a sudden system shutdown, or an unexpect-
ed serial-line hangup.  You can configure and build nvi without any sendmail
program installed on your system, but you will get warnings about its absence
when you are doing the initial configure step prior to actually building nvi.
If you want to have a fully-functional nvi which does send out notification
messages (by mail) whenever partially edited files are preserved during a
serial line hangup or system crash, then you should get the BSD sendmail
sources (via ftp from ftp.cs.berkeley.edu), build and install sendmail, and
then reconfigure, rebuild, and reinstall nvi.

Please contact me at the E-mail address below if you experience any problems in
building or using nvi on LynxOS.  I make no guarrantees, but I may be willing
to try to help.

Ron Guilmette
Roseville, California
<rfg@monkeys.com>
August 14, 1996


cut here for LynxOS 2.4.0 system include files patches
-----------------------------------------------------------------------------
*** wait.h	Fri Apr 26 10:02:45 1996
--- wait.h	Sun May 19 05:36:50 1996
***************
*** 94,104 ****
  /* Function prototypes */
  #ifndef __LYNXOS
- #ifdef _POSIX_SOURCE
  extern pid_t wait		_AP((int *));
  extern pid_t waitpid	_AP((pid_t, int *, int));
! #else
! extern int wait		_AP((union wait *));
! extern int waitpid	_AP((int, union wait *, int));
! extern int wait3	_AP((union wait *, int, struct rusage *));
  #endif
  #endif /* !__LYNXOS */
--- 94,101 ----
  /* Function prototypes */
  #ifndef __LYNXOS
  extern pid_t wait		_AP((int *));
  extern pid_t waitpid	_AP((pid_t, int *, int));
! #ifndef _POSIX_SOURCE
! extern int wait3	_AP((int *, int, struct rusage *));
  #endif
  #endif /* !__LYNXOS */
*** ioctl.h	Fri Apr 26 16:50:51 1996
--- ioctl.h	Sat May 18 17:55:16 1996
***************
*** 572,576 ****
  
  #ifndef __LYNXOS
! extern int ioctl	_AP((int, int, char *));
  #endif
  
--- 572,576 ----
  
  #ifndef __LYNXOS
! extern int ioctl	_AP((int, int, ...));
  #endif
  
*** stdarg.h	Fri Apr 26 16:51:02 1996
--- stdarg.h	Sat May 18 19:34:13 1996
***************
*** 88,92 ****
  	(((sizeof(TYPE) + sizeof(int) - 1) / sizeof(int)) * sizeof(int))
  
! #define va_start(AP, LASTARG)	(AP = ((char *) __builtin_next_arg ()))
  
  void va_end(va_list);		/* Defined in libgcc.a */
--- 88,92 ----
  	(((sizeof(TYPE) + sizeof(int) - 1) / sizeof(int)) * sizeof(int))
  
! #define va_start(AP, LASTARG)	(AP = ((char *) __builtin_next_arg (LASTARG)))
  
  void va_end(va_list);		/* Defined in libgcc.a */
***************
*** 162,166 ****
  	(((sizeof(TYPE) + sizeof(int) - 1) / sizeof(int)) * sizeof(int))
  
! #define va_start(AP, LASTARG)	(AP = ((char *) __builtin_next_arg ()))
  
  void va_end(va_list);		/* Defined in libgcc.a */
--- 162,166 ----
  	(((sizeof(TYPE) + sizeof(int) - 1) / sizeof(int)) * sizeof(int))
  
! #define va_start(AP, LASTARG)	(AP = ((char *) __builtin_next_arg (LASTARG)))
  
  void va_end(va_list);		/* Defined in libgcc.a */
There are some known problems with Solaris curses.
Please give ncurses a try when you encounter a screen output problem.

Apparently, the more recent Solaris compilers invoke the incremental linker,
`ild', when the "-g" option is used and one or more "xxx.o" files are
specified on the command line during the link phase.  Unfortunately, the
executable produced is up to 3-4 times as big as an executable generated
by the standard linker `ld'.
You can bypass this by adding "-xildoff" to LDFLAGS or by
setting CFLAGS (to something that does not contain "-g").
#	Id: README,v 8.29 2001/05/13 20:52:36 skimo Exp  (Berkeley) Date: 2001/05/13 20:52:36 

Nvi uses the GNU autoconf program for configuration and compilation.  You
should enter:

	../dist/configure
	make

and nvi will configure the system and build one or two binaries:  nvi and
tknvi.  You can use any path to the configure script, e.g., to build for
an x86 architecture, I suggest that you do:

	mkdir build.x86
	cd build.x86
	../dist/configure
	make

There are options that you can specify to the configure command.  See
the next section for a description of these options.

If you want to rebuild or reconfigure nvi, for example, because you change
your mind as to the curses library that you want to use, create a new
directory and reconfigure it using "configure" and whatever options you
choose, don't try to selectively edit the files.

By default, nvi is installed as "vi", with hard links to "ex" and "view".
To install them using different names, use the configure program options.
For example, to install them as "nvi", "nex" and "nview", use:

	configure --program-prefix=n

See the section below on installation for details.

Note, if you're building nvi on a LynxOS system, you should read the
README.LynxOS file in this directory for additional build instructions
that are specific to that operating system.

If you have trouble with this procedure, send email to the addresses
listed in ../README.  In that email, please provide a complete script
of the output for all of the above commands that you entered.

=-=-=-=-=-=-=
NVI'S OPTIONS TO THE CONFIGURE PROGRAM
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

There are many options that you can enter to the configuration program.
To see a complete list of the options, enter "configure --help".  Only
a few of them are nvi specific.  These options are as follows:

  --disable-re            DON'T use the nvi-provided RE routines.
  --enable-debug          Build a debugging version.
  --enable-perlinterp     Include a Perl interpreter in vi.
  --enable-tclinterp      Include a Tk/Tcl interpreter in vi.
  --enable-gtk		  Build a gtk front-end.
  --enable-motif	  Build a motif front-end.
  --enable-threads	  Turn on thread support.
  --enable-widechar	  Build a wide character aware vi (experimental).
  --with-curses=DIR       Path to curses installation.
  --with-db3=db3prefix    Path to db3 installation.
  --enable-dynamic-loading Load DB 3 dynamically.

disable-re:
	By default, nvi loads its own versions of the POSIX 1003.2 Regular
	Expression routines (which are Henry Spencer's implementation).
	If your C library contains an implementation of the POSIX 1003.2
	RE routines (note, this is NOT the same as the historic UNIX RE
	routines), and you want to use them instead, enter:

	--disable-re

	as an argument to configure, and the RE routines will be taken
	from whatever libraries you load.  Please ensure that your RE
	routines implement Henry Spencer's extensions for doing vi-style
	"word" searches.

enable-debug:
	If you want to build nvi with no optimization (i.e. without -O
	as a compiler flag), with -g as a compiler flag, and with DEBUG
	defined during compilation, enter:

	--enable-debug

	as an argument to configure.

enable-perlinterp:
	If you have the Perl 5 libraries and you want to compile in the
	Perl interpreter, enter:

	--enable-perlinterp

	as an argument to configure.  (Note: this is NOT possible with
	Perl 4, or even with Perl 5 versions earlier than 5.002.)

enable-tclinterp:
	If you have the Tk/Tcl libraries and you want to compile in the
	Tcl/Tk interpreter, enter:

	--enable-tclinterp

	as an argument to configure.  If your Tk/Tcl include files and
	libraries aren't in the standard library and include locations,
	see the next section of this README file for more information.

enable-gtk:
	If you have the Gtk libraries and you want to build the Gtk
	nvi front-end, enter:

	--enable-gtk

	as an argument to configure.  If your Gtk include files and
	libraries aren't in the standard library and include locations,
	see the next section of this README file for more information.
	See also the enable-threads option.

enable-motif:
	If you have the Motif libraries and you want to build the Motif
	nvi front-end, enter:

	--enable-motif

	as an argument to configure.  If your Motif include files and
	libraries aren't in the standard library and include locations,
	see the next section of this README file for more information.

enable-threads:
	If you want to be able to use multiple windows in the Gtk
	front-end, you should specify this option.

with-curses:
	Specifies the path where curses is installed.

with-db3:
	Specifies the path where DB3 is installed.
	See README.DB3 for more information about DB3.

enable-dynamic-loading:
	Dynamically load DB3 library.
	See README.DB3 for more information about DB3.

enable-widechar:
	Enables support for wide characters.
	Note that this is still rather experimental.

	If you try this out on Solaris, you will want to point nvi
	to the curses in /usr/xpg4/ which is CSI compliant.

=-=-=-=-=-=-=
ADDING OR CHANGING COMPILERS, OR COMPILE OR LOAD LINE FLAGS
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

If you want to use a specific compiler, specify the CC environment
variable before running configure.  For example:

	env CC=gcc configure

Using anything other than the native compiler will almost certainly
mean that you'll want to check the compile and load line flags, too.

If you want to specify additional load line flags, specify the ADDLDFLAGS
environment variable before running configure.  For example:

	env ADDLDFLAGS="-Q" configure

would specify the -Q flag in the load line when the nvi programs are
loaded.

If you don't want configure to use the default load line flags for the
system, specify the LDFLAGS environment variable before running configure.
For example:

	env LDFLAGS="-32" configure

will cause configure to set the load line flags to "-32", and not set
them based on the current system.

If you want to specify additional compile line flags, specify the
ADDCPPFLAGS environment variable before running configure.  For example:

	env ADDCPPFLAGS="-I../foo" configure

would cause the compiler to be passed the -I../foo flag when compiling
test programs during configuration as well as when building nvi object
files.

If you don't want configure to use the default compile line flags for the
system, specify the CPPFLAGS environment variable before running configure.
For example:

	env CPPFLAGS="-I.." configure

will cause configure to use "-I.." as the compile line flags instead of
the default values.

=-=-=-=-=-=-=
ADDING LIBRARIES AND INCLUDE FILES
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

If the Tk/Tcl or any other include files or libraries are in non-standard
places on your system, you will need to specify the directory path where
they can be found.

If you want to specify additional library paths, set the ADDLIBS environment
variable before running configure.  For example:

	env ADDLIBS="-L/a/b -L/e/f -ldb" configure

would specify two additional directories to search for libraries, /a/b
and /e/f, and one additional library to load, "db".

If you want to specify additional include paths, specify the ADDCPPFLAGS
environment variable before running configure.  For example:

	env ADDCPPFLAGS="-I/usr/local/include" LIBS="-ldb" configure

would search /usr/local/include for include files, as well as load the db
library as described above.

As a final example, let's say that you've downloaded ncurses from the net
and you've built it in a directory named ncurses which is at the same
level in the filesystem hierarchy as nvi.  You would enter something like:

	env ADDCPPFLAGS="-I../../ncurses/include" \
	    ADDLIBS="-L../../ncurses/libraries" configure

to cause nvi to look for the curses include files and the curses library
in the ncurses environment.

Notes:
	Make sure that you prepend -L to any library directory names, and
	that you prepend -I to any include file directory names!  Also,
	make sure that you quote the paths as shown above, i.e. with
	single or double quotes around the values you're specifying for
	ADDCPPFLAGS and ADDLIBS.

	=-=-=-=-=-=
	You should NOT need to add any libraries or include files to load
	the Perl5 interpreter.  The configure script will obtain that
	information directly from the Perl5 program.  This means that the
	configure script must be able to find perl in its path.  It looks
	for "perl5" first, and then "perl".  If you're building a Perl
	interpreter and neither is found, it's a fatal error.

	=-=-=-=-=-=
	You do not need to specify additional libraries to load Tk/Tcl,
	Perl or curses, as the nvi configuration script adds the
	appropriate libraries to the load line whenever you specify
	--enable-tknvi or other Perl or Tk/Tcl related option, or build
	the Tk/Tcl or curses version of nvi.  The library names that are
	automatically loaded are as follows:

	for Perl:	-lperl
	for Tk/Tcl:	-ltk -ltcl -lm
	for curses:	-lcurses

	In addition, the configure script loads:

		... the X libraries when loading the Tk/Tcl libraries,
		    if they exist.

		... the -ltermcap or -ltermlib libraries when loading
		    any curses library, if they exist.

	=-=-=-=-=-=
	The env command is available on most systems, and simply sets one
	or more environment variables before running a command.  If the
	env command is not available to you, you can set the environment
	variables in your shell before running configure.  For example,
	in sh or ksh, you could do:

		ADDLIBS="-L/a/b -L/e/f -ldb" configure

	and in csh or tcsh, you could do:

		setenv ADDLIBS "-L/a/b -L/e/f -ldb"
		configure

	See your shell manual page for further information.

=-=-=-=-=-=-=
INSTALLING NVI
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Nvi installs the following files into the following locations, with
the following default values:

Variables:		Default value:
prefix			/usr/local
exec_prefix		$(prefix)
bindir			$(prefix)/bin
datadir			$(prefix)/share
mandir			$(prefix)/man

File(s):		Default location
----------------------------------------
vi			$(bindir)/vi
vi.1			$(mandir)/man1/vi.1
vi.0			$(mandir)/cat1/vi.0
Perl scripts		$(datadir)/vi/perl/
Tcl scripts		$(datadir)/vi/tcl/
Message Catalogs	$(datadir)/vi/catalog/

Notes:
	There are two hard links to the vi program, named ex and view.
	Similarly, there are two hard links to the unformatted vi manual
	page, named ex.1 and view.1, and two hard links to the formatted
	manual page, named ex.0 and view.0.  These links are created when
	the program and man pages are installed.

	If you want to install vi, ex, view and the man pages as nvi, nex,
	nview, use the configure option --program-prefix=n.  Other, more
	complex transformations are possible -- use configure --help to
	see more options.

	To move the entire installation tree somewhere besides /usr/local,
	change the value of both "exec_prefix" and "prefix".  To move the
	binaries to a different place, change the value of "bindir".
	Similarly, to put the datafiles (the message catalogs, Perl and
	Tcl scripts) or the man pages in a different place, change the
	value of "datadir" or "mandir".  These values can be changed as
	part of configuration:

		configure --exec_prefix=/usr/contrib --prefix=/usr/share

	or when doing the install itself:

		make exec_prefix=/usr/contrib prefix=/usr/contrib install

	The datafile directory (e.g., /usr/local/share/vi by default) is
	completely removed and then recreated as part of the installation
	process.

=-=-=-=-=-=-=
NVI AND THE CURSES LIBRARY
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

The major portability problem for nvi is selecting a curses library.
Unfortunately, it is common to find broken versions of curses -- the
original System V curses was broken, resulting in all vendors whose
implementations are derived from System V having broken implementations
in turn.

If you use the vendor's or other curses library, and you see any of the
following symptoms:

	+ Core dumps in curses routines.
	+ Missing routines when compiling.
	+ Repainting the wrong characters on the screen.
	+ Displaying inverse video in the wrong places.
	+ Failure to reset your terminal to the correct modes on exit.

you have a broken curses implementation, and you should reconfigure nvi
to use another curses library.

An alternative for your vendor's curses is ncurses, available from
ftp://ftp.gnu.org/pub/gnu/ncurses/

One final note.  If you see the following symptoms:

	+ Line-by-line screen repainting instead of scrolling.

it usually means that your termcap or terminfo information is insufficient
for the terminal.
Product name:	pdisk
Version:	0.8
Ship date:	16 May 2000
Company name:	n/a
Author name:	Eryk Vershen

Description:	A low-level Apple partition table editor for Linux.
		A MacOS version exists for "standalone" use.

What's New:	Clean up sources - fix naming, delete old email addresses
		Added support for display of Mac volume names
		Added cvt_pt target (for LinuxPPC team)
		Fix block 0 display to show logical offset of drivers
		Require confimation of quit without write
		Fix iteration to not complain about missing devices
		Warn when creating/writing a map with more than 15 entries
		Make initial window larger in Mac version
		Fix ATA support to scan buses correctly
		Fix linux names (in MacOS) to work right when many devices
		Change so WORM devices are considered 'CDs'

Last time:	Added support for ATA/IDE disks without LBA capability
		Fixed bug - create partition with unmodified size failed
		Added support for new (DR3) MkLinux names - show MkLinux
		name when displaying under another name and allow the
		MkLinux name to be used on input.

Requirements:	Linux PPC - just run the binary
		MacOS - Distributed binaries for PowerPC or 68000
			I haven't tried it except on 7.6.1 and 8.0

Price:		Free

Legalese:	
    Modifications copyright 2000 by Eryk Vershen
    
    Copyright 1996,1997,1998 by Apple Computer, Inc.
    All Rights Reserved 

    Permission to use, copy, modify, and distribute this software and 
    its documentation for any purpose and without fee is hereby granted, 
    provided that the above copyright notice appears in all copies and 
    that both the copyright notice and this permission notice appear in 
    supporting documentation. 

    APPLE COMPUTER DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE 
    INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS 
    FOR A PARTICULAR PURPOSE. 

    IN NO EVENT SHALL APPLE COMPUTER BE LIABLE FOR ANY SPECIAL, INDIRECT, OR 
    CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM 
    LOSS OF USE, DATA OR PROFITS, WHETHER IN ACTION OF CONTRACT, 
    NEGLIGENCE, OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION 
    WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. 


Contact Info:	You can send mail to the author. There is  no guarantee of
		a response, but it is your best hope of getting a bug fixed
		or a feature added.


Other info:


READ the html file or the man page.


Finding out about apple partitioning
------------------------------------
The best curently available documentation on the Apple disk partitioning scheme
is "Technote 1189: The Monster Disk Drive Technote".  This release is not
completely in sync with that technote.  Maybe next time.
		

Building the macintosh application
----------------------------------
I have only built this under Code Warrior Pro.  The project file is included.
Thanks to Martin Minow for the SCSI support code.


Some notes on the apple partitioning
------------------------------------
The apple disk partitioning scheme was developed in 1986. It attempted to
be forward thinking as it was intended to handle drives of sizes up to several
hundred megabytes.  There was a design document, but like most such documents
it was neither complete nor unambiguous.

While the original intent was to handle various block sizes, in practice
most devices use a partitioning block size of 512 bytes.
Since the various address fields are 32 bits unsigned this means the format
can handle disks up to 2 Terabytes in size.  (32bits + 9 bits = 41 bits)
Because the format was designed around SCSI, there is no knowledge of
cylinders or heads, all block address are in absolute sector form.
A correct map should describe every block on the disk except for block zero.

An aside on CDROMs.  Most old apple CDROMs have incorrect data in block zero.
Since the HFS file-system could only handle 512 byte blocks, apple drives had
a special mode where they would do deblocking (i.e. converting 2k blocks
into four 512byte blocks and accepting 512byte block addresses.)  The partition
maps laid down on these disks are for the deblocked form.  In many cases the
partition maps they contain have only the minimum number of fields correct.
At least one CDROM I have seen doesn't even contain a partition map at all,
but is simply an HFS volume.
Bootable CD-ROMs have even stranger partition maps since two are laid down:
one at 2K offsets and one at 512-byte offsets.  If you notice that these
overlap then you begin to get an idea of how wierd these maps can be.
Apple refers to this "technique" as ghost partitioning.

The documentation in Inside Macintosh is only partially correct.
The boot-arguments field was left out.  A/UX used the boot arguments field
for something that was called the bzb (block zero block - don't ask me why).
This structure evolved over the course of A/UX.  I have recapitulated this
in the dpme.h header file.


Making a disk with Apple & Intel partitioning
---------------------------------------------
Don't cringe. I know it is an awful hack, but sometimes...
While I don't recommend doing this, it can be useful.
The procedure below is what we did.

The intel map can contain NO MORE THAN FOUR PRIMARY PARTITIONS.
You can't have any extended or logical partitions.  (Well, you might get it
to work but I wouldn't want to try it.)  The disk will NOT BE INTEL BOOTABLE.

1) Use pdisk to initialize an apple partition map.  Don't add any partitions
   yet, just write the map out and quit.

2) Use fdisk to create the primary partitions.  Go into the expert 'x' menu
   in fdisk and print out the table with the sector addresses.  Write the
   start and lengths down some where.  Write the table out.

3) Use pdisk again.  Shrink the partition map down, if necessary, so it
   does not overlap any intel partition.  Create an apple partition for each
   intel partition using the start and length value you got from fdisk.
   Write out the map and quit.

At present file systems are not compatible between Linux & MkLinux, but you
can tar stuff into these partitions and tar them out on another machine.



Good luck,

-eryk vershen
 software mechanic
 eryk@cfcl.com
# dhcpcd

dhcpcd is a
[DHCP](http://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol) and a
[DHCPv6](http://en.wikipedia.org/wiki/DHCPv6) client.
It's also an IPv4LL (aka [ZeroConf](http://en.wikipedia.org/wiki/Zeroconf))
client.
In layman's terms, dhcpcd runs on your machine and silently configures your
computer to work on the attached networks without trouble and mostly without
configuration.

If you're a desktop user then you may also be interested in
[Network Configurator (dhcpcd-ui)](http://roy.marples.name/projects/dhcpcd-ui)
which sits in the notification area and monitors the state of the network via
dhcpcd.
It also has a nice configuration dialog and the ability to enter a pass phrase
for wireless networks.

dhcpcd may not be the only daemon running that wants to configure DNS on the
host, so it uses [openresolv](http://roy.marples.name/projects/openresolv)
to ensure they can co-exist.

See [BUILDING.md](BUILDING.md) for how to build dhcpcd.

If you wish to file a support ticket or help out with development, please
[visit the Development Area](https://dev.marples.name/project/profile/101/)
or join the mailing list below.

## Configuration

You should read the
[dhcpcd.conf man page](http://roy.marples.name/man/html5/dhcpcd.conf.html)
and put your options into `/etc/dhcpcd.conf`.
The default configuration file should work for most people just fine.
Here it is, in case you lose it.

```
# A sample configuration for dhcpcd.
# See dhcpcd.conf(5) for details.

# Allow users of this group to interact with dhcpcd via the control socket.
#controlgroup wheel

# Inform the DHCP server of our hostname for DDNS.
hostname

# Use the hardware address of the interface for the Client ID.
#clientid
# or
# Use the same DUID + IAID as set in DHCPv6 for DHCPv4 ClientID as per RFC4361.
# Some non-RFC compliant DHCP servers do not reply with this set.
# In this case, comment out duid and enable clientid above.
duid

# Persist interface configuration when dhcpcd exits.
persistent

# Rapid commit support.
# Safe to enable by default because it requires the equivalent option set
# on the server to actually work.
option rapid_commit

# A list of options to request from the DHCP server.
option domain_name_servers, domain_name, domain_search, host_name
option classless_static_routes
# Respect the network MTU. This is applied to DHCP routes.
option interface_mtu

# Most distributions have NTP support.
#option ntp_servers

# A ServerID is required by RFC2131.
require dhcp_server_identifier

# Generate Stable Private IPv6 Addresses instead of hardware based ones
slaac private
```

The [dhcpcd man page](/man/html8/dhcpcd.html) has a lot of the same options and more, which only apply to calling dhcpcd from the command line.


## Compatibility
dhcpcd-5 is only fully command line compatible with dhcpcd-4
For compatibility with older versions, use dhcpcd-4

## Upgrading
dhcpcd-7 defaults the database directory to `/var/db/dhcpcd` instead of
`/var/db` and now stores dhcpcd.duid and dhcpcd.secret in there instead of
in /etc.
The Makefile `_confinstall` target will attempt to move the files correctly from
the old locations to the new locations.
Of course this won't work if dhcpcd-7 is packaged up, so packagers will need to
install similar logic into their dhcpcd package.

## ChangeLog
We no longer supply a ChangeLog.
However, you're more than welcome to read the
[commit log](http://roy.marples.name/git/dhcpcd.git/log/) and
[archived release announcements](http://roy.marples.name/archives/dhcpcd-discuss/).
# dhcpcd Test Suite

Currently this just tests the RFC2202 MD5 implementation in dhcpcd.
This is important, because dhcpcd will either use the system MD5
implementation if found, otherwise some compat code.

This test suit ensures that it works in accordance with known standards
on your platform.
# eloop-bench

eloop is a portable event loop designed to be dropped into the code of a
program. It is not in any library to date.
The basic requirement of eloop is a descriptor polling mechanism which
allows the safe delivery of signals.
As such, select(2) and poll(2) are not suitable.

This is an eloop benchmark to test the performance of the various
polling mechanisms. It's inspired by libevent/bench.

eloop needs to be compiled for a specific polling mechanism.
eloop will try and work out which one to use, but you can influence which one
by giving one of these CPPFLAGS to the Makefile:
  *  `HAVE_KQUEUE`
  *  `HAVE_EPOLL`
  *  `HAVE_PSELECT`
  *  `HAVE_POLLTS`
  *  `HAVE_PPOLL`

kqueue(2) is found on modern BSD kernels.
epoll(7) is found on modern Linux and Solaris kernels.
These two *should* be the best performers.

pselect(2) *should* be found on any POSIX libc.
This *should* be the worst performer.

pollts(2) and ppoll(2) are NetBSD and Linux specific variants on poll(2),
but allow safe signal delivery like pselect(2).
Aside from the function name, the arguments and functionality are identical.
They are of little use as both platforms have kqueue(2) and epoll(2),
but there is an edge case where system doesn't have epoll(2) compiled hence
it's inclusion here.

## using eloop-bench

The benchmark runs by setting up npipes to read/write to and attaching
an eloop callback for each pipe reader.
Once setup, it will perform a run by writing to nactive pipes.
For each successful pipe read, if nwrites >0 then the reader will reduce
nwrites by one on successful write back to itself.
Once nwrites is 0, the timed run will end once the last write has been read.
At the end of run, the time taken in seconds and nanoseconds is printed.

The following arguments can influence the benchmark:
  *  `-a active`  
     The number of active pipes, default 1.
  *  `-n pipes`  
     The number of pipes to create and attach an eloop callback to, defalt 100.
  *  `-r runs`  
     The number of timed runs to make, default 25.
  *  `-w writes`  
     The number of writes to make by the read callback, default 100.
# Welcome to libarchive!

The libarchive project develops a portable, efficient C library that
can read and write streaming archives in a variety of formats.  It
also includes implementations of the common `tar`, `cpio`, and `zcat`
command-line tools that use the libarchive library.

## Questions?  Issues?

* http://www.libarchive.org is the home for ongoing
  libarchive development, including documentation,
  and links to the libarchive mailing lists.
* To report an issue, use the issue tracker at
  https://github.com/libarchive/libarchive/issues
* To submit an enhancement to libarchive, please
  submit a pull request via GitHub: https://github.com/libarchive/libarchive/pulls

## Contents of the Distribution

This distribution bundle includes the following major components:

* **libarchive**: a library for reading and writing streaming archives
* **tar**: the 'bsdtar' program is a full-featured 'tar' implementation built on libarchive
* **cpio**: the 'bsdcpio' program is a different interface to essentially the same functionality
* **cat**: the 'bsdcat' program is a simple replacement tool for zcat, bzcat, xzcat, and such
* **examples**: Some small example programs that you may find useful.
* **examples/minitar**: a compact sample demonstrating use of libarchive.
* **contrib**:  Various items sent to me by third parties; please contact the authors with any questions.

The top-level directory contains the following information files:

* **NEWS** - highlights of recent changes
* **COPYING** - what you can do with this
* **INSTALL** - installation instructions
* **README** - this file
* **CMakeLists.txt** - input for "cmake" build tool, see INSTALL
* **configure** - configuration script, see INSTALL for details.  If your copy of the source lacks a `configure` script, you can try to construct it by running the script in `build/autogen.sh` (or use `cmake`).

The following files in the top-level directory are used by the 'configure' script:
* `Makefile.am`, `aclocal.m4`, `configure.ac` - used to build this distribution, only needed by maintainers
* `Makefile.in`, `config.h.in` - templates used by configure script

## Documentation

In addition to the informational articles and documentation
in the online [libarchive Wiki](https://github.com/libarchive/libarchive/wiki),
the distribution also includes a number of manual pages:

 * bsdtar.1 explains the use of the bsdtar program
 * bsdcpio.1 explains the use of the bsdcpio program
 * bsdcat.1 explains the use of the bsdcat program
 * libarchive.3 gives an overview of the library as a whole
 * archive_read.3, archive_write.3, archive_write_disk.3, and
   archive_read_disk.3 provide detailed calling sequences for the read
   and write APIs
 * archive_entry.3 details the "struct archive_entry" utility class
 * archive_internals.3 provides some insight into libarchive's
   internal structure and operation.
 * libarchive-formats.5 documents the file formats supported by the library
 * cpio.5, mtree.5, and tar.5 provide detailed information about these
   popular archive formats, including hard-to-find details about
   modern cpio and tar variants.

The manual pages above are provided in the 'doc' directory in
a number of different formats.

You should also read the copious comments in `archive.h` and the
source code for the sample programs for more details.  Please let us
know about any errors or omissions you find.

## Supported Formats

Currently, the library automatically detects and reads the following fomats:
  * Old V7 tar archives
  * POSIX ustar
  * GNU tar format (including GNU long filenames, long link names, and sparse files)
  * Solaris 9 extended tar format (including ACLs)
  * POSIX pax interchange format
  * POSIX octet-oriented cpio
  * SVR4 ASCII cpio
  * POSIX octet-oriented cpio
  * Binary cpio (big-endian or little-endian)
  * ISO9660 CD-ROM images (with optional Rockridge or Joliet extensions)
  * ZIP archives (with uncompressed or "deflate" compressed entries, including support for encrypted Zip archives)
  * GNU and BSD 'ar' archives
  * 'mtree' format
  * 7-Zip archives
  * Microsoft CAB format
  * LHA and LZH archives
  * RAR archives (with some limitations due to RAR's proprietary status)
  * XAR archives

The library also detects and handles any of the following before evaluating the archive:
  * uuencoded files
  * files with RPM wrapper
  * gzip compression
  * bzip2 compression
  * compress/LZW compression
  * lzma, lzip, and xz compression
  * lz4 compression
  * lzop compression

The library can create archives in any of the following formats:
  * POSIX ustar
  * POSIX pax interchange format
  * "restricted" pax format, which will create ustar archives except for
    entries that require pax extensions (for long filenames, ACLs, etc).
  * Old GNU tar format
  * Old V7 tar format
  * POSIX octet-oriented cpio
  * SVR4 "newc" cpio
  * shar archives
  * ZIP archives (with uncompressed or "deflate" compressed entries)
  * GNU and BSD 'ar' archives
  * 'mtree' format
  * ISO9660 format
  * 7-Zip archives
  * XAR archives

When creating archives, the result can be filtered with any of the following:
  * uuencode
  * gzip compression
  * bzip2 compression
  * compress/LZW compression
  * lzma, lzip, and xz compression
  * lz4 compression
  * lzop compression

## Notes about the Library Design

The following notes address many of the most common
questions we are asked about libarchive:

* This is a heavily stream-oriented system.  That means that
  it is optimized to read or write the archive in a single
  pass from beginning to end.  For example, this allows
  libarchive to process archives too large to store on disk
  by processing them on-the-fly as they are read from or
  written to a network or tape drive.  This also makes
  libarchive useful for tools that need to produce
  archives on-the-fly (such as webservers that provide
  archived contents of a users account).

* In-place modification and random access to the contents
  of an archive are not directly supported.  For some formats,
  this is not an issue: For example, tar.gz archives are not
  designed for random access.  In some other cases, libarchive
  can re-open an archive and scan it from the beginning quickly
  enough to provide the needed abilities even without true
  random access.  Of course, some applications do require true
  random access; those applications should consider alternatives
  to libarchive.

* The library is designed to be extended with new compression and
  archive formats.  The only requirement is that the format be
  readable or writable as a stream and that each archive entry be
  independent.  There are articles on the libarchive Wiki explaining
  how to extend libarchive.

* On read, compression and format are always detected automatically.

* The same API is used for all formats; in particular, it's very
  easy for software using libarchive to transparently handle
  any of libarchive's archiving formats.

* Libarchive's automatic support for decompression can be used
  without archiving by explicitly selecting the "raw" and "empty"
  formats.

* I've attempted to minimize static link pollution.  If you don't
  explicitly invoke a particular feature (such as support for a
  particular compression or format), it won't get pulled in to
  statically-linked programs.  In particular, if you don't explicitly
  enable a particular compression or decompression support, you won't
  need to link against the corresponding compression or decompression
  libraries.  This also reduces the size of statically-linked
  binaries in environments where that matters.

* The library is generally _thread safe_ depending on the platform:
  it does not define any global variables of its own.  However, some
  platforms do not provide fully thread-safe versions of key C library
  functions.  On those platforms, libarchive will use the non-thread-safe
  functions.  Patches to improve this are of great interest to us.

* In particular, libarchive's modules to read or write a directory
  tree do use `chdir()` to optimize the directory traversals.  This
  can cause problems for programs that expect to do disk access from
  multiple threads.  Of course, those modules are completely
  optional and you can use the rest of libarchive without them.

* The library is _not_ thread aware, however.  It does no locking
  or thread management of any kind.  If you create a libarchive
  object and need to access it from multiple threads, you will
  need to provide your own locking.

* On read, the library accepts whatever blocks you hand it.
  Your read callback is free to pass the library a byte at a time
  or mmap the entire archive and give it to the library at once.
  On write, the library always produces correctly-blocked output.

* The object-style approach allows you to have multiple archive streams
  open at once.  bsdtar uses this in its "@archive" extension.

* The archive itself is read/written using callback functions.
  You can read an archive directly from an in-memory buffer or
  write it to a socket, if you wish.  There are some utility
  functions to provide easy-to-use "open file," etc, capabilities.

* The read/write APIs are designed to allow individual entries
  to be read or written to any data source:  You can create
  a block of data in memory and add it to a tar archive without
  first writing a temporary file.  You can also read an entry from
  an archive and write the data directly to a socket.  If you want
  to read/write entries to disk, there are convenience functions to
  make this especially easy.

* Note: The "pax interchange format" is a POSIX standard extended tar
  format that should be used when the older _ustar_ format is not
  appropriate.  It has many advantages over other tar formats
  (including the legacy GNU tar format) and is widely supported by
  current tar implementations.

$FreeBSD: src/lib/libarchive/test/README,v 1.3 2008/01/01 22:28:04 kientzle Exp $

This is the test harness for libarchive.

It compiles into a single program "libarchive_test" that is intended
to exercise as much of the library as possible.  It is, of course,
very much a work in progress.

Each test is a function named test_foo in a file named test_foo.c.
Note that the file name is the same as the function name.
Each file must start with this line:

  #include "test.h"

The test function must be declared with a line of this form

  DEFINE_TEST(test_foo)

Nothing else should appear on that line.

When you add a test, please update the top-level Makefile.am and the
CMakeLists.txt in this directory to add your file to the list of
tests.  The Makefile and main.c use various macro trickery to
automatically collect a list of test functions to be invoked.

Each test function can rely on the following:

  * The current directory will be a freshly-created empty directory
    suitable for that test.  (The top-level main() creates a
    directory for each separate test and chdir()s to that directory
    before running the test.)

  * The test function should use assert(), assertA() and similar macros
    defined in test.h.  If you need to add new macros of this form, feel
    free to do so.  The current macro set includes assertEqualInt() and
    assertEqualString() that print out additional detail about their
    arguments if the assertion does fail.  'A' versions also accept
    a struct archive * and display any error message from there on
    failure.

  * You are encouraged to document each assertion with a failure() call
    just before the assert.  The failure() function is a printf-like
    function whose text is displayed only if the assertion fails.  It
    can be used to display additional information relevant to the failure:

       failure("The data read from file %s did not match the data written to that file.", filename);
       assert(strcmp(buff1, buff2) == 0);

  * Tests are encouraged to be economical with their memory and disk usage,
    though this is not essential.  The test is occasionally run under
    a memory debugger to try to locate memory leaks in the library;
    as a result, tests should be careful to release any memory they
    allocate.

  * Disable tests on specific platforms as necessary.  Please avoid
    using config.h to adjust feature requirements, as I want the tests
    to also serve as a check on the configure process.  The following
    form is usually more appropriate:

#if !defined(__PLATFORM) && !defined(__Platform2__)
    assert(xxxx)
#endif

BIND 9

	BIND version 9 is a major rewrite of nearly all aspects of the
	underlying BIND architecture.  Some of the important features of
	BIND 9 are:

		- DNS Security
			DNSSEC (signed zones)
			TSIG (signed DNS requests)

		- IP version 6
			Answers DNS queries on IPv6 sockets
			IPv6 resource records (AAAA)
			Experimental IPv6 Resolver Library

		- DNS Protocol Enhancements
			IXFR, DDNS, Notify, EDNS0
			Improved standards conformance

		- Views
			One server process can provide multiple "views" of
			the DNS namespace, e.g. an "inside" view to certain
			clients, and an "outside" view to others.

		- Multiprocessor Support

		- Improved Portability Architecture


	BIND version 9 development has been underwritten by the following
	organizations:

		Sun Microsystems, Inc.
		Hewlett Packard
		Compaq Computer Corporation
		IBM
		Process Software Corporation
		Silicon Graphics, Inc.
		Network Associates, Inc.
		U.S. Defense Information Systems Agency
		USENIX Association
		Stichting NLnet - NLnet Foundation
		Nominum, Inc.

	For a summary of functional enhancements in previous
	releases, see the HISTORY file.

	For a detailed list of user-visible changes from
	previous releases, see the CHANGES file.

	For up-to-date release notes and errata, see
	http://www.isc.org/software/bind9/releasenotes

BIND 9.10.5-P2

        This version contains a fix for the security flaws
        disclosed in CVE-2017-3142 and CVE-2017-3143.

BIND 9.10.5-P1

        This version contains a fix for the security flaws
        disclosed in CVE-2017-3140 and CVE-2017-3141.

BIND 9.10.5
	
	BIND 9.10.5 is a maintenance release and addresses the security
	flaws disclosed in CVE-2016-2775, CVE-2016-2776, CVE-2016-6170,
	CVE-2016-8864, CVE-2016-9131, CVE-2016-9147, CVE-2016-9444,
	CVE-2017-3135, CVE-2017-3136, CVE-2017-3137, and CVE-2017-3138.

BIND 9.10.4

	BIND 9.10.4 is a maintenance release and addresses bugs
	found in BIND 9.10.3 and earlier, as well as the security
	flaws described in CVE-2015-8000, CVE-2015-8461, CVE-2015-8704,
	CVE-2015-8705, CVE-2016-1285, CVE-2016-1286, CVE-2016-2088,
	CVE-2016-2775 and CVE-2016-2776.

BIND 9.10.3

	BIND 9.10.3 is a maintenance release and addresses bugs
	found in BIND 9.10.2 and earlier, as well as the security
	flaws described in CVE-2015-4620, CVE-2015-5477,
	CVE-2015-5722, and CVE-2015-5986.

	It also makes the following new features available:

	- New "fetchlimit" quotas are now available for the use of
	  recursive resolvers that are are under high query load for
	  domains whose authoritative servers are nonresponsive or are
	  experiencing a denial of service attack.

	  + "fetches-per-server" limits the number of simultaneous queries
	    that can be sent to any single authoritative server.  The
	    configured value is a starting point; it is automatically
	    adjusted downward if the server is partially or completely
	    non-responsive. The algorithm used to adjust the quota can be
	    configured via the "fetch-quota-params" option.
	  + "fetches-per-zone" limits the number of simultaneous queries
	    that can be sent for names within a single domain.  (Note:
	    Unlike "fetches-per-server", this value is not self-tuning.)
	  + New stats counters have been added to count
	    queries spilled due to these quotas.

	  NOTE: These features are NOT built in by default; use
	  "configure --enable-fetchlimit" to enable them.

	- Dig now supports sending of arbitrary EDNS options by specifying
	  them on the command line.

BIND 9.10.2

	BIND 9.10.2 is a maintenance release and addresses bugs
	found in BIND 9.10.1 and earlier, as well as the security
	flaws described in CVE-2014-8500, CVE-2014-8680 and
	CVE-2015-1349.

BIND 9.10.1

	BIND 9.10.1 is a maintenance release and addresses bugs
	found in BIND 9.10.0 and earlier.

	This release addresses the security flaws described in
	CVE-2014-3214 and CVE-2014-3859.

BIND 9.10.0

	BIND 9.10.0 includes a number of changes from BIND 9.9 and earlier
	releases.  New features include:

	 - DNS Response-rate limiting (DNS RRL), which blunts the
	   impact of reflection and amplification attacks, is always
	   compiled in and no longer requires a compile-time option
	   to enable it.
	 - An experimental "Source Identity Token" (SIT) EDNS option
	   is now available.  Similar to DNS Cookies as invented by
	   Donald Eastlake 3rd, these are designed to enable clients
	   to detect off-path spoofed responses, and to enable servers
	   to detect spoofed-source queries.  Servers can be configured
	   to send smaller responses to clients that have not identified
	   themselves using a SIT option, reducing the effectiveness of
	   amplification attacks.  RRL processing has also been updated;
	   clients proven to be legitimate via SIT are not subject to
	   rate limiting.  Use "configure --enable-sit" to enable this
	   feature in BIND.
	 - A new zone file format, "map", stores zone data in a
	   format that can be mapped directly into memory, allowing
	   significantly faster zone loading.
	 - "delv" (domain entity lookup and validation) is a new tool
	   with dig-like semantics for looking up DNS data and performing
	   internal DNSSEC validation.  This allows easy validation in
	   environments where the resolver may not be trustworthy, and
	   assists with troubleshooting of DNSSEC problems. (NOTE:
	   In previous development releases of BIND 9.10, this utility
	   was called "delve". The spelling has been changed to avoid
	   confusion with the "delve" utility included with the Xapian
	   search engine.)
	 - Improved EDNS(0) processing for better resolver performance
	   and reliability over slow or lossy connections.
	 - A new "configure --with-tuning=large" option tunes certain
	   compiled-in constants and default settings to values better
	   suited to large servers with abundant memory.  This can
	   improve performance on such servers, but will consume more
	   memory and may degrade performance on smaller systems.
	 - Substantial improvement in response-policy zone (RPZ)
	   performance.  Up to 32 response-policy zones can be
	   configured with minimal performance loss.
	 - To improve recursive resolver performance, cache records
	   which are still being requested by clients can now be
	   automatically refreshed from the authoritative server
	   before they expire, reducing or eliminating the time
	   window in which no answer is available in the cache.
	 - New "rpz-client-ip" triggers and drop policies allowing
	   response policies based on the IP address of the client.
	 - ACLs can now be specified based on geographic location
	   using the MaxMind GeoIP databases.  Use "configure
	   --with-geoip" to enable.
	 - Zone data can now be shared between views, allowing
	   multiple views to serve the same zones authoritatively
	   without storing multiple copies in memory.
	 - New XML schema (version 3) for the statistics channel
	   includes many new statistics and uses a flattened XML tree
	   for faster parsing. The older schema is now deprecated.
	 - A new stylesheet, based on the Google Charts API, displays
	   XML statistics in charts and graphs on javascript-enabled
	   browsers.
	 - The statistics channel can now provide data in JSON
	   format as well as XML.
	 - New stats counters track TCP and UDP queries received
	   per zone, and EDNS options received in total.
	 - The internal and export versions of the BIND libraries
	   (libisc, libdns, etc) have been unified so that external
	   library clients can use the same libraries as BIND itself.
	 - A new compile-time option, "configure --enable-native-pkcs11",
	   allows BIND 9 cryptography functions to use the PKCS#11 API
	   natively, so that BIND can drive a cryptographic hardware
	   service module (HSM) directly instead of using a modified
	   OpenSSL as an intermediary. (Note: This feature requires an
	   HSM to have a full implementation of the PKCS#11 API; many
	   current HSMs only have partial implementations. The new
	   "pkcs11-tokens" command can be used to check API completeness.
	   Native PKCS#11 is known to work with the Thales nShield HSM
	   and with SoftHSM version 2 from the Open DNSSEC project.)
	 - The new "max-zone-ttl" option enforces maximum TTLs for
	   zones. This can simplify the process of rolling DNSSEC keys
	   by guaranteeing that cached signatures will have expired
	   within the specified amount of time.
	 - "dig +subnet" sends an EDNS CLIENT-SUBNET option when
	   querying.
	 - "dig +expire" sends an EDNS EXPIRE option when querying.
	   When this option is sent with an SOA query to a server
	   that supports it, it will report the expiry time of
	   a slave zone.
	 - New "dnssec-coverage" tool to check DNSSEC key coverage
	   for a zone and report if a lapse in signing coverage has
	   been inadvertently scheduled.
	 - Signing algorithm flexibility and other improvements
	   for the "rndc" control channel.
	 - "named-checkzone" and "named-compilezone" can now read
	   journal files, allowing them to process dynamic zones.
	 - Multiple DLZ databases can now be configured.  Individual
	   zones can be configured to be served from a specific DLZ
	   database.  DLZ databases now serve zones of type "master"
	   and "redirect".
	 - "rndc zonestatus" reports information about a specified zone.
	 - "named" now listens on IPv6 as well as IPv4 interfaces
	   by default.
	 - "named" now preserves the capitalization of names
	   when responding to queries: for instance, a query for
	   "example.com" may be answered with "example.COM" if the
	   name was configured that way in the zone file.  Some
	   clients have a bug causing them to depend on the older
	   behavior, in which the case of the answer always matched
	   the case of the query, rather than the case of the name
	   configured in the DNS.  Such clients can now be specified
	   in the new "no-case-compress" ACL; this will restore the
	   older behavior of "named" for those clients only.
	 - new "dnssec-importkey" command allows the use of offline
	   DNSSEC keys with automatic DNSKEY management.
	 - New "named-rrchecker" tool to verify the syntactic
	   correctness of individual resource records.
	 - When re-signing a zone, the new "dnssec-signzone -Q" option
	   drops signatures from keys that are still published but are
	   no longer active.
	 - "named-checkconf -px" will print the contents of configuration
	   files with the shared secrets obscured, making it easier to
	   share configuration (e.g. when submitting a bug report)
	   without revealing private information.
	 - "rndc scan" causes named to re-scan network interfaces for
	   changes in local addresses.
	 - On operating systems with support for routing sockets,
	   network interfaces are re-scanned automatically whenever
	   they change.
	 - "tsig-keygen" is now available as an alternate command
	   name to use for "ddns-confgen".

BIND 9.9.0

	BIND 9.9.0 includes a number of changes from BIND 9.8 and earlier
	releases.  New features include:

	- Inline signing, allowing automatic DNSSEC signing of
	  master zones without modification of the zonefile, or
	  "bump in the wire" signing in slaves.
	- NXDOMAIN redirection.
	- New 'rndc flushtree' command clears all data under a given
	  name from the DNS cache.
	- New 'rndc sync' command dumps pending changes in a dynamic
	  zone to disk without a freeze/thaw cycle.
	- New 'rndc signing' command displays or clears signing status
	  records in 'auto-dnssec' zones.
	- NSEC3 parameters for 'auto-dnssec' zones can now be set prior
	  to signing, eliminating the need to initially sign with NSEC.
	- Startup time improvements on large authoritative servers.
	- Slave zones are now saved in raw format by default.
	- Several improvements to response policy zones (RPZ).
	- Improved hardware scalability by using multiple threads
	  to listen for queries and using finer-grained client locking
	- The 'also-notify' option now takes the same syntax as
	  'masters', so it can used named masterlists and TSIG keys.
	- 'dnssec-signzone -D' writes an output file containing only DNSSEC
	  data, which can be included by the primary zone file.
	- 'dnssec-signzone -R' forces removal of signatures that are
	  not expired but were created by a key which no longer exists.
	- 'dnssec-signzone -X' allows a separate expiration date to
	  be specified for DNSKEY signatures from other signatures.
	- New '-L' option to dnssec-keygen, dnssec-settime, and
	  dnssec-keyfromlabel sets the default TTL for the key.
	- dnssec-dsfromkey now supports reading from standard input,
	  to make it easier to convert DNSKEY to DS.
	- RFC 1918 reverse zones have been added to the empty-zones
	  table per RFC 6303.
	- Dynamic updates can now optionally set the zone's SOA serial
	  number to the current UNIX time.
	- DLZ modules can now retrieve the source IP address of
	  the querying client.
	- 'request-ixfr' option can now be set at the per-zone level.
	- 'dig +rrcomments' turns on comments about DNSKEY records,
	  indicating their key ID, algorithm and function
	- Simplified nsupdate syntax and added readline support

Building

	BIND 9 currently requires a UNIX system with an ANSI C compiler,
	basic POSIX support, and a 64 bit integer type.

	We've had successful builds and tests on the following systems:

		COMPAQ Tru64 UNIX 5.1B
		Fedora Core 6
		FreeBSD 4.10, 5.2.1, 6.2
		HP-UX 11.11
		Mac OS X 10.5
		NetBSD 3.x, 4.0-beta, 5.0-beta
		OpenBSD 3.3 and up
		Solaris 8, 9, 9 (x86), 10
		Ubuntu 7.04, 7.10
		Windows XP/2003/2008

	NOTE:  As of BIND 9.5.1, 9.4.3, and 9.3.6, older versions of
	Windows, including Windows NT and Windows 2000, are no longer
	supported.

	We have recent reports from the user community that a supported
	version of BIND will build and run on the following systems:

		AIX 4.3, 5L
		CentOS 4, 4.5, 5
		Darwin 9.0.0d1/ARM
		Debian 4, 5, 6
		Fedora Core 5, 7, 8
		FreeBSD 6, 7, 8
		HP-UX 11.23 PA
		MacOS X 10.5, 10.6, 10.7
		Red Hat Enterprise Linux 4, 5, 6
		SCO OpenServer 5.0.6
		Slackware 9, 10
		SuSE 9, 10

	To build, just

		./configure
		make

	Do not use a parallel "make".

	Several environment variables that can be set before running
	configure will affect compilation:

	    CC
		The C compiler to use.  configure tries to figure
		out the right one for supported systems.

	    CFLAGS
		C compiler flags.  Defaults to include -g and/or -O2
		as supported by the compiler.  Please include '-g'
		if you need to set CFLAGS.

	    STD_CINCLUDES
		System header file directories.  Can be used to specify
		where add-on thread or IPv6 support is, for example.
		Defaults to empty string.

	    STD_CDEFINES
		Any additional preprocessor symbols you want defined.
		Defaults to empty string.

		Possible settings:
		Change the default syslog facility of named/lwresd.
		  -DISC_FACILITY=LOG_LOCAL0
		Enable DNSSEC signature chasing support in dig.
		  (This feature is deprecated. Use `delv` instead.)
		  -DDIG_SIGCHASE=1 (sets -DDIG_SIGCHASE_TD=1 and
				    -DDIG_SIGCHASE_BU=1)
		Disable dropping queries from particular well known ports.
		  -DNS_CLIENT_DROPPORT=0
		Sibling glue checking in named-checkzone is enabled by default.
		To disable the default check set.  -DCHECK_SIBLING=0
		named-checkzone checks out-of-zone addresses by default.
		To disable this default set.  -DCHECK_LOCAL=0
		To create the default pid files in ${localstatedir}/run rather
		than ${localstatedir}/run/{named,lwresd}/ set.
		  -DNS_RUN_PID_DIR=0
		Enable workaround for Solaris kernel bug about /dev/poll
		  -DISC_SOCKET_USE_POLLWATCH=1
		  The watch timeout is also configurable, e.g.,
		  -DISC_SOCKET_POLLWATCH_TIMEOUT=20

	    LDFLAGS
		Linker flags. Defaults to empty string.

	The following need to be set when cross compiling.

	    BUILD_CC
		The native C compiler.
	    BUILD_CFLAGS (optional)
	    BUILD_CPPFLAGS (optional)
		Possible Settings:
		-DNEED_OPTARG=1         (optarg is not declared in <unistd.h>)
	    BUILD_LDFLAGS (optional)
	    BUILD_LIBS (optional)

	On most platforms, BIND 9 is built with multithreading
	support, allowing it to take advantage of multiple CPUs.
	You can configure this by specifying "--enable-threads" or
	"--disable-threads" on the configure command line.  The default
	is to enable threads, except on some older operating systems
	on which threads are known to have had problems in the past.
	(Note: Prior to BIND 9.10, the default was to disable threads on
	Linux systems; this has been reversed.  On Linux systems, the
	threaded build is known to change BIND's behavior with respect
	to file permissions; it may be necessary to specify a user with
	the -u option when running named.)

	To build shared libraries, specify "--with-libtool" on the
	configure command line.

	Certain compiled-in constants and default settings can be
	increased to values better suited to large servers with abundant
	memory resources (e.g, 64-bit servers with 12G or more of memory)
	by specifying "--with-tuning=large" on the configure command
	line. This can improve performance on big servers, but will
	consume more memory and may degrade performance on smaller
	systems.

	For the server to support DNSSEC, you need to build it
	with crypto support.  You must have OpenSSL 1.0.1t
	or newer installed and specify "--with-openssl" on the
	configure command line.  If OpenSSL is installed under
	a nonstandard prefix, you can tell configure where to
	look for it using "--with-openssl=/prefix".

	To support the HTTP statistics channel, the server must
	be linked with at least one of the following: libxml2
	(http://xmlsoft.org) or json-c (https://github.com/json-c).
	If these are installed at a nonstandard prefix, use
	"--with-libxml2=/prefix" or "--with-libjson=/prefix".

	Python requires 'argparse' to be available.  'argparse' is
	a standard module as of Python 2.7 and Python 3.2.

	On some platforms it is necessary to explicitly request large
	file support to handle files bigger than 2GB.  This can be
	done by "--enable-largefile" on the configure command line.

	Support for the "fixed" rrset-order option can be enabled
	or disabled by specifying "--enable-fixed-rrset" or
	"--disable-fixed-rrset" on the configure command line.
	The default is "disabled", to reduce memory footprint.

	If your operating system has integrated support for IPv6, it
	will be used automatically.  If you have installed KAME IPv6
	separately, use "--with-kame[=PATH]" to specify its location.

	"make install" will install "named" and the various BIND 9 libraries.
	By default, installation is into /usr/local, but this can be changed
	with the "--prefix" option when running "configure".

	You may specify the option "--sysconfdir" to set the directory
	where configuration files like "named.conf" go by default,
	and "--localstatedir" to set the default parent directory
	of "run/named.pid".   For backwards compatibility with BIND 8,
	--sysconfdir defaults to "/etc" and --localstatedir defaults to
	"/var" if no --prefix option is given.  If there is a --prefix
	option, sysconfdir defaults to "$prefix/etc" and localstatedir
	defaults to "$prefix/var".

	To see additional configure options, run "configure --help".
	Note that the help message does not reflect the BIND 8
	compatibility defaults for sysconfdir and localstatedir.

	If you're planning on making changes to the BIND 9 source, you
	should also "make depend".  If you're using Emacs, you might find
	"make tags" helpful.

	If you need to re-run configure please run "make distclean" first.
	This will ensure that all the option changes take.

	Building with gcc is not supported, unless gcc is the vendor's usual
	compiler (e.g. the various BSD systems, Linux).

	Known compiler issues:
	* gcc-3.2.1 and gcc-3.1.1 is known to cause problems with solaris-x86.
	* gcc prior to gcc-3.2.3 ultrasparc generates incorrect code at -02.
	* gcc-3.3.5 powerpc generates incorrect code at -02.
	* Irix, MipsPRO 7.4.1m is known to cause problems.

	A limited test suite can be run with "make test".  Many of
	the tests require you to configure a set of virtual IP addresses
	on your system, and some require Perl; see bin/tests/system/README
	for details.

	SunOS 4 requires "printf" to be installed to make the shared
	libraries.  sh-utils-1.16 provides a "printf" which compiles
	on SunOS 4.

Known limitations

	Linux requires kernel build 2.6.39 or later to get the
	performance benefits from using multiple sockets.

Documentation

	The BIND 9 Administrator Reference Manual is included with the
	source distribution in DocBook XML and HTML format, in the
	doc/arm directory.

	Some of the programs in the BIND 9 distribution have man pages
	in their directories.  In particular, the command line
	options of "named" are documented in /bin/named/named.8.
	There is now also a set of man pages for the lwres library.

	If you are upgrading from BIND 8, please read the migration
	notes in doc/misc/migration.  If you are upgrading from
	BIND 4, read doc/misc/migration-4to9.

	Frequently asked questions and their answers can be found in
	FAQ.

	Additional information on various subjects can be found
	in the other README files.


Change Log

	A detailed list of all changes to BIND 9 is included in the
	file CHANGES, with the most recent changes listed first.
	Change notes include tags indicating the category of the
	change that was made; these categories are:

	   [func]         New feature

	   [bug]          General bug fix

	   [security]     Fix for a significant security flaw

	   [experimental] Used for new features when the syntax
			  or other aspects of the design are still
			  in flux and may change

	   [port]         Portability enhancement

	   [maint]        Updates to built-in data such as root
			  server addresses and keys

	   [tuning]       Changes to built-in configuration defaults
			  and constants to improve performance

	   [performance]  Other changes to improve server performance

	   [protocol]     Updates to the DNS protocol such as new
			  RR types

	   [test]         Changes to the automatic tests, not
			  affecting server functionality

	   [cleanup]      Minor corrections and refactoring

	   [doc]          Documentation

	   [contrib]	  Changes to the contributed tools and
			  libraries in the 'contrib' subdirectory

	   [placeholder]  Used in the master development branch to
			  reserve change numbers for use in other
			  branches, e.g. when fixing a bug that only
			  exists in older releases

	In general, [func] and [experimental] tags will only appear
	in new-feature releases (i.e., those with version numbers
	ending in zero).  Some new functionality may be backported to
	older releases on a case-by-case basis.  All other change
	types may be applied to all currently-supported releases.


Bug Reports and Mailing Lists

	Bug reports should be sent to:

		bind9-bugs@isc.org

	Feature requests can be sent to:

		bind-suggest@isc.org

	To join or view the archives of the BIND Users mailing list,
	visit:

		https://lists.isc.org/mailman/listinfo/bind-users

	If you're planning on making changes to the BIND 9 source
	code, you may also want to join the BIND Workers mailing
	list:

		https://lists.isc.org/mailman/listinfo/bind-workers

	Information on read-only Git access, coding style and developer
	guidelines can be found at:

		http://www.isc.org/git/


Acknowledgments

	- This product includes software developed by the OpenSSL Project
	  for use in the OpenSSL Toolkit. (http://www.OpenSSL.org/).
	- This product includes cryptographic software written by Eric
	  Young (eay@cryptsoft.com).
	- This product includes software written by Tim Hudson
	  (tjh@cryptsoft.com).
Copyright (C) 2016, 2017  Internet Systems Consortium, Inc. ("ISC")
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

How to use site.h for the PKCS#11 provider of your HSM
------------------------------------------------------

First run "pkcs11-tokens" (in bin/pkcs11). This tool is built when BIND9
is configured with the --with-pcks11 flag.  It prints the addresses of
selected tokens per algorithm:

 - random number generation
 - RSA (sign/verify)
 - DSA (sign/verify)
 - DH (secret derivation)
 - digest (hash)
 - EC (ECDSA, sign/verify)
 - GOST (Russian hash and sign/verify)
 - AES (encrypt/decrypt)

...and a summary of PKCS#11 tokens that have been found.

Current well-known HSMs are predefined in site.h according to HSM "flavors":

 - Thales nCipher (default)
 - OpenDNSSEC SoftHSMv2

...and with experimental status:

 - OpenDNSSEC SoftHSMv1 with SHA224 support added
 - Cryptech
 - AEP Keyper

If BIND9 is configured with native PKCS#11 support (--enable-native-pkcs11),
then pkcs11-tokens will raise an error when a mandatory algorithm is not
supported.  (The usual error is 0x70, or CKR_MECHANISM_INVALID; 0x0
indicates that a required flag is not available.)  The following steps
may be taken, depending on which algorithms indicate failures:

 - rand or RSA: nothing can be done; native PKCS#11 is not supported
   in BIND9 with this HSM.

 - DSA or DH: run pkcs11-tokens with the -v (verbose) flag.  If the
   parameter generation mechanism is not supported you can make the token
   selection to ignore the error.  Note DSA and DH are not critical
   algorithms; you can use BIND9 in production without them.

 - digest: run pkcs11-tokens with the -v (verbose) flag.  If the problem is
   with HMAC mechanisms, use the corresponding REPLACE flags in site.h.
   If the problem is with MD5, use the corresponding DISABLE flag in
   site.h. If the problem is with SHA224, contact the implementor of the
   PKCS#11 provider and ask to have this hash algorithm implemented.  For
   any other problem, nothing can be done; native PKCS#11 is not supported
   with this HSM.

 - EC: you may wish to configure BIND9 without ECDSA support by adding
   --without-ecdsa to the "configure" arguments.

 - GOST: you SHOULD configure BIND9 without GOST support by adding
   --without-gost to the "configure" arguments.

 - AES: you MUST reconfigure bind9 without AES support by adding
   --without-aes to configure arguments.

You can disable some algorithms (e.g. DSA, DH and MD5) using the
"disable-algorithms" option in named.conf, and some other algorithms can be
disabled at compile time (ECDSA, GOST, AES).  Note, however, that disabling
algorithms can have unwanted side effects; for instance, disabling DH breaks
TKEY support.

A final note: the DISABLE flags in site.h work for OpenSSL code too, but
this feature is not officially supported yet and should not be relied on.
This directory contains contributed scripts, tools, libraries, and other
useful accessories to BIND 9.  Contrib software is not supported by ISC,
but reported bugs will be fixed as time permits.

    - scripts/

      Assorted useful scripts, including 'nanny' which monitors 
      named and restarts it in the event of a crash, 'zone-edit'
      which enables editing of a dynamic zone, and others

    - queryperf/

      A DNS query performance testing tool

    - perftcpdns/

      A performance testing tool for DNS over TCP

    - dane/

      mkdane.sh generates TLSA records for use with DNS-based
      Authentication of Named Entities (DANE)

    - dlz/modules

      Dynamically linkable DLZ modules that can be configured into
      named at runtime, enabling access to external data sources including
      LDAP, MySQL, Berkeley DB, perl scripts, etc

    - dlz/drivers

      Old-style DLZ drivers that can be linked into named at compile
      time.  (These are no longer actively maintained and are expected
      to be deprecated eventually.)

    - sdb/

      SDB drivers: another mechanism for accessing external data
      sources. (These are no longer actively maintained and are
      expected to be deprecated eventually.)

    - idn/

      Contains source for 'idnkit', which provides support for
      Internationalized Domain Name processing.

    - dnsperf-2.1.0.0-1/

      DNS server performance testing tools, like 'queryperf' but more
      advanced: 'dnsperf' focuses on authoritative server performance
      and 'resperf' on recursive server performance.

    - nslint-3.0a2

      A lint-like tool for checking DNS files

    - query-loc-0.4.0

      A tool for retrieving location information stored in the DNS

    - zkt-1.1.2

      DNSSEC Zone Key Tools, an alternate method for managing keys
      and signatures

This is dnsperf, a collection of DNS server performance testing tools.
For more information, see the dnsperf(1) and resperf(1) man pages.

To configure, compile, and install these programs, follow these steps.

1. Make sure that BIND 9 (9.4.0 or greater) is installed, including libraries
   and header files, and that the isc-config.sh program distributed with BIND
   is in your path.
   
   Note: many versions of bind do not correctly install the <isc/hmacsha.h>
   header file, so if the compilation fails, obtain this file from the BIND
   source distribution, and install it in the appropriate place.

2. Run "sh configure" to configure the software.  Most standard configure
   options are supported.

3. Run "make" to build dnsperf and resperf

4. Run "make install" to install dnsperf and resperf.

Additional software is available in the contrib/ directory.
#
#	README  dnssec zone key tool
#
#	(c) March 2005 - Aug 2014 by  Holger Zuleger  hznet
#	(c) domaincmp() Aug 2005 by Karle Boss & H. Zuleger (kaho)
#	(c) zconf.c by Jeroen Masar & Holger Zuleger
#

For more information about the DNSSEC Zone Key Tool please
have a look at "http://www.hznet.de/dns/zkt/"

You can also subscribe to the zkt-users@sourceforge.net mailing list
on the following website: https://lists.sourceforge.net/lists/listinfo/zkt-users

The ZKT software is licenced under BSD (see LICENCE file)

To build the software:
a) Get the current version of zkt
	$ wget http://www.hznet.de/dns/zkt/zkt-1.1.tar.gz

b) Unpack
	$ tar xzvf zkt-1.1.tar.gz

c) Change to source directory
	$ cd zkt-1.1

d) Run configure script
	$ ./configure

e) Compile
	$ make

f) Install
	# make install
	# make install-man


Prepare your setup:
a) (optional) Install or rebuild the default dnssec.conf file 
	$ zkt-conf -d -w	# Install new file
		or
	$ zkt-conf -s -w	# rebuild existing file

b) (optional) Change default parameters
	$ zkt-conf -s -O "Zonedir: /var/named/zones" -w
		or use your prefered editor 
	$ vi /var/named/dnssec.conf
   (optional) You'll probably want to have zkt-ls work recursively
	$ zkt-conf -s -O "Recursive: True" -w

c) Prepare one of your zone for zkt
	$ cd /var/named/zones/net/example.net	# change dir to zone directory
	$ cp <zonefile> zone.db 		# copy and rename existing zone file to "zone.db"
	$ zkt-conf -w zone.db			# create local dnssec.conf file and include dnskey.db into zone file 

d) Prepare for initial signing
	$ cd /var/named/zones/net/example.net
	$ touch zone.db.signed
	$ zkt-signer -v -v -o example.net	# -o is ORIGIN (i.e. zone name)

e) Publish your zone
	@ add `zone.db.signed' as zone file to your name server
	@ publish DS contained in `dsset-example.net.' at your zone's parent

#
#	README.logging
#
#	Introduction into the new logging feature 
#	available since v0.96
#	Per domain logging is enabled since v1.0
#	

In previous version of dnssec-signer every message was written
to the default stdout and stderr channels, and the logging itself
was handled by a redirection of those chanels to the logger command
or to a file.

Since v0.96, the dnssec-signer command is able to log all messages
by itself. File and SYSLOG logging is supported.

To enable the logging into a file channel, you have to specify
the file or directory name via the commandline option -L (--logfile)
or via the config file parameter "LogFile".
	LogFile: ""|"<file>"|"<directory>"	(default is "")
If a file is specified, than each run of dnssec-signer will append the
messages to that file. If a directory is specified, than a file with a
name of zkt-<ISOdate&timeUTC>+log" will be created on each dnssec-signer run.

Since v1.0 per domain logging is possible.
If the parameter "LogDomainDir:" is not empty, than the domain specific messages
are written to a separate log file with a name like "zkt-<domainname>+log" in the
directory specified by the parameter.
If "LogDomainDir:" is set to ".", then the logfile will be created in the domain
directory of the zone.

Logging into the syslog channel could be enabled via the config file
parameter "SyslogFacility".
	SyslogFacility:	NONE|USER|DAEMON|LOCAL0|..|LOCAL7 (default is USER)

For both channels, the log level could be set to one of six log levels:
	LG_FATAL, LG_ERROR, LG_WARNING
	LB_NOTICE, LG_INFO, LG_DEBUG

The loglevel is settable via the config file parameter :
	SyslogLevel: FATAL|ERROR|WARNING|NOTICE|INFO|DEBUG
		(default is ERROR)
and
	LogLevel: FATAL|ERROR|WARNING|NOTICE|INFO|DEBUG
   		(default is NOTICE)

All the log parameters are settable on the commandline via the generic
option -O "optstring" (--config-option="optstring").

A verbose message output to stdout could be achieved by the commandline
option -v (or -v -v).
If you like to have this verbose messages also logged with a level of LG_DEBUG
you should enable this by setting the config file option
"VerboseLog" to a value of 1 or 2.

Current logging messages:
	LG_FATAL: Not all of the fatal errors are logged
		(e.g.: config file or command line option fatal errors are
		not logged)
	LG_ERROR: All error messages will be logged
	LG_WARNING: KSK lifetime expiration
	LG_NOTICE:
		Start and stop of dnssec-signer
		Re-signing events 
		Key rollover events
		KSK key generation and revoking
		Zone reload resp. freeze/thaw of dynamic zone
	LG_INFO:
		Messages for key generation/removal and ksk rollover
	LG_DEBUG: all "verbose" (-v) and "very verbose" (-v -v)  messages

Some recomended and useful logging settings

- The default setting
	LogFile: ""
	SyslogFacility: USER
	SyslogLevel: NOTICE
	VerboseLog: 0

- Setting as in version v0.95
	LogFile: "zkt-error.log"	# or a directory for separate logfiles
	LogLevel: ERROR
	SyslogFacility: NONE
	VerboseLog: 0

- Setting as in previous versions
	LogFile: ""
	SyslogFacility: NONE
	VerboseLog: 0

- Recommended setting for normal usage
	LogFile: "zkt.log"	# or a directory for separate logfiles
	LogLevel: ERROR
	SyslogFacility: USER
	SyslogLevel: NOTICE
	VerboseLog: 0
	
- Recommended setting for debugging
	LogFile: "zkt.log"	# or a directory for separate logfiles
	LogLevel: DEBUG
	SyslogFacility: USER
	SyslogLevel: NOTICE
	VerboseLog: 2

                          BIND 9 IDN support

	       Japan Network Information Center (JPNIC)


* Compilation & installation

0. Prerequisite

You have to build and install idnkit before building bind9.

1. Running configure script

Run `configure' in the top directory.  See `README' for the
configuration options.

The following four options to `configure' are relevant to IDN.  You
should at least specify `--with-idn' option to enable IDN support.

    --with-idn[=IDN_PREFIX]
	To enable IDN support, you have to specify `--with-idn' option.
	The argument IDN_PREFIX is the install prefix of idnkit.  If
	IDN_PREFIX is omitted, PREFIX (derived from `--prefix=PREFIX')
	is assumed.

    --with-libiconv[=LIBICONV_PREFIX]
	Specify this option if idnkit you have installed links GNU
	libiconv.  The argument LIBICONV_PREFIX is install prefix of
	GNU libiconv.  If the argument is omitted, PREFIX (derived
	from `--prefix=PREFIX') is assumed.

	`--with-libiconv' is shorthand option for GNU libiconv.

	    --with-libiconv=/usr/local

	This is equivalent to:

	    --with-iconv='-L/usr/local/lib -R/usr/local/lib -liconv'

	`--with-libiconv' assumes that your C compiler has `-R'
	option, and that the option adds the specified run-time path
	to an executable binary.  If `-R' option of your compiler has
	different meaning, or your compiler lacks the option, you
	should use `--with-iconv' option instead.  Binary command
	without run-time path information might be unexecutable.
	In that case, you would see an error message like:

	    error in loading shared libraries: libiconv.so.2: cannot
	    open shared object file

	If both `--with-libiconv' and `--with-iconv' options are
	specified, `--with-iconv' is prior to `--with-libiconv'.

    --with-iconv=ICONV_LIBSPEC
	If your libc doesn't provide iconv(), you need to specify the
	library containing iconv() with this option.  `ICONV_LIBSPEC'
	is the argument(s) to `cc' or `ld' to link the library, for
	example, `--with-iconv="-L/usr/local/lib -liconv"'.
	You don't need to specify the header file directory for "iconv.h"
	to the compiler, as it isn't included directly by bind9.

    --with-idnlib=IDN_LIBSPEC
	With this option, you can explicitly specify the argument(s)
	to `cc' or `ld' to link the idnkit's library, `libidnkit'.  If
	this option is not specified, `-L${PREFIX}/lib -lidnkit' is
	assumed, where ${PREFIX} is the installation prefix specified
	with `--with-idn' option above.  You may need to use this
	option to specify extra arguments, for example,
	`--with-idnlib="-L/usr/local/lib -R/usr/local/lib -lidnkit"'.

Please consult `README' for other configuration options.

Note that if you want to specify some extra header file directories,
you should use the environment variable STD_CINCLUDES instead of
CFLAGS, as described in README.

2. Compilation and installation

After running "configure", just do

	make
	make install

for compiling and installing.


* Contact information

Please see http://www.nic.ad.jp/en/idn/ for the latest news
about idnkit.

Bug reports and comments on this kit should be sent to
mdnkit-bugs@nic.ad.jp and idn-cmt@nic.ad.jp, respectively.

; Id: README.idnkit,v 1.1 2009/12/04 20:14:28 each Exp 

				idnkit
	     -- internationalized domain name toolkit --
			     version 1.0
	       Japan Network Information Center (JPNIC)


Table of contents of this file:

  1. Overview
  2. Directory structure of this distribution
  3. Supported systems
  4. Contact information

See the file INSTALL for compilation and installation instructions.
See the file NEWS for a list of major changes in the current release.


1. Overview

idnkit, which was originally known as mDNkit, is a toolkit for handling
internationalized domain names.  To handle such names, the following
features are required:

    + Encoding conversion
	Multilingualized domain names have to be converted from
	the encoding application uses (local encoding) to
	the encoding used for name resolution (IDN encoding), and
	vice versa.  Since domain names in IDN encoding just look
	like good old ASCII domain names, the encoding is also known
	as ASCII-compatible encoding (ACE).

    + NAMEPREP
	Name preparation of domain names before converting to
	IDN encoding.  Basically this is a normalization process
	of the domain names.

These conversion/nameprep processes to domain names have to be
performed before they are sent to DNS servers.  And since the
processed domain names (in IDN encoding) consist of only legal ASCII
characters, no changes are required to DNS servers.
	
idnkit provides several ways for adding these features.

This kit consists of following components.

    + library for handling internationalized domain names
      (libidnkit, libidnkitlite)
	Those libraries implement encoding conversion and nameprep.
	They provide easy-to-use APIs for handling internationalized
	domain name in your applications.

	Both libraries provide almost the same API.  The difference
	between them is that libidnkit internally uses iconv() function
	to provide encoding conversion from UTF-8 to the local encoding
	(such as iso-8859-1, usually determined by the current locale),
	and vise versa.  The idnconv and runidn commands in this toolkit
	use libidnkit.

	libidnkitlite is lightweight version of libidnkit.  It assumes
	local encoding is UTF-8 so that it never uses iconv().

    + codeset conversion utility (idnconv)
	This command can convert internationalized domain name written
	in local encoding (e.g. EUC-JP) to ACE or the opposite direction.
	It can also convert named.conf and zone master files for BIND.

    + command which adds IDN feature dynamically to unix applications 
      (runidn)
	This command enables normal applications to handle
	internationalized domain names by dynamically attaching special
	library to them.  See ``2. using runidn'' below.

	Note that this command is not installed unless you specify the
	`--enable-runidn' option to `configure'.  See the file `INSTALL'
	for more information about installation.

    + patch for BIND9 that adds IDN capability
	This patch adds IDN capability to BIND9.  It adds encoding
	conversion and nameprep features to `dig', `host' and
	`nslookup'.  With the patch, those commands become capable of
	internationalized domain names.

    + IDN wrapper for Windows applications
        On windows, name resolving request is passed to WINSOCK DLL. So,
        replacing WINSOCK DLL with multi-lingual domain name version 
        makes legacy windows applications compatible with IDN.  This is
	wrapper DLL for WINSOCK's name resolving functions. See
	``3. using IDN wrapper'' below.


2. Directory structure of this distribution

Below is a directory structure of this distribution with some 
important files and their brief description.

    +README             this file
    +README.ja          .. in Japanese
    +INSTALL            compilation and installation instructions.
    +INSTALL.ja         .. in Japanese
    +DISTFILES          list of files in this distribution
    +NEWS               what's new in this version
    +ChangeLog          list of changes
    +Makefile.in        toplevel makefile template
    +configure          a `configure' script
    +include/
    |   +config.h.in    template header file for configuration
    |   +idn/           header files for libidnkit and libindkitlite
    |   +mdn/           header files for backward compatibility
    +lib/               source for libidnkit and libidnkitlite
    +patch/
    |   +bind9/         BIND9 patch
    +tools/
    |   +idnconv/       source for idnconv command
    |   +runidn/        source for runidn command
    +util/              utilities
    +wsock/             source for IDN wrapper


3. Supported systems

We've had successful builds on the following systems:

    -------------------------------------------------------------------
    OS                      iconv library      configure options
    ===================================================================
    FreeBSD 4.7-RELEASE     GNU libiconv       --with-libiconv
    for Intel               1.8
    -------------------------------------------------------------------
    Red Hat Linux 7.3       standard library   none
    for Intel               (glibc)
    -------------------------------------------------------------------
    Solaris 9               standard library   none
    for Sparc               (libc)
    -------------------------------------------------------------------

The latest information about supported/unsupported systems will be
found at the mdnkit FAQ page linked from:

	http://www.nic.ad.jp/en/idn/


4. Contact information

Please see

	http://www.nic.ad.jp/en/idn/

for the latest news about this kit.
Bug reports and comments on this kit should be sent to

	mdnkit-bugs@nic.ad.jp
and
	idn-cmt@nic.ad.jp

respectively.


; Id: README,v 1.1 2003/06/04 00:24:58 marka Exp 

				idnkit
		  -- $B9q:]2=%I%a%$%sL>%D!<%k%-%C%H(B --
			    $B%P!<%8%g%s(B 1.0
       ($B<R(B) $BF|K\%M%C%H%o!<%/%$%s%U%)%a!<%7%g%s%;%s%?!<(B (JPNIC)


$B$3$N%U%!%$%k$NL\<!(B:

  1. $B35MW(B
  2. $B%G%#%l%/%H%j9=@.(B
  3. $BK\%-%C%H$,BP1~$7$F$$$k%7%9%F%`(B
  4. $BK\%-%C%H$K4X$9$k:G?7>pJs!"Ld$$9g$o$;$K$D$$$F(B

$B%3%s%Q%$%k$*$h$S%$%s%9%H!<%kJ}K!$K$D$$$F$O!"(BINSTALL.ja $B$H$$$&%U%!%$%k$r(B
$B$4Mw2<$5$$!#$3$N%P!<%8%g%s$G$N<g$JJQ99E@$KIU$$$F$O!"(B($B1QJ8$G$9$,(B) NEWS 
$B$H$$$&%U%!%$%k$r$4Mw2<$5$$!#(B


1. $B35MW(B

idnkit ($B5l>N(B mDNkit) $B$H$O9q:]2=%I%a%$%sL>$r07$&$?$a$N%D!<%k%-%C%H$G$9!#(B
$B9q:]2=%I%a%$%sL>$r07$&$?$a$K$O!"<!$N$h$&$J5!G=$,MW5a$5$l$^$9!#(B

$B!&%(%s%3!<%G%#%s%0JQ49(B
  $B9q:]2=%I%a%$%sL>$KBP$7$F!"$=$N%(%s%3!<%G%#%s%0$r%"%W%j%1!<%7%g%s$,;H(B
  $BMQ$7$F$$$k$b$N(B ($B%m!<%+%k%(%s%3!<%G%#%s%0(B) $B$+$iL>A02r7h$KMQ$$$i$l$k$b(B
  $B$N(B (IDN $B%(%s%3!<%G%#%s%0(B) $B$X$NJQ49!"$^$?$=$N5UJ}8~$NJQ49$r9T$$$^$9!#(B
  $B%I%a%$%sL>$r(B IDN $B%(%s%3!<%G%#%s%0$K$7$?7k2L$O!"=>Mh$N%I%a%$%sL>$HF1(B
  $B$8$/(B ASCII $BJ8;z$N$_$G9=@.$5$l$k$N$G!"(BIDN $B%(%s%3!<%G%#%s%0$O(BASCII $B8_(B
  $B49%(%s%3!<%G%#%s%0(B (ACE) $B$H$b8F$P$l$^$9!#(B

$B!&(BNAMEPREP
  $B%I%a%$%sL>$r(B IDN $B%(%s%3!<%G%#%s%0$KJQ49$9$kA0$K!"L>A0$N@55,2=$r9T$$(B
  $B$^$9!#$3$l$r(B NAMEPREP $B$H8F$S$^$9!#(B

$B>e5-$N=hM}$O%I%a%$%sL>$r(B DNS $B%5!<%P$KAw$kA0$K<B9T$9$kI,MW$,$"$j$^$9!#(B
$B=hM}:Q$N%I%a%$%sL>$O(B ASCII $BJ8;z$+$i9=@.$5$l!"=>Mh$N(B ASCII $B%I%a%$%sL>$H(B
$B$7$F$b@5$7$$7A<0$K$J$C$F$$$k$N$G!"(BDNS $B%5!<%PB&$K$O$J$s$NJQ99$bI,MW$"$j(B
$B$^$;$s!#(B

$B$3$NG[I[%-%C%H$K$O<!$N$h$&$J$b$N$,4^$^$l$^$9!#(B

$B!&9q:]2=%I%a%$%sL>$r=hM}$9$k$?$a$N%i%$%V%i%j(B (libidnkit, libidnkitlite)
  $B$3$l$i$N%i%$%V%i%j$G$O!"%(%s%3!<%G%#%s%0JQ49$d(B NAMEPREP $B$N5!G=$r<B(B
  $BAu$7$F$*$j!"%"%W%j%1!<%7%g%s$,9q:]2=%I%a%$%sL>$r4JC1$K07$($k$h$&$K(B
  $B$9$k$?$a$N(B API $B$rHw$($F$$$^$9!#(B

  $B$I$A$i$N%i%$%V%i%j$b$[$H$s$IF1$8(B API $B$rDs6!$7$^$9!#N><T$NAj0cE@$G$9(B
  $B$,!"(Blibidnkit $B$N$[$&$O(B iconv() $B4X?t$r;H$C$F(B UTF-8 $B$H%m!<%+%k%(%s%3!<(B
  $B%G%#%s%0(B (iso-8859-1 $B$J$I!"DL>o$O(B locale $B$+$i7hDj$5$l$^$9(B) $B$H$N4V$N(B
  $BJQ495!G=$rDs6!$7$F$$$^$9!#K\%D!<%k%-%C%HIUB0$N(B idnconv $B$*$h$S(B runidn
  $B$O(B libidnkit $B$r;HMQ$7$F$$$^$9!#(B

  libidnkitlite $B$O(B libidnkit $B$N7ZNLHG$G$9!#%m!<%+%k%(%s%3!<%G%#%s%0$O(B
  $B>o$K(B UTF-8 $B$@$H2>Dj$7$F$*$j!"(Biconv() $B$O;HMQ$7$^$;$s!#(B

$B!&%3!<%I%;%C%H%3%s%P!<%?(B (idnconv)  
  $B%m!<%+%k%(%s%3!<%G%#%s%0(B ($B$?$H$($P(B EUC-JP) $B$G=q$+$l$?9q:]2=%I%a%$%s(B
  $BL>$+$i(B ACE $B$X$NJQ49$d!"$=$N5UJQ49$r9T$($k%3%^%s%I$G$9!#(BBIND $BMQ$N(B
  named.conf $B$d%>!<%s%^%9%?%U%!%$%k$rJQ49$9$k$3$H$b2DG=$G$9!#(B

$B!&(BUNIX $B%"%W%j%1!<%7%g%s$K(B IDN $B5!G=$rF0E*$KDI2C$9$k%3%^%s%I(B (runidn)
  UNIX $B$NDL>o$N%"%W%j%1!<%7%g%s$G9q:]2=%I%a%$%sL>$r<h$j07$&$?$a$K!"FC(B
  $BJL$J%i%$%V%i%j$rF0E*$K%j%s%/$9$k$?$a$N%3%^%s%I$G$9!#>\$7$/$O!"8e=R$N(B
  $B!V(B2. runidn $B$r;H$&!W$r8fMw$/$@$5$$!#(B

  $B$J$*!"(Bconfigure $B<B9T;~$K(B `--enable-runidn' $B$r;XDj$7$J$$$H!"(Brunidn $B$O(B
  $B%$%s%9%H!<%k$5$l$^$;$s!#%$%s%9%H!<%k$K4X$7$F!">\$7$/$O(B INSTALL.ja $B$H(B
  $B$$$&%U%!%$%k$r$4Mw$/$@$5$$!#(B

$B!&(BBIND9 $B$K(B IDN $B5!G=$rDI2C$9$k%Q%C%A(B
  BIND $B$K(B IDN $B$N5!G=$rDI2C$9$k%Q%C%A$G$9!#(B`dig'$B!"(B`host'$B!"(B`nslookup' $B$K(B
  $B%(%s%3!<%G%#%s%0JQ49$H(B NAMEPREP $B$N5!G=$rDI2C$7!"9q:]2=%I%a%$%sL>$,07(B
  $B$($k$h$&$K$7$^$9!#(B

$B!&(BWindows $B%"%W%j%1!<%7%g%sMQ$N(B IDN $B%i%C%Q!<(B
  WINSOCK $B$NL>A02r7h5!G=$KBP$9$k%i%C%Q!<(B DLL $B$G$9!#(BWindows $B>e$G$O!"L>(B
  $BA02r7h$NMW5a$O!"(BWINSOCK DLL $B$rDL$7$F9T$o$l$^$9!#$=$3$G!"$3$l$r9q:]2=(B
  $B%I%a%$%sMQ$N(B WINSOCK DLL $B$KCV$-49$($k$3$H$G!"=>Mh$N(B Windows $B%"%W%j%1!<(B
  $B%7%g%s$G$b(B IDN $B5!G=$r;HMQ$G$-$k$h$&$K$7$^$9!#>\$7$/$O!"8e=R$N(B
  $B!V(B3. IDN $B%i%C%Q!<$r;H$&!W$r8fMw$/$@$5$$!#(B


2. $B%G%#%l%/%H%j9=@.(B

$BG[I[%-%C%H$N%G%#%l%/%H%j9=@.$H!"<gMW$J%U%!%$%k$r<($7$^$9!#(B

    +README             $B1Q8lHG$N(B README
    +README.ja          $B$3$N%U%!%$%k(B
    +INSTALL            $B1Q8lHG$N(B INSTALL
    +INSTALL.ja         $B%3%s%Q%$%k$H%$%s%9%H!<%kJ}K!(B
    +DISTFILES          $BG[I[$5$l$k%U%!%$%k0lMw(B
    +NEWS               $B<g$JJQ99(B
    +ChangeLog          $BJQ99>\:Y(B
    +configure          configure $B%9%/%j%W%H(B
    +Makefile.in        $B%H%C%W%l%Y%k$N(B Makefile $B$N%F%s%W%l!<%H(B
    +include/
    |   +config.h.in    config.h $B$N%F%s%W%l!<%H(B
    |   +idn/           libidnkit, libidnkitlite $B$N%X%C%@%U%!%$%k(B
    |   +mdn/           $B5l%P!<%8%g%s$H$N8_49MQ%X%C%@%U%!%$%k(B
    +lib/               libidnkit, libidnkitlite $B$N%=!<%9(B
    +patch/
    |   +bind9/         BIND9 $BMQ%Q%C%A(B
    +tools/
    |   +idnconv/       idnconv $B%3%^%s%I$N%=!<%9(B
    |   +runidn/        runidn $B%3%^%s%I$N%=!<%9(B
    +util/              $B%f!<%F%#%j%F%#(B
    +wsock/             IDN $B%i%C%Q!<$N%=!<%9(B


3. $BK\%-%C%H$,BP1~$7$F$$$k%7%9%F%`(B

$B@5>o$K%$%s%9%H!<%k$G$-$k$3$H$,3NG'$G$-$F$$$k$N$O!"<!$N%7%9%F%`$G$9!#(B

    ------------------------------------------------------------------
    OS                      iconv              configure $B$N%*%W%7%g%s(B
    ==================================================================
    FreeBSD 4.7-RELEASE     GNU libiconv       --with-libiconv
    Intel                   1.8
    -------------------------------------------------------------------
    Red Hat Linux 7.3       $BI8=`%i%$%V%i%j(B     $B$J$7(B
    Intel                   (glibc)
    ------------------------------------------------------------------
    Solaris 9               $BI8=`%i%$%V%i%j(B     $B$J$7(B
    Sparc                   (libc)
    -------------------------------------------------------------------

$BK\%-%C%H$,BP1~$7$F$$$k(B/$B$7$F$$$J$$%7%9%F%`$K4X$9$k:G?7$N>pJs$O!"<!$N$H(B
$B$3$m$+$iC)$l$k(B mdnkit FAQ $B$N%Z!<%8$K5-$5$l$F$$$^$9!#(B

	http://www.nic.ad.jp/ja/idn/


4. $BK\%-%C%H$K4X$9$k:G?7>pJs!"Ld$$9g$o$;$K$D$$$F(B

$BK\%-%C%H$K4X$9$k:G?7>pJs$K$D$$$F$O!"(B

	http://www.nic.ad.jp/ja/idn/

$B$r;2>H$7$F$/$@$5$$(B
$BK\%-%C%H$K4X$9$k%P%0%l%]!<%H$*$h$S%3%a%s%H$O!"$=$l$>$l(B 

	mdnkit-bugs@nic.ad.jp
$B$*$h$S(B
	idn-cmt@nic.ad.jp

$B$X$*4j$$$7$^$9!#(B


; Id: README.ja,v 1.1 2003/06/04 00:24:59 marka Exp 
To build idnkit for Windows, follow the instruction below.

To build Windows version, you need `iconv' library.  A LGPL
implemenation is available from the following place.

    http://www.gnu.org/software/libiconv/

Follow the instructions described in README.woe32 file which can be
found in the distribution, and you'll get a DLL vesion of `libiconv'.
Copy the DLL (iconv.dll), the header (iconv.h) and the import library
(iconv.lib) here.

Then go to the top directory and run the following command.

	nmake -f make.wnt

; Id: README.WIN,v 1.1 2003/06/04 00:27:32 marka Exp 

    idn wrapper - Client Side IDN Conversion Software for Windows

    Copyright (c) 2000,2001,2002 Japan Network Information Center.
                All rights reserved.

    *** NOTICE ******************************************************
    If you have installed mDN Wrapper (former version of idn wrapper)
    on your system, you should unwrap all the programs before
    installing idn wrapper.
    *****************************************************************


1. Introduction

    For supporting internationalized domain names, each client
    application should convert domain names (their encodings) to that
    DNS server accepts.  This requires applications to handle
    internationalized domain names in its core, and it is the vendor's
    responsibility to make their programs IDN-compatible.

    Although there are ongoing efforts in IETF to standardize IDN
    framework (architecture, encoding etc.) and several RFCs are
    expected to be published soon as the result, not many applications
    support IDN to this date.

    So, there are needs for some helper application which makes legacy
    applications IDN-aware.  `runidn' in idnkit is one of such
    solutions for Unix-like operating systems, and this software, `idn
    wrapper' is the one for Windows.

    On windows, name resolving request is passed to WINSOCK DLL.  idn
    wrapper replaces WINSOCK DLL with the one that can handle IDN,
    which makes legacy windows applications compatible with IDN.

2. Architecture

2.1. Wrapper DLL

    Wrapper DLL resides between application and original DLL.  It
    intercept application's calls to original DLL, and preforms some
    additional processing on those calls.

    +------------+  Call  +------------+  Call  +------------+
    |            |------->|            |------->|            |
    |Application |        |Wrapper DLL |        |Original DLL|
    |            |<-------|            |<-------|            |
    +------------+ Return +------------+ Return +------------+
                           additional
			   processing
			   here

    DLL call from apllication is passed to wrapper DLL.  Wrapper DLL
    then performs some additional processing on that call, and then
    calls original DLL.  Also, result from original DLL will once passed
    to wrapper DLL and wrapper does additional process on that result,
    and finally result will passed to the application.

    idn wrapper provides wrapper DLLs for WINSOCK,
    
        WSOCK32.DLL     WINSOCK V1.1
	WS2_32.DLL      WINSOCK V2.0

    to resolve multi-lingual domain names.

2.2. Wrapping APIs

    idn wrapper performs additional processing on name resolving APIs in
    WINSOCK, listed below.

    both WINSOCK 1.1, WINSOCK 2.0
    
        gethostbyaddr
	gethostbyname
	WSAAsyncGetHostByAddr
	WSAAsyncGetHostByName
	
    only in WINSOCK 2.0
    
	getaddrinfo
	freeaddrinfo
	getnameinfo
        WSALookupServiceBeginA
	WSALookupServiceNextA
	WSALookupServiceEnd

    Some applications do not use these APIs to resolve domain names. 
    `nslookup' is one of those programs. `nslookup' builds and parse DNS
    messages internally and does not use WINSOCK's name resolver APIs.
    idn wrapper cannot make those programs IDN-aware.
    
    NOTE:
      WINSOCK 2.0 also contains WIDE-CHARACTER based name resolution
      APIs,

          WSALookupServiceBeginW
          WSALookupServiceNextW

      idn wrapper does not wrap these APIs.  These APIs are used in
      Microsoft's own internationalization framework.  It is dangerous
      to convert to another internationalization framework.
    
2.3. Other APIs in WINSOCK

    For other APIs in WINSOCK, idn wrapper does nothing, only calls
    original DLL's entries.

    idn wrapper copies original WINSOCK DLLs with renaming
    as below, and forward requests to them.

        wsock32.dll     ->  wsock32o.dll
	ws2_32.dll      ->  ws2_32o.dll

    Wrappper DLL will be installed with original DLL names. So after
    installation of idn wrapper, WINSOCK DLLs should be

        wsock32.dll         idn wrapper for WINSOCK V1.1
	ws2_32.dll          idn wrapper for WINSOCK V2.0
	wsock32o.dll        Original WINSOCK V1.1 DLL
	ws2_32o.dll         Original WINSOCK V2.0 DLL 

2.4. Asynchronous API

    Domain name conversion take place on
    
        request to DNS

            convert from local encoding to DNS compatible encoding

        response from DNS

            convert from DNS encoding to local encoding

    For synchronous APIs, local to DNS conversion is done before calling
    original API, and after return from original API, name should be
    converted from DNS encoding to local encoding.

    But WINSOCK having some asynchronous APIs, such as

	WSAAsyncGetHostByAddr
	WSAAsyncGetHostByName

    In these APIs, completion is notified with windows message.  To
    perform DNS to local conversion, wrapper should hook target window
    procedure to capture those completion messages.
    
    So, if asynchronous API was called, idn wrapper set hook to target
    window procedure (passed with API parameter).  If hook found
    notify message (also given with API parameter), then convert
    resulting name (in DNS encoding) to local encoding.
    
2.5. Installing Wrapper DLLs

    WINSOCK DLLs are placed at Windows's system directory.  To wrap
    WINSOCK DLLs, one could do following sequence at system directory.

        + Rename Original WINSOCK DLLs

	    ren wsock32.dll wsock32o.dll
	    ren ws2_32.dll  ws2_32o.dll

        + Install (copy in) Wrapper DLLs

	    copy somewhere\wsock32.dll wsock32.dll
	    copy somewhere\ws2_32.dll  ws2_32.dll
	    copy another DLLs also

    However, replacing DLLs in Window's system directory is very
    dangerous:

    a)  If you re-install idn wrapper again, original WINSOCK DLLs
        may be lost.

    b)  Some application or service pack will replace WINSOCK DLLs.  It
        may corrupt WINSOCK environment.

    If these happen, at least networking does not work, and worse,
    Windows never startup again.

    So, idn wrapper usually does not wrap in the system directory, but wrap in
    each indivisual application's directory.

    In Windows, DLL will be searched in the following places:
    
        Application's Load Directory
	%SystemRoot%\System32
	%SystemRoot%
	Directories in PATH

    and loaded & linked first found one.  So if installed wrapper DLLs is
    found on application's load directory, the application's call to
    WINSOCK will wrapped.

    But some applications or DLLs are binded to specific DLL, they do
    not rely on above DLL's search path.  For those applcaitons or DLLs,
    idn wrapper (in standard installation) cannot wrap them.

    NOTE:   Netscape is one of those program.  It cannot be wrapped if
            installed to applications directory.  Also WINSOCK DLLs are
            also binded to related DLLs in system directory.  On the
            other hand, Internet Explore or Window Media Player relys on
            standard DLL search path, and well wrapped with idn wrapper.

2.6. At which point conversion applied

    If windows supporting WINSOCK 2.0, there are DLLs one for 1.1 and
    another for 2.0, and call to WINSOCK 1.1 will redirected to 2.0 DLL.

        +------------+  Call  +------------+  Call  +------------+
        |            |------->|            |------->|            |
        |Application |        |WINSOCK 1.1 |        |WINSOCK 2.0 |
        |            |<-------|            |<-------|            |
        +------------+ Return +------------+ Return +------------+

    In this case, calls to 1.1 and 2.0 are both passed to 2.0 DLL.  So
    conversion will done in WINSOCK 2.0 DLL side.

    If windows only supports WINSOCK 1.1, there's 1.1 DLL only.

        +------------+  Call  +------------+
        |            |------->|            |
        |Application |        |WINSOCK 1.1 |
        |            |<-------|            |
        +------------+ Return +------------+

    In this case, conversion must done in 1.1 DLL.

    If idn wrapper was installed on system directory, DLLs will work as
    described above.  But if wrapper was installed on application's
    directory, call/return sequence changes.  Original WINSOCK 1.1 DLL
    in windows seems binded to specific WINSOCK 2.0 DLL, placed at
    window's system diretory.  So call from WINSOCK 1.1 to WINSOCK 2.0
    will passed to original DLL (in system directory) and never passed
    to wrapper DLL in application's directory.  So in this case, both
    1.1 and 2.0 DLLs should coonvert domain name encodings.
    
    These DLL binding is not documented.  It may be change on OS
    versions or DLL versions.  So, mDn wrapper determines place of
    conversion on registry value.  With this registry value, idn
    wrappper absolb OS/DLL variations.
    
    Registry values for idn wrapper will placed under

        HKEY_LOCAL_MACHINE\SOFTWARE\JPNIC\IDN
	HKEY_CURRENT_USER\SOFTWARE\JPNIC\IDN

    Place of conversion is determined with registry value "Where",
    
        Registry Value "Where"   REG_DWORD
	    
	    0       both on WINSOCK 1.1 and WINSOCK 2.0
	    1       if WINSOCK 2.0 exist, only in WINSOCK 2.0
	            otherwise, convert on WINSOCK 1.1
            2       only in WINSOCK 1.1
	    3       only in WINSOCK 2.0

    If you install idn wrapper into application's directory, use "0".
    If you install idn wrapper into system directory, use "1".  If there
    are no "Where" value, idn wrapper uses "0" as default, it is suited
    to installation into application's directory (default installation).

2.7. Converting From/To

    Wrapper DLL convert resolving domain name encoded with local code to
    DNS server's encoding.  Also, wrapper DLL convert resulting name (
    encoded with DNS's encoding) back to local encoding.
    
    There are several proposals for DNS encodings to handle multi-lingual
    domain names.  Wrapper DLL should be configured to convert to one of
    those encodings.  This DNS side encoding will specified with
    registry.  When installing idn wrapper, this registry will set to
    some (yet undefined) DNS encoding.
    
    Registry values for idn wrapper will placed under

        HKEY_LOCAL_MACHINE\SOFTWARE\JPNIC\IDN
	HKEY_CURRENT_USER\SOFTWARE\JPNIC\IDN

    DNS encoding name will given with registry value (REG_SZ) of "Encoding",
    this name must be one of encoding names which 'libmdn' recognize.

        Registry Value "Encoding"   REG_SZ
	
	    Encoding name of DNS server accepts.
    
    Local encodings (Windows Apllication Encodings) is generally
    acquired from process's code page.  'iconv' library, used for idn
    wrapper, generally accepts MS's codepage names.

    Some windows apllication encode domain name with some specific multi-
    lingual encoding. For example, if you configured IE to use UTF-8,
    then domain names are encoded with UTF-8. UTF-8 is one of proposed
    DNS encoding, but DNS server may expect another encoding.
    
    For those cases, idn wrapper accept program specific encoding as
    local encoding.  These program specific local encoding should be
    marked in registry.
    
    Program specific registry setting will placed under

        HKEY_LOCAL_MACHINE\SOFTWARE\JPNIC\IDN\PerProg
	HKEY_CURRENT_USER\SOFTWARE\JPNIC\IDN\PerProg
    
    using program name (executable file name) as key.  For example,
    setting specific to Internet Explore, it executable name is 
    "IEXPLORE", will plcaed at

        HKEY_LOCAL_MACHINE\SOFTWARE\JPNIC\IDN\PerProg\IEXPLORE

    Local encoding name will specified with registry value (REG_SZ) of 
    "Encoding".  This name must be one of encoding names which '
    recognize.libmdn'

        Registry Value "Encoding"   REG_SZ
	
	    Encoding name of application program encodes, if it is not
            system's default encoding.

3. Setup and Configuration

    idn wrapper wraps WINSOCK DLL by placing wrapper (fake) DLLs in
    the application's directory.  For the installation, idn wrapper
    comes with a setup program and a configuration program.

    NOTE:   You can also install idn wrapper DLLs in the Windows
            system directory.  But this installation is very dangerous
	    and may cause severe problems in your system.
	    You should try it at your own risk.

3.1. Setup Program

    To install idn wrapper, run "setup.exe".  Setup program will do:
    
    Installing Files
    
        Copy idn wrapper files (DLL, Program EXE, etc) into diretory
	
	    "\Program Files\JPNIC\idn wrapper"

        This directory may be changed on setup sequence.

    Setting registry entries

        Setup program will create keys and values under registry:
	
	    "HKEY_LOCAL_MACHINES\Software\JPNIC\IDN"

	InstallDir	REG_SZ	"<installation directory>"
	    Pathname of the idn wrapper's installation directory.
	    The installer makes copies of the original WINSOCK DLLs
	    in that directory, which is referenced by the idn wrapper's
	    fake DLLs.
    
        ConfFile        REG_SZ  "<installation directory>\idn.conf"
	    Name of the idnkit's configuration file, which defines
	    various parameter regarding multilingual domain name
	    handling.  See the contents of the file for details.
            This value can be changed with the Configuration Program
	    or the registry editor.

	LogFile		REG_SZ	"<installation directory>\idn_wrapper.log"
	    Name of the idn wrapper's log file.
            This value can be changed with the Configuration Program
	    or the registry editor.

	LogLevel	DWORD	-1
	    Logging level.  Default is -1, which indicates no logging
	    is made.  This value can be changed with the Configuration
	    Program or the registry editor.

        PerProg         KEY
	
	    Under this key, idn wrapper set program specific values. idn
            wrapper uses program's executable name as key, and put
            values under that key.
	    
	    PerProg\<progname>\Where    REG_DWORD Encoding Position
	    PerProg\>progname>\Encoding REG_SZ    Local Encoding Name

            Configuration program set local encpoding name.  "Where"
            value is usually not required in standard installation.  If
            you installed idn wrapper in system directory, chanage
            "Where" values to fit your environment.

    Creating ICON
    
        Setup program will create program icon for idn wrapper's
        configuration program, and put it into "Start Menu".  You can
        start configuration program with it.
	   
3.2. Configuration Program

    Configuration program is a tool for wrap specific program, or unwrap
    programs.  If you start "Configuration Program", you'll get window
    like this.

    +---+-------------------------------------------------+---+---+---+
    |   | idn wrapper - Configuration                     | _ | O | X |
    +---+-------------------------------------------------+---+---+---+
    |          idn wrapper Configuration Program version X.X          |
    +-----------------------------------------------------------------+
    |                  Wrapped Program                    +---------+ |
    | +---------------------------------------------+---+ | Wrap..  | |
    | |                                             | A | +---------+ |
    | |                                             +---+ +---------+ |
    | |                                             |   | | Unwrap..| |
    | |                                             |   | +---------+ |
    | |                                             |   | +---------+ |
    | |                                             |   | |UnwrapAll| |
    | |                                             |   | +---------+ |
    | |                                             |   | +---------+ |
    | |                                             |   | |RewrapAll| |
    | |                                             |   | +---------+ |
    | |                                             |   | +---------+ |
    | |                                             |   | |  Log..  | |
    | |                                             |   | +---------+ |
    | |                                             |   | +---------+ |
    | |                                             +---+ |Advanced.| |
    | |                                             | V | +---------+ |
    | +---+-------------------------------------+---+---+ +---------+ |
    | | < |                                     | > |     |  Exit   | |
    | +---+-------------------------------------+---+     +---------+ |
    +-----------------------------------------------------------------+

    Listbox contains list of current wrapped programs.  Initially it is
    empty.  
    
    To wrap a program, press button "wrap".  You'll get following dialog.
    
    +---+-------------------------------------------------+---+---+---+
    |   | idn wrapper - Wrap Executable                   | _ | O | X |
    +---+-------------------------------------------------+---+---+---+
    |           +----------------------------------------+ +--------+ |
    |  Program: |                                        | |Browse..| |
    |           +----------------------------------------+ +--------+ |
    |           +----------+                                          |
    | Encoding: |          |  o Default  o UTF-8                      |
    |           +----------+                                          |
    |           [] Force local DLL reference                          |
    +-----------------------------------------------------------------+
    |                                           +--------+ +--------+ |
    |                                           |  Wrap  | | Cancel | |
    |                                           +--------+ +--------+ |
    +-----------------------------------------------------------------+

    First, enter program (executable name with full path) or browse
    wrapping exectable from file browser. Then set local encoding of
    that program.  Usually use "Default" as local encoding. If target
    program uses internationalized encoding, then specify "UFT-8". 

    The "Force local DLL reference" button controls the DLL search
    order of the program to be wrapped (Windows95 does not have this
    capability, hence this button does not appear).  If it is checked,
    DLLs in the local directory (the directory which the executable
    file is in) are always preferred, even if the executable specifies
    otherwise.  If you have problem with wrapping, checking this
    button may solve the problem, but it is also possible that it
    causes other problem.

    Finally, put "wrap" button to wrap specified program with given
    encoding. Wrapped program will be listed in listbox of the first
    window.

    When you install a new version of idn wrapper, you have to re-wrap
    your programs in order to update DLLs used for wrapping.  "Rewrap
    all" button is provided for this purpose.  Just press the button,
    and all the currently wrapped programs will be re-wrapped.

    To unwrap a program, press button "unwrap".  You'll get following 
    confirmating dialog.
    
    +---+-------------------------------------------------+---+---+---+
    |   | idn wrapper - Unwrap Executable                 | _ | O | X |
    +---+-------------------------------------------------+---+---+---+
    |           +---------------------------------------------------+ |
    | Program:  |                                                   | |
    |           +---------------------------------------------------+ |
    +-----------------------------------------------------------------+
    |                                           +--------+ +--------+ |
    |                                           | Unwrap | | Cancel | |
    |                                           +--------+ +--------+ |
    +-----------------------------------------------------------------+

    If you unwrap a program, the program will be vanished from listbox
    of the first window.

    Also "Unwrap all" button is provided to unwrap all the programs
    that are currently wrapped.

    To configure logging, press button "log".  You'll get the following
    dialog.

    +---+-------------------------------------------------+---+---+---+
    |   | idn wrapper - Log Configuration                 | _ | O | X |
    +---+-------------------------------------------------+---+---+---+
    |    Log Level: o None o Fatal o Error o Warning o Info o Trace   |
    |                                                                 |
    |              +------------------------------------+ +---------+ |
    |     Log File:|                                    | | Browse..| |
    |              +------------------------------------+ +---------+ |
    |               +------+ +--------+                               |
    |Log Operation: | View | | Delete |                               |
    |               +------+ +--------+                               |
    +-----------------------------------------------------------------+
    |                                           +--------+ +--------+ |
    |                                           |   OK   | | Cancel | |
    |                                           +--------+ +--------+ |
    +-----------------------------------------------------------------+

    Logging level can be selected from the followings.
	None	no logging at all
	Fatal   only records fatal errors
	Error	also records non-fatal errors
	Warning	also records warning mssages
	Info	also records informational messages
	Trace	also records trace information
    Note that these levels are for log output from IDN library (idnkit.dll).
    idn wrapper itself supports only off (None) and on (the rest).

    Pathname of the log file can also be specified with this dialog.

    You can view the current log file contents by pressing "View" button,
    or delete it by "Delete" button.

    Note that log level and log file configuration doesn't affect already
    running processes.

    Press "advanced" button to invoke the advanced configuration dialog.
    This dialog is for advanced users and enables customization for
    some basic parameters which normal users need not change, since
    appropriate defaults are provided.

    +---+-------------------------------------------------+---+---+---+
    |   | idn wrapper - Advanced Configuration            | _ | O | X |
    +---+-------------------------------------------------+---+---+---+
    |                    IDN Wrapping Mode                            |
    |  o Wrap both WINSOCK 1.1 and WINSOCK 2.0                        |
    |  o Wrap only WINSOCK 1.1                                        |
    |  o Wrap only WINSOCK 2.0                                        |
    |  o Wrap only WINSOCK 2.0 if it exists.                          |
    |    Otherwise wrap only WINSOCK 1.1                              |
    +-----------------------------------------------------------------+
    |                       IDN Configuration                         |
    |               +--------------------------------+ +----------+   |
    |  Config File: |                                | | Browse.. |   |
    |               +--------------------------------+ +----------+   |
    |               +------+                                          |
    |               | Edit |                                          |
    |               +------+                                          |
    +-----------------------------------------------------------------+
    |                                           +--------+ +--------+ |
    |                                           |   OK   | | Cancel | |
    |                                           +--------+ +--------+ |
    +-----------------------------------------------------------------+

    With the dialog users can do the following configuration.

    Wrapping Mode
	Customize wrapping mode.  Normally the default item should be
	appropriate.  Changing it to other item may help when you
	have problems.

    IDN Configuration
	Set the configuration file for multilingual domain name handling.
	By pressing "Edit" button, you can edit then contents of the file.

4. Limitations

4.1. DLL Versions

    Wrapper DLL is tightly coupled with specific DLL version, because
    it must export all the entries including un-documented ones.
    If WINSOCK DLL version changed, idn wrapper may not work correctly.

    Current idn wrapper is tested on
    
        Win2000         (WINSOCK 1.1 + 2.0)
        WinME           (WINSOCK 1.1 + 2.0)

    But there are no assuarance for future versions of Windows.

4.2. DNS, WINS, LMHOSTS

    There are three name resolving methods in windows, DNS, WINS and
    LMHOSTS. Using idn wrapper, domain name conversion will performed 
    on all of thoses methods.  It may cause some trouble if windows 
    using WINS or LMHOSTS.  We recommend use DNS oly if you want to use
    idn wrapper.

4.3. Converting Names other than Domain Name

    In WINSOCK 2.0, there are generic name resolution APIs are
    introduced.
    
        WSALookupServiceBeginA
	WSALookupServiceNextA
	WSALookupServiceEnd

    They are use mainly domain name conversion now, but not limited to
    resolving domain name.  idn wrapper hooks this API and convert
    given name anyway.  This causes some trouble if conversion name is
    not domain name.

4.4. Applications don't use these APIa

    Some applications don't use these APIs to resolving domain names.
    For example, 'nslookup' issue DNS request locally.  For these
    applications, idn wrapper does not work.

4.5. Applications bound to specific WINSOCK DLL

    Some applications are bound to specific DLL, not relying on
    standard DLL search path. Netscape Communicator seems to be one of
    such programs.  idn wrapper in standard installation cannot wrap
    such programs.
    
    If you want to wrap those programs, you may use installation into
    system directory.  But this installation is very dangerous, for
    it is possible that your system cannot boot again.

5. Registry Setting - Summary

5.1. Priority of Setting

    Settings of idn wrapper is placed on registry 
    
        Software\JPNIC\IDN
	
    under HKEY_LOCAL_MACHINE or HKEY_CURRENT_USER.  idn wrapper first
    read HKEY_LOCAL_MACHINE, and if HKEY_CURRENT_USER exist, overwrite
    with this one.  Usually set HKEY_LOCAL_MACHINE only.  But if you
    need per user setting, then set HKEY_CURRENT_USER.

    Note that the configuration program reads/writes only
    HKEY_LOCAL_MACHINE.

5.2. Registry Key

    There's common settings and per program settings.
    
_Common Settings

	Software\JPNIC\IDN\InstallDir	 Installation directory
        Software\JPNIC\IDN\Where         Where to convert encoding
	                    0: both WINSOCK 1.1 and WINSOCK 2.0
                            1: if WINSOCK 2.0 exist, convert at 2.0 DLL
                               if WINSOCK 1.1 only, convert at 1.1 DLL
			    2: only in WINSOCK1.1
			    3: only in WINSOCK2.0
        Software\JPNIC\IDN\ConfFile	 idnkit Configuration File
        Software\JPNIC\IDN\LogFile       Log File
        Software\JPNIC\IDN\LogLevel      Log Level

_Per Program Settings

    Converting position and program's local encoding may be set per
    program bases.

        Software\JPNIC\IDN\PerProg\<name>\Where
        Software\JPNIC\IDN\PerProg\<name>\Encoding

    If not specified, the following values are assumed.
    
        Where       0 (both 1.1 DLL and 2.0 DLL)
	Encoding    [process's code page]
OVERVIEW:

DLZ (Dynamically Loadable Zones) is an extention to BIND 9 that
allows zone data to be retrieved directly from an external database.
There is no required format or schema.  DLZ drivers exist for several
different database backends including PostgreSQL, MySQL, and LDAP and
can be written for any other.

Historically, DLZ drivers had to be statically linked with the named
binary and were turned on via a configure option at compile time (for
example, "configure --with-dlz-ldap").  Currently, the drivers provided
in the BIND 9 tarball in contrib/dlz/drivers are still linked this way.

However, as of BIND 9.8, it is also possible to link some DLZ modules
dynamically at runtime, via the DLZ "dlopen" driver, which acts as a
generic wrapper around a shared object that implements the DLZ API.  The
"dlopen" driver is linked into named by default, so configure options are
no longer necessary unless using older DLZ drivers.

When the DLZ module provides data to named, it does so in text format.
The response is converted to DNS wire format by named.  This conversion,
and the lack of any internal caching, places significant limits on the
query performance of DLZ modules.  Consequently, DLZ is not recommended
for use on high-volume servers.  However, it can be used in a hidden
master configuration, with slaves retrieving zone updates via AXFR.
(Note, however, that DLZ has no built-in support for DNS notify; slaves
are not automatically informed of changes to the zones in the database.)

CONFIGURING DLZ:

A DLZ database is configured with a "dlz" statement in named.conf.

    dlz example {
        database "dlopen driver.so <args>";
        search yes;
    };

This specifies a DLZ module to search when answering queries; the module
is implemented in "driver.so" and is loaded at runtime by the dlopen DLZ
driver.  Multiple "dlz" statements can be specified; when answering a
query, all DLZ modules with the "search" option set to "yes" will be
checked for an answer, and the best available answer will be returned
to the client.

The "search" option in this example can be omitted, as "yes" is the
default value.  If it is set to "no", then this DLZ module is *not*
searched for best-match when a query is received.  Instead, zones in
this DLZ must be separately specified in a zone statement.  This can
be useful when conventional zone semantics are desired but you wish
to use a different back-end storage mechanism than the standard zone
database.  For example, to use a DLZ module for an NXDOMAIN redirection
zone:

    dlz other {
        database "dlopen driver.so <args>";
        search no;
    };

    zone "." {
        type redirect;
        dlz other;
    };

EXAMPLE DRIVER:

This directory contains an example of an externally-lodable DLZ module,
dlz_example.c, which demonstrates the features of the DLZ API.  It sets up
a single zone, whose name is configured in named.conf.  The zone can answer
queries and AXFR requests, and accept DDNS updates.

By default, at runtime, the zone implemented by this driver will contain
an SOA, NS, and a single A record at the apex.  If configured in named.conf
to use the name "example.nil", then, the zone will look like this:

 example.nil.  3600    IN      SOA     example.nil. hostmaster.example.nil. (
                                               123 900 600 86400 3600
                                       )
 example.nil.  3600    IN      NS      example.nil.
 example.nil.  1800    IN      A       10.53.0.1

The driver is also capable of retrieving information about the querying
client, and altering its response on the basis of this information.  To
demonstrate this feature, the example driver responds to queries for
"source-addr.<zonename>/TXT" with the source address of the query.
Note, however, that this record will *not* be included in AXFR or ANY
responses.  (Normally, this feature would be used to alter responses in
some other fashion, e.g., by providing different address records for
a particular name depending on the network from which the query arrived.)

IMPLEMENTATION NOTES:

The minimal set of type definitions, prototypes, and macros needed
for implementing a DLZ driver is in ../modules/dlz_minimal.h.  Copy this
header file into your source tree when creating an external DLZ module.

The DLZ dlopen driver provides a set of callback functions:

  - void log(int level, const char *fmt, ...);

    Writes the specified string to the named log, at the specified
    log level.  Uses printf() format semantics.

  - isc_result_t putrr(dns_sdlzlookup_t *lookup, const char *type,
                       dns_ttl_t ttl, const char *data);

    Puts a DNS resource record into the query response, which
    referenced by the opaque structure 'lookup' provided by named.

  - isc_result_t putnamedrr(dns_sdlzallnotes_t *allnodes,
                            const char *name, const char *type,
                            dns_ttl_t ttl, const char *data);

    Puts a DNS resource record into an AXFR response, which is
    referenced by the opaque structure 'allnodes' provided by named.

  - isc_result_t writeable_zone(dns_view_t *view, const char *zone_name);

    Allows the DLZ module to inform named that a given zone can recieve
    DDNS updates.  (Note: This is not currently supported for DLZ
    databases that are configured as 'search no;')

The external DLZ module can define the following functions (some of these
are mandatory, others optional).

  - int dlz_version(unsigned int *flags);

    Required for alL external DLZ modules, to indicate the version number
    of the DLZ dlopen driver that this module supports.  It should return
    the value DLZ_DLOPEN_VERSION, which is defined in the file
    contrib/dlz/modules/dlz_minimal.h and is currently 3.  'flags' is
    updated to indicate capabilities of the module.  In particular, if
    the module is thread-safe then it sets 'flags' to include
    DNS_SDLZFLAG_THREADSAFE.  (Other capability flags may be added in
    the future.)

  - isc_result_t dlz_create(const char *dlzname,
                            unsigned int argc, char *argv[],
                            void **dbdata, ...);

    Required for all external DLZ modules; this call initializes the
    module.

  - void dlz_destroy(void *dbdata);

    Optional.  If supplied, this will be called when the driver is
    unloaded.

  - isc_result_t dlz_findzonedb(void *dbdata, const char *name,
                                dns_clientinfomethods_t *methods,
                                dns_clientinfo_t *clientinfo);

    Required for all external DLZ modules.  This indicates whether
    the DLZ module can answer for the given name.  Returns ISC_R_SUCCESS
    if so, and ISC_R_NOTFOUND if not.  As an optimization, it can
    also return ISC_R_NOMORE: this indicates that the DLZ module has
    no data for the given name or for any name above it in the DNS.
    This prevents named from searching for a zone cut.

   - isc_result_t dlz_lookup(const char *zone, const char *name, void *dbdata,
                             dns_sdlzlookup_t *lookup,
                             dns_clientinfomethods_t *methods,
                             dns_clientinfo_t *clientinfo);

    Required for all external DLZ modules.  This carries out the database
    lookup for a query.

  - isc_result_t dlz_allowzonexfr(void *dbdata, const char *name,
                                  const char *client);

    Optional.  Supply this if you want the module to support AXFR
    for the specified zone and client.  A return value of ISC_R_SUCCESS
    means AXFR is allowed, any other value means it isn't.

  -  isc_result_t dlz_allnodes(const char *zone, void *dbdata,
                               dns_sdlzallnodes_t *allnodes);

     Optional, but must be supplied dlz_allowzonexfr() is.  This function
     returns all nodes in the zone in order to perform a zone transfer.

  - isc_result_t dlz_newversion(const char *zone, void *dbdata,
                                void **versionp);

    Optional.  Supply this if you want the module to support DDNS
    updates.  This function starts a transaction in the database.


  - void dlz_closeversion(const char *zone, isc_boolean_t commit,
                          void *dbdata, void **versionp);

    Optional, but must be supplied if dlz_newversion() is.  This function
    closes a transaction.  'commit' indicates whether to commit the changes
    to the database, or ignore them.

  - isc_result_t dlz_configure(dns_view_t *view, void *dbdata);

    Optional, but must be supplied in order to support DDNS updates.

  - isc_boolean_t dlz_ssumatch(const char *signer, const char *name,
                               const char *tcpaddr, const char *type,
                               const char *key, uint32_t keydatalen,
                               uint8_t *keydata, void *dbdata);

    Optional, but must be supplied in order to support DDNS updates.

  - isc_result_t dlz_addrdataset(const char *name, const char *rdatastr,
                                 void *dbdata, void *version);

    Optional, but must be supplied in order to support DDNS updates.
    Adds the data in 'rdatastr' to a database node.

  - isc_result_t dlz_subrdataset(const char *name, const char *rdatastr,
                                 void *dbdata, void *version);

    Optional, but must be supplied in order to support DDNS updates.
    Removes the data in 'rdatastr' from a database node.

  - isc_result_t dlz_delrdataset(const char *name, const char *rdatastr,
                                 void *dbdata, void *version);

    Optional, but must be supplied in order to support DDNS updates.
    Deletes all data matching the type specified in 'rdatastr' from
    the database.
BIND 9 DLZ Perl module (bind-dlz-tools)

Written by John Eaglesham <dns@8192.net>

A dynamically loadable zone (DLZ) plugin embedding a Perl
interpreter in BIND, allowing Perl scripts to be written to
integrate with BIND and serve DNS data.

More information/updates at http://bind-dlz-tools.sourceforge.net/
These files were used for testing on Ubuntu Linux using OpenLDAP.

- Move aside /etc/ldap/slapd.d
- Move slapd.conf to /etc/ldap
- Move dlz.schema to /etc/ldap/schema/dlz.schema
- Run "/etc/init.d/slapd restart"
- Run "ldapadd -x -f example.ldif -D 'cn=Manager,o=bind-dlz' -w secret"

LDAP server is now loaded with example.com data from the file example.ldif

The "wildcard" DLZ module provides a "template" zone for domains matching
a wildcard name.  For example, the following DLZ configuration would match
any zone name containing the string "example" and ending with .com, such
as "thisexample.com", "exampleofthat.com", or "anexampleoftheotherthing.com".

    dlz "test" {
	database "dlopen ../dlz_wildcard_dynamic.so
        *example*.com 10.53.* 1800
        @      3600    SOA   {ns3.example.nil. support.example.nil. 42 14400 7200 2592000 600}
        @      3600    NS     ns3.example.nil.
        @      3600    NS     ns4.example.nil.
        @      3600    NS     ns8.example.nil.
        @      3600    MX     {5 mail.example.nil.}
        ftp    86400   A      192.0.0.1
        sql    86400   A      192.0.0.2
        tmp    {}      A      192.0.0.3
        www    86400   A      192.0.0.3
        www    86400   AAAA   ::1
        txt    300     TXT    {\"you requested $record$ in $zone$\"}
        *      86400   A      192.0.0.100";
    };

For any zone name matchin the wildcard, it would return the data from
the template.  "$zone$" is replaced with zone name: i.e., the shortest
possible string of labels in the query name that matches the wildcard.
"$record$" is replaced with the remainder of the query name.  In the
example above, a query for "txt.thisexample.com/TXT" would return the
string "you requested txt in thisexample.com".

Any client whose source address matches the second wildcard ("10.53.*")
is allowed to request a zone transfer.
These files were used for testing on Ubuntu Linux using SQLite3

- Install SQLite3: sudo apt-get install sqlite3 libsqlite3-dev
- Build sqlite3 DLZ module
- Run "sqlite3 BindDB < dlz.schema" to set up database
- Run "sqlite3 BindDB < dlz.data" to populate it
- Run "named -gc named.conf"
- Send test queries, e.g "dig @localhost -p 5300 example.com",
   "dig @localhost -p 5300 axfr example.com" (AXFR should be
   allowed from 127.0.0.1 only).
These files were used for testing on Ubuntu Linux using MySQL

- Install MySQL: sudo apt-get install mysql-server
- Run "mysql --user=USER --password=PASSWORD < dlz.schema" to set up database
- Run "mysql --user=USER --password=PASSWORD < dlz.data" to populate it
- update named.conf with correct USER and PASSWORD

dlz-bdbhpt-dynamic
==================

A Bind 9 Dynamically Loadable BerkeleyDB High Performance Text Driver

Summary
-------

This is an attempt to port the original Bind 9 DLZ bdbhpt_driver.c as
found in the Bind 9 source tree into the new DLZ dlopen driver API.
The goals of this project are as follows:

* Provide DLZ facilities to OEM-supported Bind distributions
* Support both v1 (Bind 9.8) and v2 (Bind 9.9) of the dlopen() DLZ API

Requirements
------------

You will need the following:
 * Bind 9.8 or higher with the DLZ dlopen driver enabled
 * BerkeleyDB libraries and header files
 * A C compiler

This distribution have been successfully installed and tested on
Ubuntu 12.04.

Installation
------------

With the above requirements satisfied perform the following steps:

1. Ensure the symlink for dlz_minimal.h points at the correct header
   file matching your Bind version
2. Run: make
3. Run: sudo make install # this will install dlz_bdbhpt_dynamic.so
   into /usr/lib/bind9/
4. Add a DLZ statement similar to the example below into your
   Bind configuration
5. Ensure your BerkeleyDB home-directory exists and can be written to
   by the bind user
6. Use the included testing/bdbhpt-populate.pl script to provide some
   data for initial testing

Usage
-----

Example usage is as follows:

```
dlz "bdbhpt_dynamic" {
        database "dlopen /usr/lib/bind9/dlz_bdbhpt_dynamic.so T /var/cache/bind/dlz dnsdata.db";
};
```

The arguments for the "database" line above are as follows:

1. dlopen - Use the dlopen DLZ driver to dynamically load our compiled
   driver
2. The full path to your built dlz_bdbhpt_dynamic.so
3. Single character specifying the mode to open your BerkeleyDB
   environment:
   * T - Transactional Mode - Highest safety, lowest speed.
   * C - Concurrent Mode - Lower safety (no rollback), higher speed.
   * P - Private Mode - No interprocess communication & no locking.
     Lowest safety, highest speed.
4. Directory containing your BerkeleyDB - this is where the BerkeleyDB
   environment will be created.
5. Filename within this directory containing your BerkeleyDB tables.

A copy of the above Bind configuration is included within
example/dlz.conf.

Author
------

The person responsible for this is:

 Mark Goldfinch <g@g.org.nz>

The code is maintained at:

 https://github.com/goldie80/dlz-bdbhpt-dynamic

There is very little in the way of original code in this work,
however, original license conditions from both bdbhpt_driver.c and
dlz_example.c are maintained in the dlz_bdbhpt_dynamic.c.
These files were used for testing on Ubuntu Linux using BDB 5.1 and
BerkeleyDB 0.54 for perl.

- Populate the database from dns-data.txt for zone example.com:

      perl bdbhpt-populate.pl \
          --bdb=test.db --input=dns-data.txt --zones=example.com

- Run "named -g -c named.conf"

BDB server is now loaded with example.com data from the file test.db

Id: README,v 1.1 2001/07/12 02:02:09 gson Exp 

This is queryperf, a DNS server query performance testing tool.

It is primarily intended for measuring the performance of
authoritative DNS servers, but it has also been used for measuring
caching server performance.  This document describes the use of
queryperf for authoritative server performance testing.


Building

To build queryperf, just do

  sh configure
  make


The test environment

It is recommended that you run queryperf and the name server under
test on separate machines, so that the CPU usage of queryperf itself
does not slow down the name server.  The two machines should be
connected with a fast network, preferably a dedicated 100baseT
segment.  Testing through a router or firewall is not advisable.


Configuring the server

The name server under test should be set up as an authoritative
server, serving one or more zones similar in size and number to
what the server is expected to serve in production.

Be sure to turn off recursion in the server's configuration 
(in BIND 8/9, specify "recursion no;" in the options block).
In BIND 8, you should also specify "fetch-glue no;"; otherwise
the server may attempt to retrieve glue information from the
Internet during the test, slowing it down by an unpredictable
factor.


Constructing the input file

You need to construct a queryperf input file containing a large and
realistic set of queries, on the order of ten thousand to a million.
The input file contains one line per query, consisting of a domain 
name and an RR type name separated by a space.  The class of the 
query is implicitly IN.

When measuring the performance serving non-terminal zones such as the
root zone or TLDs, note that such servers spend most of their time
providing referral responses, not authoritative answers.  Therefore, a
realistic input file might consist mostly of queries for type A for
names *below*, not at, the delegations present in the zone.  For
example, when testing the performance of a server configured to be
authoritative for the top-level domain "fi.", which contains
delegations for domains like "helsinki.fi" and "turku.fi", the input
file could contain lines like

  www.turku.fi A
  www.helsinki.fi A

where the "www" prefix ensures that the server will respond with a
referral.  Ideally, a realistic proportion of queries for nonexistent
domains should be mixed in with those for existing ones, and the lines
of the input file should be in a random order.


Running the tests

Queryperf is run specifying the input file using the "-d" option, as
in

  queryperf -d input_file -s server

The output of queryperf is mostly self-explanatory.  Pay attention to
the number of dropped packets reported - when running the test over a
local Ethernet connection, it should be zero.  If one or more packets
has been dropped, there may be a problem with the network connection.
In that case, the results should be considered suspect and the test
repeated.
@(#) Id: README 237 2009-03-14 05:38:15Z leres  (LBL)

NSLINT 2.0
Lawrence Berkeley National Laboratory
Network Research Group
nslint@ee.lbl.gov
ftp://ftp.ee.lbl.gov/nslint.tar.gz

This directory contains source code for nslint, a lint program for dns
files.

Please send bugs and comments to nslint@ee.lbl.gov.

 - Craig Leres
(Message trash:37216)

Date:    Wed, 15 May 2002 20:44:45 +0100
To:      <bind-workers@isc.org>
From:    Nuno Miguel Rodrigues <nmr@co.sapo.pt>
Subject: Berkeley DB BIND9 SDB

Replied: Thu, 16 May 2002 11:47:35 +1000
Replied: Nuno Miguel Rodrigues <nmr@co.sapo.pt>
Return-Path: <bind-workers-bounce@isc.org>
X-X-Sender: <nmr@angelina.sl.pt>
MIME-Version: 1.0
X-ecartis-version: Ecartis v1.0.0
Sender:  bind-workers-bounce@isc.org
Errors-To: bind-workers-bounce@isc.org
X-original-sender: nmr@co.sapo.pt
Precedence: bulk
X-list:  bind-workers
Status:  



Hello all,

I'm making available a BIND9 SDB using Berkeley DB 4.0.

You can get it at http://www.dhis.org/~nmr/bind9_berkeleydb_sdb-1.0.tar

Thanks,


-- 
Nuno M. Rodrigues <nmr@co.sapo.pt>




INSTALLATION

To Compile zone2ldap from contrib/sdb directory:

   gcc -g `../../../isc-config.sh --cflags isc dns` -c zone2ldap.c
   gcc -g -o zone2ldap zone2ldap.o `../../../isc-config.sh --libs isc dns` -lldap -llber -lresolv

USAGE:

See zone2ldap.1

BUGS:

Jeff McNeil <jeff@snapcase.g-rock.net>



This is an attempt at an LDAP back-end for BIND 9 using the new simplified
database interface "sdb". This is release 1.0-beta and should be pretty
stable. Note that since version 0.4 a new schema is used. It is not
backwards compatible with versions before 0.4.

1.0-beta fixes a large memory leak. An extension x-tls for enabling TLS
has been added.

1.0-alpha uses LDAPv3 by default and also supports LDAP simple bind. That
is, one can use plain text password for authentication. The bind dn and
password is coded into the URL using extensions bindname and x-bindpw
per RFC 2255.

In 0.9 the code has been cleaned up a bit and should be slightly faster
than previous versions. It also fixes an error with zone transfers (AXFR)
and entries with multiple relativeDomainName values. The problem was
that it would only use the first value in the result. There's no need
to upgrade unless you use such entries.

0.8 uses asynchronous LDAP search which should give better performance.
Thanks to Ashley Burston for providing patch. Another new feature is
allowing filters in URLs. The syntax is as in RFC 2255. Few people will
need this, but if you have say an internal and external version of the
same zone, you could stick say o=internal and o=external into different
entries, and specify for instance ldap://host/base???(o=internal)
Some error logging has also been added.

0.7 allows space and other characters to be used in URLs by use of %-quoting.
For instance space can be written as %20. It also fixes a problem with some
servers and/or APIs that do not preserve attribute casing.

0.6 fixes some memory leaks present in older versions unless compiled with
the RFC 1823 API.

The big changes in 0.5 are thread support and improved connection handling.
Multiple threads can now access the back-end simultaneously, and rather than
having one connection per zone, there is now one connection per thread per
LDAP server. This should help people with multiple CPUs and people with a
huge number of zones. One final change is support for literal IPv6 addresses
in LDAP URLs. At least OpenLDAP 2 has IPv6 support, so if you use OpenLDAP 2
libraries and server, you got all you need.

If you have bug reports, fixes, comments, questions or whatever, please
contact me. See also http://www.venaas.no/ldap/bind-sdb/ for information.

See INSTALL for how to build, install and use.

Stig Venaas <venaas@uninett.no> 2004-08-15
			SQLite BIND SDB driver

The SQLite BIND SDB "driver" is intended as an alternative both to the
pgsqldb and dirdb drivers, for situations that would like the management
simplicity and convenience of single filesystem files, with the additional
capability of SQL databases.  It is also intended as an alternative to
the standard dynamic DNS update capability in bind, which effectively
requires use of DNSSEC keys for authorization and is limited to 'nsupdate'
for updates.  An sqlite database, by contrast, uses and requires only
normal filesystem permissions, and may be updated however a typical SQLite
database might be updated, e.g., via a web service with an SQLite backend.

This driver is not considered suitable for very high volume public
nameserver use, while likely useful for smaller private nameserver
applications, whether or not in a production environment.  It should
generally be suitable wherever SQLite is preferable over larger database
engines, and not suitable where SQLite is not preferable.

Usage:

o Use the named_sdb process ( put ENABLE_SDB=yes in /etc/sysconfig/named )

o Edit your named.conf to contain a database zone, eg.:
  
zone "mydomain.net." IN {
        type master;
        database "sqlite /etc/named.d/mydomain.db mydomain";
        #                ^- DB file               ^-Table
};

o Create the database zone table
  The table must contain the columns "name", "rdtype", and "rdata", and
  is expected to contain a properly constructed zone.  The program
  "zone2sqlite" creates such a table.
  
  zone2sqlite usage:
    
    zone2sqlite origin zonefile dbfile dbtable

    where
	origin   : zone origin, eg "mydomain.net."
	zonefile : master zone database file, eg. mydomain.net.zone
	dbfile   : name of SQLite database file
        dbtable  : name of table in database

---
# mydomain.net.zone:
$TTL 1H
@       SOA     localhost.      root.localhost. (       1
                                                3H
                                                1H
                                                1W
                                                1H )
        NS      localhost.
host1   A       192.168.2.1
host2   A       192.168.2.2
host3   A       192.168.2.3
host4   A       192.168.2.4
host5   A       192.168.2.5
host6   A       192.168.2.6
host7   A       192.168.2.7
---

# zone2sqlite mydomain.net. mydomain.net.zone mydomain.net.db mydomain

will create/update the 'mydomain' table in database file 'mydomain.net.db'.

These unit tests for BIND 9 are based on the NetBSD Automated Test Framework 
release 0.17.

To build an external copy of ATF for use by BIND 9:

  $ cd atf-src
  $ configure --prefix=<prefix> --enable-tools --disable-shared
  $ make
  $ make install

Subsequently, specify the ATF prefix when building BIND 9:

  $ configure --with-atf=<prefix>

ATF can also be built automatically during the BIND 9 build,
by specifying --with-atf without an argument:

  $ configure --with-atf

This causes BIND 9 to build ATF in the atf-src directory and
link to it directly.
Introductory information                        Automated Testing Framework
===========================================================================


Introduction
************

The Automated Testing Framework (ATF) is a collection of libraries and
utilities designed to ease unattended application testing in the hands of
developers and end users of a specific piece of software.

As regards developers, ATF provides the necessary means to easily create
test suites composed of multiple test programs, which in turn are a
collection of test cases.  It also attempts to simplify the debugging of
problems when these test cases detect an error by providing as much
information as possible about the failure.

As regards users, it simplifies the process of running the test suites and,
in special, encourages end users to run them often: they do not need to
have source trees around nor any other development tools installed to be
able to certify that a given piece of software works on their machine as
advertised.


Other documents
***************

* AUTHORS: List of authors and contributors for this project.

* COPYING: License information.

* INSTALL: Compilation and installation instructions.  These is not the
  standard document shipped with many packages, so be sure to read it for
  things that are specific to ATF's build.

* NEWS: List of major changes between formal, published releases.


===========================================================================
vim: filetype=text:textwidth=75:expandtab:shiftwidth=2:softtabstop=2
Copyright (C) 2004, 2015  Internet Systems Consortium, Inc. ("ISC")
Copyright (C) 2000, 2001  Internet Software Consortium.
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

The BIND v9 ARM master document is now kept in DocBook 5 XML format.

Most of the ARM is in the single file "Bv9ARM-book.xml", with certain
other files included into it:

    - dlz.xml
    - dnssec.xml
    - libdns.xml
    - logging-categories.xml
    - managed-keys.xml
    - notes.xml
    - pkcs11.xml
    - BIND man pages

All of the published ARM formats - HTML, PDF, etc - are generated from
this master source.

The file "notes.xml" contains the release notes for the current release. In
addition to being included in the ARM as an appendix, it is also built into
a stand-alone document: "notes.pdf" and "notes.html".

Building these these files requires DocBook 5 and dblatex. These are
available as packages in many OS distributes; in debian, for example:

    $ sudo apt-get install docbook5-xml docbook-xml docbook-xsl-ns \
                           docbook-utils dblatex

To build all documentation, run "make doc".

When committing changes or submitting patches, it is only necessary to
edit the XML source (i.e., the files with ".docbook" or ".xml" suffixes);
the files in HTML and man page format are built from the XML source by a
cron job.

If you are familiar with SGML or HTML, editing the DocBook XML is quite
straightforward.  You only need to know what the tags are and how to use
them.  You can find a good resource either for this either online or in
printed form:

    DocBook: The Definitive Guide
    By Norman Walsh and Leonard Muellner
    ISBN: 156592-580-7
    1st Edition, October 1999
    Copyright (C) 1999 by O'Reilly & Associates, Inc. All rights reserved.

The book is available online in HTML format:

    http://docbook.org/

After editing documentation, it is useful to check the correctness of the
XML; this can be done using the "xmllint" utility.
These scripts generate a named.conf file with an arbitrary number of
small zones, for testing startup performance.

To generate a test server with 1000 zones each of which contains 5 A
records, run:

   $ sh setup.sh 1000 5 > named.conf

Zones are generated with random names, and the zone files are created
in the subdirectory "zones".

Or, to generate a test server with 100 zones which all load from the same
generic file (smallzone.db):

   $ sh setup.sh -s 100 > named.conf

The "number of records" argument is ignored if -s is used.
Copyright (C) 2004, 2010, 2011, 2013, 2015  Internet Systems Consortium, Inc. ("ISC")
Copyright (C) 2000, 2001  Internet Software Consortium.
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

This is a simple test environment for running bind9 system tests
involving multiple name servers.

There are multiple test suites, each in a separate subdirectory and
involving a different DNS setup.  They are:

  dnssec/	DNSSEC tests
  forward/	Forwarding tests
  glue/		Glue handling tests
  limits/	Tests of handling of large data (close to server limits)
  lwresd/	Tests of the lightweight resolver library and daemon
  notify/	More NOTIFY tests
  nsupdate/	Dynamic update and IXFR tests
  resolver/     Regression tests for resolver bugs that have been fixed
		(not a complete resolver test suite)
  rrl/		query rate limiting
  rpz/		Tests of response policy zone (RPZ) rewriting
  rpzrecurse/	Another set of RPZ tests to check recursion behavior
  stub/		Tests of stub zone functionality
  unknown/	Unknown type and class tests
  upforwd/	Update forwarding tests
  views/	Tests of the "views" statement
  xfer/		Zone transfer tests
  xferquota/	Zone transfer quota tests

Typically each test suite sets up 2-5 name servers and then performs
one or more tests against them.  Within the test suite subdirectory,
each name server has a separate subdirectory containing its
configuration data.  By convention, these subdirectories are named
"ns1", "ns2", etc.

The tests are completely self-contained and do not require access to
the real DNS.  Generally, one of the test servers (ns1) is set up as a
root name server and is listed in the hints file of the others.

To enable all servers to run on the same machine, they bind to
separate virtual IP address on the loopback interface.  ns1 runs on
10.53.0.1, ns2 on 10.53.0.2, etc.  Before running any tests, you must
set up these addresses by running "ifconfig.sh up" as root.

Mac OS X:
If you wish to make the interfaces survive across reboots
copy org.isc.bind.system and org.isc.bind.system to
/Library/LaunchDaemons then run
"launchctl load /Library/LaunchDaemons/org.isc.bind.system.plist" as
root.

The servers use port 5300 instead of the usual port 53, so they can be
run without root privileges once the interfaces have been set up.

The tests can be run individually like this:

  sh run.sh xfer
  sh run.sh notify
  etc.

To run all the tests, just type "make test".

When running system tests, named and lwresd can be run under
Valgrind. The output from Valgrind are sent to per-process files that
can be reviewed after the test has completed. To enable this, set the
USE_VALGRIND environment variable to "helgrind" to run the Helgrind
tool, or any other value to run the Memcheck tool. To use "helgrind"
effectively, build BIND with --disable-atomic.
Copyright (C) 2014  Internet Systems Consortium, Inc. ("ISC")
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

system test for recursion limits

ns1  -- root server
ans2 -- delegate to ns1.(n+1).example.com for all n, up to
        the value specified in ans.limit (or forever if limit is 0)
ns3  -- resolver under test
ans4 -- delegates every query to 16 more name servers, with "victim" address
ans7 -- "victim" server
Copyright (C) 2013, 2014  Internet Systems Consortium, Inc. ("ISC")
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

The data data files in this directory are sample GeoIP databases,
generated from the corresponding CSV files.  Thanks to MaxMind, Inc.
for assistance with producing these files.

Unless otherwise noted, the databases only support IPv4:

GeoIP.dat: Country (IPv4)
GeoIPv6.dat: Country (IPv6)
GeoIPCity.dat: City (IPv4)
GeoIPCityv6.dat: City (IPv6)
GeoIPRegion.dat: Region
GeoIPISP.dat: ISP
GeoIPOrg.dat: Organization
GeoIPDoain.dat: Domain Name
GeoIPASNum.dat: AS Number
GeoIPNetSpeed.dat: Net Speed

GeoIP.dat can also be generated using the open source 'geoip-csv-to-dat'
utility:

$ geoip-csv-to-dat -i "BIND9 geoip test data v1" -o GeoIP.dat << EOF
"10.53.0.1","10.53.0.1","171245569","171245569","AU","Australia"
"10.53.0.2","10.53.0.2","171245570","171245570","US","United States"
"10.53.0.3","10.53.0.3","171245571","171245571","GB","United Kingdom"
"10.53.0.4","10.53.0.4","171245572","171245572","CA","Canada"
"10.53.0.5","10.53.0.5","171245573","171245573","CL","Chile"
"10.53.0.6","10.53.0.6","171245574","171245574","DE","Germany"
"10.53.0.7","10.53.0.7","171245575","171245575","EH","Western Sahara"
EOF
This set includes one KSK rollover.  The first KSK is deleted
and its successor published prior to the first KSK being deactivated
and its successor activated.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
ERROR: After 2012-05-Dec (21:22:19):
    Delete: example.com/007/06219 (KSK)
    Publish: example.com/007/20559 (KSK)
No KSK's are both active and published

Checking ZSK events for zone example.com, algorithm 7:
OK
This set includes one ZSK rollover.  The first ZSK is deactivated
prior to its replacement being activated.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
OK

Checking ZSK events for zone example.com, algorithm 7:
ERROR: After 2012-05-Dec (20:39:32):
    Inactive: example.com/005/08376 (ZSK)
No ZSK's are active
This set includes one KSK rollover.  The first KSK is deleted
and its successor published prior to the first KSK being deactivated
and its successor activated.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
OK

Checking ZSK events for zone example.com, algorithm 7:
ERROR: After 2012-05-Dec (20:44:18):
    Delete: example.com/007/26369 (ZSK)
    Publish: example.com/007/21029 (ZSK)
No ZSK's are both active and published
This set includes one KSK rollover.  The KSK is deactivated prior to
its replacement being activated.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
ERROR: After 2012-31-Jul (20:59:14):
    Inactive: example.com/007/45435 (KSK)
No KSK's are active

Checking ZSK events for zone example.com, algorithm 7:
OK
This set includes a KSK rollover, with insufficient delay between
prepublication and rollover.

Expected tool output TBD.
This set includes a KSK rollover, with insufficient delay between
prepublication and rollover.

Expected tool output TBD.
This set includes one KSK rollover.  The KSK is deactivated prior to
its replacement being activated.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
ERROR: After 2012-31-Jul (20:59:14):
    Inactive: example.com/007/45435 (KSK)
No KSK's are active

Checking ZSK events for zone example.com, algorithm 7:
OK
This set includes one ZSK rollover.  The first ZSK is deactivated
prior to its replacement being activated; however, as we are only
checking KSKs, we should not detect the error.  Tool output should
resemble:

Checking KSK events for zone example.com, algorithm 7:
OK
This set includes one KSK rollover.  The KSK is deactivated prior to
its replacement being activated; however, as we are only checking ZSK's,
we should not detect the error.  Tool output should resemble:

Checking ZSK events for zone example.com, algorithm 7:
OK
This set contains one ZSK rollover.  The ZSK is unpublished before its
successor is published.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
OK

Checking ZSK events for zone example.com, algorithm 7:
ERROR: After 2012-06-Oct (21:13:45):
    Delete: example.com/007/25967 (ZSK)
No ZSK's are published
This set contains one KSK rollover.  The KSK is unpublished before its
successor is published.  Tool output should resemble:

Checking KSK events for zone example.com, algorithm 7:
ERROR: After 2012-06-Oct (21:07:57):
    Delete: example.com/007/23040 (KSK)
No KSK's are published

Checking ZSK events for zone example.com, algorithm 7:
OK
Copyright (C) 2004, 2011  Internet Systems Consortium, Inc. ("ISC")
Copyright (C) 2000-2002  Internet Software Consortium.
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

Id: README,v 1.10 2011/01/04 23:47:13 tbox Exp 

The test setup for the DNSSEC tests has a secure root.

ns1 is the root server.

ns2 and ns3 are authoritative servers for the various test domains.

ns4 is a caching-only server, configured with the correct trusted key
for the root.

ns5 is a caching-only server, configured with the an incorrect trusted
key for the root.  It is used for testing failure cases.

ns6 is a caching-only server configured to use DLV.

ns7 is used for checking non-cacheable answers.
Copyright (C) 2015  Internet Systems Consortium, Inc. ("ISC")
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

These tests check RPZ recursion behavior (including skipping
recursion when appropriate).

The general structure of the tests is:

* The resolver (ns2) with an unqualified view containing the policy
  zones, the response-policy statement, and a root hint zone

* The auth server that contains two authoritative zones, l1.l0 and
  l2.l1.l0, both delegated to itself. l2.l1.l0 specifies a non-existent
  zone data file and so will generate SERVFAILs for any queries to it.

The l2.l1.l0 zone was chosen to generate SERVFAIL responses because RPZ
evaluation will use that error response whenever it encounters it during
processing, thus making it a binary indicator for whether or not
recursion was attempted.  This also allows us to not worry about having
to craft 'ip', 'nsdname', and 'nsip' rules that matched the queries.

Each test is intended to be fed a number of queries constructed as
qXX.l2.l1.l0, where XX is the 1-based query sequence number (e.g. the
first query of each test is q01.l2.l1.l0).

For all the tests the triggers are constructed as follows:
client-ip - match 127.0.0.1/32
ip - match 255.255.255.255/32 (does not matter due to SERVFAIL)
nsdname - match ns.example.org (also does not matter)
nsip - match 255.255.255.255/32 (also does not matter)
qname - match qXX.l2.l1.l0, where XX is the query sequence number that
is intended to be matched by this qname rule.

Here's the detail on the test cases:

Group 1 - testing skipping recursion for a single policy zone with only
records that allow recursion to be skipped

Test 1a:
    1 policy zone containing 1 'client-ip' trigger
    1 query, expected to skip recursion

Test 1b:
    1 policy zone containing 1 'qname' trigger (q01)
    2 queries, q01 is expected to skip recursion, q02 is expected to
      recurse

Test 1c:
    1 policy zone containing both a 'client-ip' and 'qname' trigger (q02)
    1 query, expected to skip recursion

Group 2 - testing skipping recursion with multiple policy zones when all
zones have only trigger types eligible to skip recursion with

Test 2a:
    32 policy zones, each containing 1 'qname' trigger (qNN, where NN is
       the zone's sequence 1-based sequence number formatted to 2 digits,
       so each of the first 32 queries should match a different zone)
    33 queries, the first 32 of which are expected to skip recursion
       while the 33rd is expected to recurse

Group 3 - Testing interaction of triggers that require recursion when in
a single zone, both alone and with triggers that allow recursion to be
skipped

Test 3a:
    1 policy zone containing 1 'ip' trigger
    1 query, expected to recurse

Test 3b:
    1 policy zone containing 1 'nsdname' trigger
    1 query, expected to recurse

Test 3c:
    1 policy zone containing 1 'nsip' trigger
    1 query, expected to recurse

Test 3d:
    1 policy zone containing 1 'ip' trigger and 1 'qname' trigger (q02)
    2 queries, the first should not recurse and the second should recurse

Test 3e:
    1 policy zone containing 1 'nsdname' trigger and 1 'qname' trigger
      (q02)
    2 queries, the first should not recurse and the second should recurse

Test 3f:
    1 policy zone containing 1 'nsip' trigger and 1 'qname' trigger (q02)
    2 queries, the first should not recurse and the second should recurse

Group 4 - contains 32 subtests designed to verify that recursion is
skippable for only the appropriate zones based on the order specified in
the 'response-policy' statement

Tests 4aa to 4bf:
    32 policy zones per test, one of which is configured with 1 'ip'
       trigger and one 'qname' trigger while the others are configured
       only with 1 'qname' trigger.  The zone with both triggers starts
       listed first and is moved backwards by one position with each
       test.  The 'qname' triggers in the zones are structured so that
       the zones are tested starting with the first zone and the 'ip'
       trigger is tested before the 'qname' trigger for that zone.
    33 queries per test, where the number expected to skip recursion
       matches the test sequence number: e.g. 1 skip for 4aa, 26 skips
       for 4az, and 32 skips for 4bf

Group 5 - This test verifies that the "pivot" policy zone for whether or
not recursion can be skipped is the first listed zone with applicable
trigger types rather than a later listed zone.

Test 5a:
    5 policy zones, the 1st, 3rd, and 5th configured with 1 'qname'
      trigger each (q01, q04, and q06, respectively), the 2nd and 4th
      each configured with an 'ip' and 'qname' trigger (q02 and q05,
      respectively for the 'qname' triggers
    6 queries, of which only q01 and q02 are expected to skip recursion
Copyright (C) 2010  Internet Systems Consortium, Inc. ("ISC")
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

This is copied from ../system.

This test suite uses a virtual time, gettimeofday(), select(),
poll(), kevent() and epoll_wait() Unix system calls are redirected:
gettimeofday() returns a date in virtual/exponentially inflated
delay from an epoch, select(), poll(), kevent() and epoll_wait()
timeouts are deflated down to at least 10ms.

These tests depends on LD_PRELOAD being supported by the runtime
loader.

Beware BIND clock uses unsigned integer, in 22 seconds isc_time_now()
overflows and breaks assertions. Note 22 real seconds is 136 virtual
years...

Id: README,v 1.2 2010/06/17 05:38:04 marka Exp 
"pkcs11-hmacmd5" is here to check for the presence of a known bug in
the Thales nCipher PKCS#11 provider library.  To test for the bug, use
pkcs11-hmacmd5 to hash a test vector from RFC 2104, and determine
whether the resulting digest is is correct.  For instance:

    echo -n "Hi There" | \
        ./pkcs11-hmacmd5 -p <PIN> -k '0b0b0b0b0b0b0b0b0b0b0b0b0b0b0b0b'

...must return "9294727a3638bb1c13f48ef8158bfc9d".

If any other value is returned, then the provider library is buggy,
and theflag PK11_MD5_HMAC_REPLACE must be defined in
lib/isc/include/pk11/site.h
However, if the correct value is returned, then it is safe to turn
off PK11_MD5_HMAC_REPLACE. (It is on by default.)
Copyright (C) 2013  Internet Systems Consortium, Inc. ("ISC")
See COPYRIGHT in the source root or http://isc.org/copyright.html for terms.

	bash buildzones.sh < zones	# creates setup, run, servers/* master/*
					# named.conf
	sudo sh setup			# configure interfaces
	sh run				# setup

	../named/named [-g] -c named.conf

	sh tests.sh < zones

	sudo sh teardown		# teardown interfaces

The test server can controlled with

	rndc -k rndc.key -s 127.127.0.0 -p 5300 
OpenPAM is an open source PAM library that focuses on simplicity,
correctness, and cleanliness.

OpenPAM aims to gather the best features of Solaris PAM, XSSO and
Linux-PAM, plus some innovations of its own.  In areas where these
implementations disagree, OpenPAM tries to remain compatible with
Solaris, at the expense of XSSO conformance and Linux-PAM
compatibility.

Please direct bug reports and inquiries to <des@des.no>.

                            Less, version 458

    This is the distribution of less, version 458, released 04 Apr 2013.
    This program is part of the GNU project (http://www.gnu.org).

    This program is free software.  You may redistribute it and/or
    modify it under the terms of either:

    1. The GNU General Public License, as published by the Free
       Software Foundation; either version 3, or (at your option) any
       later version.  A copy of this license is in the file COPYING.
    or
    2. The Less License, in the file LICENSE.

    Please report any problems to bug-less@gnu.org.
    See http://www.greenwoodsoftware.com/less for the latest info.

=========================================================================

This is the distribution of "less", a paginator similar to "more" or "pg".

The formatted manual page is in less.man.
The manual page nroff source is in less.nro.
Major changes made since the last posted version are in NEWS.

=======================================================================
INSTALLATION (Unix systems only):

1. Move the distributed source to its own directory and unpack it,
   if you have not already done so.  

2. Type "sh configure".
   This will generate a Makefile and a defines.h.
   Warning: if you have a GNU sed, make sure it is version 2.05 or later.

   The file INSTALL describes the usage of the configure program in
   general.  In addition, these options to configure are supported:

   --with-editor=program
     Specifies the default editor program used by the "v" command.
     The default is "vi".

   --with-regex=lib
     Specifies the regular expression library used by less for pattern
     matching.  The default is "auto", which means the configure program 
     finds a regular expression library automatically.  Other values are:
        posix          Use the POSIX-compatible regcomp.
        pcre           Use the PCRE library.
        regcmp         Use the regcmp library.
        re_comp        Use the re_comp library.
        regcomp        Use the V8-compatible regcomp.
        regcomp-local  Use Henry Spencer's V8-compatible regcomp
                       (source is supplied with less).
        none           No regular expressions, only simple string matching.
   --with-secure
     Builds a "secure" version of less, with some features disabled
     to prevent users from viewing other files, accessing shell
     commands, etc.


3. It is a good idea to look over the generated Makefile and defines.h
   and make sure they look ok.  If you know of any peculiarities of
   your system that configure might not have detected, you may fix the
   Makefile now.  Take particular notice of the list of "terminal" 
   libraries in the LIBS definition in the Makefile; these may need 
   to be edited.  The terminal libraries will be some subset of
       -lncurses  -lcurses  -ltermcap  -ltermlib

   If you wish, you may edit defines.h to remove some optional features.
   If you choose not to include some features in your version, you may
   wish to edit the manual page "less.nro" and the help page "less.hlp" 
   to remove the descriptions of the features which you are removing.
   If you edit less.hlp, you should run "make -f Makefile.aut help.c".

4. Type "make" and watch the fun.

5. If the make succeeds, it will generate the programs "less",
   "lesskey" and "lessecho" in your current directory.  Test the 
   generated programs.

6. When satisfied that it works, if you wish to install it
   in a public place, type "make install".

   The default install destinations are:
        Executables (less, lesskey, lessecho) in /usr/local/bin
        Documentation (less.nro, lesskey.nro) in /usr/local/man/man1
   If you want to install any of these files elsewhere, define
   bindir and/or mandir to the appropriate directories.

If you have any problems building or running "less", suggestions, 
complaints, etc., you may mail to bug-less@gnu.org.

Note to hackers: comments noting possible improvements are enclosed
in double curly brackets {{ like this }}.

(Note that the above note was originally written at a time when 
"hackers" most commonly meant "enthusiastic and dedicated computer 
programmers", not "persons who attempt to circumvent computer security".)



=======================================================================
INSTALLATION (MS-DOS systems only,
              with Microsoft C, Borland C, or DJGPP)

1. Move the distributed source to its own directory.
   Depending on your compiler, you may need to convert the source 
   to have CR-LF rather than LF as line terminators.

2. If you are using Microsoft C, rename MAKEFILE.DSU to MAKEFILE.
   If you are using Borland C, rename MAKEFILE.DSB to MAKEFILE.
   If you are using DJGPP, rename MAKEFILE.DSG to MAKEFILE.

3. Look at MAKEFILE to make sure that the definitions for CC and LIBDIR
   are correct.  CC should be the name of your C compiler and
   LIBDIR should be the directory where the C libraries reside (for
   Microsoft C only).  If these definitions need to be changed, you can
   either modify the definitions directly in MAKEFILE, or set your
   environment variables CC and/or LIBDIR to override the definitions
   in MAKEFILE.

4. If you wish, you may edit DEFINES.DS to remove some optional features.
   If you choose not to include some features in your version, you may
   wish to edit the manual page LESS.MAN and the help page HELP.C
   to remove the descriptions of the features which you are removing.

5. Run your "make" program and watch the fun.
   If your "make" requires a flag to import environment variables,
   you should use that flag.
   If your compiler runs out of memory, try running "make -n >cmds.bat" 
   and then run cmds.bat.

6. If the make succeeds, it will generate the programs "LESS.EXE" and
   "LESSKEY.EXE" in your current directory.  Test the generated programs.

7. When satisfied that it works, you may wish to install LESS.EXE and
   LESSKEY.EXE in a directory which is included in your PATH.



=======================================================================
INSTALLATION (Windows-95, Windows-98 and Windows-NT systems only,
              with Borland C or Microsoft Visual C++)

1. Move the distributed source to its own directory.

2. If you are using Borland C, rename Makefile.wnb to Makefile.
   If you are using Microsoft Visual C++, rename Makefile.wnm to Makefile.

3. Check the Makefile to make sure the definitions look ok.

4. If you wish, you may edit defines.wn to remove some optional features.
   If you choose not to include some features in your version, you may
   wish to edit the manual page less.man and the help page help.c
   to remove the descriptions of the features which you are removing.

5. Type "make" and watch the fun.

6. If the make succeeds, it will generate the programs "less.exe" and
   "lesskey.exe" in your current directory.  Test the generated programs.

7. When satisfied that it works, if you wish to install it
   in a public place, type "make install".
   See step 6 of the Unix installation instructions for details
   on how to change the default installation directories.



=======================================================================
INSTALLATION (OS/2 systems only,
              with EMX C)

1. Move the distributed source to its own directory.

2. Rename Makefile.o2e to Makefile.

3. Check the Makefile to make sure the definitions look ok.

4. If you wish, you may edit defines.o2 to remove some optional features.
   If you choose not to include some features in your version, you may
   wish to edit the manual page less.man and the help page help.c
   to remove the descriptions of the features which you are removing.

5. Type "make" and watch the fun.

6. If the make succeeds, it will generate the programs "less.exe" and
   "lesskey.exe" in your current directory.  Test the generated programs.

7. Make sure you have the emx runtime installed. You need the emx DLLs
   emx.dll and emxlibcs.dll and also the termcap database, termcap.dat.
   Make sure you have termcap.dat either in the default location or
   somewhere in a directory listed in the PATH or INIT environment 
   variables.

8. When satisfied that it works, you may wish to install less.exe,
   lesskey.exe and scrsize.exe in a directory which is included in 
   your PATH.  scrsize.exe is required only if you use a terminal
   emulator such as xterm or rxvt.



=======================================================================
INSTALLATION (OS-9 systems only,
              with Microware C or Ultra C)

1. Move the distributed source to its own directory.

2. If you are using Microware C, rename Makefile.o9c to Makefile.
   If you are using Ultra C, rename Makefile.o9u to Makefile.

3. Check the Makefile to make sure the definitions look ok.

4. If you wish, you may edit defines.o9 to remove some optional features.
   If you choose not to include some features in your version, you may
   wish to edit the manual page less.man and the help page help.c
   to remove the descriptions of the features which you are removing.

5. Type "dmake" and watch the fun.
   The standard OS-9 "make" will probably not work.  If you don't
   have dmake, you can get a copy from os9archive.rtsi.com.

6. If the make succeeds, it will generate the programs "less" and
   "lesskey" in your current directory.  Test the generated programs.

7. When satisfied that it works, if you wish to install it
   in a public place, type "dmake install".
   See step 6 of the Unix installation instructions for details
   on how to change the default installation directories.

=======================================================================
ACKNOWLEDGMENTS:
  Some versions of the less distribution are packaged using 
  Info-ZIP's compression utility.
  Info-ZIP's software is free and can be obtained as source 
  code or executables from various anonymous-ftp sites,
  including ftp.uu.net:/pub/archiving/zip.
# $NetBSD: README,v 1.8 2017/04/13 17:59:34 christos Exp $

This package contains library that can be used by network daemons to
communicate with a packet filter via a daemon to enforce opening and
closing ports dynamically based on policy.

The interface to the packet filter is in libexec/blacklistd-helper
(this is currently designed for npf) and the configuration file
(inspired from inetd.conf) is in etc/blacklistd.conf.

On NetBSD you can find an example npf.conf and blacklistd.conf in
/usr/share/examples/blacklistd; you need to adjust the interface
in npf.conf and copy both files to /etc; then you just enable
blacklistd=YES in /etc/rc.conf, start it up, and you are all set.

There is also a startup file in etc/rc.d/blacklistd

Patches to various daemons to add blacklisting capabilitiers are in the
"diff" directory:
    - OpenSSH: diff/ssh.diff [tcp socket example]
    - Bind: diff/named.diff [both tcp and udp]
    - ftpd: diff/ftpd.diff [tcp]

These patches have been applied to NetBSD-current.

The network daemon (for example sshd) communicates to blacklistd, via
a unix socket like syslog. The library calls are simple and everything
is handled by the library. In the simplest form the only thing the
daemon needs to do is to call:

	blacklist(action, acceptedfd, message);

Where:
	action = 0 -> successful login clear blacklist state
		 1 -> failed login, add to the failed count
	acceptedfd -> the file descriptor where the server is
		      connected to the remote client. It is used
		      to determine the listening socket, and the
		      remote address. This allows any program to
		      contact the blacklist daemon, since the verification
		      if the program has access to the listening
		      socket is done by virtue that the port
		      number is retrieved from the kernel.
	message    -> an optional string that is used in debugging logs.

Unfortunately there is no way to get information about the "peer"
from a udp socket, because there is no connection and that information
is kept with the server. In that case the daemon can provide the
peer information to blacklistd via:

	blacklist_sa(action, acceptedfd, sockaddr, sockaddr_len, message);

The configuration file contains entries of the form:

# Blacklist rule
# host/Port	type	protocol	owner	name	nfail	disable
192.168.1.1:ssh	stream	tcp		*	-int	10	1m
8.8.8.8:ssh	stream	tcp		*	-ext	6	60m
ssh		stream	tcp6		*	*	6	60m
http		stream	tcp		*	*	6	60m

Here note that owner is * because the connection is done from the
child ssh socket which runs with user privs. We treat ipv4 connections
differently by maintaining two different rules one for the external
interface and one from the internal We also register for both tcp
and tcp6 since those are different listening sockets and addresses;
we don't bother with ipv6 and separate rules. We use nfail = 6,
because ssh allows 3 password attempts per connection, and this
will let us have 2 connections before blocking. Finally we block
for an hour; we could block forever too by specifying * in the
duration column.

blacklistd and the library use syslog(3) to report errors. The
blacklist filter state is persisted automatically in /var/db/blacklistd.db
so that if the daemon is restarted, it remembers what connections
is currently handling. To start from a fresh state (if you restart
npf too for example), you can use -f. To watch the daemon at work,
you can use -d.

The current control file is designed for npf, and it uses the
dynamic rule feature. You need to create a dynamic rule in your
/etc/npf.conf on the group referring to the interface you want to block
called blacklistd as follows:

ext_if=bge0
int_if=sk0
	
group "external" on $ext_if {
	...
        ruleset "blacklistd-ext" 
        ruleset "blacklistd" 
	...
}

group "internal" on $int_if {
	...
        ruleset "blacklistd-int" 
	...
}

You can use 'blacklistctl dump -a' to list all the current entries
in the database; the ones that have nfail <c>/<t> where <c>urrent
>= <t>otal, should have an id assosiated with them; this means that
there is a packet filter rule added for that entry. For npf, you
can examine the packet filter dynamic rule entries using 'npfctl
rule <rulename> list'.  The number of current entries can exceed
the total. This happens because entering packet filter rules is
asynchronous; there could be other connection before the rule
becomes activated.

Enjoy,

christos
This is flex, the fast lexical analyzer generator.

flex is a tool for generating scanners: programs which recognize
lexical patterns in text.

The flex codebase is kept in
[Git on GitHub.](https://github.com/westes/flex)

Use GitHub's [issues](https://github.com/westes/flex/issues) and
[pull request](https://github.com/westes/flex) features to file bugs
and submit patches.

There are several mailing lists available as well:

* flex-announce@lists.sourceforge.net - where posts will be made
  announcing new releases of flex.
* flex-help@lists.sourceforge.net - where you can post questions about
  using flex
* flex-devel@lists.sourceforge.net - where you can discuss development
  of flex itself

Find information on subscribing to the mailing lists at:

http://sourceforge.net/mail/?group_id=97492

The flex distribution contains the following files which may be of
interest:

* README - This file.
* NEWS - current version number and list of user-visible changes.
* INSTALL - basic installation information.
* ABOUT-NLS - description of internationalization support in flex.
* COPYING - flex's copyright and license.
* doc/ - user documentation.
* examples/ - containing examples of some possible flex scanners and a
	      few other things. See the file examples/README for more
              details.
* tests/ - regression tests. See TESTS/README for details.
* po/ - internationalization support files.

You need the following tools to build flex from the maintainer's
repository:

* compiler suite - flex is built with gcc
* bash, or a good Bourne-style shell
* m4 - m4 -p needs to work; GNU m4 and a few others are suitable
* GNU bison;  to generate parse.c from parse.y
* autoconf; for handling the build system
* automake; for Makefile generation
* gettext; for i18n support
* help2man; to generate the flex man page
* tar, gzip, lzip, etc.; for packaging of the source distribution
* GNU texinfo; to build and test the flex manual. Note that if you want
  to build the dvi/ps/pdf versions of the documentation you will need
  texi2dvi and related programs, along with a sufficiently powerful
  implementation of TeX to process them. See your operating system
  documentation for how to achieve this. The printable versions of the
  manual are not built unless specifically requested, but the targets
  are included by automake.
* GNU indent; for indenting the flex source the way we want it done

In cases where the versions of the above tools matter, the file
configure.ac will specify the minimum required versions.

Once you have all the necessary tools installed, life becomes
simple. To prepare the flex tree for building, run the script:

```bash
./autogen.sh
```

in the top level of the flex source tree.

This script calls the various tools needed to get flex ready for the
GNU-style configure script to be able to work.

From this point on, building flex follows the usual  routine:

```bash
configure && make && make install
```

This file is part of flex.

This code is derived from software contributed to Berkeley by
Vern Paxson.

The United States Government has rights in this work pursuant
to contract no. DE-AC03-76SF00098 between the United States
Department of Energy and the University of California.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

Neither the name of the University nor the names of its contributors
may be used to endorse or promote products derived from this software
without specific prior written permission.

THIS SOFTWARE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.
This directory contains some examples of what you can do with
flex. These files are not tested regularly so you might have to tinker
a bit before they work for you. Updates, new files and patches are welcome.

	- debflex.awk, an awk script for anotating flex debug output.
	  It presently only works with gawk and mawk, not with "old"
	  or "new" awk.

	- testxxLexer.l, a sample C++ program that uses flex's scanner
	  class option ("-+").

	- fastwc/, a subdirectory containing examples of how to use flex
	  to write progressively higher-performance versions of the Unix
	  "wc" utility.  This certainly should work with 2.5, but hasn't
	  been tested.
This directory contains some examples illustrating techniques for extracting
high-performance from flex scanners.  Each program implements a simplified
version of the Unix "wc" tool: read text from stdin and print the number of
characters, words, and lines present in the text.  All programs were compiled
using gcc (version unavailable, sorry) with the -O flag, and run on a
SPARCstation 1+.  The input used was a PostScript file, mainly containing
figures, with the following "wc" counts:

	lines  words  characters
	214217 635954 2592172


The basic principles illustrated by these programs are:

	- match as much text with each rule as possible
	- adding rules does not slow you down!
	- avoid backing up

and the big caveat that comes with them is:

	- you buy performance with decreased maintainability; make
	  sure you really need it before applying the above techniques.

See the "Performance Considerations" section of flexdoc for more
details regarding these principles.


The different versions of "wc":

	mywc.c
		a simple but fairly efficient C version

	wc1.l	a naive flex "wc" implementation

	wc2.l	somewhat faster; adds rules to match multiple tokens at once

	wc3.l	faster still; adds more rules to match longer runs of tokens

	wc4.l	fastest; still more rules added; hard to do much better
		using flex (or, I suspect, hand-coding)

	wc5.l	identical to wc3.l except one rule has been slightly
		shortened, introducing backing-up

Timing results (all times in user CPU seconds):

	program	  time 	 notes
	-------   ----   -----
	wc1       16.4   default flex table compression (= -Cem)
	wc1        6.7   -Cf compression option
	/bin/wc	   5.8	 Sun's standard "wc" tool
	mywc	   4.6   simple but better C implementation!
	wc2	   4.6   as good as C implementation; built using -Cf
	wc3	   3.8   -Cf
	wc4	   3.3   -Cf
	wc5	   5.7   -Cf; ouch, backing up is expensive
This directory contains the example programs from the manual, and a
few other things as well. To make all the programs, simply type "make
-f Makefile.examples", and assuming you have flex and gcc, all will be
well.

To build the programs individually, type

   make -f Makefile.examples program_name

For example:

   make -f Makefile.examples expr
This file describes the flex test suite.

* WHO SHOULD USE THE TEST SUITE?

The test suite is intended to be used by flex developers, i.e., anyone hacking
the flex distribution. If you are simply installing flex, then you can ignore
this directory and its contents.

* STRUCTURE OF THE TEST SUITE

The testsuite consists of several tests. Each test is centered around
a scanner known to work with the most recent version of flex. In
general, after you modify your copy of the flex distribution, you
should re-run the test suite. Some of the tests may require certain
tools to be available (e.g., bison, diff). If any test returns an
error or generates an error message, then your modifications *may*
have broken a feature of flex. At a minimum, you'll want to
investigate the failure and determine if it's truly significant.

* HOW TO RUN THE TEST SUITE

To build and execute all tests from the top level of the flex source tree:

  $ make check

To build and execute a single test:

  $ cd tests/ # from the top level of the flex tree.
  $ make testname.log

  where "testname" is the name of the test. This is an automake-ism
  that will create (or re-create, if need be), a log of the particular
  test run that you're working on.

* HOW TO ADD A NEW TEST TO THE TEST SUITE

** List your test in the TESTS variable in Makefile.am in this
   directory. Note that due to the large number of tests, we use
   variables to group similar tests together. This also helps with
   handling the automake test suite requirements. Hopefully your test
   can be listed in SIMPLE_TESTS. You'll need to add the appropriate
   automake _SOURCES variable as well. If you're unsure, then consult
   the automake manual, paying attention to the parallel test harness
   section.

** On success, your test should return zero.

** On error, your test should return 1 (one) and print a message to
stderr, which will have been redirected to the log file created by the
automake test suite harness.

** If your test is skipped (e.g., because bison was not found), then
   the test should return 2 (two).

** Once your work is done, submit a patch via the flex development
   mailing list, the github pull request mechanism or some other
   suitable means.
openresolv is a resolvconf implementation which manages resolv.conf
You can find the latest version at http://roy.marples.name/projects/openresolv
It is written and maintained by Roy Marples <roy@marples.name>

This resolvconf implementation, along with its subscribers, work with a
POSIX compliant shell and userland utilities. It is designed to work without
tools such as sed as it *has* to work without /usr being available.

On systems where resolvconf is expected to be used before /var/run is available
for writing, you can configure openresolv to write somewhere else, like say a
ramdisk.
DESCRIPTION
    LZF is an extremely fast (not that much slower than a pure memcpy)
    compression algorithm. It is ideal for applications where you want to
    save *some* space but not at the cost of speed. It is ideal for
    repetitive data as well. The module is self-contained and very small.

    It's written in ISO-C with no external dependencies other than what
    C provides and can easily be #include'd into your code, no makefile
    changes or library builds requires.

    A C♯ implementation without external dependencies is available, too.

    I do not know for certain wether any patents in any countries apply
    to this algorithm, but at the moment it is believed that it is free
    from any patents. More importantly, it is also free to use in every
    software package (see LICENSE).

    See the lzf.h file for details on how the functions in this
    mini-library are to be used.

    NOTE: This package contains a very bare-bones command-line utility
    which is neither optimized for speed nor for compression. This library
    is really intented to be used inside larger programs.

AUTHOR
    This library was written by Marc Lehmann <schmorp@schmorp.de> (See also
    http://software.schmorp.de/pkg/liblzf).


The C♯ implementation of the LZF en-/decoder functions in this
directory was written (and is maintained) by

    Oren J. Maurice <oymaurice@hazorea.org.il>.

If you have any questions or improvements, you should contact the
original author (and maybe CC me, Marc Lehmann <liblzf@schmorp.de>).
Upgrading autoconf
===============================================================================

If you are in the mood to upgrade autoconf, you should:

 1. Consider not upgrading.
 2. No really, this is a hassle, you don't want to do it.
 3. Get the new version of autoconf and put it in <SRC>
 4. configure/build/install autoconf with --prefix=<PFX>
 5. Run autoupdate on all the m4 macros in llvm/autoconf/m4
 6. Run autoupdate on llvm/autoconf/configure.ac
 7. Regenerate configure script with AutoRegen.sh
 8. If there are any warnings from AutoRegen.sh, fix them and go to step 7.
 9. Test, test, test.
Low Level Virtual Machine (LLVM)
================================

This directory and its subdirectories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and runtime environments.

LLVM is open source software. You may freely distribute it under the terms of
the license agreement found in LICENSE.txt.

Please see the documentation provided in docs/ for further
assistance with LLVM, and in particular docs/GettingStarted.rst for getting
started with LLVM and docs/README.txt for an overview of LLVM's
documentation setup.

If you are writing a package for LLVM, see docs/Packaging.rst for our
suggestions.

//===----------------------------------------------------------------------===//
//                         ModuleMaker Sample project
//===----------------------------------------------------------------------===//

This project is an extremely simple example of using some simple pieces of the 
LLVM API.  The actual executable generated by this project simply emits an 
LLVM bitcode file to standard output.  It is designed to show some basic 
usage of LLVM APIs, and how to link to LLVM libraries.
//===----------------------------------------------------------------------===/
//                          Kaleidoscope with MCJIT
//===----------------------------------------------------------------------===//

The files in this directory are meant to accompany a series of blog posts
that describe the process of porting the Kaleidoscope tutorial to use the MCJIT
execution engine instead of the older JIT engine.

When the blog posts are ready this file will be updated with links to the posts.

These directories contain Makefiles that allow the code to be built in a
standalone manner, independent of the larger LLVM build infrastructure.//===----------------------------------------------------------------------===/
//                          Kaleidoscope with MCJIT
//===----------------------------------------------------------------------===//

The files in this directory are meant to accompany the first in a series of
three blog posts that describe the process of porting the Kaleidoscope tutorial
to use the MCJIT execution engine instead of the older JIT engine.

When the blog post is ready this file will be updated with a link to the post.

The source code in this directory demonstrates the third version of the
program, now modified to accept an input IR file on the command line and,
optionally, to use a basic caching mechanism to store generated object images.

The toy-jit.cpp file contains a version of the original JIT-based source code
that has been modified to support the input IR file command line option.

This directory contain a Makefile that allow the code to be built in a
standalone manner, independent of the larger LLVM build infrastructure. To build
the program you will need to have 'clang++' and 'llvm-config' in your path. If
you attempt to build using the LLVM 3.3 release, some minor modifications will
be required.

This directory also contains a Python script that may be used to generate random
input for the program and test scripts to capture data for rough performance
comparisons.  Another Python script will split generated input files into
definitions and function calls for the purpose of testing the IR input and
caching facilities.//===----------------------------------------------------------------------===/
//                          Kaleidoscope with MCJIT
//===----------------------------------------------------------------------===//

The files in this directory are meant to accompany the first in a series of
three blog posts that describe the process of porting the Kaleidoscope tutorial
to use the MCJIT execution engine instead of the older JIT engine.

When the blog post is ready this file will be updated with a link to the post.

The source code in this directory demonstrates the second version of the
program, now modified to implement a sort of 'lazy' compilation.

The toy-jit.cpp file contains a version of the original JIT-based source code
that has been modified to disable most stderr output for timing purposes.

This directory contain a Makefile that allow the code to be built in a
standalone manner, independent of the larger LLVM build infrastructure. To build
the program you will need to have 'clang++' and 'llvm-config' in your path. If
you attempt to build using the LLVM 3.3 release, some minor modifications will
be required.

This directory also contains a Python script that may be used to generate random
input for the program and test scripts to capture data for rough performance
comparisons.//===----------------------------------------------------------------------===/
//                          Kaleidoscope with MCJIT
//===----------------------------------------------------------------------===//

The files in this directory are meant to accompany the first in a series of
three blog posts that describe the process of porting the Kaleidoscope tutorial
to use the MCJIT execution engine instead of the older JIT engine.

When the blog post is ready this file will be updated with a link to the post.

The source code in this directory demonstrates the initial working version of
the program before subsequent performance improvements are applied.

This directory contain a Makefile that allow the code to be built in a
standalone manner, independent of the larger LLVM build infrastructure. To build
the program you will need to have 'clang++' and 'llvm-config' in your path. If
you attempt to build using the LLVM 3.3 release, some minor modifications will
be required, as mentioned in the blog posts.//===----------------------------------------------------------------------===/
//                          Kaleidoscope with MCJIT
//===----------------------------------------------------------------------===//

The files in this directory are meant to accompany the first in a series of
three blog posts that describe the process of porting the Kaleidoscope tutorial
to use the MCJIT execution engine instead of the older JIT engine.

When the blog post is ready this file will be updated with a link to the post.

The source code in this directory combines all previous versions, including the
old JIT-based implementation, into a single file for easy comparison with
command line options to select between the various possibilities.

This directory contain a Makefile that allow the code to be built in a
standalone manner, independent of the larger LLVM build infrastructure. To build
the program you will need to have 'clang++' and 'llvm-config' in your path. If
you attempt to build using the LLVM 3.3 release, some minor modifications will
be required.

This directory also contains a Python script that may be used to generate random
input for the program and test scripts to capture data for rough performance
comparisons.  Another Python script will split generated input files into
definitions and function calls for the purpose of testing the IR input and
caching facilities.This directory contains tests for the MIR file format parser and printer. It
was necessary to split the tests across different targets as no single target
covers all features available in machine IR.

Tests for codegen passes should NOT be here but in test/CodeGen/sometarget. As
a rule of thumb this directory should only contain tests using
'llc -run-pass none'.
+==============================================================================+
| How to organize the lit tests                                                |
+==============================================================================+

- If you write a test for matching a single DAG opcode or intrinsic, it should
  go in a file called {opcode_name,intrinsic_name}.ll (e.g. fadd.ll)

- If you write a test that matches several DAG opcodes and checks for a single
  ISA instruction, then that test should go in a file called {ISA_name}.ll (e.g.
  bfi_int.ll

- For all other tests, use your best judgement for organizing tests and naming
  the files.

+==============================================================================+
| Naming conventions                                                           |
+==============================================================================+

- Use dash '-' and not underscore '_' to separate words in file names, unless
  the file is named after a DAG opcode or ISA instruction that has an
  underscore '_' in its name.
This directory contains test cases for the instcombine transformation.  The
dated tests are actual bug tests, whereas the named tests are used to test
for features that the this pass should be capable of performing.

This directory contains test cases for individual source features of LLVM.
It is designed to make sure that the major components of LLVM support all of the
features of LLVM, for very small examples.  Entire programs should not go here.

Regression tests for individual bug fixes should go into the test/Regression dir.

These inputs were pre-generated to allow for easier testing of llvm-cov.

The files used to test the gcov compatible code coverage tool were generated
using the following method:

  test.gcno and test.gcda were create by running clang:
    clang++ -g -ftest-coverage -fprofile-arcs test.cpp

  test.cpp.gcov was created by running gcov 4.2.1:
    gcov test.cpp

The 'covmapping' files that are used to test llvm-cov contain raw sections
with the coverage mapping data generated by the compiler and linker. They are
created by running clang and llvm-cov:
  clang++ -fprofile-instr-generate -fcoverage-mapping -o test test.cpp
  llvm-cov convert-for-testing -o test.covmapping test

The 'profdata' files were generated by running an instrumented version of the
program and merging the raw profile data using llvm-profdata.
  ./test
  llvm-profdata merge -o test.profdata default.profraw
This directory contains testcases that the verifier is supposed to detect as
malformed LLVM code.  Testcases for situations that the verifier incorrectly
identifies as malformed should go in the test/Assembler directory.
This directory contains bindings for the LLVM compiler infrastructure to allow
programs written in languages other than C or C++ to take advantage of the LLVM
infrastructure--for instance, a self-hosted compiler front-end.
This directory contains LLVM bindings for the Go programming language
(http://golang.org).

Prerequisites
-------------

* Go 1.2+.
* CMake (to build LLVM).

Using the bindings
------------------

The package path "llvm.org/llvm/bindings/go/llvm" can be used to
import the latest development version of LLVM from SVN. Paths such as
"llvm.org/llvm.v36/bindings/go/llvm" refer to released versions of LLVM.

It is recommended to use the "-d" flag with "go get" to download the
package or a dependency, as an additional step is required to build LLVM
(see "Building LLVM" below).

Building LLVM
-------------

The script "build.sh" in this directory can be used to build LLVM and prepare
it to be used by the bindings. If you receive an error message from "go build"
like this:

    ./analysis.go:4:84: fatal error: llvm-c/Analysis.h: No such file or directory
     #include <llvm-c/Analysis.h> // If you are getting an error here read bindings/go/README.txt

or like this:

    ./llvm_dep.go:5: undefined: run_build_sh

it means that LLVM needs to be built or updated by running the script.

    $ $GOPATH/src/llvm.org/llvm/bindings/go/build.sh

Any command line arguments supplied to the script are passed to LLVM's CMake
build system. A good set of arguments to use during development are:

    $ $GOPATH/src/llvm.org/llvm/bindings/go/build.sh -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD=host -DBUILD_SHARED_LIBS=ON

Note that CMake keeps a cache of build settings so once you have built
LLVM there is no need to pass these arguments again after updating.

Alternatively, you can build LLVM yourself, but you must then set the
CGO_CPPFLAGS, CGO_CXXFLAGS and CGO_LDFLAGS environment variables:

    $ export CGO_CPPFLAGS="`/path/to/llvm-build/bin/llvm-config --cppflags`"
    $ export CGO_CXXFLAGS=-std=c++11
    $ export CGO_LDFLAGS="`/path/to/llvm-build/bin/llvm-config --ldflags --libs --system-libs all`"
    $ go build -tags byollvm
This directory contains LLVM bindings for the OCaml programming language
(http://ocaml.org).

Prerequisites
-------------

* OCaml 4.00.0+.
* ctypes 0.4+.
* oUnit 2+ (only required for tests).
* CMake (to build LLVM).

Building the bindings
---------------------

If all dependencies are present, the bindings will be built and installed
as a part of the default CMake configuration, with no further action.
They will only work with the specific OCaml compiler detected during the build.

The bindings can also be built out-of-tree, i.e. targeting a preinstalled
LLVM. To do this, configure the LLVM build tree as follows:

    $ cmake -DLLVM_OCAML_OUT_OF_TREE=TRUE \
            -DCMAKE_INSTALL_PREFIX=[OCaml install prefix] \
            [... any other options]

then build and install it as:

    $ make ocaml_all
    $ cmake -P bindings/ocaml/cmake_install.cmake
This directory contains Python bindings for LLVM's C library.

The bindings are currently a work in progress and are far from complete.
Use at your own risk.

Developer Info
==============

The single Python package is "llvm." Modules inside this package roughly
follow the names of the modules/headers defined by LLVM's C API.

Testing
-------

All test code is location in llvm/tests. Tests are written as classes
which inherit from llvm.tests.base.TestBase, which is a convenience base
class that provides common functionality.

Tests can be executed by installing nose:

    pip install nosetests

Then by running nosetests:

    nosetests

To see more output:

    nosetests -v

To step into the Python debugger while running a test, add the following
to your test at the point you wish to enter the debugger:

    import pdb; pdb.set_trace()

Then run nosetests:

    nosetests -s -v

You should strive for high code coverage. To see current coverage:

    pip install coverage
    nosetests --with-coverage --cover-html

Then open cover/index.html in your browser of choice to see the code coverage.

Style Convention
----------------

All code should pass PyFlakes. First, install PyFlakes:

    pip install pyflakes

Then at any time run it to see a report:

    pyflakes .

Eventually we'll provide a Pylint config file. In the meantime, install
Pylint:

    pip install pylint

And run:

    pylint llvm

And try to keep the number of violations to a minimum.
Target Independent Opportunities:

//===---------------------------------------------------------------------===//

We should recognized various "overflow detection" idioms and translate them into
llvm.uadd.with.overflow and similar intrinsics.  Here is a multiply idiom:

unsigned int mul(unsigned int a,unsigned int b) {
 if ((unsigned long long)a*b>0xffffffff)
   exit(0);
  return a*b;
}

The legalization code for mul-with-overflow needs to be made more robust before
this can be implemented though.

//===---------------------------------------------------------------------===//

Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and
precision don't matter (ffastmath).  Misc/mandel will like this. :)  This isn't
safe in general, even on darwin.  See the libm implementation of hypot for
examples (which special case when x/y are exactly zero to get signed zeros etc
right).

//===---------------------------------------------------------------------===//

On targets with expensive 64-bit multiply, we could LSR this:

for (i = ...; ++i) {
   x = 1ULL << i;

into:
 long long tmp = 1;
 for (i = ...; ++i, tmp+=tmp)
   x = tmp;

This would be a win on ppc32, but not x86 or ppc64.

//===---------------------------------------------------------------------===//

Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0)

//===---------------------------------------------------------------------===//

Reassociate should turn things like:

int factorial(int X) {
 return X*X*X*X*X*X*X*X;
}

into llvm.powi calls, allowing the code generator to produce balanced
multiplication trees.

First, the intrinsic needs to be extended to support integers, and second the
code generator needs to be enhanced to lower these to multiplication trees.

//===---------------------------------------------------------------------===//

Interesting? testcase for add/shift/mul reassoc:

int bar(int x, int y) {
  return x*x*x+y+x*x*x*x*x*y*y*y*y;
}
int foo(int z, int n) {
  return bar(z, n) + bar(2*z, 2*n);
}

This is blocked on not handling X*X*X -> powi(X, 3) (see note above).  The issue
is that we end up getting t = 2*X  s = t*t   and don't turn this into 4*X*X,
which is the same number of multiplies and is canonical, because the 2*X has
multiple uses.  Here's a simple example:

define i32 @test15(i32 %X1) {
  %B = mul i32 %X1, 47   ; X1*47
  %C = mul i32 %B, %B
  ret i32 %C
}


//===---------------------------------------------------------------------===//

Reassociate should handle the example in GCC PR16157:

extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; 
void f () {  /* this can be optimized to four additions... */ 
        b4 = a4 + a3 + a2 + a1 + a0; 
        b3 = a3 + a2 + a1 + a0; 
        b2 = a2 + a1 + a0; 
        b1 = a1 + a0; 
} 

This requires reassociating to forms of expressions that are already available,
something that reassoc doesn't think about yet.


//===---------------------------------------------------------------------===//

These two functions should generate the same code on big-endian systems:

int g(int *j,int *l)  {  return memcmp(j,l,4);  }
int h(int *j, int *l) {  return *j - *l; }

this could be done in SelectionDAGISel.cpp, along with other special cases,
for 1,2,4,8 bytes.

//===---------------------------------------------------------------------===//

It would be nice to revert this patch:
http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html

And teach the dag combiner enough to simplify the code expanded before 
legalize.  It seems plausible that this knowledge would let it simplify other
stuff too.

//===---------------------------------------------------------------------===//

For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal
to the type size. It works but can be overly conservative as the alignment of
specific vector types are target dependent.

//===---------------------------------------------------------------------===//

We should produce an unaligned load from code like this:

v4sf example(float *P) {
  return (v4sf){P[0], P[1], P[2], P[3] };
}

//===---------------------------------------------------------------------===//

Add support for conditional increments, and other related patterns.  Instead
of:

	movl 136(%esp), %eax
	cmpl $0, %eax
	je LBB16_2	#cond_next
LBB16_1:	#cond_true
	incl _foo
LBB16_2:	#cond_next

emit:
	movl	_foo, %eax
	cmpl	$1, %edi
	sbbl	$-1, %eax
	movl	%eax, _foo

//===---------------------------------------------------------------------===//

Combine: a = sin(x), b = cos(x) into a,b = sincos(x).

Expand these to calls of sin/cos and stores:
      double sincos(double x, double *sin, double *cos);
      float sincosf(float x, float *sin, float *cos);
      long double sincosl(long double x, long double *sin, long double *cos);

Doing so could allow SROA of the destination pointers.  See also:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17687

This is now easily doable with MRVs.  We could even make an intrinsic for this
if anyone cared enough about sincos.

//===---------------------------------------------------------------------===//

quantum_sigma_x in 462.libquantum contains the following loop:

      for(i=0; i<reg->size; i++)
	{
	  /* Flip the target bit of each basis state */
	  reg->node[i].state ^= ((MAX_UNSIGNED) 1 << target);
	} 

Where MAX_UNSIGNED/state is a 64-bit int.  On a 32-bit platform it would be just
so cool to turn it into something like:

   long long Res = ((MAX_UNSIGNED) 1 << target);
   if (target < 32) {
     for(i=0; i<reg->size; i++)
       reg->node[i].state ^= Res & 0xFFFFFFFFULL;
   } else {
     for(i=0; i<reg->size; i++)
       reg->node[i].state ^= Res & 0xFFFFFFFF00000000ULL
   }
   
... which would only do one 32-bit XOR per loop iteration instead of two.

It would also be nice to recognize the reg->size doesn't alias reg->node[i],
but this requires TBAA.

//===---------------------------------------------------------------------===//

This isn't recognized as bswap by instcombine (yes, it really is bswap):

unsigned long reverse(unsigned v) {
    unsigned t;
    t = v ^ ((v << 16) | (v >> 16));
    t &= ~0xff0000;
    v = (v << 24) | (v >> 8);
    return v ^ (t >> 8);
}

//===---------------------------------------------------------------------===//

[LOOP DELETION]

We don't delete this output free loop, because trip count analysis doesn't
realize that it is finite (if it were infinite, it would be undefined).  Not
having this blocks Loop Idiom from matching strlen and friends.  

void foo(char *C) {
  int x = 0;
  while (*C)
    ++x,++C;
}

//===---------------------------------------------------------------------===//

[LOOP RECOGNITION]

These idioms should be recognized as popcount (see PR1488):

unsigned countbits_slow(unsigned v) {
  unsigned c;
  for (c = 0; v; v >>= 1)
    c += v & 1;
  return c;
}

unsigned int popcount(unsigned int input) {
  unsigned int count = 0;
  for (unsigned int i =  0; i < 4 * 8; i++)
    count += (input >> i) & i;
  return count;
}

This should be recognized as CLZ:  rdar://8459039

unsigned clz_a(unsigned a) {
  int i;
  for (i=0;i<32;i++)
    if (a & (1<<(31-i)))
      return i;
  return 32;
}

This sort of thing should be added to the loop idiom pass.

//===---------------------------------------------------------------------===//

These should turn into single 16-bit (unaligned?) loads on little/big endian
processors.

unsigned short read_16_le(const unsigned char *adr) {
  return adr[0] | (adr[1] << 8);
}
unsigned short read_16_be(const unsigned char *adr) {
  return (adr[0] << 8) | adr[1];
}

//===---------------------------------------------------------------------===//

-instcombine should handle this transform:
   icmp pred (sdiv X / C1 ), C2
when X, C1, and C2 are unsigned.  Similarly for udiv and signed operands. 

Currently InstCombine avoids this transform but will do it when the signs of
the operands and the sign of the divide match. See the FIXME in 
InstructionCombining.cpp in the visitSetCondInst method after the switch case 
for Instruction::UDiv (around line 4447) for more details.

The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of
this construct. 

//===---------------------------------------------------------------------===//

[LOOP OPTIMIZATION]

SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization
opportunities in its double_array_divs_variable function: it needs loop
interchange, memory promotion (which LICM already does), vectorization and
variable trip count loop unrolling (since it has a constant trip count). ICC
apparently produces this very nice code with -ffast-math:

..B1.70:                        # Preds ..B1.70 ..B1.69
       mulpd     %xmm0, %xmm1                                  #108.2
       mulpd     %xmm0, %xmm1                                  #108.2
       mulpd     %xmm0, %xmm1                                  #108.2
       mulpd     %xmm0, %xmm1                                  #108.2
       addl      $8, %edx                                      #
       cmpl      $131072, %edx                                 #108.2
       jb        ..B1.70       # Prob 99%                      #108.2

It would be better to count down to zero, but this is a lot better than what we
do.

//===---------------------------------------------------------------------===//

Consider:

typedef unsigned U32;
typedef unsigned long long U64;
int test (U32 *inst, U64 *regs) {
    U64 effective_addr2;
    U32 temp = *inst;
    int r1 = (temp >> 20) & 0xf;
    int b2 = (temp >> 16) & 0xf;
    effective_addr2 = temp & 0xfff;
    if (b2) effective_addr2 += regs[b2];
    b2 = (temp >> 12) & 0xf;
    if (b2) effective_addr2 += regs[b2];
    effective_addr2 &= regs[4];
     if ((effective_addr2 & 3) == 0)
        return 1;
    return 0;
}

Note that only the low 2 bits of effective_addr2 are used.  On 32-bit systems,
we don't eliminate the computation of the top half of effective_addr2 because
we don't have whole-function selection dags.  On x86, this means we use one
extra register for the function when effective_addr2 is declared as U64 than
when it is declared U32.

PHI Slicing could be extended to do this.

//===---------------------------------------------------------------------===//

Tail call elim should be more aggressive, checking to see if the call is
followed by an uncond branch to an exit block.

; This testcase is due to tail-duplication not wanting to copy the return
; instruction into the terminating blocks because there was other code
; optimized out of the function after the taildup happened.
; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call

define i32 @t4(i32 %a) {
entry:
	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]
	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]
	br i1 %tmp.2, label %then.0, label %else.0

then.0:		; preds = %entry
	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]
	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]
	br label %return

else.0:		; preds = %entry
	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]
	br i1 %tmp.7, label %then.1, label %return

then.1:		; preds = %else.0
	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]
	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]
	br label %return

return:		; preds = %then.1, %else.0, %then.0
	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],
                            [ %tmp.9, %then.1 ]
	ret i32 %result.0
}

//===---------------------------------------------------------------------===//

Tail recursion elimination should handle:

int pow2m1(int n) {
 if (n == 0)
   return 0;
 return 2 * pow2m1 (n - 1) + 1;
}

Also, multiplies can be turned into SHL's, so they should be handled as if
they were associative.  "return foo() << 1" can be tail recursion eliminated.

//===---------------------------------------------------------------------===//

Argument promotion should promote arguments for recursive functions, like 
this:

; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val

define internal i32 @foo(i32* %x) {
entry:
	%tmp = load i32* %x		; <i32> [#uses=0]
	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]
	ret i32 %tmp.foo
}

define i32 @bar(i32* %x) {
entry:
	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]
	ret i32 %tmp3
}

//===---------------------------------------------------------------------===//

We should investigate an instruction sinking pass.  Consider this silly
example in pic mode:

#include <assert.h>
void foo(int x) {
  assert(x);
  //...
}

we compile this to:
_foo:
	subl	$28, %esp
	call	"L1$pb"
"L1$pb":
	popl	%eax
	cmpl	$0, 32(%esp)
	je	LBB1_2	# cond_true
LBB1_1:	# return
	# ...
	addl	$28, %esp
	ret
LBB1_2:	# cond_true
...

The PIC base computation (call+popl) is only used on one path through the 
code, but is currently always computed in the entry block.  It would be 
better to sink the picbase computation down into the block for the 
assertion, as it is the only one that uses it.  This happens for a lot of 
code with early outs.

Another example is loads of arguments, which are usually emitted into the 
entry block on targets like x86.  If not used in all paths through a 
function, they should be sunk into the ones that do.

In this case, whole-function-isel would also handle this.

//===---------------------------------------------------------------------===//

Investigate lowering of sparse switch statements into perfect hash tables:
http://burtleburtle.net/bob/hash/perfect.html

//===---------------------------------------------------------------------===//

We should turn things like "load+fabs+store" and "load+fneg+store" into the
corresponding integer operations.  On a yonah, this loop:

double a[256];
void foo() {
  int i, b;
  for (b = 0; b < 10000000; b++)
  for (i = 0; i < 256; i++)
    a[i] = -a[i];
}

is twice as slow as this loop:

long long a[256];
void foo() {
  int i, b;
  for (b = 0; b < 10000000; b++)
  for (i = 0; i < 256; i++)
    a[i] ^= (1ULL << 63);
}

and I suspect other processors are similar.  On X86 in particular this is a
big win because doing this with integers allows the use of read/modify/write
instructions.

//===---------------------------------------------------------------------===//

DAG Combiner should try to combine small loads into larger loads when 
profitable.  For example, we compile this C++ example:

struct THotKey { short Key; bool Control; bool Shift; bool Alt; };
extern THotKey m_HotKey;
THotKey GetHotKey () { return m_HotKey; }

into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):

__Z9GetHotKeyv:                         ## @_Z9GetHotKeyv
	movq	_m_HotKey@GOTPCREL(%rip), %rax
	movzwl	(%rax), %ecx
	movzbl	2(%rax), %edx
	shlq	$16, %rdx
	orq	%rcx, %rdx
	movzbl	3(%rax), %ecx
	shlq	$24, %rcx
	orq	%rdx, %rcx
	movzbl	4(%rax), %eax
	shlq	$32, %rax
	orq	%rcx, %rax
	ret

//===---------------------------------------------------------------------===//

We should add an FRINT node to the DAG to model targets that have legal
implementations of ceil/floor/rint.

//===---------------------------------------------------------------------===//

Consider:

int test() {
  long long input[8] = {1,0,1,0,1,0,1,0};
  foo(input);
}

Clang compiles this into:

  call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16, i1 false)
  %0 = getelementptr [8 x i64]* %input, i64 0, i64 0
  store i64 1, i64* %0, align 16
  %1 = getelementptr [8 x i64]* %input, i64 0, i64 2
  store i64 1, i64* %1, align 16
  %2 = getelementptr [8 x i64]* %input, i64 0, i64 4
  store i64 1, i64* %2, align 16
  %3 = getelementptr [8 x i64]* %input, i64 0, i64 6
  store i64 1, i64* %3, align 16

Which gets codegen'd into:

	pxor	%xmm0, %xmm0
	movaps	%xmm0, -16(%rbp)
	movaps	%xmm0, -32(%rbp)
	movaps	%xmm0, -48(%rbp)
	movaps	%xmm0, -64(%rbp)
	movq	$1, -64(%rbp)
	movq	$1, -48(%rbp)
	movq	$1, -32(%rbp)
	movq	$1, -16(%rbp)

It would be better to have 4 movq's of 0 instead of the movaps's.

//===---------------------------------------------------------------------===//

http://llvm.org/PR717:

The following code should compile into "ret int undef". Instead, LLVM
produces "ret int 0":

int f() {
  int x = 4;
  int y;
  if (x == 3) y = 0;
  return y;
}

//===---------------------------------------------------------------------===//

The loop unroller should partially unroll loops (instead of peeling them)
when code growth isn't too bad and when an unroll count allows simplification
of some code within the loop.  One trivial example is:

#include <stdio.h>
int main() {
    int nRet = 17;
    int nLoop;
    for ( nLoop = 0; nLoop < 1000; nLoop++ ) {
        if ( nLoop & 1 )
            nRet += 2;
        else
            nRet -= 1;
    }
    return nRet;
}

Unrolling by 2 would eliminate the '&1' in both copies, leading to a net
reduction in code size.  The resultant code would then also be suitable for
exit value computation.

//===---------------------------------------------------------------------===//

We miss a bunch of rotate opportunities on various targets, including ppc, x86,
etc.  On X86, we miss a bunch of 'rotate by variable' cases because the rotate
matching code in dag combine doesn't look through truncates aggressively 
enough.  Here are some testcases reduces from GCC PR17886:

unsigned long long f5(unsigned long long x, unsigned long long y) {
  return (x << 8) | ((y >> 48) & 0xffull);
}
unsigned long long f6(unsigned long long x, unsigned long long y, int z) {
  switch(z) {
  case 1:
    return (x << 8) | ((y >> 48) & 0xffull);
  case 2:
    return (x << 16) | ((y >> 40) & 0xffffull);
  case 3:
    return (x << 24) | ((y >> 32) & 0xffffffull);
  case 4:
    return (x << 32) | ((y >> 24) & 0xffffffffull);
  default:
    return (x << 40) | ((y >> 16) & 0xffffffffffull);
  }
}

//===---------------------------------------------------------------------===//

This (and similar related idioms):

unsigned int foo(unsigned char i) {
  return i | (i<<8) | (i<<16) | (i<<24);
} 

compiles into:

define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {
entry:
  %conv = zext i8 %i to i32
  %shl = shl i32 %conv, 8
  %shl5 = shl i32 %conv, 16
  %shl9 = shl i32 %conv, 24
  %or = or i32 %shl9, %conv
  %or6 = or i32 %or, %shl5
  %or10 = or i32 %or6, %shl
  ret i32 %or10
}

it would be better as:

unsigned int bar(unsigned char i) {
  unsigned int j=i | (i << 8); 
  return j | (j<<16);
}

aka:

define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {
entry:
  %conv = zext i8 %i to i32
  %shl = shl i32 %conv, 8
  %or = or i32 %shl, %conv
  %shl5 = shl i32 %or, 16
  %or6 = or i32 %shl5, %or
  ret i32 %or6
}

or even i*0x01010101, depending on the speed of the multiplier.  The best way to
handle this is to canonicalize it to a multiply in IR and have codegen handle
lowering multiplies to shifts on cpus where shifts are faster.

//===---------------------------------------------------------------------===//

We do a number of simplifications in simplify libcalls to strength reduce
standard library functions, but we don't currently merge them together.  For
example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy.  This can only
be done safely if "b" isn't modified between the strlen and memcpy of course.

//===---------------------------------------------------------------------===//

We compile this program: (from GCC PR11680)
http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487

Into code that runs the same speed in fast/slow modes, but both modes run 2x
slower than when compile with GCC (either 4.0 or 4.2):

$ llvm-g++ perf.cpp -O3 -fno-exceptions
$ time ./a.out fast
1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w

$ g++ perf.cpp -O3 -fno-exceptions
$ time ./a.out fast
0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w

It looks like we are making the same inlining decisions, so this may be raw
codegen badness or something else (haven't investigated).

//===---------------------------------------------------------------------===//

Divisibility by constant can be simplified (according to GCC PR12849) from
being a mulhi to being a mul lo (cheaper).  Testcase:

void bar(unsigned n) {
  if (n % 3 == 0)
    true();
}

This is equivalent to the following, where 2863311531 is the multiplicative
inverse of 3, and 1431655766 is ((2^32)-1)/3+1:
void bar(unsigned n) {
  if (n * 2863311531U < 1431655766U)
    true();
}

The same transformation can work with an even modulo with the addition of a
rotate: rotate the result of the multiply to the right by the number of bits
which need to be zero for the condition to be true, and shrink the compare RHS
by the same amount.  Unless the target supports rotates, though, that
transformation probably isn't worthwhile.

The transformation can also easily be made to work with non-zero equality
comparisons: just transform, for example, "n % 3 == 1" to "(n-1) % 3 == 0".

//===---------------------------------------------------------------------===//

Better mod/ref analysis for scanf would allow us to eliminate the vtable and a
bunch of other stuff from this example (see PR1604): 

#include <cstdio>
struct test {
    int val;
    virtual ~test() {}
};

int main() {
    test t;
    std::scanf("%d", &t.val);
    std::printf("%d\n", t.val);
}

//===---------------------------------------------------------------------===//

These functions perform the same computation, but produce different assembly.

define i8 @select(i8 %x) readnone nounwind {
  %A = icmp ult i8 %x, 250
  %B = select i1 %A, i8 0, i8 1
  ret i8 %B 
}

define i8 @addshr(i8 %x) readnone nounwind {
  %A = zext i8 %x to i9
  %B = add i9 %A, 6       ;; 256 - 250 == 6
  %C = lshr i9 %B, 8
  %D = trunc i9 %C to i8
  ret i8 %D
}

//===---------------------------------------------------------------------===//

From gcc bug 24696:
int
f (unsigned long a, unsigned long b, unsigned long c)
{
  return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);
}
int
f (unsigned long a, unsigned long b, unsigned long c)
{
  return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);
}
Both should combine to ((a|b) & (c-1)) != 0.  Currently not optimized with
"clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

From GCC Bug 20192:
#define PMD_MASK    (~((1UL << 23) - 1))
void clear_pmd_range(unsigned long start, unsigned long end)
{
   if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK))
       f();
}
The expression should optimize to something like
"!((start|end)&~PMD_MASK). Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return
i;}
unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}
These should combine to the same thing.  Currently, the first function
produces better code on X86.

//===---------------------------------------------------------------------===//

From GCC Bug 15784:
#define abs(x) x>0?x:-x
int f(int x, int y)
{
 return (abs(x)) >= 0;
}
This should optimize to x == INT_MIN. (With -fwrapv.)  Currently not
optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

From GCC Bug 14753:
void
rotate_cst (unsigned int a)
{
 a = (a << 10) | (a >> 22);
 if (a == 123)
   bar ();
}
void
minus_cst (unsigned int a)
{
 unsigned int tem;

 tem = 20 - a;
 if (tem == 5)
   bar ();
}
void
mask_gt (unsigned int a)
{
 /* This is equivalent to a > 15.  */
 if ((a & ~7) > 8)
   bar ();
}
void
rshift_gt (unsigned int a)
{
 /* This is equivalent to a > 23.  */
 if ((a >> 2) > 5)
   bar ();
}

All should simplify to a single comparison.  All of these are
currently not optimized with "clang -emit-llvm-bc | opt
-O3".

//===---------------------------------------------------------------------===//

From GCC Bug 32605:
int c(int* x) {return (char*)x+2 == (char*)x;}
Should combine to 0.  Currently not optimized with "clang
-emit-llvm-bc | opt -O3" (although llc can optimize it).

//===---------------------------------------------------------------------===//

int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}
Should be combined to  "((b >> 1) | b) & 1".  Currently not optimized
with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}
Should combine to "x | (y & 3)".  Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}
Should fold to "(~a & c) | (a & b)".  Currently not optimized with
"clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int a,int b) {return (~(a|b))|a;}
Should fold to "a|~b".  Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int a, int b) {return (a&&b) || (a&&!b);}
Should fold to "a".  Currently not optimized with "clang -emit-llvm-bc
| opt -O3".

//===---------------------------------------------------------------------===//

int a(int a, int b, int c) {return (a&&b) || (!a&&c);}
Should fold to "a ? b : c", or at least something sane.  Currently not
optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}
Should fold to a && (b || c).  Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int x) {return x | ((x & 8) ^ 8);}
Should combine to x | 8.  Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int x) {return x ^ ((x & 8) ^ 8);}
Should also combine to x | 8.  Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int a(int x) {return ((x | -9) ^ 8) & x;}
Should combine to x & -9.  Currently not optimized with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}
Should combine to "a * 0x88888888 >> 31".  Currently not optimized
with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

unsigned a(char* x) {if ((*x & 32) == 0) return b();}
There's an unnecessary zext in the generated code with "clang
-emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

unsigned a(unsigned long long x) {return 40 * (x >> 1);}
Should combine to "20 * (((unsigned)x) & -2)".  Currently not
optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int g(int x) { return (x - 10) < 0; }
Should combine to "x <= 9" (the sub has nsw).  Currently not
optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int g(int x) { return (x + 10) < 0; }
Should combine to "x < -10" (the add has nsw).  Currently not
optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

int f(int i, int j) { return i < j + 1; }
int g(int i, int j) { return j > i - 1; }
Should combine to "i <= j" (the add/sub has nsw).  Currently not
optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }
The & 15 part should be optimized away, it doesn't change the result. Currently
not optimized with "clang -emit-llvm-bc | opt -O3".

//===---------------------------------------------------------------------===//

This was noticed in the entryblock for grokdeclarator in 403.gcc:

        %tmp = icmp eq i32 %decl_context, 4          
        %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context 
        %tmp1 = icmp eq i32 %decl_context_addr.0, 1 
        %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0

tmp1 should be simplified to something like:
  (!tmp || decl_context == 1)

This allows recursive simplifications, tmp1 is used all over the place in
the function, e.g. by:

        %tmp23 = icmp eq i32 %decl_context_addr.1, 0            ; <i1> [#uses=1]
        %tmp24 = xor i1 %tmp1, true             ; <i1> [#uses=1]
        %or.cond8 = and i1 %tmp23, %tmp24               ; <i1> [#uses=1]

later.

//===---------------------------------------------------------------------===//

[STORE SINKING]

Store sinking: This code:

void f (int n, int *cond, int *res) {
    int i;
    *res = 0;
    for (i = 0; i < n; i++)
        if (*cond)
            *res ^= 234; /* (*) */
}

On this function GVN hoists the fully redundant value of *res, but nothing
moves the store out.  This gives us this code:

bb:		; preds = %bb2, %entry
	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	
	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]
	%1 = load i32* %cond, align 4
	%2 = icmp eq i32 %1, 0
	br i1 %2, label %bb2, label %bb1

bb1:		; preds = %bb
	%3 = xor i32 %.rle, 234	
	store i32 %3, i32* %res, align 4
	br label %bb2

bb2:		; preds = %bb, %bb1
	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	
	%indvar.next = add i32 %i.05, 1	
	%exitcond = icmp eq i32 %indvar.next, %n
	br i1 %exitcond, label %return, label %bb

DSE should sink partially dead stores to get the store out of the loop.

Here's another partial dead case:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395

//===---------------------------------------------------------------------===//

Scalar PRE hoists the mul in the common block up to the else:

int test (int a, int b, int c, int g) {
  int d, e;
  if (a)
    d = b * c;
  else
    d = b - c;
  e = b * c + g;
  return d + e;
}

It would be better to do the mul once to reduce codesize above the if.
This is GCC PR38204.


//===---------------------------------------------------------------------===//
This simple function from 179.art:

int winner, numf2s;
struct { double y; int   reset; } *Y;

void find_match() {
   int i;
   winner = 0;
   for (i=0;i<numf2s;i++)
       if (Y[i].y > Y[winner].y)
              winner =i;
}

Compiles into (with clang TBAA):

for.body:                                         ; preds = %for.inc, %bb.nph
  %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]
  %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]
  %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0
  %tmp5 = load double* %tmp4, align 8, !tbaa !4
  %idxprom7 = sext i32 %i.01718 to i64
  %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0
  %tmp11 = load double* %tmp10, align 8, !tbaa !4
  %cmp12 = fcmp ogt double %tmp5, %tmp11
  br i1 %cmp12, label %if.then, label %for.inc

if.then:                                          ; preds = %for.body
  %i.017 = trunc i64 %indvar to i32
  br label %for.inc

for.inc:                                          ; preds = %for.body, %if.then
  %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]
  %indvar.next = add i64 %indvar, 1
  %exitcond = icmp eq i64 %indvar.next, %tmp22
  br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body


It is good that we hoisted the reloads of numf2's, and Y out of the loop and
sunk the store to winner out.

However, this is awful on several levels: the conditional truncate in the loop
(-indvars at fault? why can't we completely promote the IV to i64?).

Beyond that, we have a partially redundant load in the loop: if "winner" (aka 
%i.01718) isn't updated, we reload Y[winner].y the next time through the loop.
Similarly, the addressing that feeds it (including the sext) is redundant. In
the end we get this generated assembly:

LBB0_2:                                 ## %for.body
                                        ## =>This Inner Loop Header: Depth=1
	movsd	(%rdi), %xmm0
	movslq	%edx, %r8
	shlq	$4, %r8
	ucomisd	(%rcx,%r8), %xmm0
	jbe	LBB0_4
	movl	%esi, %edx
LBB0_4:                                 ## %for.inc
	addq	$16, %rdi
	incq	%rsi
	cmpq	%rsi, %rax
	jne	LBB0_2

All things considered this isn't too bad, but we shouldn't need the movslq or
the shlq instruction, or the load folded into ucomisd every time through the
loop.

On an x86-specific topic, if the loop can't be restructure, the movl should be a
cmov.

//===---------------------------------------------------------------------===//

[STORE SINKING]

GCC PR37810 is an interesting case where we should sink load/store reload
into the if block and outside the loop, so we don't reload/store it on the
non-call path.

for () {
  *P += 1;
  if ()
    call();
  else
    ...
->
tmp = *P
for () {
  tmp += 1;
  if () {
    *P = tmp;
    call();
    tmp = *P;
  } else ...
}
*P = tmp;

We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but
we don't sink the store.  We need partially dead store sinking.

//===---------------------------------------------------------------------===//

[LOAD PRE CRIT EDGE SPLITTING]

GCC PR37166: Sinking of loads prevents SROA'ing the "g" struct on the stack
leading to excess stack traffic. This could be handled by GVN with some crazy
symbolic phi translation.  The code we get looks like (g is on the stack):

bb2:		; preds = %bb1
..
	%9 = getelementptr %struct.f* %g, i32 0, i32 0		
	store i32 %8, i32* %9, align  bel %bb3

bb3:		; preds = %bb1, %bb2, %bb
	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]
	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]
	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0
	%11 = load i32* %10, align 4

%11 is partially redundant, an in BB2 it should have the value %8.

GCC PR33344 and PR35287 are similar cases.


//===---------------------------------------------------------------------===//

[LOAD PRE]

There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the
GCC testsuite, ones we don't get yet are (checked through loadpre25):

[CRIT EDGE BREAKING]
predcom-4.c

[PRE OF READONLY CALL]
loadpre5.c

[TURN SELECT INTO BRANCH]
loadpre14.c loadpre15.c 

actually a conditional increment: loadpre18.c loadpre19.c

//===---------------------------------------------------------------------===//

[LOAD PRE / STORE SINKING / SPEC HACK]

This is a chunk of code from 456.hmmer:

int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,
     int *tpdm, int xmb, int *bp, int *ms) {
 int k, sc;
 for (k = 1; k <= M; k++) {
     mc[k] = mpp[k-1]   + tpmm[k-1];
     if ((sc = ip[k-1]  + tpim[k-1]) > mc[k])  mc[k] = sc;
     if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k])  mc[k] = sc;
     if ((sc = xmb  + bp[k])         > mc[k])  mc[k] = sc;
     mc[k] += ms[k];
   }
}

It is very profitable for this benchmark to turn the conditional stores to mc[k]
into a conditional move (select instr in IR) and allow the final store to do the
store.  See GCC PR27313 for more details.  Note that this is valid to xform even
with the new C++ memory model, since mc[k] is previously loaded and later
stored.

//===---------------------------------------------------------------------===//

[SCALAR PRE]
There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the
GCC testsuite.

//===---------------------------------------------------------------------===//

There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the
GCC testsuite.  For example, we get the first example in predcom-1.c, but 
miss the second one:

unsigned fib[1000];
unsigned avg[1000];

__attribute__ ((noinline))
void count_averages(int n) {
  int i;
  for (i = 1; i < n; i++)
    avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;
}

which compiles into two loads instead of one in the loop.

predcom-2.c is the same as predcom-1.c

predcom-3.c is very similar but needs loads feeding each other instead of
store->load.


//===---------------------------------------------------------------------===//

[ALIAS ANALYSIS]

Type based alias analysis:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705

We should do better analysis of posix_memalign.  At the least it should
no-capture its pointer argument, at best, we should know that the out-value
result doesn't point to anything (like malloc).  One example of this is in
SingleSource/Benchmarks/Misc/dt.c

//===---------------------------------------------------------------------===//

Interesting missed case because of control flow flattening (should be 2 loads):
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629
With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | 
             opt -mem2reg -gvn -instcombine | llvm-dis
we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT
VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS

//===---------------------------------------------------------------------===//

http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633
We could eliminate the branch condition here, loading from null is undefined:

struct S { int w, x, y, z; };
struct T { int r; struct S s; };
void bar (struct S, int);
void foo (int a, struct T b)
{
  struct S *c = 0;
  if (a)
    c = &b.s;
  bar (*c, a);
}

//===---------------------------------------------------------------------===//

simplifylibcalls should do several optimizations for strspn/strcspn:

strcspn(x, "a") -> inlined loop for up to 3 letters (similarly for strspn):

size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,
                     int __reject3) {
  register size_t __result = 0;
  while (__s[__result] != '\0' && __s[__result] != __reject1 &&
         __s[__result] != __reject2 && __s[__result] != __reject3)
    ++__result;
  return __result;
}

This should turn into a switch on the character.  See PR3253 for some notes on
codegen.

456.hmmer apparently uses strcspn and strspn a lot.  471.omnetpp uses strspn.

//===---------------------------------------------------------------------===//

simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917)

char buf1[6], buf2[6], buf3[4], buf4[4];
int i;

int foo (void) {
  int ret = snprintf (buf1, sizeof buf1, "abcde");
  ret += snprintf (buf2, sizeof buf2, "abcdef") * 16;
  ret += snprintf (buf3, sizeof buf3, "%s", i++ < 6 ? "abc" : "def") * 256;
  ret += snprintf (buf4, sizeof buf4, "%s", i++ > 10 ? "abcde" : "defgh")*4096;
  return ret;
}

//===---------------------------------------------------------------------===//

"gas" uses this idiom:
  else if (strchr ("+-/*%|&^:[]()~", *intel_parser.op_string))
..
  else if (strchr ("<>", *intel_parser.op_string)

Those should be turned into a switch.  SimplifyLibCalls only gets the second
case.

//===---------------------------------------------------------------------===//

252.eon contains this interesting code:

        %3072 = getelementptr [100 x i8]* %tempString, i32 0, i32 0
        %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind
        %strlen = call i32 @strlen(i8* %3072)    ; uses = 1
        %endptr = getelementptr [100 x i8]* %tempString, i32 0, i32 %strlen
        call void @llvm.memcpy.i32(i8* %endptr, 
          i8* getelementptr ([5 x i8]* @"\01LC42", i32 0, i32 0), i32 5, i32 1)
        %3074 = call i32 @strlen(i8* %endptr) nounwind readonly 
        
This is interesting for a couple reasons.  First, in this:

The memcpy+strlen strlen can be replaced with:

        %3074 = call i32 @strlen([5 x i8]* @"\01LC42") nounwind readonly 

Because the destination was just copied into the specified memory buffer.  This,
in turn, can be constant folded to "4".

In other code, it contains:

        %endptr6978 = bitcast i8* %endptr69 to i32*            
        store i32 7107374, i32* %endptr6978, align 1
        %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly    

Which could also be constant folded.  Whatever is producing this should probably
be fixed to leave this as a memcpy from a string.

Further, eon also has an interesting partially redundant strlen call:

bb8:            ; preds = %_ZN18eonImageCalculatorC1Ev.exit
        %682 = getelementptr i8** %argv, i32 6          ; <i8**> [#uses=2]
        %683 = load i8** %682, align 4          ; <i8*> [#uses=4]
        %684 = load i8* %683, align 1           ; <i8> [#uses=1]
        %685 = icmp eq i8 %684, 0               ; <i1> [#uses=1]
        br i1 %685, label %bb10, label %bb9

bb9:            ; preds = %bb8
        %686 = call i32 @strlen(i8* %683) nounwind readonly          
        %687 = icmp ugt i32 %686, 254           ; <i1> [#uses=1]
        br i1 %687, label %bb10, label %bb11

bb10:           ; preds = %bb9, %bb8
        %688 = call i32 @strlen(i8* %683) nounwind readonly          

This could be eliminated by doing the strlen once in bb8, saving code size and
improving perf on the bb8->9->10 path.

//===---------------------------------------------------------------------===//

I see an interesting fully redundant call to strlen left in 186.crafty:InputMove
which looks like:
       %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 
 

bb62:           ; preds = %bb55, %bb53
        %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ]             
        %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1
        %172 = add i32 %171, -1         ; <i32> [#uses=1]
        %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172       

...  no stores ...
       br i1 %or.cond, label %bb65, label %bb72

bb65:           ; preds = %bb62
        store i8 0, i8* %173, align 1
        br label %bb72

bb72:           ; preds = %bb65, %bb62
        %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ]            
        %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1

Note that on the bb62->bb72 path, that the %177 strlen call is partially
redundant with the %171 call.  At worst, we could shove the %177 strlen call
up into the bb65 block moving it out of the bb62->bb72 path.   However, note
that bb65 stores to the string, zeroing out the last byte.  This means that on
that path the value of %177 is actually just %171-1.  A sub is cheaper than a
strlen!

This pattern repeats several times, basically doing:

  A = strlen(P);
  P[A-1] = 0;
  B = strlen(P);
  where it is "obvious" that B = A-1.

//===---------------------------------------------------------------------===//

186.crafty has this interesting pattern with the "out.4543" variable:

call void @llvm.memcpy.i32(
        i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),
       i8* getelementptr ([7 x i8]* @"\01LC28700", i32 0, i32 0), i32 7, i32 1) 
%101 = call@printf(i8* ...   @out.4543, i32 0, i32 0)) nounwind 

It is basically doing:

  memcpy(globalarray, "string");
  printf(...,  globalarray);
  
Anyway, by knowing that printf just reads the memory and forward substituting
the string directly into the printf, this eliminates reads from globalarray.
Since this pattern occurs frequently in crafty (due to the "DisplayTime" and
other similar functions) there are many stores to "out".  Once all the printfs
stop using "out", all that is left is the memcpy's into it.  This should allow
globalopt to remove the "stored only" global.

//===---------------------------------------------------------------------===//

This code:

define inreg i32 @foo(i8* inreg %p) nounwind {
  %tmp0 = load i8* %p
  %tmp1 = ashr i8 %tmp0, 5
  %tmp2 = sext i8 %tmp1 to i32
  ret i32 %tmp2
}

could be dagcombine'd to a sign-extending load with a shift.
For example, on x86 this currently gets this:

	movb	(%eax), %al
	sarb	$5, %al
	movsbl	%al, %eax

while it could get this:

	movsbl	(%eax), %eax
	sarl	$5, %eax

//===---------------------------------------------------------------------===//

GCC PR31029:

int test(int x) { return 1-x == x; }     // --> return false
int test2(int x) { return 2-x == x; }    // --> return x == 1 ?

Always foldable for odd constants, what is the rule for even?

//===---------------------------------------------------------------------===//

PR 3381: GEP to field of size 0 inside a struct could be turned into GEP
for next field in struct (which is at same address).

For example: store of float into { {{}}, float } could be turned into a store to
the float directly.

//===---------------------------------------------------------------------===//

The arg promotion pass should make use of nocapture to make its alias analysis
stuff much more precise.

//===---------------------------------------------------------------------===//

The following functions should be optimized to use a select instead of a
branch (from gcc PR40072):

char char_int(int m) {if(m>7) return 0; return m;}
int int_char(char m) {if(m>7) return 0; return m;}

//===---------------------------------------------------------------------===//

int func(int a, int b) { if (a & 0x80) b |= 0x80; else b &= ~0x80; return b; }

Generates this:

define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {
entry:
  %0 = and i32 %a, 128                            ; <i32> [#uses=1]
  %1 = icmp eq i32 %0, 0                          ; <i1> [#uses=1]
  %2 = or i32 %b, 128                             ; <i32> [#uses=1]
  %3 = and i32 %b, -129                           ; <i32> [#uses=1]
  %b_addr.0 = select i1 %1, i32 %3, i32 %2        ; <i32> [#uses=1]
  ret i32 %b_addr.0
}

However, it's functionally equivalent to:

         b = (b & ~0x80) | (a & 0x80);

Which generates this:

define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {
entry:
  %0 = and i32 %b, -129                           ; <i32> [#uses=1]
  %1 = and i32 %a, 128                            ; <i32> [#uses=1]
  %2 = or i32 %0, %1                              ; <i32> [#uses=1]
  ret i32 %2
}

This can be generalized for other forms:

     b = (b & ~0x80) | (a & 0x40) << 1;

//===---------------------------------------------------------------------===//

These two functions produce different code. They shouldn't:

#include <stdint.h>
 
uint8_t p1(uint8_t b, uint8_t a) {
  b = (b & ~0xc0) | (a & 0xc0);
  return (b);
}
 
uint8_t p2(uint8_t b, uint8_t a) {
  b = (b & ~0x40) | (a & 0x40);
  b = (b & ~0x80) | (a & 0x80);
  return (b);
}

define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {
entry:
  %0 = and i8 %b, 63                              ; <i8> [#uses=1]
  %1 = and i8 %a, -64                             ; <i8> [#uses=1]
  %2 = or i8 %1, %0                               ; <i8> [#uses=1]
  ret i8 %2
}

define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {
entry:
  %0 = and i8 %b, 63                              ; <i8> [#uses=1]
  %.masked = and i8 %a, 64                        ; <i8> [#uses=1]
  %1 = and i8 %a, -128                            ; <i8> [#uses=1]
  %2 = or i8 %1, %0                               ; <i8> [#uses=1]
  %3 = or i8 %2, %.masked                         ; <i8> [#uses=1]
  ret i8 %3
}

//===---------------------------------------------------------------------===//

IPSCCP does not currently propagate argument dependent constants through
functions where it does not not all of the callers.  This includes functions
with normal external linkage as well as templates, C99 inline functions etc.
Specifically, it does nothing to:

define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {
entry:
  %0 = add nsw i32 %y, %z                         
  %1 = mul i32 %0, %x                             
  %2 = mul i32 %y, %z                             
  %3 = add nsw i32 %1, %2                         
  ret i32 %3
}

define i32 @test2() nounwind {
entry:
  %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind
  ret i32 %0
}

It would be interesting extend IPSCCP to be able to handle simple cases like
this, where all of the arguments to a call are constant.  Because IPSCCP runs
before inlining, trivial templates and inline functions are not yet inlined.
The results for a function + set of constant arguments should be memoized in a
map.

//===---------------------------------------------------------------------===//

The libcall constant folding stuff should be moved out of SimplifyLibcalls into
libanalysis' constantfolding logic.  This would allow IPSCCP to be able to
handle simple things like this:

static int foo(const char *X) { return strlen(X); }
int bar() { return foo("abcd"); }

//===---------------------------------------------------------------------===//

functionattrs doesn't know much about memcpy/memset.  This function should be
marked readnone rather than readonly, since it only twiddles local memory, but
functionattrs doesn't handle memset/memcpy/memmove aggressively:

struct X { int *p; int *q; };
int foo() {
 int i = 0, j = 1;
 struct X x, y;
 int **p;
 y.p = &i;
 x.q = &j;
 p = __builtin_memcpy (&x, &y, sizeof (int *));
 return **p;
}

This can be seen at:
$ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -functionattrs -S


//===---------------------------------------------------------------------===//

Missed instcombine transformation:
define i1 @a(i32 %x) nounwind readnone {
entry:
  %cmp = icmp eq i32 %x, 30
  %sub = add i32 %x, -30
  %cmp2 = icmp ugt i32 %sub, 9
  %or = or i1 %cmp, %cmp2
  ret i1 %or
}
This should be optimized to a single compare.  Testcase derived from gcc.

//===---------------------------------------------------------------------===//

Missed instcombine or reassociate transformation:
int a(int a, int b) { return (a==12)&(b>47)&(b<58); }

The sgt and slt should be combined into a single comparison. Testcase derived
from gcc.

//===---------------------------------------------------------------------===//

Missed instcombine transformation:

  %382 = srem i32 %tmp14.i, 64                    ; [#uses=1]
  %383 = zext i32 %382 to i64                     ; [#uses=1]
  %384 = shl i64 %381, %383                       ; [#uses=1]
  %385 = icmp slt i32 %tmp14.i, 64                ; [#uses=1]

The srem can be transformed to an and because if %tmp14.i is negative, the
shift is undefined.  Testcase derived from 403.gcc.

//===---------------------------------------------------------------------===//

This is a range comparison on a divided result (from 403.gcc):

  %1337 = sdiv i32 %1336, 8                       ; [#uses=1]
  %.off.i208 = add i32 %1336, 7                   ; [#uses=1]
  %1338 = icmp ult i32 %.off.i208, 15             ; [#uses=1]
  
We already catch this (removing the sdiv) if there isn't an add, we should
handle the 'add' as well.  This is a common idiom with it's builtin_alloca code.
C testcase:

int a(int x) { return (unsigned)(x/16+7) < 15; }

Another similar case involves truncations on 64-bit targets:

  %361 = sdiv i64 %.046, 8                        ; [#uses=1]
  %362 = trunc i64 %361 to i32                    ; [#uses=2]
...
  %367 = icmp eq i32 %362, 0                      ; [#uses=1]

//===---------------------------------------------------------------------===//

Missed instcombine/dagcombine transformation:
define void @lshift_lt(i8 zeroext %a) nounwind {
entry:
  %conv = zext i8 %a to i32
  %shl = shl i32 %conv, 3
  %cmp = icmp ult i32 %shl, 33
  br i1 %cmp, label %if.then, label %if.end

if.then:
  tail call void @bar() nounwind
  ret void

if.end:
  ret void
}
declare void @bar() nounwind

The shift should be eliminated.  Testcase derived from gcc.

//===---------------------------------------------------------------------===//

These compile into different code, one gets recognized as a switch and the
other doesn't due to phase ordering issues (PR6212):

int test1(int mainType, int subType) {
  if (mainType == 7)
    subType = 4;
  else if (mainType == 9)
    subType = 6;
  else if (mainType == 11)
    subType = 9;
  return subType;
}

int test2(int mainType, int subType) {
  if (mainType == 7)
    subType = 4;
  if (mainType == 9)
    subType = 6;
  if (mainType == 11)
    subType = 9;
  return subType;
}

//===---------------------------------------------------------------------===//

The following test case (from PR6576):

define i32 @mul(i32 %a, i32 %b) nounwind readnone {
entry:
 %cond1 = icmp eq i32 %b, 0                      ; <i1> [#uses=1]
 br i1 %cond1, label %exit, label %bb.nph
bb.nph:                                           ; preds = %entry
 %tmp = mul i32 %b, %a                           ; <i32> [#uses=1]
 ret i32 %tmp
exit:                                             ; preds = %entry
 ret i32 0
}

could be reduced to:

define i32 @mul(i32 %a, i32 %b) nounwind readnone {
entry:
 %tmp = mul i32 %b, %a
 ret i32 %tmp
}

//===---------------------------------------------------------------------===//

We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.
See GCC PR34949

Another interesting case is that something related could be used for variables
that go const after their ctor has finished.  In these cases, globalopt (which
can statically run the constructor) could mark the global const (so it gets put
in the readonly section).  A testcase would be:

#include <complex>
using namespace std;
const complex<char> should_be_in_rodata (42,-42);
complex<char> should_be_in_data (42,-42);
complex<char> should_be_in_bss;

Where we currently evaluate the ctors but the globals don't become const because
the optimizer doesn't know they "become const" after the ctor is done.  See
GCC PR4131 for more examples.

//===---------------------------------------------------------------------===//

In this code:

long foo(long x) {
  return x > 1 ? x : 1;
}

LLVM emits a comparison with 1 instead of 0. 0 would be equivalent
and cheaper on most targets.

LLVM prefers comparisons with zero over non-zero in general, but in this
case it choses instead to keep the max operation obvious.

//===---------------------------------------------------------------------===//

define void @a(i32 %x) nounwind {
entry:
  switch i32 %x, label %if.end [
    i32 0, label %if.then
    i32 1, label %if.then
    i32 2, label %if.then
    i32 3, label %if.then
    i32 5, label %if.then
  ]
if.then:
  tail call void @foo() nounwind
  ret void
if.end:
  ret void
}
declare void @foo()

Generated code on x86-64 (other platforms give similar results):
a:
	cmpl	$5, %edi
	ja	LBB2_2
	cmpl	$4, %edi
	jne	LBB2_3
.LBB0_2:
	ret
.LBB0_3:
	jmp	foo  # TAILCALL

If we wanted to be really clever, we could simplify the whole thing to
something like the following, which eliminates a branch:
	xorl    $1, %edi
	cmpl	$4, %edi
	ja	.LBB0_2
	ret
.LBB0_2:
	jmp	foo  # TAILCALL

//===---------------------------------------------------------------------===//

We compile this:

int foo(int a) { return (a & (~15)) / 16; }

Into:

define i32 @foo(i32 %a) nounwind readnone ssp {
entry:
  %and = and i32 %a, -16
  %div = sdiv i32 %and, 16
  ret i32 %div
}

but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case
should be instcombined into just "a >> 4".

We do get this at the codegen level, so something knows about it, but 
instcombine should catch it earlier:

_foo:                                   ## @foo
## BB#0:                                ## %entry
	movl	%edi, %eax
	sarl	$4, %eax
	ret

//===---------------------------------------------------------------------===//

This code (from GCC PR28685):

int test(int a, int b) {
  int lt = a < b;
  int eq = a == b;
  if (lt)
    return 1;
  return eq;
}

Is compiled to:

define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {
entry:
  %cmp = icmp slt i32 %a, %b
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %cmp5 = icmp eq i32 %a, %b
  %conv6 = zext i1 %cmp5 to i32
  ret i32 %conv6

return:                                           ; preds = %entry
  ret i32 1
}

it could be:

define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {
entry:
  %0 = icmp sle i32 %a, %b
  %retval = zext i1 %0 to i32
  ret i32 %retval
}

//===---------------------------------------------------------------------===//

This code can be seen in viterbi:

  %64 = call noalias i8* @malloc(i64 %62) nounwind
...
  %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind
  %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind

llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to
fold to %62.  This is a security win (overflows of malloc will get caught)
and also a performance win by exposing more memsets to the optimizer.

This occurs several times in viterbi.

Note that this would change the semantics of @llvm.objectsize which by its
current definition always folds to a constant. We also should make sure that
we remove checking in code like

  char *p = malloc(strlen(s)+1);
  __strcpy_chk(p, s, __builtin_objectsize(p, 0));

//===---------------------------------------------------------------------===//

clang -O3 currently compiles this code

int g(unsigned int a) {
  unsigned int c[100];
  c[10] = a;
  c[11] = a;
  unsigned int b = c[10] + c[11];
  if(b > a*2) a = 4;
  else a = 8;
  return a + 7;
}

into

define i32 @g(i32 a) nounwind readnone {
  %add = shl i32 %a, 1
  %mul = shl i32 %a, 1
  %cmp = icmp ugt i32 %add, %mul
  %a.addr.0 = select i1 %cmp, i32 11, i32 15
  ret i32 %a.addr.0
}

The icmp should fold to false. This CSE opportunity is only available
after GVN and InstCombine have run.

//===---------------------------------------------------------------------===//

memcpyopt should turn this:

define i8* @test10(i32 %x) {
  %alloc = call noalias i8* @malloc(i32 %x) nounwind
  call void @llvm.memset.p0i8.i32(i8* %alloc, i8 0, i32 %x, i32 1, i1 false)
  ret i8* %alloc
}

into a call to calloc.  We should make sure that we analyze calloc as
aggressively as malloc though.

//===---------------------------------------------------------------------===//

clang -O3 doesn't optimize this:

void f1(int* begin, int* end) {
  std::fill(begin, end, 0);
}

into a memset.  This is PR8942.

//===---------------------------------------------------------------------===//

clang -O3 -fno-exceptions currently compiles this code:

void f(int N) {
  std::vector<int> v(N);

  extern void sink(void*); sink(&v);
}

into

define void @_Z1fi(i32 %N) nounwind {
entry:
  %v2 = alloca [3 x i32*], align 8
  %v2.sub = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 0
  %tmpcast = bitcast [3 x i32*]* %v2 to %"class.std::vector"*
  %conv = sext i32 %N to i64
  store i32* null, i32** %v2.sub, align 8, !tbaa !0
  %tmp3.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 1
  store i32* null, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0
  %tmp4.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 2
  store i32* null, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0
  %cmp.i.i.i.i = icmp eq i32 %N, 0
  br i1 %cmp.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.thread.i.i, label %cond.true.i.i.i.i

_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.thread.i.i: ; preds = %entry
  store i32* null, i32** %v2.sub, align 8, !tbaa !0
  store i32* null, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0
  %add.ptr.i5.i.i = getelementptr inbounds i32* null, i64 %conv
  store i32* %add.ptr.i5.i.i, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0
  br label %_ZNSt6vectorIiSaIiEEC1EmRKiRKS0_.exit

cond.true.i.i.i.i:                                ; preds = %entry
  %cmp.i.i.i.i.i = icmp slt i32 %N, 0
  br i1 %cmp.i.i.i.i.i, label %if.then.i.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i

if.then.i.i.i.i.i:                                ; preds = %cond.true.i.i.i.i
  call void @_ZSt17__throw_bad_allocv() noreturn nounwind
  unreachable

_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i:    ; preds = %cond.true.i.i.i.i
  %mul.i.i.i.i.i = shl i64 %conv, 2
  %call3.i.i.i.i.i = call noalias i8* @_Znwm(i64 %mul.i.i.i.i.i) nounwind
  %0 = bitcast i8* %call3.i.i.i.i.i to i32*
  store i32* %0, i32** %v2.sub, align 8, !tbaa !0
  store i32* %0, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0
  %add.ptr.i.i.i = getelementptr inbounds i32* %0, i64 %conv
  store i32* %add.ptr.i.i.i, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0
  call void @llvm.memset.p0i8.i64(i8* %call3.i.i.i.i.i, i8 0, i64 %mul.i.i.i.i.i, i32 4, i1 false)
  br label %_ZNSt6vectorIiSaIiEEC1EmRKiRKS0_.exit

This is just the handling the construction of the vector. Most surprising here
is the fact that all three null stores in %entry are dead (because we do no
cross-block DSE).

Also surprising is that %conv isn't simplified to 0 in %....exit.thread.i.i.
This is a because the client of LazyValueInfo doesn't simplify all instruction
operands, just selected ones.

//===---------------------------------------------------------------------===//

clang -O3 -fno-exceptions currently compiles this code:

void f(char* a, int n) {
  __builtin_memset(a, 0, n);
  for (int i = 0; i < n; ++i)
    a[i] = 0;
}

into:

define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {
entry:
  %conv = sext i32 %n to i64
  tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false)
  %cmp8 = icmp sgt i32 %n, 0
  br i1 %cmp8, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %tmp10 = add i32 %n, -1
  %tmp11 = zext i32 %tmp10 to i64
  %tmp12 = add i64 %tmp11, 1
  call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false)
  ret void

for.end:                                          ; preds = %entry
  ret void
}

This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold
the two memset's together.

The issue with the addition only occurs in 64-bit mode, and appears to be at
least partially caused by Scalar Evolution not keeping its cache updated: it
returns the "wrong" result immediately after indvars runs, but figures out the
expected result if it is run from scratch on IR resulting from running indvars.

//===---------------------------------------------------------------------===//

clang -O3 -fno-exceptions currently compiles this code:

struct S {
  unsigned short m1, m2;
  unsigned char m3, m4;
};

void f(int N) {
  std::vector<S> v(N);
  extern void sink(void*); sink(&v);
}

into poor code for zero-initializing 'v' when N is >0. The problem is that
S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and
4 stores on each iteration. If the struct were 8 bytes, this gets turned into
a memset.

In order to handle this we have to:
  A) Teach clang to generate metadata for memsets of structs that have holes in
     them.
  B) Teach clang to use such a memset for zero init of this struct (since it has
     a hole), instead of doing elementwise zeroing.

//===---------------------------------------------------------------------===//

clang -O3 currently compiles this code:

extern const int magic;
double f() { return 0.0 * magic; }

into

@magic = external constant i32

define double @_Z1fv() nounwind readnone {
entry:
  %tmp = load i32* @magic, align 4, !tbaa !0
  %conv = sitofp i32 %tmp to double
  %mul = fmul double %conv, 0.000000e+00
  ret double %mul
}

We should be able to fold away this fmul to 0.0.  More generally, fmul(x,0.0)
can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and
not an INF.  The CannotBeNegativeZero predicate in value tracking should be
extended to support general "fpclassify" operations that can return 
yes/no/unknown for each of these predicates.

In this predicate, we know that uitofp is trivially never NaN or -0.0, and
we know that it isn't +/-Inf if the floating point type has enough exponent bits
to represent the largest integer value as < inf.

//===---------------------------------------------------------------------===//

When optimizing a transformation that can change the sign of 0.0 (such as the
0.0*val -> 0.0 transformation above), it might be provable that the sign of the
expression doesn't matter.  For example, by the above rules, we can't transform
fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the
expression is defined to be -0.0.

If we look at the uses of the fmul for example, we might be able to prove that
all uses don't care about the sign of zero.  For example, if we have:

  fadd(fmul(sitofp(x), 0.0), 2.0)

Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can
transform the fmul to 0.0, and then the fadd to 2.0.

//===---------------------------------------------------------------------===//

We should enhance memcpy/memcpy/memset to allow a metadata node on them
indicating that some bytes of the transfer are undefined.  This is useful for
frontends like clang when lowering struct copies, when some elements of the
struct are undefined.  Consider something like this:

struct x {
  char a;
  int b[4];
};
void foo(struct x*P);
struct x testfunc() {
  struct x V1, V2;
  foo(&V1);
  V2 = V1;

  return V2;
}

We currently compile this to:
$ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S


%struct.x = type { i8, [4 x i32] }

define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {
entry:
  %V1 = alloca %struct.x, align 4
  call void @foo(%struct.x* %V1)
  %tmp1 = bitcast %struct.x* %V1 to i8*
  %0 = bitcast %struct.x* %V1 to i160*
  %srcval1 = load i160* %0, align 4
  %tmp2 = bitcast %struct.x* %agg.result to i8*
  %1 = bitcast %struct.x* %agg.result to i160*
  store i160 %srcval1, i160* %1, align 4
  ret void
}

This happens because SRoA sees that the temp alloca has is being memcpy'd into
and out of and it has holes and it has to be conservative.  If we knew about the
holes, then this could be much much better.

Having information about these holes would also improve memcpy (etc) lowering at
llc time when it gets inlined, because we can use smaller transfers.  This also
avoids partial register stalls in some important cases.

//===---------------------------------------------------------------------===//

We don't fold (icmp (add) (add)) unless the two adds only have a single use.
There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for
example:

 %indvar.next90 = add i64 %indvar89, 1     ;; Has 2 uses
 %tmp96 = add i64 %tmp95, 1                ;; Has 1 use
 %exitcond97 = icmp eq i64 %indvar.next90, %tmp96

We don't fold this because we don't want to introduce an overlapped live range
of the ivar.  However if we can make this more aggressive without causing
performance issues in two ways:

1. If *either* the LHS or RHS has a single use, we can definitely do the
   transformation.  In the overlapping liverange case we're trading one register
   use for one fewer operation, which is a reasonable trade.  Before doing this
   we should verify that the llc output actually shrinks for some benchmarks.
2. If both ops have multiple uses, we can still fold it if the operations are
   both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't
   increase register pressure.

There are a ton of icmp's we aren't simplifying because of the reg pressure
concern.  Care is warranted here though because many of these are induction
variables and other cases that matter a lot to performance, like the above.
Here's a blob of code that you can drop into the bottom of visitICmp to see some
missed cases:

  { Value *A, *B, *C, *D;
    if (match(Op0, m_Add(m_Value(A), m_Value(B))) && 
        match(Op1, m_Add(m_Value(C), m_Value(D))) &&
        (A == C || A == D || B == C || B == D)) {
      errs() << "OP0 = " << *Op0 << "  U=" << Op0->getNumUses() << "\n";
      errs() << "OP1 = " << *Op1 << "  U=" << Op1->getNumUses() << "\n";
      errs() << "CMP = " << I << "\n\n";
    }
  }

//===---------------------------------------------------------------------===//

define i1 @test1(i32 %x) nounwind {
  %and = and i32 %x, 3
  %cmp = icmp ult i32 %and, 2
  ret i1 %cmp
}

Can be folded to (x & 2) == 0.

define i1 @test2(i32 %x) nounwind {
  %and = and i32 %x, 3
  %cmp = icmp ugt i32 %and, 1
  ret i1 %cmp
}

Can be folded to (x & 2) != 0.

SimplifyDemandedBits shrinks the "and" constant to 2 but instcombine misses the
icmp transform.

//===---------------------------------------------------------------------===//

This code:

typedef struct {
int f1:1;
int f2:1;
int f3:1;
int f4:29;
} t1;

typedef struct {
int f1:1;
int f2:1;
int f3:30;
} t2;

t1 s1;
t2 s2;

void func1(void)
{
s1.f1 = s2.f1;
s1.f2 = s2.f2;
}

Compiles into this IR (on x86-64 at least):

%struct.t1 = type { i8, [3 x i8] }
@s2 = global %struct.t1 zeroinitializer, align 4
@s1 = global %struct.t1 zeroinitializer, align 4
define void @func1() nounwind ssp noredzone {
entry:
  %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4
  %bf.val.sext5 = and i32 %0, 1
  %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4
  %2 = and i32 %1, -4
  %3 = or i32 %2, %bf.val.sext5
  %bf.val.sext26 = and i32 %0, 2
  %4 = or i32 %3, %bf.val.sext26
  store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4
  ret void
}

The two or/and's should be merged into one each.

//===---------------------------------------------------------------------===//

Machine level code hoisting can be useful in some cases.  For example, PR9408
is about:

typedef union {
 void (*f1)(int);
 void (*f2)(long);
} funcs;

void foo(funcs f, int which) {
 int a = 5;
 if (which) {
   f.f1(a);
 } else {
   f.f2(a);
 }
}

which we compile to:

foo:                                    # @foo
# BB#0:                                 # %entry
       pushq   %rbp
       movq    %rsp, %rbp
       testl   %esi, %esi
       movq    %rdi, %rax
       je      .LBB0_2
# BB#1:                                 # %if.then
       movl    $5, %edi
       callq   *%rax
       popq    %rbp
       ret
.LBB0_2:                                # %if.else
       movl    $5, %edi
       callq   *%rax
       popq    %rbp
       ret

Note that bb1 and bb2 are the same.  This doesn't happen at the IR level
because one call is passing an i32 and the other is passing an i64.

//===---------------------------------------------------------------------===//

I see this sort of pattern in 176.gcc in a few places (e.g. the start of
store_bit_field).  The rem should be replaced with a multiply and subtract:

  %3 = sdiv i32 %A, %B
  %4 = srem i32 %A, %B

Similarly for udiv/urem.  Note that this shouldn't be done on X86 or ARM,
which can do this in a single operation (instruction or libcall).  It is
probably best to do this in the code generator.

//===---------------------------------------------------------------------===//

unsigned foo(unsigned x, unsigned y) { return (x & y) == 0 || x == 0; }
should fold to (x & y) == 0.

//===---------------------------------------------------------------------===//

unsigned foo(unsigned x, unsigned y) { return x > y && x != 0; }
should fold to x > y.

//===---------------------------------------------------------------------===//
//===---------------------------------------------------------------------===//
// Random ideas for the X86 backend.
//===---------------------------------------------------------------------===//

Improvements to the multiply -> shift/add algorithm:
http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html

//===---------------------------------------------------------------------===//

Improve code like this (occurs fairly frequently, e.g. in LLVM):
long long foo(int x) { return 1LL << x; }

http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html
http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html
http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html

Another useful one would be  ~0ULL >> X and ~0ULL << X.

One better solution for 1LL << x is:
        xorl    %eax, %eax
        xorl    %edx, %edx
        testb   $32, %cl
        sete    %al
        setne   %dl
        sall    %cl, %eax
        sall    %cl, %edx

But that requires good 8-bit subreg support.

Also, this might be better.  It's an extra shift, but it's one instruction
shorter, and doesn't stress 8-bit subreg support.
(From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,
but without the unnecessary and.)
        movl %ecx, %eax
        shrl $5, %eax
        movl %eax, %edx
        xorl $1, %edx
        sall %cl, %eax
        sall %cl. %edx

64-bit shifts (in general) expand to really bad code.  Instead of using
cmovs, we should expand to a conditional branch like GCC produces.

//===---------------------------------------------------------------------===//

Some isel ideas:

1. Dynamic programming based approach when compile time is not an
   issue.
2. Code duplication (addressing mode) during isel.
3. Other ideas from "Register-Sensitive Selection, Duplication, and
   Sequencing of Instructions".
4. Scheduling for reduced register pressure.  E.g. "Minimum Register
   Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs"
   and other related papers.
   http://citeseer.ist.psu.edu/govindarajan01minimum.html

//===---------------------------------------------------------------------===//

Should we promote i16 to i32 to avoid partial register update stalls?

//===---------------------------------------------------------------------===//

Leave any_extend as pseudo instruction and hint to register
allocator. Delay codegen until post register allocation.
Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach
the coalescer how to deal with it though.

//===---------------------------------------------------------------------===//

It appears icc use push for parameter passing. Need to investigate.

//===---------------------------------------------------------------------===//

The instruction selector sometimes misses folding a load into a compare.  The
pattern is written as (cmp reg, (load p)).  Because the compare isn't
commutative, it is not matched with the load on both sides.  The dag combiner
should be made smart enough to canonicalize the load into the RHS of a compare
when it can invert the result of the compare for free.

//===---------------------------------------------------------------------===//

In many cases, LLVM generates code like this:

_test:
        movl 8(%esp), %eax
        cmpl %eax, 4(%esp)
        setl %al
        movzbl %al, %eax
        ret

on some processors (which ones?), it is more efficient to do this:

_test:
        movl 8(%esp), %ebx
        xor  %eax, %eax
        cmpl %ebx, 4(%esp)
        setl %al
        ret

Doing this correctly is tricky though, as the xor clobbers the flags.

//===---------------------------------------------------------------------===//

We should generate bts/btr/etc instructions on targets where they are cheap or
when codesize is important.  e.g., for:

void setbit(int *target, int bit) {
    *target |= (1 << bit);
}
void clearbit(int *target, int bit) {
    *target &= ~(1 << bit);
}

//===---------------------------------------------------------------------===//

Instead of the following for memset char*, 1, 10:

	movl $16843009, 4(%edx)
	movl $16843009, (%edx)
	movw $257, 8(%edx)

It might be better to generate

	movl $16843009, %eax
	movl %eax, 4(%edx)
	movl %eax, (%edx)
	movw al, 8(%edx)
	
when we can spare a register. It reduces code size.

//===---------------------------------------------------------------------===//

Evaluate what the best way to codegen sdiv X, (2^C) is.  For X/8, we currently
get this:

define i32 @test1(i32 %X) {
    %Y = sdiv i32 %X, 8
    ret i32 %Y
}

_test1:
        movl 4(%esp), %eax
        movl %eax, %ecx
        sarl $31, %ecx
        shrl $29, %ecx
        addl %ecx, %eax
        sarl $3, %eax
        ret

GCC knows several different ways to codegen it, one of which is this:

_test1:
        movl    4(%esp), %eax
        cmpl    $-1, %eax
        leal    7(%eax), %ecx
        cmovle  %ecx, %eax
        sarl    $3, %eax
        ret

which is probably slower, but it's interesting at least :)

//===---------------------------------------------------------------------===//

We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl
We should leave these as libcalls for everything over a much lower threshold,
since libc is hand tuned for medium and large mem ops (avoiding RFO for large
stores, TLB preheating, etc)

//===---------------------------------------------------------------------===//

Optimize this into something reasonable:
 x * copysign(1.0, y) * copysign(1.0, z)

//===---------------------------------------------------------------------===//

Optimize copysign(x, *y) to use an integer load from y.

//===---------------------------------------------------------------------===//

The following tests perform worse with LSR:

lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor.

//===---------------------------------------------------------------------===//

Adding to the list of cmp / test poor codegen issues:

int test(__m128 *A, __m128 *B) {
  if (_mm_comige_ss(*A, *B))
    return 3;
  else
    return 4;
}

_test:
	movl 8(%esp), %eax
	movaps (%eax), %xmm0
	movl 4(%esp), %eax
	movaps (%eax), %xmm1
	comiss %xmm0, %xmm1
	setae %al
	movzbl %al, %ecx
	movl $3, %eax
	movl $4, %edx
	cmpl $0, %ecx
	cmove %edx, %eax
	ret

Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There
are a number of issues. 1) We are introducing a setcc between the result of the
intrisic call and select. 2) The intrinsic is expected to produce a i32 value
so a any extend (which becomes a zero extend) is added.

We probably need some kind of target DAG combine hook to fix this.

//===---------------------------------------------------------------------===//

We generate significantly worse code for this than GCC:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150
http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701

There is also one case we do worse on PPC.

//===---------------------------------------------------------------------===//

For this:

int test(int a)
{
  return a * 3;
}

We currently emits
	imull $3, 4(%esp), %eax

Perhaps this is what we really should generate is? Is imull three or four
cycles? Note: ICC generates this:
	movl	4(%esp), %eax
	leal	(%eax,%eax,2), %eax

The current instruction priority is based on pattern complexity. The former is
more "complex" because it folds a load so the latter will not be emitted.

Perhaps we should use AddedComplexity to give LEA32r a higher priority? We
should always try to match LEA first since the LEA matching code does some
estimate to determine whether the match is profitable.

However, if we care more about code size, then imull is better. It's two bytes
shorter than movl + leal.

On a Pentium M, both variants have the same characteristics with regard
to throughput; however, the multiplication has a latency of four cycles, as
opposed to two cycles for the movl+lea variant.

//===---------------------------------------------------------------------===//

It appears gcc place string data with linkonce linkage in
.section __TEXT,__const_coal,coalesced instead of
.section __DATA,__const_coal,coalesced.
Take a look at darwin.h, there are other Darwin assembler directives that we
do not make use of.

//===---------------------------------------------------------------------===//

define i32 @foo(i32* %a, i32 %t) {
entry:
	br label %cond_true

cond_true:		; preds = %cond_true, %entry
	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]
	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]
	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]
	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]
	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]
	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]
	%tmp9 = add i32 %x.0.0, 1		; <i32> [#uses=2]
	%tmp = icmp sgt i32 %tmp9, 39		; <i1> [#uses=1]
	br i1 %tmp, label %bb12, label %cond_true

bb12:		; preds = %cond_true
	ret i32 %tmp7
}
is pessimized by -loop-reduce and -indvars

//===---------------------------------------------------------------------===//

u32 to float conversion improvement:

float uint32_2_float( unsigned u ) {
  float fl = (int) (u & 0xffff);
  float fh = (int) (u >> 16);
  fh *= 0x1.0p16f;
  return fh + fl;
}

00000000        subl    $0x04,%esp
00000003        movl    0x08(%esp,1),%eax
00000007        movl    %eax,%ecx
00000009        shrl    $0x10,%ecx
0000000c        cvtsi2ss        %ecx,%xmm0
00000010        andl    $0x0000ffff,%eax
00000015        cvtsi2ss        %eax,%xmm1
00000019        mulss   0x00000078,%xmm0
00000021        addss   %xmm1,%xmm0
00000025        movss   %xmm0,(%esp,1)
0000002a        flds    (%esp,1)
0000002d        addl    $0x04,%esp
00000030        ret

//===---------------------------------------------------------------------===//

When using fastcc abi, align stack slot of argument of type double on 8 byte
boundary to improve performance.

//===---------------------------------------------------------------------===//

GCC's ix86_expand_int_movcc function (in i386.c) has a ton of interesting
simplifications for integer "x cmp y ? a : b".

//===---------------------------------------------------------------------===//

Consider the expansion of:

define i32 @test3(i32 %X) {
        %tmp1 = urem i32 %X, 255
        ret i32 %tmp1
}

Currently it compiles to:

...
        movl $2155905153, %ecx
        movl 8(%esp), %esi
        movl %esi, %eax
        mull %ecx
...

This could be "reassociated" into:

        movl $2155905153, %eax
        movl 8(%esp), %ecx
        mull %ecx

to avoid the copy.  In fact, the existing two-address stuff would do this
except that mul isn't a commutative 2-addr instruction.  I guess this has
to be done at isel time based on the #uses to mul?

//===---------------------------------------------------------------------===//

Make sure the instruction which starts a loop does not cross a cacheline
boundary. This requires knowning the exact length of each machine instruction.
That is somewhat complicated, but doable. Example 256.bzip2:

In the new trace, the hot loop has an instruction which crosses a cacheline
boundary.  In addition to potential cache misses, this can't help decoding as I
imagine there has to be some kind of complicated decoder reset and realignment
to grab the bytes from the next cacheline.

532  532 0x3cfc movb     (1809(%esp, %esi), %bl   <<<--- spans 2 64 byte lines
942  942 0x3d03 movl     %dh, (1809(%esp, %esi)
937  937 0x3d0a incl     %esi
3    3   0x3d0b cmpb     %bl, %dl
27   27  0x3d0d jnz      0x000062db <main+11707>

//===---------------------------------------------------------------------===//

In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE.

//===---------------------------------------------------------------------===//

This could be a single 16-bit load.

int f(char *p) {
    if ((p[0] == 1) & (p[1] == 2)) return 1;
    return 0;
}

//===---------------------------------------------------------------------===//

We should inline lrintf and probably other libc functions.

//===---------------------------------------------------------------------===//

This code:

void test(int X) {
  if (X) abort();
}

is currently compiled to:

_test:
        subl $12, %esp
        cmpl $0, 16(%esp)
        jne LBB1_1
        addl $12, %esp
        ret
LBB1_1:
        call L_abort$stub

It would be better to produce:

_test:
        subl $12, %esp
        cmpl $0, 16(%esp)
        jne L_abort$stub
        addl $12, %esp
        ret

This can be applied to any no-return function call that takes no arguments etc.
Alternatively, the stack save/restore logic could be shrink-wrapped, producing
something like this:

_test:
        cmpl $0, 4(%esp)
        jne LBB1_1
        ret
LBB1_1:
        subl $12, %esp
        call L_abort$stub

Both are useful in different situations.  Finally, it could be shrink-wrapped
and tail called, like this:

_test:
        cmpl $0, 4(%esp)
        jne LBB1_1
        ret
LBB1_1:
        pop %eax   # realign stack.
        call L_abort$stub

Though this probably isn't worth it.

//===---------------------------------------------------------------------===//

Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with
a neg instead of a sub instruction.  Consider:

int test(char X) { return 7-X; }

we currently produce:
_test:
        movl $7, %eax
        movsbl 4(%esp), %ecx
        subl %ecx, %eax
        ret

We would use one fewer register if codegen'd as:

        movsbl 4(%esp), %eax
	neg %eax
        add $7, %eax
        ret

Note that this isn't beneficial if the load can be folded into the sub.  In
this case, we want a sub:

int test(int X) { return 7-X; }
_test:
        movl $7, %eax
        subl 4(%esp), %eax
        ret

//===---------------------------------------------------------------------===//

Leaf functions that require one 4-byte spill slot have a prolog like this:

_foo:
        pushl   %esi
        subl    $4, %esp
...
and an epilog like this:
        addl    $4, %esp
        popl    %esi
        ret

It would be smaller, and potentially faster, to push eax on entry and to
pop into a dummy register instead of using addl/subl of esp.  Just don't pop 
into any return registers :)

//===---------------------------------------------------------------------===//

The X86 backend should fold (branch (or (setcc, setcc))) into multiple 
branches.  We generate really poor code for:

double testf(double a) {
       return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);
}

For example, the entry BB is:

_testf:
        subl    $20, %esp
        pxor    %xmm0, %xmm0
        movsd   24(%esp), %xmm1
        ucomisd %xmm0, %xmm1
        setnp   %al
        sete    %cl
        testb   %cl, %al
        jne     LBB1_5  # UnifiedReturnBlock
LBB1_1: # cond_true


it would be better to replace the last four instructions with:

	jp LBB1_1
	je LBB1_5
LBB1_1:

We also codegen the inner ?: into a diamond:

       cvtss2sd        LCPI1_0(%rip), %xmm2
        cvtss2sd        LCPI1_1(%rip), %xmm3
        ucomisd %xmm1, %xmm0
        ja      LBB1_3  # cond_true
LBB1_2: # cond_true
        movapd  %xmm3, %xmm2
LBB1_3: # cond_true
        movapd  %xmm2, %xmm0
        ret

We should sink the load into xmm3 into the LBB1_2 block.  This should
be pretty easy, and will nuke all the copies.

//===---------------------------------------------------------------------===//

This:
        #include <algorithm>
        inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b)
        { return std::make_pair(a + b, a + b < a); }
        bool no_overflow(unsigned a, unsigned b)
        { return !full_add(a, b).second; }

Should compile to:
	addl	%esi, %edi
	setae	%al
	movzbl	%al, %eax
	ret

on x86-64, instead of the rather stupid-looking:
	addl	%esi, %edi
	setb	%al
	xorb	$1, %al
	movzbl	%al, %eax
	ret


//===---------------------------------------------------------------------===//

The following code:

bb114.preheader:		; preds = %cond_next94
	%tmp231232 = sext i16 %tmp62 to i32		; <i32> [#uses=1]
	%tmp233 = sub i32 32, %tmp231232		; <i32> [#uses=1]
	%tmp245246 = sext i16 %tmp65 to i32		; <i32> [#uses=1]
	%tmp252253 = sext i16 %tmp68 to i32		; <i32> [#uses=1]
	%tmp254 = sub i32 32, %tmp252253		; <i32> [#uses=1]
	%tmp553554 = bitcast i16* %tmp37 to i8*		; <i8*> [#uses=2]
	%tmp583584 = sext i16 %tmp98 to i32		; <i32> [#uses=1]
	%tmp585 = sub i32 32, %tmp583584		; <i32> [#uses=1]
	%tmp614615 = sext i16 %tmp101 to i32		; <i32> [#uses=1]
	%tmp621622 = sext i16 %tmp104 to i32		; <i32> [#uses=1]
	%tmp623 = sub i32 32, %tmp621622		; <i32> [#uses=1]
	br label %bb114

produces:

LBB3_5:	# bb114.preheader
	movswl	-68(%ebp), %eax
	movl	$32, %ecx
	movl	%ecx, -80(%ebp)
	subl	%eax, -80(%ebp)
	movswl	-52(%ebp), %eax
	movl	%ecx, -84(%ebp)
	subl	%eax, -84(%ebp)
	movswl	-70(%ebp), %eax
	movl	%ecx, -88(%ebp)
	subl	%eax, -88(%ebp)
	movswl	-50(%ebp), %eax
	subl	%eax, %ecx
	movl	%ecx, -76(%ebp)
	movswl	-42(%ebp), %eax
	movl	%eax, -92(%ebp)
	movswl	-66(%ebp), %eax
	movl	%eax, -96(%ebp)
	movw	$0, -98(%ebp)

This appears to be bad because the RA is not folding the store to the stack 
slot into the movl.  The above instructions could be:
	movl    $32, -80(%ebp)
...
	movl    $32, -84(%ebp)
...
This seems like a cross between remat and spill folding.

This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't
change, so we could simply subtract %eax from %ecx first and then use %ecx (or
vice-versa).

//===---------------------------------------------------------------------===//

This code:

	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]
	br i1 %tmp659, label %cond_true662, label %cond_next715

produces this:

	testw	%cx, %cx
	movswl	%cx, %esi
	jns	LBB4_109	# cond_next715

Shark tells us that using %cx in the testw instruction is sub-optimal. It
suggests using the 32-bit register (which is what ICC uses).

//===---------------------------------------------------------------------===//

We compile this:

void compare (long long foo) {
  if (foo < 4294967297LL)
    abort();
}

to:

compare:
        subl    $4, %esp
        cmpl    $0, 8(%esp)
        setne   %al
        movzbw  %al, %ax
        cmpl    $1, 12(%esp)
        setg    %cl
        movzbw  %cl, %cx
        cmove   %ax, %cx
        testb   $1, %cl
        jne     .LBB1_2 # UnifiedReturnBlock
.LBB1_1:        # ifthen
        call    abort
.LBB1_2:        # UnifiedReturnBlock
        addl    $4, %esp
        ret

(also really horrible code on ppc).  This is due to the expand code for 64-bit
compares.  GCC produces multiple branches, which is much nicer:

compare:
        subl    $12, %esp
        movl    20(%esp), %edx
        movl    16(%esp), %eax
        decl    %edx
        jle     .L7
.L5:
        addl    $12, %esp
        ret
        .p2align 4,,7
.L7:
        jl      .L4
        cmpl    $0, %eax
        .p2align 4,,8
        ja      .L5
.L4:
        .p2align 4,,9
        call    abort

//===---------------------------------------------------------------------===//

Tail call optimization improvements: Tail call optimization currently
pushes all arguments on the top of the stack (their normal place for
non-tail call optimized calls) that source from the callers arguments
or  that source from a virtual register (also possibly sourcing from
callers arguments).
This is done to prevent overwriting of parameters (see example
below) that might be used later.

example:  

int callee(int32, int64); 
int caller(int32 arg1, int32 arg2) { 
  int64 local = arg2 * 2; 
  return callee(arg2, (int64)local); 
}

[arg1]          [!arg2 no longer valid since we moved local onto it]
[arg2]      ->  [(int64)
[RETADDR]        local  ]

Moving arg1 onto the stack slot of callee function would overwrite
arg2 of the caller.

Possible optimizations:


 - Analyse the actual parameters of the callee to see which would
   overwrite a caller parameter which is used by the callee and only
   push them onto the top of the stack.

   int callee (int32 arg1, int32 arg2);
   int caller (int32 arg1, int32 arg2) {
       return callee(arg1,arg2);
   }

   Here we don't need to write any variables to the top of the stack
   since they don't overwrite each other.

   int callee (int32 arg1, int32 arg2);
   int caller (int32 arg1, int32 arg2) {
       return callee(arg2,arg1);
   }

   Here we need to push the arguments because they overwrite each
   other.

//===---------------------------------------------------------------------===//

main ()
{
  int i = 0;
  unsigned long int z = 0;

  do {
    z -= 0x00004000;
    i++;
    if (i > 0x00040000)
      abort ();
  } while (z > 0);
  exit (0);
}

gcc compiles this to:

_main:
	subl	$28, %esp
	xorl	%eax, %eax
	jmp	L2
L3:
	cmpl	$262144, %eax
	je	L10
L2:
	addl	$1, %eax
	cmpl	$262145, %eax
	jne	L3
	call	L_abort$stub
L10:
	movl	$0, (%esp)
	call	L_exit$stub

llvm:

_main:
	subl	$12, %esp
	movl	$1, %eax
	movl	$16384, %ecx
LBB1_1:	# bb
	cmpl	$262145, %eax
	jge	LBB1_4	# cond_true
LBB1_2:	# cond_next
	incl	%eax
	addl	$4294950912, %ecx
	cmpl	$16384, %ecx
	jne	LBB1_1	# bb
LBB1_3:	# bb11
	xorl	%eax, %eax
	addl	$12, %esp
	ret
LBB1_4:	# cond_true
	call	L_abort$stub

1. LSR should rewrite the first cmp with induction variable %ecx.
2. DAG combiner should fold
        leal    1(%eax), %edx
        cmpl    $262145, %edx
   =>
        cmpl    $262144, %eax

//===---------------------------------------------------------------------===//

define i64 @test(double %X) {
	%Y = fptosi double %X to i64
	ret i64 %Y
}

compiles to:

_test:
	subl	$20, %esp
	movsd	24(%esp), %xmm0
	movsd	%xmm0, 8(%esp)
	fldl	8(%esp)
	fisttpll	(%esp)
	movl	4(%esp), %edx
	movl	(%esp), %eax
	addl	$20, %esp
	#FP_REG_KILL
	ret

This should just fldl directly from the input stack slot.

//===---------------------------------------------------------------------===//

This code:
int foo (int x) { return (x & 65535) | 255; }

Should compile into:

_foo:
        movzwl  4(%esp), %eax
        orl     $255, %eax
        ret

instead of:
_foo:
	movl	$65280, %eax
	andl	4(%esp), %eax
	orl	$255, %eax
	ret

//===---------------------------------------------------------------------===//

We're codegen'ing multiply of long longs inefficiently:

unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {
  return arg1 *  arg2;
}

We compile to (fomit-frame-pointer):

_LLM:
	pushl	%esi
	movl	8(%esp), %ecx
	movl	16(%esp), %esi
	movl	%esi, %eax
	mull	%ecx
	imull	12(%esp), %esi
	addl	%edx, %esi
	imull	20(%esp), %ecx
	movl	%esi, %edx
	addl	%ecx, %edx
	popl	%esi
	ret

This looks like a scheduling deficiency and lack of remat of the load from
the argument area.  ICC apparently produces:

        movl      8(%esp), %ecx
        imull     12(%esp), %ecx
        movl      16(%esp), %eax
        imull     4(%esp), %eax 
        addl      %eax, %ecx  
        movl      4(%esp), %eax
        mull      12(%esp) 
        addl      %ecx, %edx
        ret

Note that it remat'd loads from 4(esp) and 12(esp).  See this GCC PR:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236

//===---------------------------------------------------------------------===//

We can fold a store into "zeroing a reg".  Instead of:

xorl    %eax, %eax
movl    %eax, 124(%esp)

we should get:

movl    $0, 124(%esp)

if the flags of the xor are dead.

Likewise, we isel "x<<1" into "add reg,reg".  If reg is spilled, this should
be folded into: shl [mem], 1

//===---------------------------------------------------------------------===//

In SSE mode, we turn abs and neg into a load from the constant pool plus a xor
or and instruction, for example:

	xorpd	LCPI1_0, %xmm2

However, if xmm2 gets spilled, we end up with really ugly code like this:

	movsd	(%esp), %xmm0
	xorpd	LCPI1_0, %xmm0
	movsd	%xmm0, (%esp)

Since we 'know' that this is a 'neg', we can actually "fold" the spill into
the neg/abs instruction, turning it into an *integer* operation, like this:

	xorl 2147483648, [mem+4]     ## 2147483648 = (1 << 31)

you could also use xorb, but xorl is less likely to lead to a partial register
stall.  Here is a contrived testcase:

double a, b, c;
void test(double *P) {
  double X = *P;
  a = X;
  bar();
  X = -X;
  b = X;
  bar();
  c = X;
}

//===---------------------------------------------------------------------===//

The generated code on x86 for checking for signed overflow on a multiply the
obvious way is much longer than it needs to be.

int x(int a, int b) {
  long long prod = (long long)a*b;
  return  prod > 0x7FFFFFFF || prod < (-0x7FFFFFFF-1);
}

See PR2053 for more details.

//===---------------------------------------------------------------------===//

We should investigate using cdq/ctld (effect: edx = sar eax, 31)
more aggressively; it should cost the same as a move+shift on any modern
processor, but it's a lot shorter. Downside is that it puts more
pressure on register allocation because it has fixed operands.

Example:
int abs(int x) {return x < 0 ? -x : x;}

gcc compiles this to the following when using march/mtune=pentium2/3/4/m/etc.:
abs:
        movl    4(%esp), %eax
        cltd
        xorl    %edx, %eax
        subl    %edx, %eax
        ret

//===---------------------------------------------------------------------===//

Take the following code (from 
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=16541):

extern unsigned char first_one[65536];
int FirstOnet(unsigned long long arg1)
{
  if (arg1 >> 48)
    return (first_one[arg1 >> 48]);
  return 0;
}


The following code is currently generated:
FirstOnet:
        movl    8(%esp), %eax
        cmpl    $65536, %eax
        movl    4(%esp), %ecx
        jb      .LBB1_2 # UnifiedReturnBlock
.LBB1_1:        # ifthen
        shrl    $16, %eax
        movzbl  first_one(%eax), %eax
        ret
.LBB1_2:        # UnifiedReturnBlock
        xorl    %eax, %eax
        ret

We could change the "movl 8(%esp), %eax" into "movzwl 10(%esp), %eax"; this
lets us change the cmpl into a testl, which is shorter, and eliminate the shift.

//===---------------------------------------------------------------------===//

We compile this function:

define i32 @foo(i32 %a, i32 %b, i32 %c, i8 zeroext  %d) nounwind  {
entry:
	%tmp2 = icmp eq i8 %d, 0		; <i1> [#uses=1]
	br i1 %tmp2, label %bb7, label %bb

bb:		; preds = %entry
	%tmp6 = add i32 %b, %a		; <i32> [#uses=1]
	ret i32 %tmp6

bb7:		; preds = %entry
	%tmp10 = sub i32 %a, %c		; <i32> [#uses=1]
	ret i32 %tmp10
}

to:

foo:                                    # @foo
# BB#0:                                 # %entry
	movl	4(%esp), %ecx
	cmpb	$0, 16(%esp)
	je	.LBB0_2
# BB#1:                                 # %bb
	movl	8(%esp), %eax
	addl	%ecx, %eax
	ret
.LBB0_2:                                # %bb7
	movl	12(%esp), %edx
	movl	%ecx, %eax
	subl	%edx, %eax
	ret

There's an obviously unnecessary movl in .LBB0_2, and we could eliminate a
couple more movls by putting 4(%esp) into %eax instead of %ecx.

//===---------------------------------------------------------------------===//

See rdar://4653682.

From flops:

LBB1_15:        # bb310
        cvtss2sd        LCPI1_0, %xmm1
        addsd   %xmm1, %xmm0
        movsd   176(%esp), %xmm2
        mulsd   %xmm0, %xmm2
        movapd  %xmm2, %xmm3
        mulsd   %xmm3, %xmm3
        movapd  %xmm3, %xmm4
        mulsd   LCPI1_23, %xmm4
        addsd   LCPI1_24, %xmm4
        mulsd   %xmm3, %xmm4
        addsd   LCPI1_25, %xmm4
        mulsd   %xmm3, %xmm4
        addsd   LCPI1_26, %xmm4
        mulsd   %xmm3, %xmm4
        addsd   LCPI1_27, %xmm4
        mulsd   %xmm3, %xmm4
        addsd   LCPI1_28, %xmm4
        mulsd   %xmm3, %xmm4
        addsd   %xmm1, %xmm4
        mulsd   %xmm2, %xmm4
        movsd   152(%esp), %xmm1
        addsd   %xmm4, %xmm1
        movsd   %xmm1, 152(%esp)
        incl    %eax
        cmpl    %eax, %esi
        jge     LBB1_15 # bb310
LBB1_16:        # bb358.loopexit
        movsd   152(%esp), %xmm0
        addsd   %xmm0, %xmm0
        addsd   LCPI1_22, %xmm0
        movsd   %xmm0, 152(%esp)

Rather than spilling the result of the last addsd in the loop, we should have
insert a copy to split the interval (one for the duration of the loop, one
extending to the fall through). The register pressure in the loop isn't high
enough to warrant the spill.

Also check why xmm7 is not used at all in the function.

//===---------------------------------------------------------------------===//

Take the following:

target datalayout = "e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:32:64-f32:32:32-f64:32:64-v64:64:64-v128:128:128-a0:0:64-f80:128:128-S128"
target triple = "i386-apple-darwin8"
@in_exit.4870.b = internal global i1 false		; <i1*> [#uses=2]
define fastcc void @abort_gzip() noreturn nounwind  {
entry:
	%tmp.b.i = load i1* @in_exit.4870.b		; <i1> [#uses=1]
	br i1 %tmp.b.i, label %bb.i, label %bb4.i
bb.i:		; preds = %entry
	tail call void @exit( i32 1 ) noreturn nounwind 
	unreachable
bb4.i:		; preds = %entry
	store i1 true, i1* @in_exit.4870.b
	tail call void @exit( i32 1 ) noreturn nounwind 
	unreachable
}
declare void @exit(i32) noreturn nounwind 

This compiles into:
_abort_gzip:                            ## @abort_gzip
## BB#0:                                ## %entry
	subl	$12, %esp
	movb	_in_exit.4870.b, %al
	cmpb	$1, %al
	jne	LBB0_2

We somehow miss folding the movb into the cmpb.

//===---------------------------------------------------------------------===//

We compile:

int test(int x, int y) {
  return x-y-1;
}

into (-m64):

_test:
	decl	%edi
	movl	%edi, %eax
	subl	%esi, %eax
	ret

it would be better to codegen as: x+~y  (notl+addl)

//===---------------------------------------------------------------------===//

This code:

int foo(const char *str,...)
{
 __builtin_va_list a; int x;
 __builtin_va_start(a,str); x = __builtin_va_arg(a,int); __builtin_va_end(a);
 return x;
}

gets compiled into this on x86-64:
	subq    $200, %rsp
        movaps  %xmm7, 160(%rsp)
        movaps  %xmm6, 144(%rsp)
        movaps  %xmm5, 128(%rsp)
        movaps  %xmm4, 112(%rsp)
        movaps  %xmm3, 96(%rsp)
        movaps  %xmm2, 80(%rsp)
        movaps  %xmm1, 64(%rsp)
        movaps  %xmm0, 48(%rsp)
        movq    %r9, 40(%rsp)
        movq    %r8, 32(%rsp)
        movq    %rcx, 24(%rsp)
        movq    %rdx, 16(%rsp)
        movq    %rsi, 8(%rsp)
        leaq    (%rsp), %rax
        movq    %rax, 192(%rsp)
        leaq    208(%rsp), %rax
        movq    %rax, 184(%rsp)
        movl    $48, 180(%rsp)
        movl    $8, 176(%rsp)
        movl    176(%rsp), %eax
        cmpl    $47, %eax
        jbe     .LBB1_3 # bb
.LBB1_1:        # bb3
        movq    184(%rsp), %rcx
        leaq    8(%rcx), %rax
        movq    %rax, 184(%rsp)
.LBB1_2:        # bb4
        movl    (%rcx), %eax
        addq    $200, %rsp
        ret
.LBB1_3:        # bb
        movl    %eax, %ecx
        addl    $8, %eax
        addq    192(%rsp), %rcx
        movl    %eax, 176(%rsp)
        jmp     .LBB1_2 # bb4

gcc 4.3 generates:
	subq    $96, %rsp
.LCFI0:
        leaq    104(%rsp), %rax
        movq    %rsi, -80(%rsp)
        movl    $8, -120(%rsp)
        movq    %rax, -112(%rsp)
        leaq    -88(%rsp), %rax
        movq    %rax, -104(%rsp)
        movl    $8, %eax
        cmpl    $48, %eax
        jb      .L6
        movq    -112(%rsp), %rdx
        movl    (%rdx), %eax
        addq    $96, %rsp
        ret
        .p2align 4,,10
        .p2align 3
.L6:
        mov     %eax, %edx
        addq    -104(%rsp), %rdx
        addl    $8, %eax
        movl    %eax, -120(%rsp)
        movl    (%rdx), %eax
        addq    $96, %rsp
        ret

and it gets compiled into this on x86:
	pushl   %ebp
        movl    %esp, %ebp
        subl    $4, %esp
        leal    12(%ebp), %eax
        movl    %eax, -4(%ebp)
        leal    16(%ebp), %eax
        movl    %eax, -4(%ebp)
        movl    12(%ebp), %eax
        addl    $4, %esp
        popl    %ebp
        ret

gcc 4.3 generates:
	pushl   %ebp
        movl    %esp, %ebp
        movl    12(%ebp), %eax
        popl    %ebp
        ret

//===---------------------------------------------------------------------===//

Teach tblgen not to check bitconvert source type in some cases. This allows us
to consolidate the following patterns in X86InstrMMX.td:

def : Pat<(v2i32 (bitconvert (i64 (vector_extract (v2i64 VR128:$src),
                                                  (iPTR 0))))),
          (v2i32 (MMX_MOVDQ2Qrr VR128:$src))>;
def : Pat<(v4i16 (bitconvert (i64 (vector_extract (v2i64 VR128:$src),
                                                  (iPTR 0))))),
          (v4i16 (MMX_MOVDQ2Qrr VR128:$src))>;
def : Pat<(v8i8 (bitconvert (i64 (vector_extract (v2i64 VR128:$src),
                                                  (iPTR 0))))),
          (v8i8 (MMX_MOVDQ2Qrr VR128:$src))>;

There are other cases in various td files.

//===---------------------------------------------------------------------===//

Take something like the following on x86-32:
unsigned a(unsigned long long x, unsigned y) {return x % y;}

We currently generate a libcall, but we really shouldn't: the expansion is
shorter and likely faster than the libcall.  The expected code is something
like the following:

	movl	12(%ebp), %eax
	movl	16(%ebp), %ecx
	xorl	%edx, %edx
	divl	%ecx
	movl	8(%ebp), %eax
	divl	%ecx
	movl	%edx, %eax
	ret

A similar code sequence works for division.

//===---------------------------------------------------------------------===//

We currently compile this:

define i32 @func1(i32 %v1, i32 %v2) nounwind {
entry:
  %t = call {i32, i1} @llvm.sadd.with.overflow.i32(i32 %v1, i32 %v2)
  %sum = extractvalue {i32, i1} %t, 0
  %obit = extractvalue {i32, i1} %t, 1
  br i1 %obit, label %overflow, label %normal
normal:
  ret i32 %sum
overflow:
  call void @llvm.trap()
  unreachable
}
declare {i32, i1} @llvm.sadd.with.overflow.i32(i32, i32)
declare void @llvm.trap()

to:

_func1:
	movl	4(%esp), %eax
	addl	8(%esp), %eax
	jo	LBB1_2	## overflow
LBB1_1:	## normal
	ret
LBB1_2:	## overflow
	ud2

it would be nice to produce "into" someday.

//===---------------------------------------------------------------------===//

Test instructions can be eliminated by using EFLAGS values from arithmetic
instructions. This is currently not done for mul, and, or, xor, neg, shl,
sra, srl, shld, shrd, atomic ops, and others. It is also currently not done
for read-modify-write instructions. It is also current not done if the
OF or CF flags are needed.

The shift operators have the complication that when the shift count is
zero, EFLAGS is not set, so they can only subsume a test instruction if
the shift count is known to be non-zero. Also, using the EFLAGS value
from a shift is apparently very slow on some x86 implementations.

In read-modify-write instructions, the root node in the isel match is
the store, and isel has no way for the use of the EFLAGS result of the
arithmetic to be remapped to the new node.

Add and subtract instructions set OF on signed overflow and CF on unsiged
overflow, while test instructions always clear OF and CF. In order to
replace a test with an add or subtract in a situation where OF or CF is
needed, codegen must be able to prove that the operation cannot see
signed or unsigned overflow, respectively.

//===---------------------------------------------------------------------===//

memcpy/memmove do not lower to SSE copies when possible.  A silly example is:
define <16 x float> @foo(<16 x float> %A) nounwind {
	%tmp = alloca <16 x float>, align 16
	%tmp2 = alloca <16 x float>, align 16
	store <16 x float> %A, <16 x float>* %tmp
	%s = bitcast <16 x float>* %tmp to i8*
	%s2 = bitcast <16 x float>* %tmp2 to i8*
	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16)
	%R = load <16 x float>* %tmp2
	ret <16 x float> %R
}

declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind

which compiles to:

_foo:
	subl	$140, %esp
	movaps	%xmm3, 112(%esp)
	movaps	%xmm2, 96(%esp)
	movaps	%xmm1, 80(%esp)
	movaps	%xmm0, 64(%esp)
	movl	60(%esp), %eax
	movl	%eax, 124(%esp)
	movl	56(%esp), %eax
	movl	%eax, 120(%esp)
	movl	52(%esp), %eax
        <many many more 32-bit copies>
      	movaps	(%esp), %xmm0
	movaps	16(%esp), %xmm1
	movaps	32(%esp), %xmm2
	movaps	48(%esp), %xmm3
	addl	$140, %esp
	ret

On Nehalem, it may even be cheaper to just use movups when unaligned than to
fall back to lower-granularity chunks.

//===---------------------------------------------------------------------===//

Implement processor-specific optimizations for parity with GCC on these
processors.  GCC does two optimizations:

1. ix86_pad_returns inserts a noop before ret instructions if immediately
   preceded by a conditional branch or is the target of a jump.
2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of
   code contains more than 3 branches.
   
The first one is done for all AMDs, Core2, and "Generic"
The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,
  Core 2, and "Generic"

//===---------------------------------------------------------------------===//
Testcase:
int x(int a) { return (a&0xf0)>>4; }

Current output:
	movl	4(%esp), %eax
	shrl	$4, %eax
	andl	$15, %eax
	ret

Ideal output:
	movzbl	4(%esp), %eax
	shrl	$4, %eax
	ret

//===---------------------------------------------------------------------===//

Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch
properly.

When the return value is not used (i.e. only care about the value in the
memory), x86 does not have to use add to implement these. Instead, it can use
add, sub, inc, dec instructions with the "lock" prefix.

This is currently implemented using a bit of instruction selection trick. The
issue is the target independent pattern produces one output and a chain and we
want to map it into one that just output a chain. The current trick is to select
it into a MERGE_VALUES with the first definition being an implicit_def. The
proper solution is to add new ISD opcodes for the no-output variant. DAG
combiner can then transform the node before it gets to target node selection.

Problem #2 is we are adding a whole bunch of x86 atomic instructions when in
fact these instructions are identical to the non-lock versions. We need a way to
add target specific information to target nodes and have this information
carried over to machine instructions. Asm printer (or JIT) can use this
information to add the "lock" prefix.

//===---------------------------------------------------------------------===//

struct B {
  unsigned char y0 : 1;
};

int bar(struct B* a) { return a->y0; }

define i32 @bar(%struct.B* nocapture %a) nounwind readonly optsize {
  %1 = getelementptr inbounds %struct.B* %a, i64 0, i32 0
  %2 = load i8* %1, align 1
  %3 = and i8 %2, 1
  %4 = zext i8 %3 to i32
  ret i32 %4
}

bar:                                    # @bar
# BB#0:
        movb    (%rdi), %al
        andb    $1, %al
        movzbl  %al, %eax
        ret

Missed optimization: should be movl+andl.

//===---------------------------------------------------------------------===//

The x86_64 abi says:

Booleans, when stored in a memory object, are stored as single byte objects the
value of which is always 0 (false) or 1 (true).

We are not using this fact:

int bar(_Bool *a) { return *a; }

define i32 @bar(i8* nocapture %a) nounwind readonly optsize {
  %1 = load i8* %a, align 1, !tbaa !0
  %tmp = and i8 %1, 1
  %2 = zext i8 %tmp to i32
  ret i32 %2
}

bar:
        movb    (%rdi), %al
        andb    $1, %al
        movzbl  %al, %eax
        ret

GCC produces

bar:
        movzbl  (%rdi), %eax
        ret

//===---------------------------------------------------------------------===//

Consider the following two functions compiled with clang:
_Bool foo(int *x) { return !(*x & 4); }
unsigned bar(int *x) { return !(*x & 4); }

foo:
	movl	4(%esp), %eax
	testb	$4, (%eax)
	sete	%al
	movzbl	%al, %eax
	ret

bar:
	movl	4(%esp), %eax
	movl	(%eax), %eax
	shrl	$2, %eax
	andl	$1, %eax
	xorl	$1, %eax
	ret

The second function generates more code even though the two functions are
are functionally identical.

//===---------------------------------------------------------------------===//

Take the following C code:
int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }

We generate the following IR with clang:
define i32 @f(i32 %a, i32 %b) nounwind readnone {
entry:
  %tmp = xor i32 %b, %a                           ; <i32> [#uses=1]
  %tmp6 = and i32 %tmp, 255                       ; <i32> [#uses=1]
  %cmp = icmp eq i32 %tmp6, 0                     ; <i1> [#uses=1]
  %conv5 = zext i1 %cmp to i32                    ; <i32> [#uses=1]
  ret i32 %conv5
}

And the following x86 code:
	xorl	%esi, %edi
	testb	$-1, %dil
	sete	%al
	movzbl	%al, %eax
	ret

A cmpb instead of the xorl+testb would be one instruction shorter.

//===---------------------------------------------------------------------===//

Given the following C code:
int f(int a, int b) { return (signed char)a == (signed char)b; }

We generate the following IR with clang:
define i32 @f(i32 %a, i32 %b) nounwind readnone {
entry:
  %sext = shl i32 %a, 24                          ; <i32> [#uses=1]
  %conv1 = ashr i32 %sext, 24                     ; <i32> [#uses=1]
  %sext6 = shl i32 %b, 24                         ; <i32> [#uses=1]
  %conv4 = ashr i32 %sext6, 24                    ; <i32> [#uses=1]
  %cmp = icmp eq i32 %conv1, %conv4               ; <i1> [#uses=1]
  %conv5 = zext i1 %cmp to i32                    ; <i32> [#uses=1]
  ret i32 %conv5
}

And the following x86 code:
	movsbl	%sil, %eax
	movsbl	%dil, %ecx
	cmpl	%eax, %ecx
	sete	%al
	movzbl	%al, %eax
	ret


It should be possible to eliminate the sign extensions.

//===---------------------------------------------------------------------===//

LLVM misses a load+store narrowing opportunity in this code:

%struct.bf = type { i64, i16, i16, i32 }

@bfi = external global %struct.bf*                ; <%struct.bf**> [#uses=2]

define void @t1() nounwind ssp {
entry:
  %0 = load %struct.bf** @bfi, align 8            ; <%struct.bf*> [#uses=1]
  %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]
  %2 = bitcast i16* %1 to i32*                    ; <i32*> [#uses=2]
  %3 = load i32* %2, align 1                      ; <i32> [#uses=1]
  %4 = and i32 %3, -65537                         ; <i32> [#uses=1]
  store i32 %4, i32* %2, align 1
  %5 = load %struct.bf** @bfi, align 8            ; <%struct.bf*> [#uses=1]
  %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]
  %7 = bitcast i16* %6 to i32*                    ; <i32*> [#uses=2]
  %8 = load i32* %7, align 1                      ; <i32> [#uses=1]
  %9 = and i32 %8, -131073                        ; <i32> [#uses=1]
  store i32 %9, i32* %7, align 1
  ret void
}

LLVM currently emits this:

  movq  bfi(%rip), %rax
  andl  $-65537, 8(%rax)
  movq  bfi(%rip), %rax
  andl  $-131073, 8(%rax)
  ret

It could narrow the loads and stores to emit this:

  movq  bfi(%rip), %rax
  andb  $-2, 10(%rax)
  movq  bfi(%rip), %rax
  andb  $-3, 10(%rax)
  ret

The trouble is that there is a TokenFactor between the store and the
load, making it non-trivial to determine if there's anything between
the load and the store which would prohibit narrowing.

//===---------------------------------------------------------------------===//

This code:
void foo(unsigned x) {
  if (x == 0) bar();
  else if (x == 1) qux();
}

currently compiles into:
_foo:
	movl	4(%esp), %eax
	cmpl	$1, %eax
	je	LBB0_3
	testl	%eax, %eax
	jne	LBB0_4

the testl could be removed:
_foo:
	movl	4(%esp), %eax
	cmpl	$1, %eax
	je	LBB0_3
	jb	LBB0_4

0 is the only unsigned number < 1.

//===---------------------------------------------------------------------===//

This code:

%0 = type { i32, i1 }

define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {
entry:
  %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x)
  %cmp = extractvalue %0 %uadd, 1
  %inc = zext i1 %cmp to i32
  %add = add i32 %x, %sum
  %z.0 = add i32 %add, %inc
  ret i32 %z.0
}

declare %0 @llvm.uadd.with.overflow.i32(i32, i32) nounwind readnone

compiles to:

_add32carry:                            ## @add32carry
	addl	%esi, %edi
	sbbl	%ecx, %ecx
	movl	%edi, %eax
	subl	%ecx, %eax
	ret

But it could be:

_add32carry:
	leal	(%rsi,%rdi), %eax
	cmpl	%esi, %eax
	adcl	$0, %eax
	ret

//===---------------------------------------------------------------------===//

The hot loop of 256.bzip2 contains code that looks a bit like this:

int foo(char *P, char *Q, int x, int y) {
  if (P[0] != Q[0])
     return P[0] < Q[0];
  if (P[1] != Q[1])
     return P[1] < Q[1];
  if (P[2] != Q[2])
     return P[2] < Q[2];
   return P[3] < Q[3];
}

In the real code, we get a lot more wrong than this.  However, even in this
code we generate:

_foo:                                   ## @foo
## BB#0:                                ## %entry
	movb	(%rsi), %al
	movb	(%rdi), %cl
	cmpb	%al, %cl
	je	LBB0_2
LBB0_1:                                 ## %if.then
	cmpb	%al, %cl
	jmp	LBB0_5
LBB0_2:                                 ## %if.end
	movb	1(%rsi), %al
	movb	1(%rdi), %cl
	cmpb	%al, %cl
	jne	LBB0_1
## BB#3:                                ## %if.end38
	movb	2(%rsi), %al
	movb	2(%rdi), %cl
	cmpb	%al, %cl
	jne	LBB0_1
## BB#4:                                ## %if.end60
	movb	3(%rdi), %al
	cmpb	3(%rsi), %al
LBB0_5:                                 ## %if.end60
	setl	%al
	movzbl	%al, %eax
	ret

Note that we generate jumps to LBB0_1 which does a redundant compare.  The
redundant compare also forces the register values to be live, which prevents
folding one of the loads into the compare.  In contrast, GCC 4.2 produces:

_foo:
	movzbl	(%rsi), %eax
	cmpb	%al, (%rdi)
	jne	L10
L12:
	movzbl	1(%rsi), %eax
	cmpb	%al, 1(%rdi)
	jne	L10
	movzbl	2(%rsi), %eax
	cmpb	%al, 2(%rdi)
	jne	L10
	movzbl	3(%rdi), %eax
	cmpb	3(%rsi), %al
L10:
	setl	%al
	movzbl	%al, %eax
	ret

which is "perfect".

//===---------------------------------------------------------------------===//

For the branch in the following code:
int a();
int b(int x, int y) {
  if (x & (1<<(y&7)))
    return a();
  return y;
}

We currently generate:
	movb	%sil, %al
	andb	$7, %al
	movzbl	%al, %eax
	btl	%eax, %edi
	jae	.LBB0_2

movl+andl would be shorter than the movb+andb+movzbl sequence.

//===---------------------------------------------------------------------===//

For the following:
struct u1 {
    float x, y;
};
float foo(struct u1 u) {
    return u.x + u.y;
}

We currently generate:
	movdqa	%xmm0, %xmm1
	pshufd	$1, %xmm0, %xmm0        # xmm0 = xmm0[1,0,0,0]
	addss	%xmm1, %xmm0
	ret

We could save an instruction here by commuting the addss.

//===---------------------------------------------------------------------===//

This (from PR9661):

float clamp_float(float a) {
        if (a > 1.0f)
                return 1.0f;
        else if (a < 0.0f)
                return 0.0f;
        else
                return a;
}

Could compile to:

clamp_float:                            # @clamp_float
        movss   .LCPI0_0(%rip), %xmm1
        minss   %xmm1, %xmm0
        pxor    %xmm1, %xmm1
        maxss   %xmm1, %xmm0
        ret

with -ffast-math.

//===---------------------------------------------------------------------===//

This function (from PR9803):

int clamp2(int a) {
        if (a > 5)
                a = 5;
        if (a < 0) 
                return 0;
        return a;
}

Compiles to:

_clamp2:                                ## @clamp2
        pushq   %rbp
        movq    %rsp, %rbp
        cmpl    $5, %edi
        movl    $5, %ecx
        cmovlel %edi, %ecx
        testl   %ecx, %ecx
        movl    $0, %eax
        cmovnsl %ecx, %eax
        popq    %rbp
        ret

The move of 0 could be scheduled above the test to make it is xor reg,reg.

//===---------------------------------------------------------------------===//

GCC PR48986.  We currently compile this:

void bar(void);
void yyy(int* p) {
    if (__sync_fetch_and_add(p, -1) == 1)
      bar();
}

into:
	movl	$-1, %eax
	lock
	xaddl	%eax, (%rdi)
	cmpl	$1, %eax
	je	LBB0_2

Instead we could generate:

	lock
	dec %rdi
	je LBB0_2

The trick is to match "fetch_and_add(X, -C) == C".

//===---------------------------------------------------------------------===//

unsigned t(unsigned a, unsigned b) {
  return a <= b ? 5 : -5;
}

We generate:
	movl	$5, %ecx
	cmpl	%esi, %edi
	movl	$-5, %eax
	cmovbel	%ecx, %eax

GCC:
	cmpl	%edi, %esi
	sbbl	%eax, %eax
	andl	$-10, %eax
	addl	$5, %eax

//===---------------------------------------------------------------------===//
//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//

AMD64 Optimization Manual 8.2 has some nice information about optimizing integer
multiplication by a constant. How much of it applies to Intel's X86-64
implementation? There are definite trade-offs to consider: latency vs. register
pressure vs. code size.

//===---------------------------------------------------------------------===//

Are we better off using branches instead of cmove to implement FP to
unsigned i64?

_conv:
	ucomiss	LC0(%rip), %xmm0
	cvttss2siq	%xmm0, %rdx
	jb	L3
	subss	LC0(%rip), %xmm0
	movabsq	$-9223372036854775808, %rax
	cvttss2siq	%xmm0, %rdx
	xorq	%rax, %rdx
L3:
	movq	%rdx, %rax
	ret

instead of

_conv:
	movss LCPI1_0(%rip), %xmm1
	cvttss2siq %xmm0, %rcx
	movaps %xmm0, %xmm2
	subss %xmm1, %xmm2
	cvttss2siq %xmm2, %rax
	movabsq $-9223372036854775808, %rdx
	xorq %rdx, %rax
	ucomiss %xmm1, %xmm0
	cmovb %rcx, %rax
	ret

Seems like the jb branch has high likelihood of being taken. It would have
saved a few instructions.

//===---------------------------------------------------------------------===//

It's not possible to reference AH, BH, CH, and DH registers in an instruction
requiring REX prefix. However, divb and mulb both produce results in AH. If isel
emits a CopyFromReg which gets turned into a movb and that can be allocated a
r8b - r15b.

To get around this, isel emits a CopyFromReg from AX and then right shift it
down by 8 and truncate it. It's not pretty but it works. We need some register
allocation magic to make the hack go away (e.g. putting additional constraints
on the result of the movb).

//===---------------------------------------------------------------------===//

The x86-64 ABI for hidden-argument struct returns requires that the
incoming value of %rdi be copied into %rax by the callee upon return.

The idea is that it saves callers from having to remember this value,
which would often require a callee-saved register. Callees usually
need to keep this value live for most of their body anyway, so it
doesn't add a significant burden on them.

We currently implement this in codegen, however this is suboptimal
because it means that it would be quite awkward to implement the
optimization for callers.

A better implementation would be to relax the LLVM IR rules for sret
arguments to allow a function with an sret argument to have a non-void
return type, and to have the front-end to set up the sret argument value
as the return value of the function. The front-end could more easily
emit uses of the returned struct value to be in terms of the function's
lowered return value, and it would free non-C frontends from a
complication only required by a C-based ABI.

//===---------------------------------------------------------------------===//

We get a redundant zero extension for code like this:

int mask[1000];
int foo(unsigned x) {
 if (x < 10)
   x = x * 45;
 else
   x = x * 78;
 return mask[x];
}

_foo:
LBB1_0:	## entry
	cmpl	$9, %edi
	jbe	LBB1_3	## bb
LBB1_1:	## bb1
	imull	$78, %edi, %eax
LBB1_2:	## bb2
	movl	%eax, %eax                    <----
	movq	_mask@GOTPCREL(%rip), %rcx
	movl	(%rcx,%rax,4), %eax
	ret
LBB1_3:	## bb
	imull	$45, %edi, %eax
	jmp	LBB1_2	## bb2
  
Before regalloc, we have:

        %reg1025<def> = IMUL32rri8 %reg1024, 45, %EFLAGS<imp-def>
        JMP mbb<bb2,0x203afb0>
    Successors according to CFG: 0x203afb0 (#3)

bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:
    Predecessors according to CFG: 0x203aec0 (#0)
        %reg1026<def> = IMUL32rri8 %reg1024, 78, %EFLAGS<imp-def>
    Successors according to CFG: 0x203afb0 (#3)

bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:
    Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2)
        %reg1027<def> = PHI %reg1025, mbb<bb,0x203af10>,
                            %reg1026, mbb<bb1,0x203af60>
        %reg1029<def> = MOVZX64rr32 %reg1027

so we'd have to know that IMUL32rri8 leaves the high word zero extended and to
be able to recognize the zero extend.  This could also presumably be implemented
if we have whole-function selectiondags.

//===---------------------------------------------------------------------===//

Take the following code
(from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):
extern unsigned long table[];
unsigned long foo(unsigned char *p) {
  unsigned long tag = *p;
  return table[tag >> 4] + table[tag & 0xf];
}

Current code generated:
	movzbl	(%rdi), %eax
	movq	%rax, %rcx
	andq	$240, %rcx
	shrq	%rcx
	andq	$15, %rax
	movq	table(,%rax,8), %rax
	addq	table(%rcx), %rax
	ret

Issues:
1. First movq should be movl; saves a byte.
2. Both andq's should be andl; saves another two bytes.  I think this was
   implemented at one point, but subsequently regressed.
3. shrq should be shrl; saves another byte.
4. The first andq can be completely eliminated by using a slightly more
   expensive addressing mode.

//===---------------------------------------------------------------------===//

Consider the following (contrived testcase, but contains common factors):

#include <stdarg.h>
int test(int x, ...) {
  int sum, i;
  va_list l;
  va_start(l, x);
  for (i = 0; i < x; i++)
    sum += va_arg(l, int);
  va_end(l);
  return sum;
}

Testcase given in C because fixing it will likely involve changing the IR
generated for it.  The primary issue with the result is that it doesn't do any
of the optimizations which are possible if we know the address of a va_list
in the current function is never taken:
1. We shouldn't spill the XMM registers because we only call va_arg with "int".
2. It would be nice if we could sroa the va_list.
3. Probably overkill, but it'd be cool if we could peel off the first five
iterations of the loop.

Other optimizations involving functions which use va_arg on floats which don't
have the address of a va_list taken:
1. Conversely to the above, we shouldn't spill general registers if we only
   call va_arg on "double".
2. If we know nothing more than 64 bits wide is read from the XMM registers,
   we can change the spilling code to reduce the amount of stack used by half.

//===---------------------------------------------------------------------===//
//===---------------------------------------------------------------------===//
// Random ideas for the X86 backend: MMX-specific stuff.
//===---------------------------------------------------------------------===//

//===---------------------------------------------------------------------===//

This:

#include <mmintrin.h>

__v2si qux(int A) {
  return (__v2si){ 0, A };
}

is compiled into:

_qux:
        subl $28, %esp
        movl 32(%esp), %eax
        movd %eax, %mm0
        movq %mm0, (%esp)
        movl (%esp), %eax
        movl %eax, 20(%esp)
        movq %mm0, 8(%esp)
        movl 12(%esp), %eax
        movl %eax, 16(%esp)
        movq 16(%esp), %mm0
        addl $28, %esp
        ret

Yuck!

GCC gives us:

_qux:
        subl    $12, %esp
        movl    16(%esp), %eax
        movl    20(%esp), %edx
        movl    $0, (%eax)
        movl    %edx, 4(%eax)
        addl    $12, %esp
        ret     $4

//===---------------------------------------------------------------------===//

We generate crappy code for this:

__m64 t() {
  return _mm_cvtsi32_si64(1);
}

_t:
	subl	$12, %esp
	movl	$1, %eax
	movd	%eax, %mm0
	movq	%mm0, (%esp)
	movl	(%esp), %eax
	movl	4(%esp), %edx
	addl	$12, %esp
	ret

The extra stack traffic is covered in the previous entry. But the other reason
is we are not smart about materializing constants in MMX registers. With -m64

	movl	$1, %eax
	movd	%eax, %mm0
	movd	%mm0, %rax
	ret

We should be using a constantpool load instead:
	movq	LC0(%rip), %rax
//===---------------------------------------------------------------------===//
// Testcases that crash the X86 backend because they aren't implemented
//===---------------------------------------------------------------------===//

These are cases we know the X86 backend doesn't handle.  Patches are welcome
and appreciated, because no one has signed up to implemented these yet.
Implementing these would allow elimination of the corresponding intrinsics,
which would be great.

1) vector shifts
2) vector comparisons
3) vector fp<->int conversions: PR2683, PR2684, PR2685, PR2686, PR2688
4) bitcasts from vectors to scalars: PR2804
5) llvm.atomic.cmp.swap.i128.p0i128: PR3462
//===---------------------------------------------------------------------===//
// Random ideas for the X86 backend: FP stack related stuff
//===---------------------------------------------------------------------===//

//===---------------------------------------------------------------------===//

Some targets (e.g. athlons) prefer freep to fstp ST(0):
http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html

//===---------------------------------------------------------------------===//

This should use fiadd on chips where it is profitable:
double foo(double P, int *I) { return P+*I; }

We have fiadd patterns now but the followings have the same cost and
complexity. We need a way to specify the later is more profitable.

def FpADD32m  : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,
                    [(set RFP:$dst, (fadd RFP:$src1,
                                     (extloadf64f32 addr:$src2)))]>;
                // ST(0) = ST(0) + [mem32]

def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,
                    [(set RFP:$dst, (fadd RFP:$src1,
                                     (X86fild addr:$src2, i32)))]>;
                // ST(0) = ST(0) + [mem32int]

//===---------------------------------------------------------------------===//

The FP stackifier should handle simple permutates to reduce number of shuffle
instructions, e.g. turning:

fld P	->		fld Q
fld Q			fld P
fxch

or:

fxch	->		fucomi
fucomi			jl X
jg X

Ideas:
http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html


//===---------------------------------------------------------------------===//

Add a target specific hook to DAG combiner to handle SINT_TO_FP and
FP_TO_SINT when the source operand is already in memory.

//===---------------------------------------------------------------------===//

Open code rint,floor,ceil,trunc:
http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html
http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html

Opencode the sincos[f] libcall.

//===---------------------------------------------------------------------===//

None of the FPStack instructions are handled in
X86RegisterInfo::foldMemoryOperand, which prevents the spiller from
folding spill code into the instructions.

//===---------------------------------------------------------------------===//

Currently the x86 codegen isn't very good at mixing SSE and FPStack
code:

unsigned int foo(double x) { return x; }

foo:
	subl $20, %esp
	movsd 24(%esp), %xmm0
	movsd %xmm0, 8(%esp)
	fldl 8(%esp)
	fisttpll (%esp)
	movl (%esp), %eax
	addl $20, %esp
	ret

This just requires being smarter when custom expanding fptoui.

//===---------------------------------------------------------------------===//
//===---------------------------------------------------------------------===//
// Random ideas for the X86 backend: SSE-specific stuff.
//===---------------------------------------------------------------------===//

//===---------------------------------------------------------------------===//

SSE Variable shift can be custom lowered to something like this, which uses a
small table + unaligned load + shuffle instead of going through memory.

__m128i_shift_right:
	.byte	  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15
	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

...
__m128i shift_right(__m128i value, unsigned long offset) {
  return _mm_shuffle_epi8(value,
               _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));
}

//===---------------------------------------------------------------------===//

SSE has instructions for doing operations on complex numbers, we should pattern
match them.   For example, this should turn into a horizontal add:

typedef float __attribute__((vector_size(16))) v4f32;
float f32(v4f32 A) {
  return A[0]+A[1]+A[2]+A[3];
}

Instead we get this:

_f32:                                   ## @f32
	pshufd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0,0,0]
	addss	%xmm0, %xmm1
	pshufd	$3, %xmm0, %xmm2        ## xmm2 = xmm0[3,0,0,0]
	movhlps	%xmm0, %xmm0            ## xmm0 = xmm0[1,1]
	movaps	%xmm0, %xmm3
	addss	%xmm1, %xmm3
	movdqa	%xmm2, %xmm0
	addss	%xmm3, %xmm0
	ret

Also, there are cases where some simple local SLP would improve codegen a bit.
compiling this:

_Complex float f32(_Complex float A, _Complex float B) {
  return A+B;
}

into:

_f32:                                   ## @f32
	movdqa	%xmm0, %xmm2
	addss	%xmm1, %xmm2
	pshufd	$1, %xmm1, %xmm1        ## xmm1 = xmm1[1,0,0,0]
	pshufd	$1, %xmm0, %xmm3        ## xmm3 = xmm0[1,0,0,0]
	addss	%xmm1, %xmm3
	movaps	%xmm2, %xmm0
	unpcklps	%xmm3, %xmm0    ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]
	ret

seems silly when it could just be one addps.


//===---------------------------------------------------------------------===//

Expand libm rounding functions inline:  Significant speedups possible.
http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html

//===---------------------------------------------------------------------===//

When compiled with unsafemath enabled, "main" should enable SSE DAZ mode and
other fast SSE modes.

//===---------------------------------------------------------------------===//

Think about doing i64 math in SSE regs on x86-32.

//===---------------------------------------------------------------------===//

This testcase should have no SSE instructions in it, and only one load from
a constant pool:

double %test3(bool %B) {
        %C = select bool %B, double 123.412, double 523.01123123
        ret double %C
}

Currently, the select is being lowered, which prevents the dag combiner from
turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'

The pattern isel got this one right.

//===---------------------------------------------------------------------===//

Lower memcpy / memset to a series of SSE 128 bit move instructions when it's
feasible.

//===---------------------------------------------------------------------===//

Codegen:
  if (copysign(1.0, x) == copysign(1.0, y))
into:
  if (x^y & mask)
when using SSE.

//===---------------------------------------------------------------------===//

Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half
of a v4sf value.

//===---------------------------------------------------------------------===//

Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.
Perhaps use pxor / xorp* to clear a XMM register first?

//===---------------------------------------------------------------------===//

External test Nurbs exposed some problems. Look for
__ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc
emits:

        movaps    (%edx), %xmm2                                 #59.21
        movaps    (%edx), %xmm5                                 #60.21
        movaps    (%edx), %xmm4                                 #61.21
        movaps    (%edx), %xmm3                                 #62.21
        movl      40(%ecx), %ebp                                #69.49
        shufps    $0, %xmm2, %xmm5                              #60.21
        movl      100(%esp), %ebx                               #69.20
        movl      (%ebx), %edi                                  #69.20
        imull     %ebp, %edi                                    #69.49
        addl      (%eax), %edi                                  #70.33
        shufps    $85, %xmm2, %xmm4                             #61.21
        shufps    $170, %xmm2, %xmm3                            #62.21
        shufps    $255, %xmm2, %xmm2                            #63.21
        lea       (%ebp,%ebp,2), %ebx                           #69.49
        negl      %ebx                                          #69.49
        lea       -3(%edi,%ebx), %ebx                           #70.33
        shll      $4, %ebx                                      #68.37
        addl      32(%ecx), %ebx                                #68.37
        testb     $15, %bl                                      #91.13
        jne       L_B1.24       # Prob 5%                       #91.13

This is the llvm code after instruction scheduling:

cond_next140 (0xa910740, LLVM BB @0xa90beb0):
	%reg1078 = MOV32ri -3
	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %NOREG, 0
	%reg1037 = MOV32rm %reg1024, 1, %NOREG, 40
	%reg1080 = IMUL32rr %reg1079, %reg1037
	%reg1081 = MOV32rm %reg1058, 1, %NOREG, 0
	%reg1038 = LEA32r %reg1081, 1, %reg1080, -3
	%reg1036 = MOV32rm %reg1024, 1, %NOREG, 32
	%reg1082 = SHL32ri %reg1038, 4
	%reg1039 = ADD32rr %reg1036, %reg1082
	%reg1083 = MOVAPSrm %reg1059, 1, %NOREG, 0
	%reg1034 = SHUFPSrr %reg1083, %reg1083, 170
	%reg1032 = SHUFPSrr %reg1083, %reg1083, 0
	%reg1035 = SHUFPSrr %reg1083, %reg1083, 255
	%reg1033 = SHUFPSrr %reg1083, %reg1083, 85
	%reg1040 = MOV32rr %reg1039
	%reg1084 = AND32ri8 %reg1039, 15
	CMP32ri8 %reg1084, 0
	JE mbb<cond_next204,0xa914d30>

Still ok. After register allocation:

cond_next140 (0xa910740, LLVM BB @0xa90beb0):
	%EAX = MOV32ri -3
	%EDX = MOV32rm <fi#3>, 1, %NOREG, 0
	ADD32rm %EAX<def&use>, %EDX, 1, %NOREG, 0
	%EDX = MOV32rm <fi#7>, 1, %NOREG, 0
	%EDX = MOV32rm %EDX, 1, %NOREG, 40
	IMUL32rr %EAX<def&use>, %EDX
	%ESI = MOV32rm <fi#5>, 1, %NOREG, 0
	%ESI = MOV32rm %ESI, 1, %NOREG, 0
	MOV32mr <fi#4>, 1, %NOREG, 0, %ESI
	%EAX = LEA32r %ESI, 1, %EAX, -3
	%ESI = MOV32rm <fi#7>, 1, %NOREG, 0
	%ESI = MOV32rm %ESI, 1, %NOREG, 32
	%EDI = MOV32rr %EAX
	SHL32ri %EDI<def&use>, 4
	ADD32rr %EDI<def&use>, %ESI
	%XMM0 = MOVAPSrm %ECX, 1, %NOREG, 0
	%XMM1 = MOVAPSrr %XMM0
	SHUFPSrr %XMM1<def&use>, %XMM1, 170
	%XMM2 = MOVAPSrr %XMM0
	SHUFPSrr %XMM2<def&use>, %XMM2, 0
	%XMM3 = MOVAPSrr %XMM0
	SHUFPSrr %XMM3<def&use>, %XMM3, 255
	SHUFPSrr %XMM0<def&use>, %XMM0, 85
	%EBX = MOV32rr %EDI
	AND32ri8 %EBX<def&use>, 15
	CMP32ri8 %EBX, 0
	JE mbb<cond_next204,0xa914d30>

This looks really bad. The problem is shufps is a destructive opcode. Since it
appears as operand two in more than one shufps ops. It resulted in a number of
copies. Note icc also suffers from the same problem. Either the instruction
selector should select pshufd or The register allocator can made the two-address
to three-address transformation.

It also exposes some other problems. See MOV32ri -3 and the spills.

//===---------------------------------------------------------------------===//

Consider:

__m128 test(float a) {
  return _mm_set_ps(0.0, 0.0, 0.0, a*a);
}

This compiles into:

movss 4(%esp), %xmm1
mulss %xmm1, %xmm1
xorps %xmm0, %xmm0
movss %xmm1, %xmm0
ret

Because mulss doesn't modify the top 3 elements, the top elements of 
xmm1 are already zero'd.  We could compile this to:

movss 4(%esp), %xmm0
mulss %xmm0, %xmm0
ret

//===---------------------------------------------------------------------===//

Here's a sick and twisted idea.  Consider code like this:

__m128 test(__m128 a) {
  float b = *(float*)&A;
  ...
  return _mm_set_ps(0.0, 0.0, 0.0, b);
}

This might compile to this code:

movaps c(%esp), %xmm1
xorps %xmm0, %xmm0
movss %xmm1, %xmm0
ret

Now consider if the ... code caused xmm1 to get spilled.  This might produce
this code:

movaps c(%esp), %xmm1
movaps %xmm1, c2(%esp)
...

xorps %xmm0, %xmm0
movaps c2(%esp), %xmm1
movss %xmm1, %xmm0
ret

However, since the reload is only used by these instructions, we could 
"fold" it into the uses, producing something like this:

movaps c(%esp), %xmm1
movaps %xmm1, c2(%esp)
...

movss c2(%esp), %xmm0
ret

... saving two instructions.

The basic idea is that a reload from a spill slot, can, if only one 4-byte 
chunk is used, bring in 3 zeros the one element instead of 4 elements.
This can be used to simplify a variety of shuffle operations, where the
elements are fixed zeros.

//===---------------------------------------------------------------------===//

This code generates ugly code, probably due to costs being off or something:

define void @test(float* %P, <4 x float>* %P2 ) {
        %xFloat0.688 = load float* %P
        %tmp = load <4 x float>* %P2
        %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3
        store <4 x float> %inFloat3.713, <4 x float>* %P2
        ret void
}

Generates:

_test:
	movl	8(%esp), %eax
	movaps	(%eax), %xmm0
	pxor	%xmm1, %xmm1
	movaps	%xmm0, %xmm2
	shufps	$50, %xmm1, %xmm2
	shufps	$132, %xmm2, %xmm0
	movaps	%xmm0, (%eax)
	ret

Would it be better to generate:

_test:
        movl 8(%esp), %ecx
        movaps (%ecx), %xmm0
	xor %eax, %eax
        pinsrw $6, %eax, %xmm0
        pinsrw $7, %eax, %xmm0
        movaps %xmm0, (%ecx)
        ret

?

//===---------------------------------------------------------------------===//

Some useful information in the Apple Altivec / SSE Migration Guide:

http://developer.apple.com/documentation/Performance/Conceptual/
Accelerate_sse_migration/index.html

e.g. SSE select using and, andnot, or. Various SSE compare translations.

//===---------------------------------------------------------------------===//

Add hooks to commute some CMPP operations.

//===---------------------------------------------------------------------===//

Apply the same transformation that merged four float into a single 128-bit load
to loads from constant pool.

//===---------------------------------------------------------------------===//

Floating point max / min are commutable when -enable-unsafe-fp-path is
specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other
nodes which are selected to max / min instructions that are marked commutable.

//===---------------------------------------------------------------------===//

We should materialize vector constants like "all ones" and "signbit" with 
code like:

     cmpeqps xmm1, xmm1   ; xmm1 = all-ones

and:
     cmpeqps xmm1, xmm1   ; xmm1 = all-ones
     psrlq   xmm1, 31     ; xmm1 = all 100000000000...

instead of using a load from the constant pool.  The later is important for
ABS/NEG/copysign etc.

//===---------------------------------------------------------------------===//

These functions:

#include <xmmintrin.h>
__m128i a;
void x(unsigned short n) {
  a = _mm_slli_epi32 (a, n);
}
void y(unsigned n) {
  a = _mm_slli_epi32 (a, n);
}

compile to ( -O3 -static -fomit-frame-pointer):
_x:
        movzwl  4(%esp), %eax
        movd    %eax, %xmm0
        movaps  _a, %xmm1
        pslld   %xmm0, %xmm1
        movaps  %xmm1, _a
        ret
_y:
        movd    4(%esp), %xmm0
        movaps  _a, %xmm1
        pslld   %xmm0, %xmm1
        movaps  %xmm1, _a
        ret

"y" looks good, but "x" does silly movzwl stuff around into a GPR.  It seems
like movd would be sufficient in both cases as the value is already zero 
extended in the 32-bit stack slot IIRC.  For signed short, it should also be
save, as a really-signed value would be undefined for pslld.


//===---------------------------------------------------------------------===//

#include <math.h>
int t1(double d) { return signbit(d); }

This currently compiles to:
	subl	$12, %esp
	movsd	16(%esp), %xmm0
	movsd	%xmm0, (%esp)
	movl	4(%esp), %eax
	shrl	$31, %eax
	addl	$12, %esp
	ret

We should use movmskp{s|d} instead.

//===---------------------------------------------------------------------===//

CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single
(aligned) vector load.  This functionality has a couple of problems.

1. The code to infer alignment from loads of globals is in the X86 backend,
   not the dag combiner.  This is because dagcombine2 needs to be able to see
   through the X86ISD::Wrapper node, which DAGCombine can't really do.
2. The code for turning 4 x load into a single vector load is target 
   independent and should be moved to the dag combiner.
3. The code for turning 4 x load into a vector load can only handle a direct 
   load from a global or a direct load from the stack.  It should be generalized
   to handle any load from P, P+4, P+8, P+12, where P can be anything.
4. The alignment inference code cannot handle loads from globals in non-static
   mode because it doesn't look through the extra dyld stub load.  If you try
   vec_align.ll without -relocation-model=static, you'll see what I mean.

//===---------------------------------------------------------------------===//

We should lower store(fneg(load p), q) into an integer load+xor+store, which
eliminates a constant pool load.  For example, consider:

define i64 @ccosf(float %z.0, float %z.1) nounwind readonly  {
entry:
 %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]
 %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly
 ret i64 %tmp20
}
declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly

This currently compiles to:

LCPI1_0:					#  <4 x float>
	.long	2147483648	# float -0
	.long	2147483648	# float -0
	.long	2147483648	# float -0
	.long	2147483648	# float -0
_ccosf:
	subl	$12, %esp
	movss	16(%esp), %xmm0
	movss	%xmm0, 4(%esp)
	movss	20(%esp), %xmm0
	xorps	LCPI1_0, %xmm0
	movss	%xmm0, (%esp)
	call	L_ccoshf$stub
	addl	$12, %esp
	ret

Note the load into xmm0, then xor (to negate), then store.  In PIC mode,
this code computes the pic base and does two loads to do the constant pool 
load, so the improvement is much bigger.

The tricky part about this xform is that the argument load/store isn't exposed
until post-legalize, and at that point, the fneg has been custom expanded into 
an X86 fxor.  This means that we need to handle this case in the x86 backend
instead of in target independent code.

//===---------------------------------------------------------------------===//

Non-SSE4 insert into 16 x i8 is atrociously bad.

//===---------------------------------------------------------------------===//

<2 x i64> extract is substantially worse than <2 x f64>, even if the destination
is memory.

//===---------------------------------------------------------------------===//

INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert
any number of 0.0 simultaneously.  Currently we only use it for simple
insertions.

See comments in LowerINSERT_VECTOR_ELT_SSE4.

//===---------------------------------------------------------------------===//

On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not
Custom.  All combinations of insert/extract reg-reg, reg-mem, and mem-reg are
legal, it'll just take a few extra patterns written in the .td file.

Note: this is not a code quality issue; the custom lowered code happens to be
right, but we shouldn't have to custom lower anything.  This is probably related
to <2 x i64> ops being so bad.

//===---------------------------------------------------------------------===//

LLVM currently generates stack realignment code, when it is not necessary
needed. The problem is that we need to know about stack alignment too early,
before RA runs.

At that point we don't know, whether there will be vector spill, or not.
Stack realignment logic is overly conservative here, but otherwise we can
produce unaligned loads/stores.

Fixing this will require some huge RA changes.

Testcase:
#include <emmintrin.h>

typedef short vSInt16 __attribute__ ((__vector_size__ (16)));

static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,
- 22725, - 12873};;

vSInt16 madd(vSInt16 b)
{
    return _mm_madd_epi16(a, b);
}

Generated code (x86-32, linux):
madd:
        pushl   %ebp
        movl    %esp, %ebp
        andl    $-16, %esp
        movaps  .LCPI1_0, %xmm1
        pmaddwd %xmm1, %xmm0
        movl    %ebp, %esp
        popl    %ebp
        ret

//===---------------------------------------------------------------------===//

Consider:
#include <emmintrin.h> 
__m128 foo2 (float x) {
 return _mm_set_ps (0, 0, x, 0);
}

In x86-32 mode, we generate this spiffy code:

_foo2:
	movss	4(%esp), %xmm0
	pshufd	$81, %xmm0, %xmm0
	ret

in x86-64 mode, we generate this code, which could be better:

_foo2:
	xorps	%xmm1, %xmm1
	movss	%xmm0, %xmm1
	pshufd	$81, %xmm1, %xmm0
	ret

In sse4 mode, we could use insertps to make both better.

Here's another testcase that could use insertps [mem]:

#include <xmmintrin.h>
extern float x2, x3;
__m128 foo1 (float x1, float x4) {
 return _mm_set_ps (x2, x1, x3, x4);
}

gcc mainline compiles it to:

foo1:
       insertps        $0x10, x2(%rip), %xmm0
       insertps        $0x10, x3(%rip), %xmm1
       movaps  %xmm1, %xmm2
       movlhps %xmm0, %xmm2
       movaps  %xmm2, %xmm0
       ret

//===---------------------------------------------------------------------===//

We compile vector multiply-by-constant into poor code:

define <4 x i32> @f(<4 x i32> %i) nounwind  {
	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >
	ret <4 x i32> %A
}

On targets without SSE4.1, this compiles into:

LCPI1_0:					##  <4 x i32>
	.long	10
	.long	10
	.long	10
	.long	10
	.text
	.align	4,0x90
	.globl	_f
_f:
	pshufd	$3, %xmm0, %xmm1
	movd	%xmm1, %eax
	imull	LCPI1_0+12, %eax
	movd	%eax, %xmm1
	pshufd	$1, %xmm0, %xmm2
	movd	%xmm2, %eax
	imull	LCPI1_0+4, %eax
	movd	%eax, %xmm2
	punpckldq	%xmm1, %xmm2
	movd	%xmm0, %eax
	imull	LCPI1_0, %eax
	movd	%eax, %xmm1
	movhlps	%xmm0, %xmm0
	movd	%xmm0, %eax
	imull	LCPI1_0+8, %eax
	movd	%eax, %xmm0
	punpckldq	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	punpckldq	%xmm2, %xmm0
	ret

It would be better to synthesize integer vector multiplication by constants
using shifts and adds, pslld and paddd here. And even on targets with SSE4.1,
simple cases such as multiplication by powers of two would be better as
vector shifts than as multiplications.

//===---------------------------------------------------------------------===//

We compile this:

__m128i
foo2 (char x)
{
  return _mm_set_epi8 (1, 0, 0, 0, 0, 0, 0, 0, 0, x, 0, 1, 0, 0, 0, 0);
}

into:
	movl	$1, %eax
	xorps	%xmm0, %xmm0
	pinsrw	$2, %eax, %xmm0
	movzbl	4(%esp), %eax
	pinsrw	$3, %eax, %xmm0
	movl	$256, %eax
	pinsrw	$7, %eax, %xmm0
	ret


gcc-4.2:
	subl	$12, %esp
	movzbl	16(%esp), %eax
	movdqa	LC0, %xmm0
	pinsrw	$3, %eax, %xmm0
	addl	$12, %esp
	ret
	.const
	.align 4
LC0:
	.word	0
	.word	0
	.word	1
	.word	0
	.word	0
	.word	0
	.word	0
	.word	256

With SSE4, it should be
      movdqa  .LC0(%rip), %xmm0
      pinsrb  $6, %edi, %xmm0

//===---------------------------------------------------------------------===//

We should transform a shuffle of two vectors of constants into a single vector
of constants. Also, insertelement of a constant into a vector of constants
should also result in a vector of constants. e.g. 2008-06-25-VecISelBug.ll.

We compiled it to something horrible:

	.align	4
LCPI1_1:					##  float
	.long	1065353216	## float 1
	.const

	.align	4
LCPI1_0:					##  <4 x float>
	.space	4
	.long	1065353216	## float 1
	.space	4
	.long	1065353216	## float 1
	.text
	.align	4,0x90
	.globl	_t
_t:
	xorps	%xmm0, %xmm0
	movhps	LCPI1_0, %xmm0
	movss	LCPI1_1, %xmm1
	movaps	%xmm0, %xmm2
	shufps	$2, %xmm1, %xmm2
	shufps	$132, %xmm2, %xmm0
	movaps	%xmm0, 0

//===---------------------------------------------------------------------===//
rdar://5907648

This function:

float foo(unsigned char x) {
  return x;
}

compiles to (x86-32):

define float @foo(i8 zeroext  %x) nounwind  {
	%tmp12 = uitofp i8 %x to float		; <float> [#uses=1]
	ret float %tmp12
}

compiles to:

_foo:
	subl	$4, %esp
	movzbl	8(%esp), %eax
	cvtsi2ss	%eax, %xmm0
	movss	%xmm0, (%esp)
	flds	(%esp)
	addl	$4, %esp
	ret

We should be able to use:
  cvtsi2ss 8($esp), %xmm0
since we know the stack slot is already zext'd.

//===---------------------------------------------------------------------===//

Consider using movlps instead of movsd to implement (scalar_to_vector (loadf64))
when code size is critical. movlps is slower than movsd on core2 but it's one
byte shorter.

//===---------------------------------------------------------------------===//

We should use a dynamic programming based approach to tell when using FPStack
operations is cheaper than SSE.  SciMark montecarlo contains code like this
for example:

double MonteCarlo_num_flops(int Num_samples) {
    return ((double) Num_samples)* 4.0;
}

In fpstack mode, this compiles into:

LCPI1_0:					
	.long	1082130432	## float 4.000000e+00
_MonteCarlo_num_flops:
	subl	$4, %esp
	movl	8(%esp), %eax
	movl	%eax, (%esp)
	fildl	(%esp)
	fmuls	LCPI1_0
	addl	$4, %esp
	ret
        
in SSE mode, it compiles into significantly slower code:

_MonteCarlo_num_flops:
	subl	$12, %esp
	cvtsi2sd	16(%esp), %xmm0
	mulsd	LCPI1_0, %xmm0
	movsd	%xmm0, (%esp)
	fldl	(%esp)
	addl	$12, %esp
	ret

There are also other cases in scimark where using fpstack is better, it is
cheaper to do fld1 than load from a constant pool for example, so
"load, add 1.0, store" is better done in the fp stack, etc.

//===---------------------------------------------------------------------===//

These should compile into the same code (PR6214): Perhaps instcombine should
canonicalize the former into the later?

define float @foo(float %x) nounwind {
  %t = bitcast float %x to i32
  %s = and i32 %t, 2147483647
  %d = bitcast i32 %s to float
  ret float %d
}

declare float @fabsf(float %n)
define float @bar(float %x) nounwind {
  %d = call float @fabsf(float %x)
  ret float %d
}

//===---------------------------------------------------------------------===//

This IR (from PR6194):

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-apple-darwin10.0.0"

%0 = type { double, double }
%struct.float3 = type { float, float, float }

define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {
entry:
  %tmp18 = extractvalue %0 %0, 0                  ; <double> [#uses=1]
  %tmp19 = bitcast double %tmp18 to i64           ; <i64> [#uses=1]
  %tmp20 = zext i64 %tmp19 to i128                ; <i128> [#uses=1]
  %tmp10 = lshr i128 %tmp20, 32                   ; <i128> [#uses=1]
  %tmp11 = trunc i128 %tmp10 to i32               ; <i32> [#uses=1]
  %tmp12 = bitcast i32 %tmp11 to float            ; <float> [#uses=1]
  %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]
  store float %tmp12, float* %tmp5
  ret void
}

Compiles to:

_test:                                  ## @test
	movd	%xmm0, %rax
	shrq	$32, %rax
	movl	%eax, 4(%rdi)
	ret

This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and
doing a shuffle from v[1] to v[0] then a float store.

//===---------------------------------------------------------------------===//

[UNSAFE FP]

void foo(double, double, double);
void norm(double x, double y, double z) {
  double scale = __builtin_sqrt(x*x + y*y + z*z);
  foo(x/scale, y/scale, z/scale);
}

We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is
slow and not pipelined. In -ffast-math mode we could compute "1.0/scale" first
and emit 3 mulsd in place of the divs. This can be done as a target-independent
transform.

If we're dealing with floats instead of doubles we could even replace the sqrtss
and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the
cost of reduced accuracy.

//===---------------------------------------------------------------------===//

This function should be matched to haddpd when the appropriate CPU is enabled:

#include <x86intrin.h>
double f (__m128d p) {
  return p[0] + p[1];
}

similarly, v[0]-v[1] should match to hsubpd, and {v[0]-v[1], w[0]-w[1]} should
turn into hsubpd also.

//===---------------------------------------------------------------------===//

define <2 x i32> @foo(<2 x double> %in) {
  %x = fptosi <2 x double> %in to <2 x i32>
  ret <2 x i32> %x
}

Should compile into cvttpd2dq instead of being scalarized into 2 cvttsd2si.

//===---------------------------------------------------------------------===//
//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//

TODO:
* lmw/stmw pass a la arm load store optimizer for prolog/epilog

===-------------------------------------------------------------------------===

This code:

unsigned add32carry(unsigned sum, unsigned x) {
 unsigned z = sum + x;
 if (sum + x < x)
     z++;
 return z;
}

Should compile to something like:

	addc r3,r3,r4
	addze r3,r3

instead we get:

	add r3, r4, r3
	cmplw cr7, r3, r4
	mfcr r4 ; 1
	rlwinm r4, r4, 29, 31, 31
	add r3, r3, r4

Ick.

===-------------------------------------------------------------------------===

We compile the hottest inner loop of viterbi to:

        li r6, 0
        b LBB1_84       ;bb432.i
LBB1_83:        ;bb420.i
        lbzx r8, r5, r7
        addi r6, r7, 1
        stbx r8, r4, r7
LBB1_84:        ;bb432.i
        mr r7, r6
        cmplwi cr0, r7, 143
        bne cr0, LBB1_83        ;bb420.i

The CBE manages to produce:

	li r0, 143
	mtctr r0
loop:
	lbzx r2, r2, r11
	stbx r0, r2, r9
	addi r2, r2, 1
	bdz later
	b loop

This could be much better (bdnz instead of bdz) but it still beats us.  If we
produced this with bdnz, the loop would be a single dispatch group.

===-------------------------------------------------------------------------===

Lump the constant pool for each function into ONE pic object, and reference
pieces of it as offsets from the start.  For functions like this (contrived
to have lots of constants obviously):

double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }

We generate:

_X:
        lis r2, ha16(.CPI_X_0)
        lfd f0, lo16(.CPI_X_0)(r2)
        lis r2, ha16(.CPI_X_1)
        lfd f2, lo16(.CPI_X_1)(r2)
        fmadd f0, f1, f0, f2
        lis r2, ha16(.CPI_X_2)
        lfd f1, lo16(.CPI_X_2)(r2)
        lis r2, ha16(.CPI_X_3)
        lfd f2, lo16(.CPI_X_3)(r2)
        fmadd f1, f0, f1, f2
        blr

It would be better to materialize .CPI_X into a register, then use immediates
off of the register to avoid the lis's.  This is even more important in PIC 
mode.

Note that this (and the static variable version) is discussed here for GCC:
http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html

Here's another example (the sgn function):
double testf(double a) {
       return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);
}

it produces a BB like this:
LBB1_1: ; cond_true
        lis r2, ha16(LCPI1_0)
        lfs f0, lo16(LCPI1_0)(r2)
        lis r2, ha16(LCPI1_1)
        lis r3, ha16(LCPI1_2)
        lfs f2, lo16(LCPI1_2)(r3)
        lfs f3, lo16(LCPI1_1)(r2)
        fsub f0, f0, f1
        fsel f1, f0, f2, f3
        blr 

===-------------------------------------------------------------------------===

PIC Code Gen IPO optimization:

Squish small scalar globals together into a single global struct, allowing the 
address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size
of the GOT on targets with one).

Note that this is discussed here for GCC:
http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html

===-------------------------------------------------------------------------===

Darwin Stub removal:

We still generate calls to foo$stub, and stubs, on Darwin.  This is not
necessary when building with the Leopard (10.5) or later linker, as stubs are
generated by ld when necessary.  Parameterizing this based on the deployment
target (-mmacosx-version-min) is probably enough.  x86-32 does this right, see
its logic.

===-------------------------------------------------------------------------===

Darwin Stub LICM optimization:

Loops like this:
  
  for (...)  bar();

Have to go through an indirect stub if bar is external or linkonce.  It would 
be better to compile it as:

     fp = &bar;
     for (...)  fp();

which only computes the address of bar once (instead of each time through the 
stub).  This is Darwin specific and would have to be done in the code generator.
Probably not a win on x86.

===-------------------------------------------------------------------------===

Simple IPO for argument passing, change:
  void foo(int X, double Y, int Z) -> void foo(int X, int Z, double Y)

the Darwin ABI specifies that any integer arguments in the first 32 bytes worth
of arguments get assigned to r3 through r10. That is, if you have a function
foo(int, double, int) you get r3, f1, r6, since the 64 bit double ate up the
argument bytes for r4 and r5. The trick then would be to shuffle the argument
order for functions we can internalize so that the maximum number of 
integers/pointers get passed in regs before you see any of the fp arguments.

Instead of implementing this, it would actually probably be easier to just 
implement a PPC fastcc, where we could do whatever we wanted to the CC, 
including having this work sanely.

===-------------------------------------------------------------------------===

Fix Darwin FP-In-Integer Registers ABI

Darwin passes doubles in structures in integer registers, which is very very 
bad.  Add something like a BITCAST to LLVM, then do an i-p transformation that
percolates these things out of functions.

Check out how horrible this is:
http://gcc.gnu.org/ml/gcc/2005-10/msg01036.html

This is an extension of "interprocedural CC unmunging" that can't be done with
just fastcc.

===-------------------------------------------------------------------------===

Fold add and sub with constant into non-extern, non-weak addresses so this:

static int a;
void bar(int b) { a = b; }
void foo(unsigned char *c) {
  *c = a;
}

So that 

_foo:
        lis r2, ha16(_a)
        la r2, lo16(_a)(r2)
        lbz r2, 3(r2)
        stb r2, 0(r3)
        blr

Becomes

_foo:
        lis r2, ha16(_a+3)
        lbz r2, lo16(_a+3)(r2)
        stb r2, 0(r3)
        blr

===-------------------------------------------------------------------------===

We should compile these two functions to the same thing:

#include <stdlib.h>
void f(int a, int b, int *P) {
  *P = (a-b)>=0?(a-b):(b-a);
}
void g(int a, int b, int *P) {
  *P = abs(a-b);
}

Further, they should compile to something better than:

_g:
        subf r2, r4, r3
        subfic r3, r2, 0
        cmpwi cr0, r2, -1
        bgt cr0, LBB2_2 ; entry
LBB2_1: ; entry
        mr r2, r3
LBB2_2: ; entry
        stw r2, 0(r5)
        blr

GCC produces:

_g:
        subf r4,r4,r3
        srawi r2,r4,31
        xor r0,r2,r4
        subf r0,r2,r0
        stw r0,0(r5)
        blr

... which is much nicer.

This theoretically may help improve twolf slightly (used in dimbox.c:142?).

===-------------------------------------------------------------------------===

PR5945: This: 
define i32 @clamp0g(i32 %a) {
entry:
        %cmp = icmp slt i32 %a, 0
        %sel = select i1 %cmp, i32 0, i32 %a
        ret i32 %sel
}

Is compile to this with the PowerPC (32-bit) backend:

_clamp0g:
        cmpwi cr0, r3, 0
        li r2, 0
        blt cr0, LBB1_2
; BB#1:                                                     ; %entry
        mr r2, r3
LBB1_2:                                                     ; %entry
        mr r3, r2
        blr

This could be reduced to the much simpler:

_clamp0g:
        srawi r2, r3, 31
        andc r3, r3, r2
        blr

===-------------------------------------------------------------------------===

int foo(int N, int ***W, int **TK, int X) {
  int t, i;
  
  for (t = 0; t < N; ++t)
    for (i = 0; i < 4; ++i)
      W[t / X][i][t % X] = TK[i][t];
      
  return 5;
}

We generate relatively atrocious code for this loop compared to gcc.

We could also strength reduce the rem and the div:
http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf

===-------------------------------------------------------------------------===

We generate ugly code for this:

void func(unsigned int *ret, float dx, float dy, float dz, float dw) {
  unsigned code = 0;
  if(dx < -dw) code |= 1;
  if(dx > dw)  code |= 2;
  if(dy < -dw) code |= 4;
  if(dy > dw)  code |= 8;
  if(dz < -dw) code |= 16;
  if(dz > dw)  code |= 32;
  *ret = code;
}

===-------------------------------------------------------------------------===

%struct.B = type { i8, [3 x i8] }

define void @bar(%struct.B* %b) {
entry:
        %tmp = bitcast %struct.B* %b to i32*              ; <uint*> [#uses=1]
        %tmp = load i32* %tmp          ; <uint> [#uses=1]
        %tmp3 = bitcast %struct.B* %b to i32*             ; <uint*> [#uses=1]
        %tmp4 = load i32* %tmp3                ; <uint> [#uses=1]
        %tmp8 = bitcast %struct.B* %b to i32*             ; <uint*> [#uses=2]
        %tmp9 = load i32* %tmp8                ; <uint> [#uses=1]
        %tmp4.mask17 = shl i32 %tmp4, i8 1          ; <uint> [#uses=1]
        %tmp1415 = and i32 %tmp4.mask17, 2147483648            ; <uint> [#uses=1]
        %tmp.masked = and i32 %tmp, 2147483648         ; <uint> [#uses=1]
        %tmp11 = or i32 %tmp1415, %tmp.masked          ; <uint> [#uses=1]
        %tmp12 = and i32 %tmp9, 2147483647             ; <uint> [#uses=1]
        %tmp13 = or i32 %tmp12, %tmp11         ; <uint> [#uses=1]
        store i32 %tmp13, i32* %tmp8
        ret void
}

We emit:

_foo:
        lwz r2, 0(r3)
        slwi r4, r2, 1
        or r4, r4, r2
        rlwimi r2, r4, 0, 0, 0
        stw r2, 0(r3)
        blr

We could collapse a bunch of those ORs and ANDs and generate the following
equivalent code:

_foo:
        lwz r2, 0(r3)
        rlwinm r4, r2, 1, 0, 0
        or r2, r2, r4
        stw r2, 0(r3)
        blr

===-------------------------------------------------------------------------===

Consider a function like this:

float foo(float X) { return X + 1234.4123f; }

The FP constant ends up in the constant pool, so we need to get the LR register.
 This ends up producing code like this:

_foo:
.LBB_foo_0:     ; entry
        mflr r11
***     stw r11, 8(r1)
        bl "L00000$pb"
"L00000$pb":
        mflr r2
        addis r2, r2, ha16(.CPI_foo_0-"L00000$pb")
        lfs f0, lo16(.CPI_foo_0-"L00000$pb")(r2)
        fadds f1, f1, f0
***     lwz r11, 8(r1)
        mtlr r11
        blr

This is functional, but there is no reason to spill the LR register all the way
to the stack (the two marked instrs): spilling it to a GPR is quite enough.

Implementing this will require some codegen improvements.  Nate writes:

"So basically what we need to support the "no stack frame save and restore" is a
generalization of the LR optimization to "callee-save regs".

Currently, we have LR marked as a callee-save reg.  The register allocator sees
that it's callee save, and spills it directly to the stack.

Ideally, something like this would happen:

LR would be in a separate register class from the GPRs. The class of LR would be
marked "unspillable".  When the register allocator came across an unspillable
reg, it would ask "what is the best class to copy this into that I *can* spill"
If it gets a class back, which it will in this case (the gprs), it grabs a free
register of that class.  If it is then later necessary to spill that reg, so be
it.

===-------------------------------------------------------------------------===

We compile this:
int test(_Bool X) {
  return X ? 524288 : 0;
}

to: 
_test:
        cmplwi cr0, r3, 0
        lis r2, 8
        li r3, 0
        beq cr0, LBB1_2 ;entry
LBB1_1: ;entry
        mr r3, r2
LBB1_2: ;entry
        blr 

instead of:
_test:
        addic r2,r3,-1
        subfe r0,r2,r3
        slwi r3,r0,19
        blr

This sort of thing occurs a lot due to globalopt.

===-------------------------------------------------------------------------===

We compile:

define i32 @bar(i32 %x) nounwind readnone ssp {
entry:
  %0 = icmp eq i32 %x, 0                          ; <i1> [#uses=1]
  %neg = sext i1 %0 to i32              ; <i32> [#uses=1]
  ret i32 %neg
}

to:

_bar:
	cntlzw r2, r3
	slwi r2, r2, 26
	srawi r3, r2, 31
	blr 

it would be better to produce:

_bar: 
        addic r3,r3,-1
        subfe r3,r3,r3
        blr

===-------------------------------------------------------------------------===

We generate horrible ppc code for this:

#define N  2000000
double   a[N],c[N];
void simpleloop() {
   int j;
   for (j=0; j<N; j++)
     c[j] = a[j];
}

LBB1_1: ;bb
        lfdx f0, r3, r4
        addi r5, r5, 1                 ;; Extra IV for the exit value compare.
        stfdx f0, r2, r4
        addi r4, r4, 8

        xoris r6, r5, 30               ;; This is due to a large immediate.
        cmplwi cr0, r6, 33920
        bne cr0, LBB1_1

//===---------------------------------------------------------------------===//

This:
        #include <algorithm>
        inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b)
        { return std::make_pair(a + b, a + b < a); }
        bool no_overflow(unsigned a, unsigned b)
        { return !full_add(a, b).second; }

Should compile to:

__Z11no_overflowjj:
        add r4,r3,r4
        subfc r3,r3,r4
        li r3,0
        adde r3,r3,r3
        blr

(or better) not:

__Z11no_overflowjj:
        add r2, r4, r3
        cmplw cr7, r2, r3
        mfcr r2
        rlwinm r2, r2, 29, 31, 31
        xori r3, r2, 1
        blr 

//===---------------------------------------------------------------------===//

We compile some FP comparisons into an mfcr with two rlwinms and an or.  For
example:
#include <math.h>
int test(double x, double y) { return islessequal(x, y);}
int test2(double x, double y) {  return islessgreater(x, y);}
int test3(double x, double y) {  return !islessequal(x, y);}

Compiles into (all three are similar, but the bits differ):

_test:
	fcmpu cr7, f1, f2
	mfcr r2
	rlwinm r3, r2, 29, 31, 31
	rlwinm r2, r2, 31, 31, 31
	or r3, r2, r3
	blr 

GCC compiles this into:

 _test:
	fcmpu cr7,f1,f2
	cror 30,28,30
	mfcr r3
	rlwinm r3,r3,31,1
	blr
        
which is more efficient and can use mfocr.  See PR642 for some more context.

//===---------------------------------------------------------------------===//

void foo(float *data, float d) {
   long i;
   for (i = 0; i < 8000; i++)
      data[i] = d;
}
void foo2(float *data, float d) {
   long i;
   data--;
   for (i = 0; i < 8000; i++) {
      data[1] = d;
      data++;
   }
}

These compile to:

_foo:
	li r2, 0
LBB1_1:	; bb
	addi r4, r2, 4
	stfsx f1, r3, r2
	cmplwi cr0, r4, 32000
	mr r2, r4
	bne cr0, LBB1_1	; bb
	blr 
_foo2:
	li r2, 0
LBB2_1:	; bb
	addi r4, r2, 4
	stfsx f1, r3, r2
	cmplwi cr0, r4, 32000
	mr r2, r4
	bne cr0, LBB2_1	; bb
	blr 

The 'mr' could be eliminated to folding the add into the cmp better.

//===---------------------------------------------------------------------===//
Codegen for the following (low-probability) case deteriorated considerably 
when the correctness fixes for unordered comparisons went in (PR 642, 58871).
It should be possible to recover the code quality described in the comments.

; RUN: llvm-as < %s | llc -march=ppc32  | grep or | count 3
; This should produce one 'or' or 'cror' instruction per function.

; RUN: llvm-as < %s | llc -march=ppc32  | grep mfcr | count 3
; PR2964

define i32 @test(double %x, double %y) nounwind  {
entry:
	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]
	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]
	ret i32 %tmp345
}

define i32 @test2(double %x, double %y) nounwind  {
entry:
	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]
	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]
	ret i32 %tmp345
}

define i32 @test3(double %x, double %y) nounwind  {
entry:
	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]
	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]
	ret i32 %tmp34
}

//===---------------------------------------------------------------------===//
for the following code:

void foo (float *__restrict__ a, int *__restrict__ b, int n) {
      a[n] = b[n]  * 2.321;
}

we load b[n] to GPR, then move it VSX register and convert it float. We should 
use vsx scalar integer load instructions to avoid direct moves

//===----------------------------------------------------------------------===//
; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg

; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and 
; should not be generated except with -enable-finite-only-fp-math or the like).
; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to
; recognize a more elaborate tree than a simple SETxx.

define double @test_FNEG_sel(double %A, double %B, double %C) {
        %D = fsub double -0.000000e+00, %A               ; <double> [#uses=1]
        %Cond = fcmp ugt double %D, -0.000000e+00               ; <i1> [#uses=1]
        %E = select i1 %Cond, double %B, double %C              ; <double> [#uses=1]
        ret double %E
}

//===----------------------------------------------------------------------===//
The save/restore sequence for CR in prolog/epilog is terrible:
- Each CR subreg is saved individually, rather than doing one save as a unit.
- On Darwin, the save is done after the decrement of SP, which means the offset
from SP of the save slot can be too big for a store instruction, which means we
need an additional register (currently hacked in 96015+96020; the solution there
is correct, but poor).
- On SVR4 the same thing can happen, and I don't think saving before the SP
decrement is safe on that target, as there is no red zone.  This is currently
broken AFAIK, although it's not a target I can exercise.
The following demonstrates the problem:
extern void bar(char *p);
void foo() {
  char x[100000];
  bar(x);
  __asm__("" ::: "cr2");
}

//===-------------------------------------------------------------------------===
Naming convention for instruction formats is very haphazard.
We have agreed on a naming scheme as follows:

<INST_form>{_<OP_type><OP_len>}+

Where:
INST_form is the instruction format (X-form, etc.)
OP_type is the operand type - one of OPC (opcode), RD (register destination),
                              RS (register source),
                              RDp (destination register pair),
                              RSp (source register pair), IM (immediate),
                              XO (extended opcode)
OP_len is the length of the operand in bits

VSX register operands would be of length 6 (split across two fields),
condition register fields of length 3.
We would not need denote reserved fields in names of instruction formats.

//===----------------------------------------------------------------------===//

Instruction fusion was introduced in ISA 2.06 and more opportunities added in
ISA 2.07.  LLVM needs to add infrastructure to recognize fusion opportunities
and force instruction pairs to be scheduled together.

-----------------------------------------------------------------------------

More general handling of any_extend and zero_extend:

See https://reviews.llvm.org/D24924#555306
//===- README_ALTIVEC.txt - Notes for improving Altivec code gen ----------===//

Implement PPCInstrInfo::isLoadFromStackSlot/isStoreToStackSlot for vector
registers, to generate better spill code.

//===----------------------------------------------------------------------===//

The first should be a single lvx from the constant pool, the second should be 
a xor/stvx:

void foo(void) {
  int x[8] __attribute__((aligned(128))) = { 1, 1, 1, 17, 1, 1, 1, 1 };
  bar (x);
}

#include <string.h>
void foo(void) {
  int x[8] __attribute__((aligned(128)));
  memset (x, 0, sizeof (x));
  bar (x);
}

//===----------------------------------------------------------------------===//

Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763

When -ffast-math is on, we can use 0.0.

//===----------------------------------------------------------------------===//

  Consider this:
  v4f32 Vector;
  v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };

Since we know that "Vector" is 16-byte aligned and we know the element offset 
of ".X", we should change the load into a lve*x instruction, instead of doing
a load/store/lve*x sequence.

//===----------------------------------------------------------------------===//

For functions that use altivec AND have calls, we are VRSAVE'ing all call
clobbered regs.

//===----------------------------------------------------------------------===//

Implement passing vectors by value into calls and receiving them as arguments.

//===----------------------------------------------------------------------===//

GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load
of C1/C2/C3, then a load and vperm of Variable.

//===----------------------------------------------------------------------===//

We need a way to teach tblgen that some operands of an intrinsic are required to
be constants.  The verifier should enforce this constraint.

//===----------------------------------------------------------------------===//

We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte
aligned stack slot, followed by a load/vperm.  We should probably just store it
to a scalar stack slot, then use lvsl/vperm to load it.  If the value is already
in memory this is a big win.

//===----------------------------------------------------------------------===//

extract_vector_elt of an arbitrary constant vector can be done with the 
following instructions:

vTemp = vec_splat(v0,2);    // 2 is the element the src is in.
vec_ste(&destloc,0,vTemp);

We can do an arbitrary non-constant value by using lvsr/perm/ste.

//===----------------------------------------------------------------------===//

If we want to tie instruction selection into the scheduler, we can do some
constant formation with different instructions.  For example, we can generate
"vsplti -1" with "vcmpequw R,R" and 1,1,1,1 with "vsubcuw R,R", and 0,0,0,0 with
"vsplti 0" or "vxor", each of which use different execution units, thus could
help scheduling.

This is probably only reasonable for a post-pass scheduler.

//===----------------------------------------------------------------------===//

For this function:

void test(vector float *A, vector float *B) {
  vector float C = (vector float)vec_cmpeq(*A, *B);
  if (!vec_any_eq(*A, *B))
    *B = (vector float){0,0,0,0};
  *A = C;
}

we get the following basic block:

	...
        lvx v2, 0, r4
        lvx v3, 0, r3
        vcmpeqfp v4, v3, v2
        vcmpeqfp. v2, v3, v2
        bne cr6, LBB1_2 ; cond_next

The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the
vcmpeqfp. result is used by a branch.  This can be improved.

//===----------------------------------------------------------------------===//

The code generated for this is truly aweful:

vector float test(float a, float b) {
 return (vector float){ 0.0, a, 0.0, 0.0}; 
}

LCPI1_0:                                        ;  float
        .space  4
        .text
        .globl  _test
        .align  4
_test:
        mfspr r2, 256
        oris r3, r2, 4096
        mtspr 256, r3
        lis r3, ha16(LCPI1_0)
        addi r4, r1, -32
        stfs f1, -16(r1)
        addi r5, r1, -16
        lfs f0, lo16(LCPI1_0)(r3)
        stfs f0, -32(r1)
        lvx v2, 0, r4
        lvx v3, 0, r5
        vmrghw v3, v3, v2
        vspltw v2, v2, 0
        vmrghw v2, v2, v3
        mtspr 256, r2
        blr

//===----------------------------------------------------------------------===//

int foo(vector float *x, vector float *y) {
        if (vec_all_eq(*x,*y)) return 3245; 
        else return 12;
}

A predicate compare being used in a select_cc should have the same peephole
applied to it as a predicate compare used by a br_cc.  There should be no
mfcr here:

_foo:
        mfspr r2, 256
        oris r5, r2, 12288
        mtspr 256, r5
        li r5, 12
        li r6, 3245
        lvx v2, 0, r4
        lvx v3, 0, r3
        vcmpeqfp. v2, v3, v2
        mfcr r3, 2
        rlwinm r3, r3, 25, 31, 31
        cmpwi cr0, r3, 0
        bne cr0, LBB1_2 ; entry
LBB1_1: ; entry
        mr r6, r5
LBB1_2: ; entry
        mr r3, r6
        mtspr 256, r2
        blr

//===----------------------------------------------------------------------===//

CodeGen/PowerPC/vec_constants.ll has an and operation that should be
codegen'd to andc.  The issue is that the 'all ones' build vector is
SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected
which prevents the vnot pattern from matching.


//===----------------------------------------------------------------------===//

An alternative to the store/store/load approach for illegal insert element 
lowering would be:

1. store element to any ol' slot
2. lvx the slot
3. lvsl 0; splat index; vcmpeq to generate a select mask
4. lvsl slot + x; vperm to rotate result into correct slot
5. vsel result together.

//===----------------------------------------------------------------------===//

Should codegen branches on vec_any/vec_all to avoid mfcr.  Two examples:

#include <altivec.h>
 int f(vector float a, vector float b)
 {
  int aa = 0;
  if (vec_all_ge(a, b))
    aa |= 0x1;
  if (vec_any_ge(a,b))
    aa |= 0x2;
  return aa;
}

vector float f(vector float a, vector float b) { 
  if (vec_any_eq(a, b)) 
    return a; 
  else 
    return b; 
}

//===----------------------------------------------------------------------===//

We should do a little better with eliminating dead stores.
The stores to the stack are dead since %a and %b are not needed

; Function Attrs: nounwind
define <16 x i8> @test_vpmsumb() #0 {
  entry:
  %a = alloca <16 x i8>, align 16
  %b = alloca <16 x i8>, align 16
  store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16
  store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16
  %0 = load <16 x i8>* %a, align 16
  %1 = load <16 x i8>* %b, align 16
  %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1)
  ret <16 x i8> %2
}


; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1


Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:
# BB#0:                                 # %entry
    addis 3, 2, .LCPI0_0@toc@ha
    addis 4, 2, .LCPI0_1@toc@ha
    addi 3, 3, .LCPI0_0@toc@l
    addi 4, 4, .LCPI0_1@toc@l
    lxvw4x 0, 0, 3
    addi 3, 1, -16
    lxvw4x 35, 0, 4
    stxvw4x 0, 0, 3
    ori 2, 2, 0
    lxvw4x 34, 0, 3
    addi 3, 1, -32
    stxvw4x 35, 0, 3
    vpmsumb 2, 2, 3
    blr
    .long   0
    .quad   0

The two stxvw4x instructions are not needed.
With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes
are present too.

//===----------------------------------------------------------------------===//

The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:

define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {
       %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0
       %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1
       %result = add <2 x i64> %x, %tmpvec2
       ret <2 x i64> %result

This will generate the following instruction sequence:
        std 5, -8(1)
        std 5, -16(1)
        addi 3, 1, -16
        ori 2, 2, 0
        lxvd2x 35, 0, 3
        vaddudm 2, 2, 3
        blr

This will almost certainly cause a load-hit-store hazard.  
Since val is a value parameter, it should not need to be saved onto
the stack, unless it's being done set up the vector register. Instead,
it would be better to splat the value into a vector register, and then
remove the (dead) stores to the stack.

//===----------------------------------------------------------------------===//

At the moment we always generate a lxsdx in preference to lfd, or stxsdx in
preference to stfd.  When we have a reg-immediate addressing mode, this is a
poor choice, since we have to load the address into an index register.  This
should be fixed for P7/P8. 

//===----------------------------------------------------------------------===//

Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.
However, we could actually support both kinds on either endianness, if we check
for the appropriate shufflevector pattern for each case ...  this would cause
some additional shufflevectors to be recognized and implemented via the
"swapped" form.

//===----------------------------------------------------------------------===//

There is a utility program called PerfectShuffle that generates a table of the
shortest instruction sequence for implementing a shufflevector operation on
PowerPC.  However, this was designed for big-endian code generation.  We could
modify this program to create a little endian version of the table.  The table
is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE().

//===----------------------------------------------------------------------===//

Opportunies to use instructions from PPCInstrVSX.td during code gen
  - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07)
  - Scalar comparisons (xscmpodp and xscmpudp)
  - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp)

Related to this: we currently do not generate the lxvw4x instruction for either
v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires
a single target type.  This should probably be addressed in the PPCISelDAGToDAG logic.

//===----------------------------------------------------------------------===//

Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only
for v2f64 with VSX available.  We should create custom lowering
support for the other vector types.  Without this support, we generate
sequences with load-hit-store hazards.

v4f32 can be supported with VSX by shifting the correct element into
big-endian lane 0, using xscvspdpn to produce a double-precision
representation of the single-precision value in big-endian
double-precision lane 0, and reinterpreting lane 0 as an FPR or
vector-scalar register.

v2i64 can be supported with VSX and P8Vector in the same manner as
v2f64, followed by a direct move to a GPR.

v4i32 can be supported with VSX and P8Vector by shifting the correct
element into big-endian lane 1, using a direct move to a GPR, and
sign-extending the 32-bit result to 64 bits.

v8i16 can be supported with VSX and P8Vector by shifting the correct
element into big-endian lane 3, using a direct move to a GPR, and
sign-extending the 16-bit result to 64 bits.

v16i8 can be supported with VSX and P8Vector by shifting the correct
element into big-endian lane 7, using a direct move to a GPR, and
sign-extending the 8-bit result to 64 bits.
//===- README_P9.txt - Notes for improving Power9 code gen ----------------===//

TODO: Instructions Need Implement Instrinstics or Map to LLVM IR

Altivec:
- Vector Compare Not Equal (Zero):
  vcmpneb(.) vcmpneh(.) vcmpnew(.)
  vcmpnezb(.) vcmpnezh(.) vcmpnezw(.)
  . Same as other VCMP*, use VCMP/VCMPo form (support intrinsic)

- Vector Extract Unsigned: vextractub vextractuh vextractuw vextractd
  . Don't use llvm extractelement because they have different semantics
  . Use instrinstics:
    (set v2i64:$vD, (int_ppc_altivec_vextractub v16i8:$vA, imm:$UIMM))
    (set v2i64:$vD, (int_ppc_altivec_vextractuh v8i16:$vA, imm:$UIMM))
    (set v2i64:$vD, (int_ppc_altivec_vextractuw v4i32:$vA, imm:$UIMM))
    (set v2i64:$vD, (int_ppc_altivec_vextractd  v2i64:$vA, imm:$UIMM))

- Vector Extract Unsigned Byte Left/Right-Indexed:
  vextublx vextubrx vextuhlx vextuhrx vextuwlx vextuwrx
  . Use instrinstics:
    // Left-Indexed
    (set i64:$rD, (int_ppc_altivec_vextublx i64:$rA, v16i8:$vB))
    (set i64:$rD, (int_ppc_altivec_vextuhlx i64:$rA, v8i16:$vB))
    (set i64:$rD, (int_ppc_altivec_vextuwlx i64:$rA, v4i32:$vB))

    // Right-Indexed
    (set i64:$rD, (int_ppc_altivec_vextubrx i64:$rA, v16i8:$vB))
    (set i64:$rD, (int_ppc_altivec_vextuhrx i64:$rA, v8i16:$vB))
    (set i64:$rD, (int_ppc_altivec_vextuwrx i64:$rA, v4i32:$vB))

- Vector Insert Element Instructions: vinsertb vinsertd vinserth vinsertw
    (set v16i8:$vD, (int_ppc_altivec_vinsertb v16i8:$vA, imm:$UIMM))
    (set v8i16:$vD, (int_ppc_altivec_vinsertd v8i16:$vA, imm:$UIMM))
    (set v4i32:$vD, (int_ppc_altivec_vinserth v4i32:$vA, imm:$UIMM))
    (set v2i64:$vD, (int_ppc_altivec_vinsertw v2i64:$vA, imm:$UIMM))

- Vector Count Leading/Trailing Zero LSB. Result is placed into GPR[rD]:
  vclzlsbb vctzlsbb
  . Use intrinsic:
    (set i64:$rD, (int_ppc_altivec_vclzlsbb v16i8:$vB))
    (set i64:$rD, (int_ppc_altivec_vctzlsbb v16i8:$vB))

- Vector Count Trailing Zeros: vctzb vctzh vctzw vctzd
  . Map to llvm cttz
    (set v16i8:$vD, (cttz v16i8:$vB))     // vctzb
    (set v8i16:$vD, (cttz v8i16:$vB))     // vctzh
    (set v4i32:$vD, (cttz v4i32:$vB))     // vctzw
    (set v2i64:$vD, (cttz v2i64:$vB))     // vctzd

- Vector Extend Sign: vextsb2w vextsh2w vextsb2d vextsh2d vextsw2d
  . vextsb2w:
    (set v4i32:$vD, (sext v4i8:$vB))

    // PowerISA_V3.0:
    do i = 0 to 3
       VR[VRT].word[i] ← EXTS32(VR[VRB].word[i].byte[3])
    end

  . vextsh2w:
    (set v4i32:$vD, (sext v4i16:$vB))

    // PowerISA_V3.0:
    do i = 0 to 3
       VR[VRT].word[i] ← EXTS32(VR[VRB].word[i].hword[1])
    end

  . vextsb2d
    (set v2i64:$vD, (sext v2i8:$vB))

    // PowerISA_V3.0:
    do i = 0 to 1
       VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].byte[7])
    end

  . vextsh2d
    (set v2i64:$vD, (sext v2i16:$vB))

    // PowerISA_V3.0:
    do i = 0 to 1
       VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].hword[3])
    end

  . vextsw2d
    (set v2i64:$vD, (sext v2i32:$vB))

    // PowerISA_V3.0:
    do i = 0 to 1
       VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].word[1])
    end

- Vector Integer Negate: vnegw vnegd
  . Map to llvm ineg
    (set v4i32:$rT, (ineg v4i32:$rA))       // vnegw
    (set v2i64:$rT, (ineg v2i64:$rA))       // vnegd

- Vector Parity Byte: vprtybw vprtybd vprtybq
  . Use intrinsic:
    (set v4i32:$rD, (int_ppc_altivec_vprtybw v4i32:$vB))
    (set v2i64:$rD, (int_ppc_altivec_vprtybd v2i64:$vB))
    (set v1i128:$rD, (int_ppc_altivec_vprtybq v1i128:$vB))

- Vector (Bit) Permute (Right-indexed):
  . vbpermd: Same as "vbpermq", use VX1_Int_Ty2:
    VX1_Int_Ty2<1484, "vbpermd", int_ppc_altivec_vbpermd, v2i64, v2i64>;

  . vpermr: use VA1a_Int_Ty3
    VA1a_Int_Ty3<59, "vpermr", int_ppc_altivec_vpermr, v16i8, v16i8, v16i8>;

- Vector Rotate Left Mask/Mask-Insert: vrlwnm vrlwmi vrldnm vrldmi
  . Use intrinsic:
    VX1_Int_Ty<389, "vrlwnm", int_ppc_altivec_vrlwnm, v4i32>;
    VX1_Int_Ty<133, "vrlwmi", int_ppc_altivec_vrlwmi, v4i32>;
    VX1_Int_Ty<453, "vrldnm", int_ppc_altivec_vrldnm, v2i64>;
    VX1_Int_Ty<197, "vrldmi", int_ppc_altivec_vrldmi, v2i64>;

- Vector Shift Left/Right: vslv vsrv
  . Use intrinsic, don't map to llvm shl and lshr, because they have different
    semantics, e.g. vslv:

      do i = 0 to 15
         sh ← VR[VRB].byte[i].bit[5:7]
         VR[VRT].byte[i] ← src.byte[i:i+1].bit[sh:sh+7]
      end

    VR[VRT].byte[i] is composed of 2 bytes from src.byte[i:i+1]

  . VX1_Int_Ty<1860, "vslv", int_ppc_altivec_vslv, v16i8>;
    VX1_Int_Ty<1796, "vsrv", int_ppc_altivec_vsrv, v16i8>;

- Vector Multiply-by-10 (& Write Carry) Unsigned Quadword:
  vmul10uq vmul10cuq
  . Use intrinsic:
    VX1_Int_Ty<513, "vmul10uq",   int_ppc_altivec_vmul10uq,  v1i128>;
    VX1_Int_Ty<  1, "vmul10cuq",  int_ppc_altivec_vmul10cuq, v1i128>;

- Vector Multiply-by-10 Extended (& Write Carry) Unsigned Quadword:
  vmul10euq vmul10ecuq
  . Use intrinsic:
    VX1_Int_Ty<577, "vmul10euq",  int_ppc_altivec_vmul10euq, v1i128>;
    VX1_Int_Ty< 65, "vmul10ecuq", int_ppc_altivec_vmul10ecuq, v1i128>;

- Decimal Convert From/to National/Zoned/Signed-QWord:
  bcdcfn. bcdcfz. bcdctn. bcdctz. bcdcfsq. bcdctsq.
  . Use instrinstics:
    (set v1i128:$vD, (int_ppc_altivec_bcdcfno  v1i128:$vB, i1:$PS))
    (set v1i128:$vD, (int_ppc_altivec_bcdcfzo  v1i128:$vB, i1:$PS))
    (set v1i128:$vD, (int_ppc_altivec_bcdctno  v1i128:$vB))
    (set v1i128:$vD, (int_ppc_altivec_bcdctzo  v1i128:$vB, i1:$PS))
    (set v1i128:$vD, (int_ppc_altivec_bcdcfsqo v1i128:$vB, i1:$PS))
    (set v1i128:$vD, (int_ppc_altivec_bcdctsqo v1i128:$vB))

- Decimal Copy-Sign/Set-Sign: bcdcpsgn. bcdsetsgn.
  . Use instrinstics:
    (set v1i128:$vD, (int_ppc_altivec_bcdcpsgno v1i128:$vA, v1i128:$vB))
    (set v1i128:$vD, (int_ppc_altivec_bcdsetsgno v1i128:$vB, i1:$PS))

- Decimal Shift/Unsigned-Shift/Shift-and-Round: bcds. bcdus. bcdsr.
  . Use instrinstics:
    (set v1i128:$vD, (int_ppc_altivec_bcdso  v1i128:$vA, v1i128:$vB, i1:$PS))
    (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB))
    (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS))

  . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]

- Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.
  . Use instrinstics:
    (set v1i128:$vD, (int_ppc_altivec_bcdso  v1i128:$vA, v1i128:$vB, i1:$PS))
    (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB))

  . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63])

VSX:
- QP Copy Sign: xscpsgnqp
  . Similar to xscpsgndp
  . (set f128:$vT, (fcopysign f128:$vB, f128:$vA)

- QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp
  . Similar to xsabsdp/xsnabsdp/xsnegdp
  . (set f128:$vT, (fabs f128:$vB))             // xsabsqp
    (set f128:$vT, (fneg (fabs f128:$vB)))      // xsnabsqp
    (set f128:$vT, (fneg f128:$vB))             // xsnegqp

- QP Add/Divide/Multiply/Subtract/Square-Root:
  xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp
  . Similar to xsadddp
  . isCommutable = 1
    (set f128:$vT, (fadd f128:$vA, f128:$vB))   // xsaddqp
    (set f128:$vT, (fmul f128:$vA, f128:$vB))   // xsmulqp

  . isCommutable = 0
    (set f128:$vT, (fdiv f128:$vA, f128:$vB))   // xsdivqp
    (set f128:$vT, (fsub f128:$vA, f128:$vB))   // xssubqp
    (set f128:$vT, (fsqrt f128:$vB)))           // xssqrtqp

- Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:
  xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo
  . Similar to xsrsqrtedp??
      def XSRSQRTEDP : XX2Form<60, 74,
                               (outs vsfrc:$XT), (ins vsfrc:$XB),
                               "xsrsqrtedp $XT, $XB", IIC_VecFP,
                               [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;

  . Define DAG Node in PPCInstrInfo.td:
    def PPCfaddrto: SDNode<"PPCISD::FADDRTO", SDTFPBinOp, []>;
    def PPCfdivrto: SDNode<"PPCISD::FDIVRTO", SDTFPBinOp, []>;
    def PPCfmulrto: SDNode<"PPCISD::FMULRTO", SDTFPBinOp, []>;
    def PPCfsubrto: SDNode<"PPCISD::FSUBRTO", SDTFPBinOp, []>;
    def PPCfsqrtrto: SDNode<"PPCISD::FSQRTRTO", SDTFPUnaryOp, []>;

    DAG patterns of each instruction (PPCInstrVSX.td):
    . isCommutable = 1
      (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB))   // xsaddqpo
      (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB))   // xsmulqpo

    . isCommutable = 0
      (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB))   // xsdivqpo
      (set f128:$vT, (PPCfsubrto f128:$vA, f128:$vB))   // xssubqpo
      (set f128:$vT, (PPCfsqrtrto f128:$vB))            // xssqrtqpo

- QP (Negative) Multiply-{Add/Subtract}: xsmaddqp xsmsubqp xsnmaddqp xsnmsubqp
  . Ref: xsmaddadp/xsmsubadp/xsnmaddadp/xsnmsubadp

  . isCommutable = 1
    // xsmaddqp
    [(set f128:$vT, (fma f128:$vA, f128:$vB, f128:$vTi))]>,
    RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
    AltVSXFMARel;

    // xsmsubqp
    [(set f128:$vT, (fma f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,
    RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
    AltVSXFMARel;

    // xsnmaddqp
    [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, f128:$vTi)))]>,
    RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
    AltVSXFMARel;

    // xsnmsubqp
    [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,
    RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
    AltVSXFMARel;

- Round to Odd of QP (Negative) Multiply-{Add/Subtract}:
  xsmaddqpo xsmsubqpo xsnmaddqpo xsnmsubqpo
  . Similar to xsrsqrtedp??

  . Define DAG Node in PPCInstrInfo.td:
    def PPCfmarto: SDNode<"PPCISD::FMARTO", SDTFPTernaryOp, []>;

    It looks like we only need to define "PPCfmarto" for these instructions,
    because according to PowerISA_V3.0, these instructions perform RTO on
    fma's result:
        xsmaddqp(o)
        v      ← bfp_MULTIPLY_ADD(src1, src3, src2)
        rnd    ← bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)
        result ← bfp_CONVERT_TO_BFP128(rnd)

        xsmsubqp(o)
        v      ← bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2))
        rnd    ← bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)
        result ← bfp_CONVERT_TO_BFP128(rnd)

        xsnmaddqp(o)
        v      ← bfp_MULTIPLY_ADD(src1,src3,src2)
        rnd    ← bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v))
        result ← bfp_CONVERT_TO_BFP128(rnd)

        xsnmsubqp(o)
        v      ← bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2))
        rnd    ← bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v))
        result ← bfp_CONVERT_TO_BFP128(rnd)

    DAG patterns of each instruction (PPCInstrVSX.td):
    . isCommutable = 1
      // xsmaddqpo
      [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, f128:$vTi))]>,
      RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
      AltVSXFMARel;

      // xsmsubqpo
      [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,
      RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
      AltVSXFMARel;

      // xsnmaddqpo
      [(set f128:$vT, (fneg (PPCfmarto f128:$vA, f128:$vB, f128:$vTi)))]>,
      RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
      AltVSXFMARel;

      // xsnmsubqpo
      [(set f128:$vT, (fneg (PPCfmarto f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,
      RegConstraint<"$vTi = $vT">, NoEncode<"$vTi">,
      AltVSXFMARel;

- QP Compare Ordered/Unordered: xscmpoqp xscmpuqp
  . ref: XSCMPUDP
      def XSCMPUDP : XX3Form_1<60, 35,
                               (outs crrc:$crD), (ins vsfrc:$XA, vsfrc:$XB),
                               "xscmpudp $crD, $XA, $XB", IIC_FPCompare, []>;

  . No SDAG, intrinsic, builtin are required??
    Or llvm fcmp order/unorder compare??

- DP/QP Compare Exponents: xscmpexpdp xscmpexpqp
  . No SDAG, intrinsic, builtin are required?

- DP Compare ==, >=, >, !=: xscmpeqdp xscmpgedp xscmpgtdp xscmpnedp
  . I checked existing instruction "XSCMPUDP". They are different in target
    register. "XSCMPUDP" write to CR field, xscmp*dp write to VSX register

  . Use instrinsic:
    (set i128:$XT, (int_ppc_vsx_xscmpeqdp f64:$XA, f64:$XB))
    (set i128:$XT, (int_ppc_vsx_xscmpgedp f64:$XA, f64:$XB))
    (set i128:$XT, (int_ppc_vsx_xscmpgtdp f64:$XA, f64:$XB))
    (set i128:$XT, (int_ppc_vsx_xscmpnedp f64:$XA, f64:$XB))

- Vector Compare Not Equal: xvcmpnedp xvcmpnedp. xvcmpnesp xvcmpnesp.
  . Similar to xvcmpeqdp:
      defm XVCMPEQDP : XX3Form_Rcr<60, 99,
                                 "xvcmpeqdp", "$XT, $XA, $XB", IIC_VecFPCompare,
                                 int_ppc_vsx_xvcmpeqdp, v2i64, v2f64>;

  . So we should use "XX3Form_Rcr" to implement instrinsic

- Convert DP -> QP: xscvdpqp
  . Similar to XSCVDPSP:
      def XSCVDPSP : XX2Form<60, 265,
                          (outs vsfrc:$XT), (ins vsfrc:$XB),
                          "xscvdpsp $XT, $XB", IIC_VecFP, []>;
  . So, No SDAG, intrinsic, builtin are required??

- Round & Convert QP -> DP (dword[1] is set to zero): xscvqpdp xscvqpdpo
  . Similar to XSCVDPSP
  . No SDAG, intrinsic, builtin are required??

- Truncate & Convert QP -> (Un)Signed (D)Word (dword[1] is set to zero):
  xscvqpsdz xscvqpswz xscvqpudz xscvqpuwz
  . According to PowerISA_V3.0, these are similar to "XSCVDPSXDS", "XSCVDPSXWS",
    "XSCVDPUXDS", "XSCVDPUXWS"

  . DAG patterns:
    (set f128:$XT, (PPCfctidz f128:$XB))    // xscvqpsdz
    (set f128:$XT, (PPCfctiwz f128:$XB))    // xscvqpswz
    (set f128:$XT, (PPCfctiduz f128:$XB))   // xscvqpudz
    (set f128:$XT, (PPCfctiwuz f128:$XB))   // xscvqpuwz

- Convert (Un)Signed DWord -> QP: xscvsdqp xscvudqp
  . Similar to XSCVSXDSP
  . (set f128:$XT, (PPCfcfids f64:$XB))     // xscvsdqp
    (set f128:$XT, (PPCfcfidus f64:$XB))    // xscvudqp

- (Round &) Convert DP <-> HP: xscvdphp xscvhpdp
  . Similar to XSCVDPSP
  . No SDAG, intrinsic, builtin are required??

- Vector HP -> SP: xvcvhpsp xvcvsphp
  . Similar to XVCVDPSP:
      def XVCVDPSP : XX2Form<60, 393,
                          (outs vsrc:$XT), (ins vsrc:$XB),
                          "xvcvdpsp $XT, $XB", IIC_VecFP, []>;
  . No SDAG, intrinsic, builtin are required??

- Round to Quad-Precision Integer: xsrqpi xsrqpix
  . These are combination of "XSRDPI", "XSRDPIC", "XSRDPIM", .., because you
    need to assign rounding mode in instruction
  . Provide builtin?
    (set f128:$vT, (int_ppc_vsx_xsrqpi f128:$vB))
    (set f128:$vT, (int_ppc_vsx_xsrqpix f128:$vB))

- Round Quad-Precision to Double-Extended Precision (fp80): xsrqpxp
  . Provide builtin?
    (set f128:$vT, (int_ppc_vsx_xsrqpxp f128:$vB))

Fixed Point Facility:

- Exploit cmprb and cmpeqb (perhaps for something like
  isalpha/isdigit/isupper/islower and isspace respectivelly). This can
  perhaps be done through a builtin.

- Provide testing for cnttz[dw]
- Insert Exponent DP/QP: xsiexpdp xsiexpqp
  . Use intrinsic?
  . xsiexpdp:
    // Note: rA and rB are the unsigned integer value.
    (set f128:$XT, (int_ppc_vsx_xsiexpdp i64:$rA, i64:$rB))

  . xsiexpqp:
    (set f128:$vT, (int_ppc_vsx_xsiexpqp f128:$vA, f64:$vB))

- Extract Exponent/Significand DP/QP: xsxexpdp xsxsigdp xsxexpqp xsxsigqp
  . Use intrinsic?
  . (set i64:$rT, (int_ppc_vsx_xsxexpdp f64$XB))    // xsxexpdp
    (set i64:$rT, (int_ppc_vsx_xsxsigdp f64$XB))    // xsxsigdp
    (set f128:$vT, (int_ppc_vsx_xsxexpqp f128$vB))  // xsxexpqp
    (set f128:$vT, (int_ppc_vsx_xsxsigqp f128$vB))  // xsxsigqp

- Vector Insert Word: xxinsertw
  - Useful for inserting f32/i32 elements into vectors (the element to be
    inserted needs to be prepared)
  . Note: llvm has insertelem in "Vector Operations"
    ; yields <n x <ty>>
    <result> = insertelement <n x <ty>> <val>, <ty> <elt>, <ty2> <idx>

    But how to map to it??
    [(set v1f128:$XT, (insertelement v1f128:$XTi, f128:$XB, i4:$UIMM))]>,
    RegConstraint<"$XTi = $XT">, NoEncode<"$XTi">,

  . Or use intrinsic?
    (set v1f128:$XT, (int_ppc_vsx_xxinsertw v1f128:$XTi, f128:$XB, i4:$UIMM))

- Vector Extract Unsigned Word: xxextractuw
  - Not useful for extraction of f32 from v4f32 (the current pattern is better -
    shift->convert)
  - It is useful for (uint_to_fp (vector_extract v4i32, N))
  - Unfortunately, it can't be used for (sint_to_fp (vector_extract v4i32, N))
  . Note: llvm has extractelement in "Vector Operations"
    ; yields <ty>
    <result> = extractelement <n x <ty>> <val>, <ty2> <idx>

    How to map to it??
    [(set f128:$XT, (extractelement v1f128:$XB, i4:$UIMM))]

  . Or use intrinsic?
    (set f128:$XT, (int_ppc_vsx_xxextractuw v1f128:$XB, i4:$UIMM))

- Vector Insert Exponent DP/SP: xviexpdp xviexpsp
  . Use intrinsic
    (set v2f64:$XT, (int_ppc_vsx_xviexpdp v2f64:$XA, v2f64:$XB))
    (set v4f32:$XT, (int_ppc_vsx_xviexpsp v4f32:$XA, v4f32:$XB))

- Vector Extract Exponent/Significand DP/SP: xvxexpdp xvxexpsp xvxsigdp xvxsigsp
  . Use intrinsic
    (set v2f64:$XT, (int_ppc_vsx_xvxexpdp v2f64:$XB))
    (set v4f32:$XT, (int_ppc_vsx_xvxexpsp v4f32:$XB))
    (set v2f64:$XT, (int_ppc_vsx_xvxsigdp v2f64:$XB))
    (set v4f32:$XT, (int_ppc_vsx_xvxsigsp v4f32:$XB))

- Test Data Class SP/DP/QP: xststdcsp xststdcdp xststdcqp
  . No SDAG, intrinsic, builtin are required?
    Because it seems that we have no way to map BF field?

    Instruction Form: [PO T XO B XO BX TX]
    Asm: xststd* BF,XB,DCMX

    BF is an index to CR register field.

- Vector Test Data Class SP/DP: xvtstdcsp xvtstdcdp
  . Use intrinsic
    (set v4f32:$XT, (int_ppc_vsx_xvtstdcsp v4f32:$XB, i7:$DCMX))
    (set v2f64:$XT, (int_ppc_vsx_xvtstdcdp v2f64:$XB, i7:$DCMX))

- Maximum/Minimum Type-C/Type-J DP: xsmaxcdp xsmaxjdp xsmincdp xsminjdp
  . PowerISA_V3.0:
    "xsmaxcdp can be used to implement the C/C++/Java conditional operation
     (x>y)?x:y for single-precision and double-precision arguments."

    Note! c type and j type have different behavior when:
    1. Either input is NaN
    2. Both input are +-Infinity, +-Zero

  . dtype map to llvm fmaxnum/fminnum
    jtype use intrinsic

  . xsmaxcdp xsmincdp
    (set f64:$XT, (fmaxnum f64:$XA, f64:$XB))
    (set f64:$XT, (fminnum f64:$XA, f64:$XB))

  . xsmaxjdp xsminjdp
    (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB))
    (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB))

- Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq
  . Use intrinsic
    (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB))
    (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB))
    (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB))
    (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB))

- Vector Permute: xxperm xxpermr
  . I have checked "PPCxxswapd" in PPCInstrVSX.td, but they are different
  . Use intrinsic
    (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB))
    (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB))

- Vector Splat Immediate Byte: xxspltib
  . Similar to XXSPLTW:
      def XXSPLTW : XX2Form_2<60, 164,
                           (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),
                           "xxspltw $XT, $XB, $UIM", IIC_VecPerm, []>;

  . No SDAG, intrinsic, builtin are required?

- Load/Store Vector: lxv stxv
  . Has likely SDAG match:
    (set v?:$XT, (load ix16addr:$src))
    (set v?:$XT, (store ix16addr:$dst))

  . Need define ix16addr in PPCInstrInfo.td
    ix16addr: 16-byte aligned, see "def memrix16" in PPCInstrInfo.td

- Load/Store Vector Indexed: lxvx stxvx
  . Has likely SDAG match:
    (set v?:$XT, (load xoaddr:$src))
    (set v?:$XT, (store xoaddr:$dst))

- Load/Store DWord: lxsd stxsd
  . Similar to lxsdx/stxsdx:
    def LXSDX : XX1Form<31, 588,
                        (outs vsfrc:$XT), (ins memrr:$src),
                        "lxsdx $XT, $src", IIC_LdStLFD,
                        [(set f64:$XT, (load xoaddr:$src))]>;

  . (set f64:$XT, (load ixaddr:$src))
    (set f64:$XT, (store ixaddr:$dst))

- Load/Store SP, with conversion from/to DP: lxssp stxssp
  . Similar to lxsspx/stxsspx:
    def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),
                         "lxsspx $XT, $src", IIC_LdStLFD,
                         [(set f32:$XT, (load xoaddr:$src))]>;

  . (set f32:$XT, (load ixaddr:$src))
    (set f32:$XT, (store ixaddr:$dst))

- Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx
  . Similar to lxsiwzx:
    def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),
                          "lxsiwzx $XT, $src", IIC_LdStLFD,
                          [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;

  . (set f64:$XT, (PPClfiwzx xoaddr:$src))

- Store as Integer Byte/Halfword Indexed: stxsibx stxsihx
  . Similar to stxsiwx:
    def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),
                          "stxsiwx $XT, $dst", IIC_LdStSTFD,
                          [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;

  . (PPCstfiwx f64:$XT, xoaddr:$dst)

- Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x
  . Similar to lxvd2x/lxvw4x:
    def LXVD2X : XX1Form<31, 844,
                         (outs vsrc:$XT), (ins memrr:$src),
                         "lxvd2x $XT, $src", IIC_LdStLFD,
                         [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;

  . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src))
    (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src))

- Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x
  . Similar to stxvd2x/stxvw4x:
    def STXVD2X : XX1Form<31, 972,
                         (outs), (ins vsrc:$XT, memrr:$dst),
                         "stxvd2x $XT, $dst", IIC_LdStSTFD,
                         [(store v2f64:$XT, xoaddr:$dst)]>;

  . (store v8i16:$XT, xoaddr:$dst)
    (store v16i8:$XT, xoaddr:$dst)

- Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll
  . Likely needs an intrinsic
  . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src))
    (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src))

  . (int_ppc_vsx_stxvl xoaddr:$dst))
    (int_ppc_vsx_stxvll xoaddr:$dst))

- Load Vector Word & Splat Indexed: lxvwsx
  . Likely needs an intrinsic
  . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src))

Atomic operations (l[dw]at, st[dw]at):
- Provide custom lowering for common atomic operations to use these
  instructions with the correct Function Code
- Ensure the operands are in the correct register (i.e. RT+1, RT+2)
- Provide builtins since not all FC's necessarily have an existing LLVM
  atomic operation

Load Doubleword Monitored (ldmx):
- Investigate whether there are any uses for this. It seems to be related to
  Garbage Collection so it isn't likely to be all that useful for most
  languages we deal with.

Move to CR from XER Extended (mcrxrx):
- Is there a use for this in LLVM?

Fixed Point Facility:

- Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last
  . Use instrinstics:
    (int_ppc_copy_first i32:$rA, i32:$rB)
    (int_ppc_copy i32:$rA, i32:$rB)

    (int_ppc_paste i32:$rA, i32:$rB)
    (int_ppc_paste_last i32:$rA, i32:$rB)

    (int_cp_abort)

- Message Synchronize: msgsync
- SLB*: slbieg slbsync
- stop
  . No instrinstics
To-do
-----

* Instruction encodings
* Tailcalls
* Investigate loop alignment
* Add builtins

# AVR backend

This experimental backend is for the 8-bit Atmel [AVR](https://en.wikipedia.org/wiki/Atmel_AVR) microcontroller.

## Useful links

* [Unresolved bugs](https://llvm.org/bugs/buglist.cgi?product=libraries&component=Backend%3A%20AVR&resolution=---&list_id=109466)
* [Architecture notes](https://github.com/avr-llvm/architecture)
To-do
-----

* Keep the address of the constant pool in a register instead of forming its
  address all of the time.
* We can fold small constant offsets into the %hi/%lo references to constant
  pool addresses as well.
* When in V9 mode, register allocate %icc[0-3].
* Add support for isel'ing UMUL_LOHI instead of marking it as Expand.
* Emit the 'Branch on Integer Register with Prediction' instructions.  It's
  not clear how to write a pattern for this though:

float %t1(int %a, int* %p) {
        %C = seteq int %a, 0
        br bool %C, label %T, label %F
T:
        store int 123, int* %p
        br label %F
F:
        ret float undef
}

codegens to this:

t1:
        save -96, %o6, %o6
1)      subcc %i0, 0, %l0
1)      bne .LBBt1_2    ! F
        nop
.LBBt1_1:       ! T
        or %g0, 123, %l0
        st %l0, [%i1]
.LBBt1_2:       ! F
        restore %g0, %g0, %g0
        retl
        nop

1) should be replaced with a brz in V9 mode.

* Same as above, but emit conditional move on register zero (p192) in V9
  mode.  Testcase:

int %t1(int %a, int %b) {
        %C = seteq int %a, 0
        %D = select bool %C, int %a, int %b
        ret int %D
}

* Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling
  with the Y register, if they are faster.

* Codegen bswap(load)/store(bswap) -> load/store ASI

* Implement frame pointer elimination, e.g. eliminate save/restore for
  leaf fns.
* Fill delay slots

* Use %g0 directly to materialize 0. No instruction is required.
//===---------------------------------------------------------------------===//
// Random ideas for the ARM backend (Thumb specific).
//===---------------------------------------------------------------------===//

* Add support for compiling functions in both ARM and Thumb mode, then taking
  the smallest.

* Add support for compiling individual basic blocks in thumb mode, when in a 
  larger ARM function.  This can be used for presumed cold code, like paths
  to abort (failure path of asserts), EH handling code, etc.

* Thumb doesn't have normal pre/post increment addressing modes, but you can
  load/store 32-bit integers with pre/postinc by using load/store multiple
  instrs with a single register.

* Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add
  and cmp instructions can use high registers. Also, we can use them as
  temporaries to spill values into.

* In thumb mode, short, byte, and bool preferred alignments are currently set
  to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple
  of 4).

//===---------------------------------------------------------------------===//

Potential jumptable improvements:

* If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit
  jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the
  function is even smaller. This also applies to ARM.

* Thumb jumptable codegen can improve given some help from the assembler. This
  is what we generate right now:

	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4))
LPCRELL0:
	mov r1, #PCRELV0
	add r1, pc
	ldr r0, [r0, r1]
	mov pc, r0 
	.align	2
LJTI1_0_0:
	.long	 LBB1_3
        ...

Note there is another pc relative add that we can take advantage of.
     add r1, pc, #imm_8 * 4

We should be able to generate:

LPCRELL0:
	add r1, LJTI1_0_0
	ldr r0, [r0, r1]
	mov pc, r0 
	.align	2
LJTI1_0_0:
	.long	 LBB1_3

if the assembler can translate the add to:
       add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc)

Note the assembler also does something similar to constpool load:
LPCRELL0:
     ldr r0, LCPI1_0
=>
     ldr r0, pc, #((LCPI1_0-(LPCRELL0+4))&0xfffffffc)


//===---------------------------------------------------------------------===//

We compile the following:

define i16 @func_entry_2E_ce(i32 %i) {
        switch i32 %i, label %bb12.exitStub [
                 i32 0, label %bb4.exitStub
                 i32 1, label %bb9.exitStub
                 i32 2, label %bb4.exitStub
                 i32 3, label %bb4.exitStub
                 i32 7, label %bb9.exitStub
                 i32 8, label %bb.exitStub
                 i32 9, label %bb9.exitStub
        ]

bb12.exitStub:
        ret i16 0

bb4.exitStub:
        ret i16 1

bb9.exitStub:
        ret i16 2

bb.exitStub:
        ret i16 3
}

into:

_func_entry_2E_ce:
        mov r2, #1
        lsl r2, r0
        cmp r0, #9
        bhi LBB1_4      @bb12.exitStub
LBB1_1: @newFuncRoot
        mov r1, #13
        tst r2, r1
        bne LBB1_5      @bb4.exitStub
LBB1_2: @newFuncRoot
        ldr r1, LCPI1_0
        tst r2, r1
        bne LBB1_6      @bb9.exitStub
LBB1_3: @newFuncRoot
        mov r1, #1
        lsl r1, r1, #8
        tst r2, r1
        bne LBB1_7      @bb.exitStub
LBB1_4: @bb12.exitStub
        mov r0, #0
        bx lr
LBB1_5: @bb4.exitStub
        mov r0, #1
        bx lr
LBB1_6: @bb9.exitStub
        mov r0, #2
        bx lr
LBB1_7: @bb.exitStub
        mov r0, #3
        bx lr
LBB1_8:
        .align  2
LCPI1_0:
        .long   642


gcc compiles to:

	cmp	r0, #9
	@ lr needed for prologue
	bhi	L2
	ldr	r3, L11
	mov	r2, #1
	mov	r1, r2, asl r0
	ands	r0, r3, r2, asl r0
	movne	r0, #2
	bxne	lr
	tst	r1, #13
	beq	L9
L3:
	mov	r0, r2
	bx	lr
L9:
	tst	r1, #256
	movne	r0, #3
	bxne	lr
L2:
	mov	r0, #0
	bx	lr
L12:
	.align 2
L11:
	.long	642
        

GCC is doing a couple of clever things here:
  1. It is predicating one of the returns.  This isn't a clear win though: in
     cases where that return isn't taken, it is replacing one condbranch with
     two 'ne' predicated instructions.
  2. It is sinking the shift of "1 << i" into the tst, and using ands instead of
     tst.  This will probably require whole function isel.
  3. GCC emits:
  	tst	r1, #256
     we emit:
        mov r1, #1
        lsl r1, r1, #8
        tst r2, r1

//===---------------------------------------------------------------------===//

When spilling in thumb mode and the sp offset is too large to fit in the ldr /
str offset field, we load the offset from a constpool entry and add it to sp:

ldr r2, LCPI
add r2, sp
ldr r2, [r2]

These instructions preserve the condition code which is important if the spill
is between a cmp and a bcc instruction. However, we can use the (potentially)
cheaper sequnce if we know it's ok to clobber the condition register.

add r2, sp, #255 * 4
add r2, #132
ldr r2, [r2, #7 * 4]

This is especially bad when dynamic alloca is used. The all fixed size stack
objects are referenced off the frame pointer with negative offsets. See
oggenc for an example.

//===---------------------------------------------------------------------===//

Poor codegen test/CodeGen/ARM/select.ll f7:

	ldr r5, LCPI1_0
LPC0:
	add r5, pc
	ldr r6, LCPI1_1
	ldr r2, LCPI1_2
	mov r3, r6
	mov lr, pc
	bx r5

//===---------------------------------------------------------------------===//

Make register allocator / spiller smarter so we can re-materialize "mov r, imm",
etc. Almost all Thumb instructions clobber condition code.

//===---------------------------------------------------------------------===//

Thumb load / store address mode offsets are scaled. The values kept in the
instruction operands are pre-scale values. This probably ought to be changed
to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions.

//===---------------------------------------------------------------------===//

We need to make (some of the) Thumb1 instructions predicable. That will allow
shrinking of predicated Thumb2 instructions. To allow this, we need to be able
to toggle the 's' bit since they do not set CPSR when they are inside IT blocks.

//===---------------------------------------------------------------------===//

Make use of hi register variants of cmp: tCMPhir / tCMPZhir.

//===---------------------------------------------------------------------===//

Thumb1 immediate field sometimes keep pre-scaled values. See
ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and
Thumb2.

//===---------------------------------------------------------------------===//

Rather than having tBR_JTr print a ".align 2" and constant island pass pad it,
add a target specific ALIGN instruction instead. That way, getInstSizeInBytes
won't have to over-estimate. It can also be used for loop alignment pass.

//===---------------------------------------------------------------------===//

We generate conditional code for icmp when we don't need to. This code:

  int foo(int s) {
    return s == 1;
  }

produces:

foo:
        cmp     r0, #1
        mov.w   r0, #0
        it      eq
        moveq   r0, #1
        bx      lr

when it could use subs + adcs. This is GCC PR46975.
//===---------------------------------------------------------------------===//
// Random ideas for the ARM backend.
//===---------------------------------------------------------------------===//

Reimplement 'select' in terms of 'SEL'.

* We would really like to support UXTAB16, but we need to prove that the
  add doesn't need to overflow between the two 16-bit chunks.

* Implement pre/post increment support.  (e.g. PR935)
* Implement smarter constant generation for binops with large immediates.

A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX

Interesting optimization for PIC codegen on arm-linux:
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129

//===---------------------------------------------------------------------===//

Crazy idea:  Consider code that uses lots of 8-bit or 16-bit values.  By the
time regalloc happens, these values are now in a 32-bit register, usually with
the top-bits known to be sign or zero extended.  If spilled, we should be able
to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part
of the reload.

Doing this reduces the size of the stack frame (important for thumb etc), and
also increases the likelihood that we will be able to reload multiple values
from the stack with a single load.

//===---------------------------------------------------------------------===//

The constant island pass is in good shape.  Some cleanups might be desirable,
but there is unlikely to be much improvement in the generated code.

1.  There may be some advantage to trying to be smarter about the initial
placement, rather than putting everything at the end.

2.  There might be some compile-time efficiency to be had by representing
consecutive islands as a single block rather than multiple blocks.

3.  Use a priority queue to sort constant pool users in inverse order of
    position so we always process the one closed to the end of functions
    first. This may simply CreateNewWater.

//===---------------------------------------------------------------------===//

Eliminate copysign custom expansion. We are still generating crappy code with
default expansion + if-conversion.

//===---------------------------------------------------------------------===//

Eliminate one instruction from:

define i32 @_Z6slow4bii(i32 %x, i32 %y) {
        %tmp = icmp sgt i32 %x, %y
        %retval = select i1 %tmp, i32 %x, i32 %y
        ret i32 %retval
}

__Z6slow4bii:
        cmp r0, r1
        movgt r1, r0
        mov r0, r1
        bx lr
=>

__Z6slow4bii:
        cmp r0, r1
        movle r0, r1
        bx lr

//===---------------------------------------------------------------------===//

Implement long long "X-3" with instructions that fold the immediate in.  These
were disabled due to badness with the ARM carry flag on subtracts.

//===---------------------------------------------------------------------===//

More load / store optimizations:
1) Better representation for block transfer? This is from Olden/power:

	fldd d0, [r4]
	fstd d0, [r4, #+32]
	fldd d0, [r4, #+8]
	fstd d0, [r4, #+40]
	fldd d0, [r4, #+16]
	fstd d0, [r4, #+48]
	fldd d0, [r4, #+24]
	fstd d0, [r4, #+56]

If we can spare the registers, it would be better to use fldm and fstm here.
Need major register allocator enhancement though.

2) Can we recognize the relative position of constantpool entries? i.e. Treat

	ldr r0, LCPI17_3
	ldr r1, LCPI17_4
	ldr r2, LCPI17_5

   as
	ldr r0, LCPI17
	ldr r1, LCPI17+4
	ldr r2, LCPI17+8

   Then the ldr's can be combined into a single ldm. See Olden/power.

Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a
double 64-bit FP constant:

	adr	r0, L6
	ldmia	r0, {r0-r1}

	.align 2
L6:
	.long	-858993459
	.long	1074318540

3) struct copies appear to be done field by field
instead of by words, at least sometimes:

struct foo { int x; short s; char c1; char c2; };
void cpy(struct foo*a, struct foo*b) { *a = *b; }

llvm code (-O2)
        ldrb r3, [r1, #+6]
        ldr r2, [r1]
        ldrb r12, [r1, #+7]
        ldrh r1, [r1, #+4]
        str r2, [r0]
        strh r1, [r0, #+4]
        strb r3, [r0, #+6]
        strb r12, [r0, #+7]
gcc code (-O2)
        ldmia   r1, {r1-r2}
        stmia   r0, {r1-r2}

In this benchmark poor handling of aggregate copies has shown up as
having a large effect on size, and possibly speed as well (we don't have
a good way to measure on ARM).

//===---------------------------------------------------------------------===//

* Consider this silly example:

double bar(double x) {
  double r = foo(3.1);
  return x+r;
}

_bar:
        stmfd sp!, {r4, r5, r7, lr}
        add r7, sp, #8
        mov r4, r0
        mov r5, r1
        fldd d0, LCPI1_0
        fmrrd r0, r1, d0
        bl _foo
        fmdrr d0, r4, r5
        fmsr s2, r0
        fsitod d1, s2
        faddd d0, d1, d0
        fmrrd r0, r1, d0
        ldmfd sp!, {r4, r5, r7, pc}

Ignore the prologue and epilogue stuff for a second. Note
	mov r4, r0
	mov r5, r1
the copys to callee-save registers and the fact they are only being used by the
fmdrr instruction. It would have been better had the fmdrr been scheduled
before the call and place the result in a callee-save DPR register. The two
mov ops would not have been necessary.

//===---------------------------------------------------------------------===//

Calling convention related stuff:

* gcc's parameter passing implementation is terrible and we suffer as a result:

e.g.
struct s {
  double d1;
  int s1;
};

void foo(struct s S) {
  printf("%g, %d\n", S.d1, S.s1);
}

'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and
then reload them to r1, r2, and r3 before issuing the call (r0 contains the
address of the format string):

	stmfd	sp!, {r7, lr}
	add	r7, sp, #0
	sub	sp, sp, #12
	stmia	sp, {r0, r1, r2}
	ldmia	sp, {r1-r2}
	ldr	r0, L5
	ldr	r3, [sp, #8]
L2:
	add	r0, pc, r0
	bl	L_printf$stub

Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?

* Return an aggregate type is even worse:

e.g.
struct s foo(void) {
  struct s S = {1.1, 2};
  return S;
}

	mov	ip, r0
	ldr	r0, L5
	sub	sp, sp, #12
L2:
	add	r0, pc, r0
	@ lr needed for prologue
	ldmia	r0, {r0, r1, r2}
	stmia	sp, {r0, r1, r2}
	stmia	ip, {r0, r1, r2}
	mov	r0, ip
	add	sp, sp, #12
	bx	lr

r0 (and later ip) is the hidden parameter from caller to store the value in. The
first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,
r2 into the address passed in. However, there is one additional stmia that
stores r0, r1, and r2 to some stack location. The store is dead.

The llvm-gcc generated code looks like this:

csretcc void %foo(%struct.s* %agg.result) {
entry:
	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]
	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]
	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]
	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 )
	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]
	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 )
	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]
	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 )
	ret void
}

llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from
constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated
into a number of load and stores, or 2) custom lower memcpy (of small size) to
be ldmia / stmia. I think option 2 is better but the current register
allocator cannot allocate a chunk of registers at a time.

A feasible temporary solution is to use specific physical registers at the
lowering time for small (<= 4 words?) transfer size.

* ARM CSRet calling convention requires the hidden argument to be returned by
the callee.

//===---------------------------------------------------------------------===//

We can definitely do a better job on BB placements to eliminate some branches.
It's very common to see llvm generated assembly code that looks like this:

LBB3:
 ...
LBB4:
...
  beq LBB3
  b LBB2

If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can
then eliminate beq and turn the unconditional branch to LBB2 to a bne.

See McCat/18-imp/ComputeBoundingBoxes for an example.

//===---------------------------------------------------------------------===//

Pre-/post- indexed load / stores:

1) We should not make the pre/post- indexed load/store transform if the base ptr
is guaranteed to be live beyond the load/store. This can happen if the base
ptr is live out of the block we are performing the optimization. e.g.

mov r1, r2
ldr r3, [r1], #4
...

vs.

ldr r3, [r2]
add r1, r2, #4
...

In most cases, this is just a wasted optimization. However, sometimes it can
negatively impact the performance because two-address code is more restrictive
when it comes to scheduling.

Unfortunately, liveout information is currently unavailable during DAG combine
time.

2) Consider spliting a indexed load / store into a pair of add/sub + load/store
   to solve #1 (in TwoAddressInstructionPass.cpp).

3) Enhance LSR to generate more opportunities for indexed ops.

4) Once we added support for multiple result patterns, write indexed loads
   patterns instead of C++ instruction selection code.

5) Use VLDM / VSTM to emulate indexed FP load / store.

//===---------------------------------------------------------------------===//

Implement support for some more tricky ways to materialize immediates.  For
example, to get 0xffff8000, we can use:

mov r9, #&3f8000
sub r9, r9, #&400000

//===---------------------------------------------------------------------===//

We sometimes generate multiple add / sub instructions to update sp in prologue
and epilogue if the inc / dec value is too large to fit in a single immediate
operand. In some cases, perhaps it might be better to load the value from a
constantpool instead.

//===---------------------------------------------------------------------===//

GCC generates significantly better code for this function.

int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {
    int i = 0;

    if (StackPtr != 0) {
       while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768)))
          Line[i++] = Stack[--StackPtr];
        if (LineLen > 32768)
        {
            while (StackPtr != 0 && i < LineLen)
            {
                i++;
                --StackPtr;
            }
        }
    }
    return StackPtr;
}

//===---------------------------------------------------------------------===//

This should compile to the mlas instruction:
int mlas(int x, int y, int z) { return ((x * y + z) < 0) ? 7 : 13; }

//===---------------------------------------------------------------------===//

At some point, we should triage these to see if they still apply to us:

http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19598
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=18560
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=27016

http://gcc.gnu.org/bugzilla/show_bug.cgi?id=11831
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=11826
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=11825
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=11824
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=11823
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=11820
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=10982

http://gcc.gnu.org/bugzilla/show_bug.cgi?id=10242
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9831
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9760
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9759
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9703
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9702
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9663

http://www.inf.u-szeged.hu/gcc-arm/
http://citeseer.ist.psu.edu/debus04linktime.html

//===---------------------------------------------------------------------===//

gcc generates smaller code for this function at -O2 or -Os:

void foo(signed char* p) {
  if (*p == 3)
     bar();
   else if (*p == 4)
    baz();
  else if (*p == 5)
    quux();
}

llvm decides it's a good idea to turn the repeated if...else into a
binary tree, as if it were a switch; the resulting code requires -1
compare-and-branches when *p<=2 or *p==5, the same number if *p==4
or *p>6, and +1 if *p==3.  So it should be a speed win
(on balance).  However, the revised code is larger, with 4 conditional
branches instead of 3.

More seriously, there is a byte->word extend before
each comparison, where there should be only one, and the condition codes
are not remembered when the same two values are compared twice.

//===---------------------------------------------------------------------===//

More LSR enhancements possible:

1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged
   in a load / store.
2. Allow iv reuse even when a type conversion is required. For example, i8
   and i32 load / store addressing modes are identical.


//===---------------------------------------------------------------------===//

This:

int foo(int a, int b, int c, int d) {
  long long acc = (long long)a * (long long)b;
  acc += (long long)c * (long long)d;
  return (int)(acc >> 32);
}

Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies
two signed 32-bit values to produce a 64-bit value, and accumulates this with
a 64-bit value.

We currently get this with both v4 and v6:

_foo:
        smull r1, r0, r1, r0
        smull r3, r2, r3, r2
        adds r3, r3, r1
        adc r0, r2, r0
        bx lr

//===---------------------------------------------------------------------===//

This:
        #include <algorithm>
        std::pair<unsigned, bool> full_add(unsigned a, unsigned b)
        { return std::make_pair(a + b, a + b < a); }
        bool no_overflow(unsigned a, unsigned b)
        { return !full_add(a, b).second; }

Should compile to:

_Z8full_addjj:
	adds	r2, r1, r2
	movcc	r1, #0
	movcs	r1, #1
	str	r2, [r0, #0]
	strb	r1, [r0, #4]
	mov	pc, lr

_Z11no_overflowjj:
	cmn	r0, r1
	movcs	r0, #0
	movcc	r0, #1
	mov	pc, lr

not:

__Z8full_addjj:
        add r3, r2, r1
        str r3, [r0]
        mov r2, #1
        mov r12, #0
        cmp r3, r1
        movlo r12, r2
        str r12, [r0, #+4]
        bx lr
__Z11no_overflowjj:
        add r3, r1, r0
        mov r2, #1
        mov r1, #0
        cmp r3, r0
        movhs r1, r2
        mov r0, r1
        bx lr

//===---------------------------------------------------------------------===//

Some of the NEON intrinsics may be appropriate for more general use, either
as target-independent intrinsics or perhaps elsewhere in the ARM backend.
Some of them may also be lowered to target-independent SDNodes, and perhaps
some new SDNodes could be added.

For example, maximum, minimum, and absolute value operations are well-defined
and standard operations, both for vector and scalar types.

The current NEON-specific intrinsics for count leading zeros and count one
bits could perhaps be replaced by the target-independent ctlz and ctpop
intrinsics.  It may also make sense to add a target-independent "ctls"
intrinsic for "count leading sign bits".  Likewise, the backend could use
the target-independent SDNodes for these operations.

ARMv6 has scalar saturating and halving adds and subtracts.  The same
intrinsics could possibly be used for both NEON's vector implementations of
those operations and the ARMv6 scalar versions.

//===---------------------------------------------------------------------===//

Split out LDR (literal) from normal ARM LDR instruction. Also consider spliting
LDR into imm12 and so_reg forms. This allows us to clean up some code. e.g.
ARMLoadStoreOptimizer does not need to look at LDR (literal) and LDR (so_reg)
while ARMConstantIslandPass only need to worry about LDR (literal).

//===---------------------------------------------------------------------===//

Constant island pass should make use of full range SoImm values for LEApcrel.
Be careful though as the last attempt caused infinite looping on lencod.

//===---------------------------------------------------------------------===//

Predication issue. This function:

extern unsigned array[ 128 ];
int     foo( int x ) {
  int     y;
  y = array[ x & 127 ];
  if ( x & 128 )
     y = 123456789 & ( y >> 2 );
  else
     y = 123456789 & y;
  return y;
}

compiles to:

_foo:
	and r1, r0, #127
	ldr r2, LCPI1_0
	ldr r2, [r2]
	ldr r1, [r2, +r1, lsl #2]
	mov r2, r1, lsr #2
	tst r0, #128
	moveq r2, r1
	ldr r0, LCPI1_1
	and r0, r2, r0
	bx lr

It would be better to do something like this, to fold the shift into the
conditional move:

	and r1, r0, #127
	ldr r2, LCPI1_0
	ldr r2, [r2]
	ldr r1, [r2, +r1, lsl #2]
	tst r0, #128
	movne r1, r1, lsr #2
	ldr r0, LCPI1_1
	and r0, r1, r0
	bx lr

it saves an instruction and a register.

//===---------------------------------------------------------------------===//

It might be profitable to cse MOVi16 if there are lots of 32-bit immediates
with the same bottom half.

//===---------------------------------------------------------------------===//

Robert Muth started working on an alternate jump table implementation that
does not put the tables in-line in the text.  This is more like the llvm
default jump table implementation.  This might be useful sometime.  Several
revisions of patches are on the mailing list, beginning at:
http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html

//===---------------------------------------------------------------------===//

Make use of the "rbit" instruction.

//===---------------------------------------------------------------------===//

Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how
to licm and cse the unnecessary load from cp#1.

//===---------------------------------------------------------------------===//

The CMN instruction sets the flags like an ADD instruction, while CMP sets
them like a subtract. Therefore to be able to use CMN for comparisons other
than the Z bit, we'll need additional logic to reverse the conditionals
associated with the comparison. Perhaps a pseudo-instruction for the comparison,
with a post-codegen pass to clean up and handle the condition codes?
See PR5694 for testcase.

//===---------------------------------------------------------------------===//

Given the following on armv5:
int test1(int A, int B) {
  return (A&-8388481)|(B&8388480);
}

We currently generate:
	ldr	r2, .LCPI0_0
	and	r0, r0, r2
	ldr	r2, .LCPI0_1
	and	r1, r1, r2
	orr	r0, r1, r0
	bx	lr

We should be able to replace the second ldr+and with a bic (i.e. reuse the
constant which was already loaded).  Not sure what's necessary to do that.

//===---------------------------------------------------------------------===//

The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:

int a(int x) { return __builtin_bswap32(x); }

a:
	mov	r1, #255, 24
	mov	r2, #255, 16
	and	r1, r1, r0, lsr #8
	and	r2, r2, r0, lsl #8
	orr	r1, r1, r0, lsr #24
	orr	r0, r2, r0, lsl #24
	orr	r0, r0, r1
	bx	lr

Something like the following would be better (fewer instructions/registers):
	eor     r1, r0, r0, ror #16
	bic     r1, r1, #0xff0000
	mov     r1, r1, lsr #8
	eor     r0, r1, r0, ror #8
	bx	lr

A custom Thumb version would also be a slight improvement over the generic
version.

//===---------------------------------------------------------------------===//

Consider the following simple C code:

void foo(unsigned char *a, unsigned char *b, int *c) {
 if ((*a | *b) == 0) *c = 0;
}

currently llvm-gcc generates something like this (nice branchless code I'd say):

       ldrb    r0, [r0]
       ldrb    r1, [r1]
       orr     r0, r1, r0
       tst     r0, #255
       moveq   r0, #0
       streq   r0, [r2]
       bx      lr

Note that both "tst" and "moveq" are redundant.

//===---------------------------------------------------------------------===//

When loading immediate constants with movt/movw, if there are multiple
constants needed with the same low 16 bits, and those values are not live at
the same time, it would be possible to use a single movw instruction, followed
by multiple movt instructions to rewrite the high bits to different values.
For example:

  volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,
  !tbaa
!0
  volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,
  !tbaa
!0

is compiled and optimized to:

    movw    r0, #32796
    mov.w    r1, #-1
    movt    r0, #20480
    str    r1, [r0]
    movw    r0, #32796    @ <= this MOVW is not needed, value is there already
    movt    r0, #20482
    str    r1, [r0]

//===---------------------------------------------------------------------===//

Improve codegen for select's:
if (x != 0) x = 1
if (x == 1) x = 1

ARM codegen used to look like this:
       mov     r1, r0
       cmp     r1, #1
       mov     r0, #0
       moveq   r0, #1

The naive lowering select between two different values. It should recognize the
test is equality test so it's more a conditional move rather than a select:
       cmp     r0, #1
       movne   r0, #0

Currently this is a ARM specific dag combine. We probably should make it into a
target-neutral one.

//===---------------------------------------------------------------------===//

Optimize unnecessary checks for zero with __builtin_clz/ctz.  Those builtins
are specified to be undefined at zero, so portable code must check for zero
and handle it as a special case.  That is unnecessary on ARM where those
operations are implemented in a way that is well-defined for zero.  For
example:

int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }

should just be implemented with a CLZ instruction.  Since there are other
targets, e.g., PPC, that share this behavior, it would be best to implement
this in a target-independent way: we should probably fold that (when using
"undefined at zero" semantics) to set the "defined at zero" bit and have
the code generator expand out the right code.

//===---------------------------------------------------------------------===//

Clean up the test/MC/ARM files to have more robust register choices.

R0 should not be used as a register operand in the assembler tests as it's then
not possible to distinguish between a correct encoding and a missing operand
encoding, as zero is the default value for the binary encoder.
e.g.,
    add r0, r0  // bad
    add r3, r5  // good

Register operands should be distinct. That is, when the encoding does not
require two syntactical operands to refer to the same register, two different
registers should be used in the test so as to catch errors where the
operands are swapped in the encoding.
e.g.,
    subs.w r1, r1, r1 // bad
    subs.w r1, r2, r3 // good

//===---------------------------------------------------------------------===//
// Random ideas for the ARM backend (Thumb2 specific).
//===---------------------------------------------------------------------===//

Make sure jumptable destinations are below the jumptable in order to make use
of tbb / tbh.
//===-- README.txt - Notes for WebAssembly code gen -----------------------===//

This WebAssembly backend is presently under development.

Currently the easiest way to use it is through Emscripten, which provides a
compilation environment that includes standard libraries, tools, and packaging
for producing WebAssembly applications that can run in browsers and other
environments. For more information, see the Emscripten documentation in
general, and this page in particular:
  * https://github.com/kripken/emscripten/wiki/New-WebAssembly-Backend

Other ways of using this backend, such as via a standalone "clang", are also
under development, though they are not generally usable yet.

For more information on WebAssembly itself, see the home page:
  * https://webassembly.github.io/

The following documents contain some information on the semantics and binary
encoding of WebAssembly itself:
  * https://github.com/WebAssembly/design/blob/master/Semantics.md
  * https://github.com/WebAssembly/design/blob/master/BinaryEncoding.md

The backend is built, tested and archived on the following waterfall:
  https://wasm-stat.us

The backend's bringup is done in part by using the GCC torture test suite, since
it doesn't require C library support. Current known failures are in
known_gcc_test_failures.txt, all other tests should pass. The waterfall will
turn red if not. Once most of these pass, further testing will use LLVM's own
test suite. The tests can be run locally using:
  https://github.com/WebAssembly/waterfall/blob/master/src/compile_torture_tests.py

//===---------------------------------------------------------------------===//

Br, br_if, and br_table instructions can support having a value on the value
stack across the jump (sometimes). We should (a) model this, and (b) extend
the stackifier to utilize it.

//===---------------------------------------------------------------------===//

The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero
behavior. The ARM target has the same kind of min/max instructions and has
implemented optimizations for them; we should do similar optimizations for
WebAssembly.

//===---------------------------------------------------------------------===//

AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.
Would these be useful to run for WebAssembly too? Also, it has an option to
run SimplifyCFG after running the AtomicExpand pass. Would this be useful for
us too?

//===---------------------------------------------------------------------===//

Register stackification uses the VALUE_STACK physical register to impose
ordering dependencies on instructions with stack operands. This is pessimistic;
we should consider alternate ways to model stack dependencies.

//===---------------------------------------------------------------------===//

Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,
there are numerous optimization-related hooks that can be overridden in
WebAssemblyTargetLowering.

//===---------------------------------------------------------------------===//

Instead of the OptimizeReturned pass, which should consider preserving the
"returned" attribute through to MachineInstrs and extending the StoreResults
pass to do this optimization on calls too. That would also let the
WebAssemblyPeephole pass clean up dead defs for such calls, as it does for
stores.

//===---------------------------------------------------------------------===//

Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,
optimizeLoadInstr, and/or getMachineCombinerPatterns.

//===---------------------------------------------------------------------===//

Find a clean way to fix the problem which leads to the Shrink Wrapping pass
being run after the WebAssembly PEI pass.

//===---------------------------------------------------------------------===//

When setting multiple local variables to the same constant, we currently get
code like this:

    i32.const   $4=, 0
    i32.const   $3=, 0

It could be done with a smaller encoding like this:

    i32.const   $push5=, 0
    tee_local   $push6=, $4=, $pop5
    copy_local  $3=, $pop6

//===---------------------------------------------------------------------===//

WebAssembly registers are implicitly initialized to zero. Explicit zeroing is
therefore often redundant and could be optimized away.

//===---------------------------------------------------------------------===//

Small indices may use smaller encodings than large indices.
WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers
according to their usage frequency to maximize the usage of smaller encodings.

//===---------------------------------------------------------------------===//

Many cases of irreducible control flow could be transformed more optimally
than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp.

It may also be worthwhile to do transforms before register coloring,
particularly when duplicating code, to allow register coloring to be aware of
the duplication.

//===---------------------------------------------------------------------===//

WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more
aggressively.

//===---------------------------------------------------------------------===//

WebAssemblyRegStackify is currently a greedy algorithm. This means that, for
example, a binary operator will stackify with its user before its operands.
However, if moving the binary operator to its user moves it to a place where
its operands can't be moved to, it would be better to leave it in place, or
perhaps move it up, so that it can stackify its operands. A binary operator
has two operands and one result, so in such cases there could be a net win by
prefering the operands.

//===---------------------------------------------------------------------===//

Instruction ordering has a significant influence on register stackification and
coloring. Consider experimenting with the MachineScheduler (enable via
enableMachineScheduler) and determine if it can be configured to schedule
instructions advantageously for this purpose.

//===---------------------------------------------------------------------===//

WebAssembly is now officially a stack machine, rather than an AST, and this
comes with additional opportunities for WebAssemblyRegStackify. Specifically,
the stack doesn't need to be empty after an instruction with no return values.
WebAssemblyRegStackify could be extended, or possibly rewritten, to take
advantage of the new opportunities.

//===---------------------------------------------------------------------===//

Add support for mergeable sections in the Wasm writer, such as for strings and
floating-point constants.

//===---------------------------------------------------------------------===//

The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll
ends up with a tee_local in its prolog which has an unused result, requiring
an extra drop:

    get_global  $push8=, 0
    tee_local   $push9=, 1, $pop8
    drop        $pop9
    [...]

The prologue code initially thinks it needs an FP register, but later it
turns out to be unneeded, so one could either approach this by being more
clever about not inserting code for an FP in the first place, or optimizing
away the copy later.

//===---------------------------------------------------------------------===//
//===---------------------------------------------------------------------===//
// MSP430 backend.
//===---------------------------------------------------------------------===//

DISCLAIMER: This backend should be considered as highly experimental. I never
seen nor worked with this MCU, all information was gathered from datasheet
only. The original intention of making this backend was to write documentation
of form "How to write backend for dummies" :) Thes notes hopefully will be
available pretty soon.

Some things are incomplete / not implemented yet (this list surely is not
complete as well):

1. Verify, how stuff is handling implicit zext with 8 bit operands (this might
be modelled currently in improper way - should we need to mark the superreg as
def for every 8 bit instruction?).

2. Libcalls: multiplication, division, remainder. Note, that calling convention
for libcalls is incomptible with calling convention of libcalls of msp430-gcc
(these cannot be used though due to license restriction).

3. Implement multiplication / division by constant (dag combiner hook?).

4. Implement non-constant shifts.

5. Implement varargs stuff.

6. Verify and fix (if needed) how's stuff playing with i32 / i64.

7. Implement floating point stuff (softfp?)

8. Implement instruction encoding for (possible) direct code emission in the
future.

9. Since almost all instructions set flags - implement brcond / select in better
way (currently they emit explicit comparison).

10. Handle imm in comparisons in better way (see comment in MSP430InstrInfo.td)

11. Implement hooks for better memory op folding, etc.

//===---------------------------------------------------------------------===//
// Random notes about and ideas for the SystemZ backend.
//===---------------------------------------------------------------------===//

The initial backend is deliberately restricted to z10.  We should add support
for later architectures at some point.

--

If an inline asm ties an i32 "r" result to an i64 input, the input
will be treated as an i32, leaving the upper bits uninitialised.
For example:

define void @f4(i32 *%dst) {
  %val = call i32 asm "blah $0", "=r,0" (i64 103)
  store i32 %val, i32 *%dst
  ret void
}

from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.
to load 103.  This seems to be a general target-independent problem.

--

The tuning of the choice between LOAD ADDRESS (LA) and addition in
SystemZISelDAGToDAG.cpp is suspect.  It should be tweaked based on
performance measurements.

--

There is no scheduling support.

--

We don't use the BRANCH ON INDEX instructions.

--

We only use MVC, XC and CLC for constant-length block operations.
We could extend them to variable-length operations too,
using EXECUTE RELATIVE LONG.

MVCIN, MVCLE and CLCLE may be worthwhile too.

--

We don't use CUSE or the TRANSLATE family of instructions for string
operations.  The TRANSLATE ones are probably more difficult to exploit.

--

We don't take full advantage of builtins like fabsl because the calling
conventions require f128s to be returned by invisible reference.

--

ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to
produce a carry.  SUBTRACT LOGICAL IMMEDIATE could be useful when we
need to produce a borrow.  (Note that there are no memory forms of
ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high
part of 128-bit memory operations would probably need to be done
via a register.)

--

We don't use ICM, STCM, or CLM.

--

We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,
or COMPARE (LOGICAL) HIGH yet.

--

DAGCombiner doesn't yet fold truncations of extended loads.  Functions like:

    unsigned long f (unsigned long x, unsigned short *y)
    {
      return (x << 32) | *y;
    }

therefore end up as:

        sllg    %r2, %r2, 32
        llgh    %r0, 0(%r3)
        lr      %r2, %r0
        br      %r14

but truncating the load would give:

        sllg    %r2, %r2, 32
        lh      %r2, 0(%r3)
        br      %r14

--

Functions like:

define i64 @f1(i64 %a) {
  %and = and i64 %a, 1
  ret i64 %and
}

ought to be implemented as:

        lhi     %r0, 1
        ngr     %r2, %r0
        br      %r14

but two-address optimizations reverse the order of the AND and force:

        lhi     %r0, 1
        ngr     %r0, %r2
        lgr     %r2, %r0
        br      %r14

CodeGen/SystemZ/and-04.ll has several examples of this.

--

Out-of-range displacements are usually handled by loading the full
address into a register.  In many cases it would be better to create
an anchor point instead.  E.g. for:

define void @f4a(i128 *%aptr, i64 %base) {
  %addr = add i64 %base, 524288
  %bptr = inttoptr i64 %addr to i128 *
  %a = load volatile i128 *%aptr
  %b = load i128 *%bptr
  %add = add i128 %a, %b
  store i128 %add, i128 *%aptr
  ret void
}

(from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296
into separate registers, rather than using %base+524288 as a base for both.

--

Dynamic stack allocations round the size to 8 bytes and then allocate
that rounded amount.  It would be simpler to subtract the unrounded
size from the copy of the stack pointer and then align the result.
See CodeGen/SystemZ/alloca-01.ll for an example.

--

If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG.

--

We might want to model all access registers and use them to spill
32-bit values.

--

We might want to use the 'overflow' condition of eg. AR to support
llvm.sadd.with.overflow.i32 and related instructions - the generated code
for signed overflow check is currently quite bad.  This would improve
the results of using -ftrapv.
//===---------------------------------------------------------------------===//

Common register allocation / spilling problem:

        mul lr, r4, lr
        str lr, [sp, #+52]
        ldr lr, [r1, #+32]
        sxth r3, r3
        ldr r4, [sp, #+52]
        mla r4, r3, lr, r4

can be:

        mul lr, r4, lr
        mov r4, lr
        str lr, [sp, #+52]
        ldr lr, [r1, #+32]
        sxth r3, r3
        mla r4, r3, lr, r4

and then "merge" mul and mov:

        mul r4, r4, lr
        str r4, [sp, #+52]
        ldr lr, [r1, #+32]
        sxth r3, r3
        mla r4, r3, lr, r4

It also increase the likelihood the store may become dead.

//===---------------------------------------------------------------------===//

bb27 ...
        ...
        %reg1037 = ADDri %reg1039, 1
        %reg1038 = ADDrs %reg1032, %reg1039, %NOREG, 10
    Successors according to CFG: 0x8b03bf0 (#5)

bb76 (0x8b03bf0, LLVM BB @0x8b032d0, ID#5):
    Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4)
        %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>

Note ADDri is not a two-address instruction. However, its result %reg1037 is an
operand of the PHI node in bb76 and its operand %reg1039 is the result of the
PHI node. We should treat it as a two-address code and make sure the ADDri is
scheduled after any node that reads %reg1039.

//===---------------------------------------------------------------------===//

Use local info (i.e. register scavenger) to assign it a free register to allow
reuse:
        ldr r3, [sp, #+4]
        add r3, r3, #3
        ldr r2, [sp, #+8]
        add r2, r2, #2
        ldr r1, [sp, #+4]  <==
        add r1, r1, #1
        ldr r0, [sp, #+4]
        add r0, r0, #2

//===---------------------------------------------------------------------===//

LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-
effects:

R1 = X + 4
R2 = X + 7
R3 = X + 15

loop:
load [i + R1]
...
load [i + R2]
...
load [i + R3]

Suppose there is high register pressure, R1, R2, R3, can be spilled. We need
to implement proper re-materialization to handle this:

R1 = X + 4
R2 = X + 7
R3 = X + 15

loop:
R1 = X + 4  @ re-materialized
load [i + R1]
...
R2 = X + 7 @ re-materialized
load [i + R2]
...
R3 = X + 15 @ re-materialized
load [i + R3]

Furthermore, with re-association, we can enable sharing:

R1 = X + 4
R2 = X + 7
R3 = X + 15

loop:
T = i + X
load [T + 4]
...
load [T + 7]
...
load [T + 15]
//===---------------------------------------------------------------------===//

It's not always a good idea to choose rematerialization over spilling. If all
the load / store instructions would be folded then spilling is cheaper because
it won't require new live intervals / registers. See 2003-05-31-LongShifts for
an example.

//===---------------------------------------------------------------------===//

With a copying garbage collector, derived pointers must not be retained across
collector safe points; the collector could move the objects and invalidate the
derived pointer. This is bad enough in the first place, but safe points can
crop up unpredictably. Consider:

        %array = load { i32, [0 x %obj] }** %array_addr
        %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n
        %old = load %obj** %nth_el
        %z = div i64 %x, %y
        store %obj* %new, %obj** %nth_el

If the i64 division is lowered to a libcall, then a safe point will (must)
appear for the call site. If a collection occurs, %array and %nth_el no longer
point into the correct object.

The fix for this is to copy address calculations so that dependent pointers
are never live across safe point boundaries. But the loads cannot be copied
like this if there was an intervening store, so may be hard to get right.

Only a concurrent mutator can trigger a collection at the libcall safe point.
So single-threaded programs do not have this requirement, even with a copying
collector. Still, LLVM optimizations would probably undo a front-end's careful
work.

//===---------------------------------------------------------------------===//

The ocaml frametable structure supports liveness information. It would be good
to support it.

//===---------------------------------------------------------------------===//

The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be
revisited. The check is there to work around a misuse of directives in inline
assembly.

//===---------------------------------------------------------------------===//

It would be good to detect collector/target compatibility instead of silently
doing the wrong thing.

//===---------------------------------------------------------------------===//

It would be really nice to be able to write patterns in .td files for copies,
which would eliminate a bunch of explicit predicates on them (e.g. no side 
effects).  Once this is in place, it would be even better to have tblgen 
synthesize the various copy insertion/inspection methods in TargetInstrInfo.

//===---------------------------------------------------------------------===//

Stack coloring improvements:

1. Do proper LiveStackAnalysis on all stack objects including those which are
   not spill slots.
2. Reorder objects to fill in gaps between objects.
   e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4

//===---------------------------------------------------------------------===//

The scheduler should be able to sort nearby instructions by their address. For
example, in an expanded memset sequence it's not uncommon to see code like this:

  movl $0, 4(%rdi)
  movl $0, 8(%rdi)
  movl $0, 12(%rdi)
  movl $0, 0(%rdi)

Each of the stores is independent, and the scheduler is currently making an
arbitrary decision about the order.

//===---------------------------------------------------------------------===//

Another opportunitiy in this code is that the $0 could be moved to a register:

  movl $0, 4(%rdi)
  movl $0, 8(%rdi)
  movl $0, 12(%rdi)
  movl $0, 0(%rdi)

This would save substantial code size, especially for longer sequences like
this. It would be easy to have a rule telling isel to avoid matching MOV32mi
if the immediate has more than some fixed number of uses. It's more involved
to teach the register allocator how to do late folding to recover from
excessive register pressure.

Move to http://llvm.org/docs/LibFuzzer.html

Analysis Opportunities:

//===---------------------------------------------------------------------===//

In test/Transforms/LoopStrengthReduce/quadradic-exit-value.ll, the
ScalarEvolution expression for %r is this:

  {1,+,3,+,2}<loop>

Outside the loop, this could be evaluated simply as (%n * %n), however
ScalarEvolution currently evaluates it as

  (-2 + (2 * (trunc i65 (((zext i64 (-2 + %n) to i65) * (zext i64 (-1 + %n) to i65)) /u 2) to i64)) + (3 * %n))

In addition to being much more complicated, it involves i65 arithmetic,
which is very inefficient when expanded into code.

//===---------------------------------------------------------------------===//

In formatValue in test/CodeGen/X86/lsr-delayed-fold.ll,

ScalarEvolution is forming this expression:

((trunc i64 (-1 * %arg5) to i32) + (trunc i64 %arg5 to i32) + (-1 * (trunc i64 undef to i32)))

This could be folded to

(-1 * (trunc i64 undef to i32))

//===---------------------------------------------------------------------===//
Design Of lib/System
====================

The software in this directory is designed to completely shield LLVM from any
and all operating system specific functionality. It is not intended to be a
complete operating system wrapper (such as ACE), but only to provide the
functionality necessary to support LLVM.

The software located here, of necessity, has very specific and stringent design
rules. Violation of these rules means that cracks in the shield could form and
the primary goal of the library is defeated. By consistently using this library,
LLVM becomes more easily ported to new platforms since the only thing requiring
porting is this library.

Complete documentation for the library can be found in the file:
  llvm/docs/SystemLibrary.html
or at this URL:
  http://llvm.org/docs/SystemLibrary.html

While we recommend that you read the more detailed documentation, for the
impatient, here's a high level summary of the library's requirements.

 1. No system header files are to be exposed through the interface.
 2. Std C++ and Std C header files are okay to be exposed through the interface.
 3. No exposed system-specific functions.
 4. No exposed system-specific data.
 5. Data in lib/System classes must use only simple C++ intrinsic types.
 6. Errors are handled by returning "true" and setting an optional std::string
 7. Library must not throw any exceptions, period.
 8. Interface functions must not have throw() specifications.
 9. No duplicate function impementations are permitted within an operating
    system class.

To accomplish these requirements, the library has numerous design criteria that
must be satisfied. Here's a high level summary of the library's design criteria:

 1. No unused functionality (only what LLVM needs)
 2. High-Level Interfaces
 3. Use Opaque Classes
 4. Common Implementations
 5. Multiple Implementations
 6. Minimize Memory Allocation
 7. No Virtual Methods
llvm/lib/Support/Unix README
===========================

This directory provides implementations of the lib/System classes that
are common to two or more variants of UNIX. For example, the directory
structure underneath this directory could look like this:

Unix           - only code that is truly generic to all UNIX platforms
  Posix        - code that is specific to Posix variants of UNIX
  SUS          - code that is specific to the Single Unix Specification
  SysV         - code that is specific to System V variants of UNIX

As a rule, only those directories actually needing to be created should be
created. Also, further subdirectories could be created to reflect versions of
the various standards. For example, under SUS there could be v1, v2, and v3
subdirectories to reflect the three major versions of SUS.
LLVM Documentation
==================

LLVM's documentation is written in reStructuredText, a lightweight
plaintext markup language (file extension `.rst`). While the
reStructuredText documentation should be quite readable in source form, it
is mostly meant to be processed by the Sphinx documentation generation
system to create HTML pages which are hosted on <http://llvm.org/docs/> and
updated after every commit. Manpage output is also supported, see below.

If you instead would like to generate and view the HTML locally, install
Sphinx <http://sphinx-doc.org/> and then do:

    cd <build-dir>
    cmake -DLLVM_ENABLE_SPHINX=true -DSPHINX_OUTPUT_HTML=true <src-dir>
    make -j3 docs-llvm-html
    $BROWSER <build-dir>/docs//html/index.html

The mapping between reStructuredText files and generated documentation is
`docs/Foo.rst` <-> `<build-dir>/docs//html/Foo.html` <-> `http://llvm.org/docs/Foo.html`.

If you are interested in writing new documentation, you will want to read
`SphinxQuickstartTemplate.rst` which will get you writing documentation
very fast and includes examples of the most important reStructuredText
markup syntax.

Manpage Output
===============

Building the manpages is similar to building the HTML documentation. The
primary difference is to use the `man` makefile target, instead of the
default (which is `html`). Sphinx then produces the man pages in the
directory `<build-dir>/docs/man/`.

    cd <build-dir>
    cmake -DLLVM_ENABLE_SPHINX=true -DSPHINX_OUTPUT_MAN=true <src-dir>
    make -j3 docs-llvm-man
    man -l >build-dir>/docs/man/FileCheck.1

The correspondence between .rst files and man pages is
`docs/CommandGuide/Foo.rst` <-> `<build-dir>/docs//man/Foo.1`.
These .rst files are also included during HTML generation so they are also
viewable online (as noted above) at e.g.
`http://llvm.org/docs/CommandGuide/Foo.html`.

Checking links
==============

The reachability of external links in the documentation can be checked by
running:

    cd docs/
    make -f Makefile.sphinx linkcheck

Doxygen page Output
==============

Install doxygen <http://www.stack.nl/~dimitri/doxygen/download.html> and dot2tex <https://dot2tex.readthedocs.io/en/latest>.

    cd <build-dir>
    cmake -DLLVM_ENABLE_DOXYGEN=On <llvm-top-src-dir>
    make doxygen-llvm # for LLVM docs
    make doxygen-clang # for clang docs

It will generate html in
    
    <build-dir>/docs/doxygen/html # for LLVM docs
    <build-dir>/tools/clang/docs/doxygen/html # for clang docs
See docs/CMake.html for instructions on how to build LLVM with CMake.
HOWTO create an LLVM crosstool from x86_64/Linux to ARM/Linux
=============================================================

1. % llvm/utils/crosstool/create-snapshots.sh

   This will create llvm-[REV_L].tar.bz2 and llvm-gcc-4.2-[REV_G].tar.bz2,
   where:
     REV_L is the revision at which "llvm" was checked out, and
     REV_G is the revision at which "llvm-gcc-4.2" was checked out

   Note that REV_L might REV_G might not be the same revision.

2. Download CodeSourcery toolchain.  The exact location depends on your
   $CROSS_TARGET but the script will tell you what the location of the file is
   if you run it without having the file available.

   For example, if you're using $CROSS_TARGET == "arm-none-linux-gnueabi" then
   you need to download:

   http://www.codesourcery.com/sgpp/lite/arm/portal/package1787/public/arm-none-linux-gnueabi/arm-2007q3-51-arm-none-linux-gnueabi-i686-pc-linux-gnu.tar.bz2

   NOTE: simply changing $CROSS_TARGET and modifying the URL accordingly will
   not work -- you'll need to go to http://www.codesourcery.com and find the
   correct file, as the release number in the file will also be different (e.g.,
   in the file above, the release number is "51").

3. You can override most values in the script without modifying it, e.g.
   $INSTALL_ROOT (if you want to install in directory other than /usr/local).

   Run the script as:

   % env INSTALL_ROOT=[dir to install in] \
         CODE_SOURCERY_PKG_PATH=[dir where you downloaded CodeSourcery tarball] \
         LLVM_PKG_PATH=[dir where you stored your LLVM and LLVM-GCC snapshots] \
         LLVM_SVN_REV=${REV_L} \
         LLVMGCC_SVN_REV=${REV_G} \
         build-install-linux.sh
This directory contains a "bundle" for doing syntax highlighting of TableGen
files for the TextMate editor for OS X. The highlighting follows that done 
by the TextMate "C" bundle.  Currently, keywords, comments, and strings are 
highlighted.

To install this bundle, copy it to the per user area:
  cp -R utils/textmate/TableGen.tmbundle \
    ~/Library/Application\ Support/TextMate/Bundles/TableGen.tmbundle 
-*- llvm/utils/kate/README -*-

These are syntax highlighting files for the Kate editor. Included are:

* llvm.xml

  Syntax Highlighting Mode for the KDE Kate editor. To install just copy
  this file to ~/.kde/share/apps/katepart/syntax (or better yet, symlink it).

Note: If you notice missing or incorrect syntax highlighting, please contact
<llvm-bugs [at] lists.llvm.org>; if you wish to provide a patch to improve the
functionality, it will be most appreciated. Thank you.
-*- llvm/utils/emacs/README -*-

These are syntax highlighting files for the Emacs and XEmacs editors. Included
are:

* llvm-mode.el

  Syntax highlighting mode for LLVM assembly files. To use, add this code to
  your ~/.emacs :

  (setq load-path
    (cons (expand-file-name "path-to-llvm/utils/emacs") load-path))
  (require 'llvm-mode)
  
* tablegen-mode.el

  Syntax highlighting mode for TableGen description files. To use, add this code
  to your ~/.emacs:

  (setq load-path
    (cons (expand-file-name "path-to-llvm/utils/emacs") load-path))
  (require 'tablegen-mode)


Note: If you notice missing or incorrect syntax highlighting, please contact
<llvm-bugs [at] lists.llvm.org>; if you wish to provide a patch to improve the
functionality, it will be most appreciated. Thank you.
-*- llvm/utils/vim/README -*-

This directory contains settings for the vim editor to work on llvm *.ll and
tablegen *.td files. It comes with filetype detection rules in the (ftdetect),
syntax highlighting (syntax), some minimal sensible default settings (ftplugin)
and indentation plugins (indent).

To install copy all subdirectories to your $HOME/.vim or if you prefer create
symlinks to the files here. Do not copy the vimrc file here it is only meant as an inspiration and starting point for those working on llvm c++ code.

Note: If you notice missing or incorrect syntax highlighting, please contact
<llvm-bugs [at] lists.llvm.org>; if you wish to provide a patch to improve the
functionality, it will be most appreciated. Thank you.

If you find yourself working with LLVM Makefiles often, but you don't get syntax
highlighting (because the files have names such as Makefile.rules or
TEST.nightly.Makefile), add the following to your ~/.vimrc:

  " LLVM Makefile highlighting mode
  augroup filetype
    au! BufRead,BufNewFile *Makefile*     set filetype=make
  augroup END
==============================
 llvm-build - LLVM Build Tool
==============================

`llvm-build` is a tool for helping build the LLVM project.
This directory contains a "bundle" for doing syntax highlighting of TableGen
files for the Microsoft VSCode editor. The highlighting follows that done by
the TextMate "C" bundle as it is a translation of the textmate bundle to VSCode
using the "yo code" npm package. Currently, keywords, comments, and strings are
highlighted.

This colorizer was generate by the vscode-generator tool "Yo Code"
(https://github.com/Microsoft/vscode-generator-code) from the existing TableGen
text TableGen.tmLanguage syntax colorizer in utils/textmate. This README was
copied from utils/textmate/README.

To install this VSCode .td file colorizer, copy it to the following locations
per your Operating System:

  - Windows: %USERPROFILE%\.vscode\extensions
  - Mac: ~/.vscode/extensions
  - Linux: ~/.vscode/extensions

# tablegen README

This VSCode colorizer extension is a translation of the textmate bunble to
VSCode using the "yo code" npm package. Currently, keywords, comments, and
strings are highlighted.

To install this VSCode .td file colorizer, copy it to the following locations
per your Operating System:

  - Windows: %USERPROFILE%\.vscode\extensions
  - Mac: ~/.vscode/extensions
  - Linux: ~/.vscode/extensions

===============================
 lit - A Software Testing Tool
===============================

lit is a portable tool for executing LLVM and Clang style test suites,
summarizing their results, and providing indication of failures. lit is designed
to be a lightweight testing tool with as simple a user interface as possible.

=====================
 Contributing to lit
=====================

Please browse the Test Suite > lit category in LLVM's Bugzilla for ideas on
what to work on.

Before submitting patches, run the test suite to ensure nothing has regressed:

    # From within your LLVM source directory.
    utils/lit/lit.py \
        --path /path/to/your/llvm/build/bin \
        utils/lit/tests

Note that lit's tests depend on 'not' and 'FileCheck', LLVM utilities.
You will need to have built LLVM tools in order to run lit's test suite
successfully.

You'll also want to confirm that lit continues to work when testing LLVM.
Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the
regression test suite:

    make check-llvm

And be sure to run the llvm-lit wrapper script as well:

    /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests

Finally, make sure lit works when installed via setuptools:

    python utils/lit/setup.py install
    lit --path /path/to/your/llvm/build/bin utils/lit/tests

==============
 lit Examples
==============

This directory contains examples of 'lit' test suite configurations. The test
suites they define can be run with 'lit examples/example-name', for more details
see the README in each example.
========================
 Many Tests lit Example
========================

This directory contains a trivial lit test suite configuration that defines a
custom test format which just generates a large (N=10000) number of tests that
do a small amount of work in the Python test execution code.

This test suite is useful for testing the performance of lit on large numbers of
tests.
Utilities for the project that aren't intended to be part of a source
distribution.
LLVM notes
----------

This directory contains Google Test 1.8.0, with all elements removed except for
the actual source code, to minimize the addition to the LLVM distribution.

Cleaned up as follows:

# Remove all the unnecessary files and directories
$ rm -f CMakeLists.txt configure* Makefile* CHANGES CONTRIBUTORS README README.md .gitignore
$ rm -rf build-aux cmake codegear m4 make msvc samples scripts test xcode docs
$ rm -f `find . -name \*\.pump`
$ rm -f src/gtest_main.cc

# Put the license in the consistent place for LLVM.
$ mv LICENSE LICENSE.TXT

Modified as follows:
* Added support for NetBSD, Minix and Haiku.
* Added raw_os_ostream support to include/gtest/internal/custom/gtest-printers.h.
LLVM notes
----------

This directory contains the 'googlemock' component of Google Test 1.8.0, with
all elements removed except for the actual source code, to minimize the
addition to the LLVM distribution.

Cleaned up as follows:

# Remove all the unnecessary files and directories
$ rm -f CMakeLists.txt configure* Makefile* CHANGES CONTRIBUTORS README README.md .gitignore
$ rm -rf build-aux make msvc scripts test docs
$ rm -f `find . -name \*\.pump`
$ rm -f src/gmock_main.cc

# Put the license in the consistent place for LLVM.
$ mv LICENSE LICENSE.TXT
-*- llvm/utils/jedit/README -*-

These are syntax highlighting files for the jEdit editor. Included are:

* tablegen.xml

  Syntax highlighting mode for TableGen description files. To use, copy this
  file to ~/.jedit/modes/ and add this code to your ~/.jedit/modes/catalog:

  <MODE NAME="tablegen" FILE="tablegen.xml" FILE_NAME_GLOB="*.td" />

Note: If you notice missing or incorrect syntax highlighting, please contact
<llvm-bugs [at] lists.llvm.org>; if you wish to provide a patch to improve the
functionality, it will be most appreciated. Thank you.
See llvm/docs/Docker.rst for details
The LLVM Gold LTO Plugin
========================

This directory contains a plugin that is designed to work with binutils
gold linker. At present time, this is not the default linker in
binutils, and the default build of gold does not support plugins.

See docs/GoldPlugin.html for complete build and usage instructions.

NOTE: libLTO and LLVMgold aren't built without PIC because they would fail
to link on x86-64 with a relocation error: PIC and non-PIC can't be combined.
As an alternative to passing --enable-pic, you can use 'make ENABLE_PIC=1' in
your entire LLVM build.
//===----------------------------------------------------------------------===//
// C Language Family Front-end
//===----------------------------------------------------------------------===//

Welcome to Clang.  This is a compiler front-end for the C family of languages
(C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM
compiler infrastructure project.

Unlike many other compiler frontends, Clang is useful for a number of things
beyond just compiling code: we intend for Clang to be host to a number of
different source-level tools.  One example of this is the Clang Static Analyzer.

If you're interested in more (including how to build Clang) it is best to read
the relevant web sites.  Here are some pointers:

Information on Clang:              http://clang.llvm.org/
Building and using Clang:          http://clang.llvm.org/get_started.html
Clang Static Analyzer:             http://clang-analyzer.llvm.org/
Information on the LLVM project:   http://llvm.org/

If you have questions or comments about Clang, a great place to discuss them is
on the Clang development mailing list:
  http://lists.llvm.org/mailman/listinfo/cfe-dev

If you find a bug in Clang, please file it in the LLVM bug tracker:
  http://llvm.org/bugs/
This is an example of Clang based interpreter, for executing standalone C
programs.

It demonstrates the following features:
 1. Parsing standard compiler command line arguments using the Driver library.

 2. Constructing a Clang compiler instance, using the appropriate arguments
    derived in step #1.

 3. Invoking the Clang compiler to lex, parse, syntax check, and then generate
    LLVM code.

 4. Use the LLVM JIT functionality to execute the final module.

The implementation has many limitations and is not designed to be a full fledged
C interpreter. It is designed to demonstrate a simple but functional use of the
Clang compiler libraries.
This is a simple example demonstrating how to use clang's facility for
providing AST consumers using a plugin.

Build the plugin by running `make` in this directory.

Once the plugin is built, you can run it using:
--
Linux:
$ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.so -plugin print-fns some-input-file.c
$ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.so -plugin print-fns -plugin-arg-print-fns help -plugin-arg-print-fns --example-argument some-input-file.c
$ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.so -plugin print-fns -plugin-arg-print-fns -an-error some-input-file.c

Mac:
$ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.dylib -plugin print-fns some-input-file.c
$ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.dylib -plugin print-fns -plugin-arg-print-fns help -plugin-arg-print-fns --example-argument some-input-file.c
$ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.dylib -plugin print-fns -plugin-arg-print-fns -an-error some-input-file.c
These are tests for instrumentation based profiling.  This specifically means
the -fprofile-instr-generate and -fprofile-instr-use driver flags.

Tests in this directory should usually test both:

  - the generation of instrumentation (-fprofile-instr-generate), and
  - the use of profile data from instrumented runs (-fprofile-instr-use).

In order to test -fprofile-instr-use without actually running an instrumented
program, .profdata files are checked into Inputs/.

The input source files must include a main function such that building with
-fprofile-instr-generate and running the resulting program generates the same
.profdata file that is consumed by the tests for -fprofile-instr-use.  Even
tests that only check -fprofile-instr-use should include such a main function,
so that profile data can be regenerated as the .profdata file format evolves.
//===----------------------------------------------------------------------===//
// Clang Python Bindings
//===----------------------------------------------------------------------===//

This directory implements Python bindings for Clang.

You may need to alter LD_LIBRARY_PATH so that the Clang library can be
found. The unit tests are designed to be run with 'nosetests'. For example:
--
$ env PYTHONPATH=$(echo ~/llvm/tools/clang/bindings/python/) \
      LD_LIBRARY_PATH=$(llvm-config --libdir) \
  nosetests -v
tests.cindex.test_index.test_create ... ok
...

OK
--
IRgen optimization opportunities.

//===---------------------------------------------------------------------===//

The common pattern of
--
short x; // or char, etc
(x == 10)
--
generates an zext/sext of x which can easily be avoided.

//===---------------------------------------------------------------------===//

Bitfields accesses can be shifted to simplify masking and sign
extension. For example, if the bitfield width is 8 and it is
appropriately aligned then is is a lot shorter to just load the char
directly.

//===---------------------------------------------------------------------===//

It may be worth avoiding creation of alloca's for formal arguments
for the common situation where the argument is never written to or has
its address taken. The idea would be to begin generating code by using
the argument directly and if its address is taken or it is stored to
then generate the alloca and patch up the existing code.

In theory, the same optimization could be a win for block local
variables as long as the declaration dominates all statements in the
block.

NOTE: The main case we care about this for is for -O0 -g compile time
performance, and in that scenario we will need to emit the alloca
anyway currently to emit proper debug info. So this is blocked by
being able to emit debug information which refers to an LLVM
temporary, not an alloca.

//===---------------------------------------------------------------------===//

We should try and avoid generating basic blocks which only contain
jumps. At -O0, this penalizes us all the way from IRgen (malloc &
instruction overhead), all the way down through code generation and
assembly time.

On 176.gcc:expr.ll, it looks like over 12% of basic blocks are just
direct branches!

//===---------------------------------------------------------------------===//
//===----------------------------------------------------------------------===//
// Clang Static Analyzer
//===----------------------------------------------------------------------===//

= Library Structure =

The analyzer library has two layers: a (low-level) static analysis
engine (GRExprEngine.cpp and friends), and some static checkers
(*Checker.cpp).  The latter are built on top of the former via the
Checker and CheckerVisitor interfaces (Checker.h and
CheckerVisitor.h).  The Checker interface is designed to be minimal
and simple for checker writers, and attempts to isolate them from much
of the gore of the internal analysis engine.

= How It Works =

The analyzer is inspired by several foundational research papers ([1],
[2]).  (FIXME: kremenek to add more links)

In a nutshell, the analyzer is basically a source code simulator that
traces out possible paths of execution.  The state of the program
(values of variables and expressions) is encapsulated by the state
(ProgramState).  A location in the program is called a program point
(ProgramPoint), and the combination of state and program point is a
node in an exploded graph (ExplodedGraph).  The term "exploded" comes
from exploding the control-flow edges in the control-flow graph (CFG).

Conceptually the analyzer does a reachability analysis through the
ExplodedGraph.  We start at a root node, which has the entry program
point and initial state, and then simulate transitions by analyzing
individual expressions.  The analysis of an expression can cause the
state to change, resulting in a new node in the ExplodedGraph with an
updated program point and an updated state.  A bug is found by hitting
a node that satisfies some "bug condition" (basically a violation of a
checking invariant).

The analyzer traces out multiple paths by reasoning about branches and
then bifurcating the state: on the true branch the conditions of the
branch are assumed to be true and on the false branch the conditions
of the branch are assumed to be false.  Such "assumptions" create
constraints on the values of the program, and those constraints are
recorded in the ProgramState object (and are manipulated by the
ConstraintManager).  If assuming the conditions of a branch would
cause the constraints to be unsatisfiable, the branch is considered
infeasible and that path is not taken.  This is how we get
path-sensitivity.  We reduce exponential blow-up by caching nodes.  If
a new node with the same state and program point as an existing node
would get generated, the path "caches out" and we simply reuse the
existing node.  Thus the ExplodedGraph is not a DAG; it can contain
cycles as paths loop back onto each other and cache out.

ProgramState and ExplodedNodes are basically immutable once created.  Once
one creates a ProgramState, you need to create a new one to get a new
ProgramState.  This immutability is key since the ExplodedGraph represents
the behavior of the analyzed program from the entry point.  To
represent these efficiently, we use functional data structures (e.g.,
ImmutableMaps) which share data between instances.

Finally, individual Checkers work by also manipulating the analysis
state.  The analyzer engine talks to them via a visitor interface.
For example, the PreVisitCallExpr() method is called by GRExprEngine
to tell the Checker that we are about to analyze a CallExpr, and the
checker is asked to check for any preconditions that might not be
satisfied.  The checker can do nothing, or it can generate a new
ProgramState and ExplodedNode which contains updated checker state.  If it
finds a bug, it can tell the BugReporter object about the bug,
providing it an ExplodedNode which is the last node in the path that
triggered the problem.

= Notes about C++ =

Since now constructors are seen before the variable that is constructed 
in the CFG, we create a temporary object as the destination region that 
is constructed into. See ExprEngine::VisitCXXConstructExpr().

In ExprEngine::processCallExit(), we always bind the object region to the
evaluated CXXConstructExpr. Then in VisitDeclStmt(), we compute the
corresponding lazy compound value if the variable is not a reference, and
bind the variable region to the lazy compound value. If the variable
is a reference, just use the object region as the initilizer value.

Before entering a C++ method (or ctor/dtor), the 'this' region is bound
to the object region. In ctors, we synthesize 'this' region with  
CXXRecordDecl*, which means we do not use type qualifiers. In methods, we
synthesize 'this' region with CXXMethodDecl*, which has getThisType() 
taking type qualifiers into account. It does not matter we use qualified
'this' region in one method and unqualified 'this' region in another
method, because we only need to ensure the 'this' region is consistent 
when we synthesize it and create it directly from CXXThisExpr in a single
method call.

= Working on the Analyzer =

If you are interested in bringing up support for C++ expressions, the
best place to look is the visitation logic in GRExprEngine, which
handles the simulation of individual expressions.  There are plenty of
examples there of how other expressions are handled.

If you are interested in writing checkers, look at the Checker and
CheckerVisitor interfaces (Checker.h and CheckerVisitor.h).  Also look
at the files named *Checker.cpp for examples on how you can implement
these interfaces.

= Debugging the Analyzer =

There are some useful command-line options for debugging.  For example:

$ clang -cc1 -help | grep analyze
 -analyze-function <value>
 -analyzer-display-progress
 -analyzer-viz-egraph-graphviz
 ...

The first allows you to specify only analyzing a specific function.
The second prints to the console what function is being analyzed.  The
third generates a graphviz dot file of the ExplodedGraph.  This is
extremely useful when debugging the analyzer and viewing the
simulation results.

Of course, viewing the CFG (Control-Flow Graph) is also useful:

$ clang -cc1 -help | grep cfg
 -cfg-add-implicit-dtors Add C++ implicit destructors to CFGs for all analyses
 -cfg-add-initializers   Add C++ initializers to CFGs for all analyses
 -cfg-dump               Display Control-Flow Graphs
 -cfg-view               View Control-Flow Graphs using GraphViz
 -unoptimized-cfg        Generate unoptimized CFGs for all analyses

-cfg-dump dumps a textual representation of the CFG to the console,
and -cfg-view creates a GraphViz representation.

= References =

[1] Precise interprocedural dataflow analysis via graph reachability,
    T Reps, S Horwitz, and M Sagiv, POPL '95,
    http://portal.acm.org/citation.cfm?id=199462

[2] A memory model for static analysis of C programs, Z Xu, T
    Kremenek, and J Zhang, http://lcs.ios.ac.cn/~xzx/memmodel.pdf
See llvm/docs/README.txt
CMake Caches
============

This directory contains CMake cache scripts that pre-populate the CMakeCache in
a build directory with commonly used settings.

You can use the caches files with the following CMake invocation:

cmake -G <build system>
  -C <path to cache file>
  [additional CMake options (i.e. -DCMAKE_INSTALL_PREFIX=<install path>)]
  <path to llvm>

Options specified on the command line will override options in the cache files.

The following cache files exist.

Apple-stage1
------------

The Apple stage1 cache configures a two stage build similar to how Apple builds
the clang shipped with Xcode. The build files generated from this invocation has
a target named "stage2" which performs an LTO build of clang.

The Apple-stage2 cache can be used directly to match the build settings Apple
uses in shipping builds without doing a full bootstrap build.

PGO
---

The PGO CMake cache can be used to generate a multi-stage instrumented compiler.
You can configure your build directory with the following invocation of CMake:

cmake -G <generator> -C <path_to_clang>/cmake/caches/PGO.cmake <source dir>

After configuration the following additional targets will be generated:

stage2-instrumented:
Builds a stage1 x86 compiler, runtime, and required tools (llvm-config,
llvm-profdata) then uses that compiler to build an instrumented stage2 compiler.

stage2-instrumented-generate-profdata:
Depends on "stage2-instrumented" and will use the instrumented compiler to
generate profdata based on the training files in <clang>/utils/perf-training

stage2:
Depends on "stage2-instrumented-generate-profdata" and will use the stage1
compiler with the stage2 profdata to build a PGO-optimized compiler.

stage2-check-llvm:
Depends on stage2 and runs check-llvm using the stage3 compiler.

stage2-check-clang:
Depends on stage2 and runs check-clang using the stage3 compiler.

stage2-check-all:
Depends on stage2 and runs check-all using the stage3 compiler.

stage2-test-suite:
Depends on stage2 and runs the test-suite using the stage3 compiler (requires
in-tree test-suite).

3-stage
-------

This cache file can be used to generate a 3-stage clang build. You can configure
using the following CMake command:

cmake -C <path to clang>/cmake/caches/3-stage.cmake -G Ninja <path to llvm>

You can then run "ninja stage3-clang" to build stage1, stage2 and stage3 clangs.

This is useful for finding non-determinism the compiler by verifying that stage2
and stage3 are identical.
==========================
 Performance Training Data
==========================

This directory contains simple source files for use as training data for
generating PGO data and linker order files for clang.
This directory contains a VSPackage project to generate a Visual Studio extension
for clang-format.

Build prerequisites are:
- Visual Studio 2015
- Extensions SDK (you'll be prompted to install it if you open ClangFormat.sln)

The extension is built using CMake to generate the usual LLVM.sln by setting
the following CMake vars:

- BUILD_CLANG_FORMAT_VS_PLUGIN=ON

- NUGET_EXE_PATH=path/to/nuget_dir (unless nuget.exe is already available in PATH)

example:
  cd /d C:\code\llvm
  mkdir build & cd build
  cmake -DBUILD_CLANG_FORMAT_VS_PLUGIN=ON -DNUGET_EXE_PATH=C:\nuget ..

Once LLVM.sln is generated, build the clang_format_vsix target, which will build
ClangFormat.sln, the C# extension application.

The CMake build will copy clang-format.exe and LICENSE.TXT into the ClangFormat/
directory so they can be bundled with the plug-in, as well as creating
ClangFormat/source.extension.vsixmanifest. Once the plug-in has been built with
CMake once, it can be built manually from the ClangFormat.sln solution in Visual
Studio.

===========
 Debugging
===========

Once you've built the clang_format_vsix project from LLVM.sln at least once,
open ClangFormat.sln in Visual Studio, then:

- Make sure the "Debug" target is selected
- Open the ClangFormat project properties
- Select the Debug tab
- Set "Start external program:" to where your devenv.exe is installed. Typically
  it's "C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\devenv.exe"
- Set "Command line arguments" to: /rootsuffix Exp
- You can now set breakpoints if you like
- Press F5 to build and run with debugger

If all goes well, a new instance of Visual Studio will be launched in a special
mode where it uses the experimental hive instead of the normal configuration hive.
By default, when you build a VSIX project in Visual Studio, it auto-registers the
extension in the experimental hive, allowing you to test it. In the new Visual Studio
instance, open or create a C++ solution, and you should now see the Clang Format
entries in the Tool menu. You can test it out, and any breakpoints you set will be
hit where you can debug as usual.
scan-build
==========

A package designed to wrap a build so that all calls to gcc/clang are
intercepted and logged into a [compilation database][1] and/or piped to
the clang static analyzer. Includes intercept-build tool, which logs
the build, as well as scan-build tool, which logs the build and runs
the clang static analyzer on it.

Portability
-----------

Should be working on UNIX operating systems.

- It has been tested on FreeBSD, GNU/Linux and OS X.
- Prepared to work on windows, but need help to make it.


Prerequisites
-------------

1. **python** interpreter (version 2.7, 3.2, 3.3, 3.4, 3.5).


How to use
----------

To run the Clang static analyzer against a project goes like this:

    $ scan-build <your build command>

To generate a compilation database file goes like this:

    $ intercept-build <your build command>

To run the Clang static analyzer against a project with compilation database
goes like this:

    $ analyze-build

Use `--help` to know more about the commands.


Limitations
-----------

Generally speaking, the `intercept-build` and `analyze-build` tools together
does the same job as `scan-build` does. So, you can expect the same output
from this line as simple `scan-build` would do:

    $ intercept-build <your build command> && analyze-build

The major difference is how and when the analyzer is run. The `scan-build`
tool has three distinct model to run the analyzer:

1.  Use compiler wrappers to make actions.
    The compiler wrappers does run the real compiler and the analyzer.
    This is the default behaviour, can be enforced with `--override-compiler`
    flag.

2.  Use special library to intercept compiler calls durring the build process.
    The analyzer run against each modules after the build finished.
    Use `--intercept-first` flag to get this model.

3.  Use compiler wrappers to intercept compiler calls durring the build process.
    The analyzer run against each modules after the build finished.
    Use `--intercept-first` and `--override-compiler` flags together to get
    this model.

The 1. and 3. are using compiler wrappers, which works only if the build
process respects the `CC` and `CXX` environment variables. (Some build
process can override these variable as command line parameter only. This case
you need to pass the compiler wrappers manually. eg.: `intercept-build
--override-compiler make CC=intercept-cc CXX=intercept-c++ all` where the
original build command would have been `make all` only.)

The 1. runs the analyzer right after the real compilation. So, if the build
process removes removes intermediate modules (generated sources) the analyzer
output still kept.

The 2. and 3. generate the compilation database first, and filters out those
modules which are not exists. So, it's suitable for incremental analysis durring
the development.

The 2. mode is available only on FreeBSD and Linux. Where library preload
is available from the dynamic loader. Not supported on OS X (unless System
Integrity Protection feature is turned off).

`intercept-build` command uses only the 2. and 3. mode to generate the
compilation database. `analyze-build` does only run the analyzer against the
captured compiler calls.


Known problems
--------------

Because it uses `LD_PRELOAD` or `DYLD_INSERT_LIBRARIES` environment variables,
it does not append to it, but overrides it. So builds which are using these
variables might not work. (I don't know any build tool which does that, but
please let me know if you do.)


Problem reports
---------------

If you find a bug in this documentation or elsewhere in the program or would
like to propose an improvement, please use the project's [issue tracker][3].
Please describing the bug and where you found it. If you have a suggestion
how to fix it, include that as well. Patches are also welcome.


License
-------

The project is licensed under University of Illinois/NCSA Open Source License.
See LICENSE.TXT for details.

  [1]: http://clang.llvm.org/docs/JSONCompilationDatabase.html
  [2]: https://pypi.python.org/pypi/scan-build
  [3]: https://llvm.org/bugs/enter_bug.cgi?product=clang
In order for libpcap to be able to capture packets on a Linux system,
the "packet" protocol must be supported by your kernel.  If it is not,
you may get error messages such as

	modprobe: can't locate module net-pf-17

in "/var/adm/messages", or may get messages such as

	socket: Address family not supported by protocol

from applications using libpcap.

You must configure the kernel with the CONFIG_PACKET option for this
protocol; the following note is from the Linux "Configure.help" file for
the 2.0[.x] kernel:

	Packet socket
	CONFIG_PACKET
	  The Packet protocol is used by applications which communicate
	  directly with network devices without an intermediate network
	  protocol implemented in the kernel, e.g. tcpdump. If you want them
	  to work, choose Y.

	  This driver is also available as a module called af_packet.o ( =
	  code which can be inserted in and removed from the running kernel
	  whenever you want). If you want to compile it as a module, say M
	  here and read Documentation/modules.txt; if you use modprobe or
	  kmod, you may also want to add "alias net-pf-17 af_packet" to
	  /etc/modules.conf.

and the note for the 2.2[.x] kernel says:

	Packet socket
	CONFIG_PACKET
	  The Packet protocol is used by applications which communicate
	  directly with network devices without an intermediate network
	  protocol implemented in the kernel, e.g. tcpdump. If you want them
	  to work, choose Y. This driver is also available as a module called
	  af_packet.o ( = code which can be inserted in and removed from the
	  running kernel whenever you want). If you want to compile it as a
	  module, say M here and read Documentation/modules.txt.  You will
	  need to add 'alias net-pf-17 af_packet' to your /etc/conf.modules
	  file for the module version to function automatically.  If unsure,
	  say Y.

In addition, there is an option that, in 2.2 and later kernels, will
allow packet capture filters specified to programs such as tcpdump to be
executed in the kernel, so that packets that don't pass the filter won't
be copied from the kernel to the program, rather than having all packets
copied to the program and libpcap doing the filtering in user mode.

Copying packets from the kernel to the program consumes a significant
amount of CPU, so filtering in the kernel can reduce the overhead of
capturing packets if a filter has been specified that discards a
significant number of packets.  (If no filter is specified, it makes no
difference whether the filtering isn't performed in the kernel or isn't
performed in user mode. :-))

The option for this is the CONFIG_FILTER option; the "Configure.help"
file says:

	Socket filtering
	CONFIG_FILTER
	  The Linux Socket Filter is derived from the Berkeley Packet Filter.
	  If you say Y here, user-space programs can attach a filter to any
	  socket and thereby tell the kernel that it should allow or disallow
	  certain types of data to get through the socket. Linux Socket
	  Filtering works on all socket types except TCP for now. See the text
	  file linux/Documentation/networking/filter.txt for more information.
	  If unsure, say N.

Note that, by default, libpcap will, if libnl is present, build with it;
it uses libnl to support monitor mode on mac80211 devices.  There is a
configuration option to disable building with libnl, but, if that option
is chosen, the monitor-mode APIs (as used by tcpdump's "-I" flag, and as
will probably be used by other applications in the future) won't work
properly on mac80211 devices.

Linux's run-time linker allows shared libraries to be linked with other
shared libraries, which means that if an older version of a shared
library doesn't require routines from some other shared library, and a
later version of the shared library does require those routines, the
later version of the shared library can be linked with that other shared
library and, if it's otherwise binary-compatible with the older version,
can replace that older version without breaking applications built with
the older version, and without breaking configure scripts or the build
procedure for applications whose configure script doesn't use the
pcap-config script if they build with the shared library.  (The build
procedure for applications whose configure scripts use the pcap-config
script if present will not break even if they build with the static
library.)

Statistics:
Statistics reported by pcap are platform specific.  The statistics
reported by pcap_stats on Linux are as follows:

2.2.x
=====
ps_recv   Number of packets that were accepted by the pcap filter
ps_drop   Always 0, this statistic is not gatherd on this platform

2.4.x
=====
ps_recv   Number of packets that were accepted by the pcap filter
ps_drop   Number of packets that had passed filtering but were not
          passed on to pcap due to things like buffer shortage, etc.
          This is useful because these are packets you are interested in
          but won't be reported by, for example, tcpdump output.
Using BPF:

(1) AIX 4.x's version of BPF is undocumented and somewhat unstandard; the
    current BPF support code includes changes that should work around
    that; it appears to compile and work on at least one AIX 4.3.3
    machine.

    Note that the BPF driver and the "/dev/bpf" devices might not exist
    on your machine; AIX's tcpdump loads the driver and creates the
    devices if they don't already exist.  Our libpcap should do the
    same, and the configure script should detect that it's on an AIX
    system and choose BPF even if the devices aren't there.

    Also note that tcpdump _binary_ compiled on AIX 4 may have a problem
    doing the initial loading of the BPF driver if copied to AIX 5 and
    run there (GH #52). tcpdump binary natively compiled on AIX 5 should
    not have this issue.

(2) If libpcap doesn't compile on your machine when configured to use
    BPF, or if the workarounds fail to make it work correctly, you
    should send to tcpdump-workers@lists.tcpdump.org a detailed bug
    report (if the compile fails, send us the compile error messages;
    if it compiles but fails to work correctly, send us as detailed as
    possible a description of the symptoms, including indications of the
    network link-layer type being wrong or time stamps being wrong).

    If you fix the problems yourself, please submit a patch by forking
    the branch at

	https://github.com/the-tcpdump-group/libpcap/issues

    and issuing a pull request, so we can incorporate the fixes into the
    next release.

    If you don't fix the problems yourself, you can, as a workaround,
    make libpcap use DLPI instead of BPF.

    This can be done by specifying the flag:

       --with-pcap=dlpi

    to the "configure" script for libpcap.

If you use DLPI:

(1) It is a good idea to have the latest version of the DLPI driver on
    your system, since certain versions may be buggy and cause your AIX
    system to crash.  DLPI is included in the fileset bos.rte.tty.  I
    found that the DLPI driver that came with AIX 4.3.2 was buggy, and
    had to upgrade to bos.rte.tty 4.3.2.4:

	    lslpp -l bos.rte.tty

	    bos.rte.tty     4.3.2.4  COMMITTED  Base TTY Support and Commands

    Updates for AIX filesets can be obtained from:
    ftp://service.software.ibm.com/aix/fixes/

    These updates can be installed with the smit program.

(2) After compiling libpcap, you need to make sure that the DLPI driver
    is loaded.  Type:

	    strload -q -d dlpi

    If the result is:

	    dlpi: yes

    then the DLPI driver is loaded correctly.

    If it is:

	    dlpi: no

    Then you need to type:

	    strload -f /etc/dlpi.conf

    Check again with strload -q -d dlpi that the dlpi driver is loaded.

    Alternatively, you can uncomment the lines for DLPI in
    /etc/pse.conf and reboot the machine; this way DLPI will always
    be loaded when you boot your system.

(3) There appears to be a problem in the DLPI code in some versions of
    AIX, causing a warning about DL_PROMISC_MULTI failing; this might
    be responsible for DLPI not being able to capture outgoing packets.
The following instructions apply if you have a Linux platform and want
libpcap to support the Septel range of passive network monitoring cards
from Intel (http://www.intel.com)

1) Install and build the Septel software distribution by following the
instructions supplied with that package.

2) Configure libcap. To allow the 'configure' script to locate the Septel
software distribution use the '--with-septel' option:

        ./configure --with-septel=DIR

where DIR is the root of the Septel software distribution, for example
/var/src/septel.

By default (if you write only ./configure --with-septel) it takes
./../septel as argument for DIR.

If the Septel software is correctly detected 'configure' will
report:

        checking whether we have Septel API... yes

If 'configure' reports that there is no Septel API, the directory may have been
incorrectly specified or the Septel software was not built before configuring
libpcap.

See also the libpcap INSTALL.txt file for further libpcap configuration
options.

Building libpcap at this stage will include support for both the native
packet capture stream and for capturing from Septel cards.  To build
libpcap with only Septel support specify the capture type as 'septel'
when configuring libpcap:

        ./configure --with-septel=DIR --with-pcap=septel

Applications built with libpcap configured in this way will only detect Septel
cards and will not capture from the native OS packet stream.

Note: As mentioned in pcap-septel.c we should first edit the system.txt
file to change the user part example (UPE) module id to 0xdd instead of
0x2d for technical reason.  So this change in system.txt is crutial and
things will go wrong if it's not done.  System.txt along with config.txt
are configuration files that are edited by the user before running the
gctload program that uses these files for initialising modules and
configuring parameters.

----------------------------------------------------------------------
for more information please contact me : gil_hoyek@hotmail.com
As with other systems using BPF, Mac OS X allows users with read access
to the BPF devices to capture packets with libpcap and allows users with
write access to the BPF devices to send packets with libpcap.

On some systems that use BPF, the BPF devices live on the root file
system, and the permissions and/or ownership on those devices can be
changed to give users other than root permission to read or write those
devices.

On newer versions of FreeBSD, the BPF devices live on devfs, and devfs
can be configured to set the permissions and/or ownership of those
devices to give users other than root permission to read or write those
devices.

On Mac OS X, the BPF devices live on devfs, but the OS X version of
devfs is based on an older (non-default) FreeBSD devfs, and that version
of devfs cannot be configured to set the permissions and/or ownership of
those devices.

Therefore, we supply:

	a "startup item" for older versions of Mac OS X;

	a launchd daemon for Tiger and later versions of Mac OS X;

Both of them will change the ownership of the BPF devices so that the
"admin" group owns them, and will change the permission of the BPF
devices to rw-rw----, so that all users in the "admin" group - i.e., all
users with "Allow user to administer this computer" turned on - have
both read and write access to them.

The startup item is in the ChmodBPF directory in the source tree.  A
/Library/StartupItems directory should be created if it doesn't already
exist, and the ChmodBPF directory should be copied to the
/Library/StartupItems directory (copy the entire directory, so that
there's a /Library/StartupItems/ChmodBPF directory, containing all the
files in the source tree's ChmodBPF directory; don't copy the individual
items in that directory to /Library/StartupItems).  The ChmodBPF
directory, and all files under it, must be owned by root.  Installing
the files won't immediately cause the startup item to be executed; it
will be executed on the next reboot.  To change the permissions before
the reboot, run

	sudo SystemStarter start ChmodBPF

The launchd daemon is the chmod_bpf script, plus the
org.tcpdump.chmod_bpf.plist launchd plist file.  chmod_bpf should be
installed in /usr/local/bin/chmod_bpf, and org.tcpdump.chmod_bpf.plist
should be installed in /Library/LaunchDaemons.  chmod_bpf, and
org.tcpdump.chmod_bpf.plist, must be owned by root.  Installing the
script and plist file won't immediately cause the script to be executed;
it will be executed on the next reboot.  To change the permissions
before the reboot, run

	sudo /usr/local/bin/chmod_bpf

or

	sudo launchctl load /Library/LaunchDaemons/org.tcpdump.chmod_bpf.plist

If you want to give a particular user permission to access the BPF
devices, rather than giving all administrative users permission to
access them, you can have the ChmodBPF/ChmodBPF script change the
ownership of /dev/bpf* without changing the permissions.  If you want to
give a particular user permission to read and write the BPF devices and
give the administrative users permission to read but not write the BPF
devices, you can have the script change the owner to that user, the
group to "admin", and the permissions to rw-r-----.  Other possibilities
are left as an exercise for the reader.

(NOTE: due to a bug in Snow Leopard, if you change the permissions not
to grant write permission to everybody who should be allowed to capture
traffic, non-root users who cannot open the BPF devices for writing will
not be able to capture outgoing packets.)
For HP-UX 11i (11.11) and later, there are no known issues with
promiscuous mode under HP-UX.  If you are using a earlier version of
HP-UX and cannot upgrade, please continue reading.

HP-UX patches to fix packet capture problems

Note that packet-capture programs such as tcpdump may, on HP-UX, not be
able to see packets sent from the machine on which they're running.
Some articles on groups.google.com discussing this are:

	http://groups.google.com/groups?selm=82ld3v%2480i%241%40mamenchi.zrz.TU-Berlin.DE

which says:

  Newsgroups: comp.sys.hp.hpux
  Subject:  Re: Did someone made tcpdump working on 10.20 ?
  Date: 12/08/1999
  From: Lutz Jaenicke <jaenicke@emserv1.ee.TU-Berlin.DE>

  In article <82ks5i$5vc$1@news1.dti.ne.jp>, mtsat <mtsat@iris.dti.ne.jp>
  wrote:
   >Hello,
   >
   >I downloaded and compiled tcpdump3.4 a couple of week ago. I tried to use
   >it, but I can only see incoming data, never outgoing.
   >Someone (raj) explained me that a patch was missing, and that this patch
   >must me "patched" (poked) in order to see outbound data in promiscuous mode.
   >Many things to do .... So the question is : did someone has already this
   >"ready to use" PHNE_**** patch ?

   Two things:
   1. You do need a late "LAN products cumulative patch" (e.g.  PHNE_18173
  for   s700/10.20).
   2. You must use
echo 'lanc_outbound_promisc_flag/W1' | /usr/bin/adb -w /stand/vmunix /dev/kmem
     You can insert this e.g. into /sbin/init.d/lan

   Best regards,
   Lutz

and

	http://groups.google.com/groups?selm=88cf4t%24p03%241%40web1.cup.hp.com

which says:

  Newsgroups: comp.sys.hp.hpux
  Subject: Re: tcpdump only shows incoming packets
  Date: 02/15/2000
  From: Rick Jones <foo@bar.baz.invalid>

  Harald Skotnes <harald@cc.uit.no> wrote:
  > I am running HPUX 11.0 on a C200 hanging on a 100Mb switch. I have
  > compiled libpcap-0.4 an tcpdump-3.4 and it seems to work. But at a
  > closer look I only get to see the incoming packets not the
  > outgoing. I have tried tcpflow-0.12 which also uses libpcap and the
  > same thing happens.  Could someone please give me a hint on how to
  > get this right?

  Search/Read the archives ?-)

  What you are seeing is expected, un-patched, behaviour for an HP-UX
  system.  On 11.00, you need to install the latest lancommon/DLPI
  patches, and then the latest driver patch for the interface(s) in use.
  At that point, a miracle happens and you should start seeing outbound
  traffic.

[That article also mentions the patch that appears below.]

and

	http://groups.google.com/groups?selm=38AA973E.96BE7DF7%40cc.uit.no

which says:

  Newsgroups: comp.sys.hp.hpux
  Subject: Re: tcpdump only shows incoming packets
  Date: 02/16/2000
  From: Harald Skotnes <harald@cc.uit.no>

  Rick Jones wrote:

	...

  > What you are seeing is expected, un-patched, behaviour for an HP-UX
  > system. On 11.00, you need to install the latest lancommon/DLPI
  > patches, and then the latest driver patch for the interface(s) in
  > use. At that point, a miracle happens and you should start seeing
  > outbound traffic.

  Thanks a lot.  I have this problem on several machines running HPUX
  10.20 and 11.00.  The machines where patched up before y2k so did not
  know what to think.  Anyway I have now installed PHNE_19766,
  PHNE_19826, PHNE_20008, PHNE_20735 on the C200 and now I can see the
  outbound traffic too.  Thanks again.

(although those patches may not be the ones to install - there may be
later patches).

And another message to tcpdump-workers@tcpdump.org, from Rick Jones:

  Date: Mon, 29 Apr 2002 15:59:55 -0700
  From: Rick Jones
  To: tcpdump-workers@tcpdump.org
  Subject: Re: [tcpdump-workers] I Can't Capture the Outbound Traffic

	...

  http://itrc.hp.com/ would be one place to start in a search for the most
  up-to-date patches for DLPI and the lan driver(s) used on your system (I
  cannot guess because 9000/800 is too generic - one hs to use the "model"
  command these days and/or an ioscan command (see manpage) to guess what
  the drivers (btlan[3456], gelan, etc) might be involved in addition to
  DLPI.

  Another option is to upgrade to 11i as outbound promiscuous mode support
  is there in the base OS, no patches required.

Another posting:

	http://groups.google.com/groups?selm=7d6gvn%24b3%241%40ocean.cup.hp.com

indicates that you need to install the optional STREAMS product to do
captures on HP-UX 9.x:

  Newsgroups: comp.sys.hp.hpux
  Subject:  Re: tcpdump HP/UX 9.x
  Date: 03/22/1999
  From: Rick Jones <foo@bar.baz>

  Dave Barr (barr@cis.ohio-state.edu) wrote:
  : Has anyone ported tcpdump (or something similar) to HP/UX 9.x?

  I'm reasonably confident that any port of tcpdump to 9.X would require
  the (then optional) STREAMS product.  This would bring DLPI, which is
  what one uses to access interfaces in promiscuous mode.

  I'm not sure that HP even sells the 9.X STREAMS product any longer,
  since HP-UX 9.X is off the pricelist (well, maybe 9.10 for the old 68K
  devices).

  Your best bet is to be up on 10.20 or better if that is at all
  possible.  If your hardware is supported by it, I'd go with HP-UX 11.
  If you want to see the system's own outbound traffic, you'll never get
  that functionality on 9.X, but it might happen at some point for 10.20
  and 11.X.

  rick jones

(as per other messages cited here, the ability to see the system's own
outbound traffic did happen).

Rick Jones reports that HP-UX 11i needs no patches for outbound
promiscuous mode support.

An additional note, from Jost Martin, for HP-UX 10.20:

	Q: How do I get ethereral on HPUX to capture the _outgoing_ packets
	   of an interface
	A: You need to get PHNE_20892,PHNE_20725 and PHCO_10947 (or
	   newer, this is as of 4.4.00) and its dependencies.  Then you can
	   enable the feature as descibed below:

	Patch Name: PHNE_20892
	Patch Description: s700 10.20 PCI 100Base-T cumulative patch
		To trace the outbound packets, please do the following
		to turn on a global promiscuous switch before running
		the promiscuous applications like snoop or tcpdump:

		adb -w /stand/vmunix /dev/mem
		lanc_outbound_promisc_flag/W 1
		(adb will echo the result showing that the flag has
		been changed)
		$quit
	(Thanks for this part to HP-support, Ratingen)

		The attached hack does this and some security-related stuff
	(thanks to hildeb@www.stahl.bau.tu-bs.de (Ralf Hildebrandt) who
	posted the security-part some time ago)

		 <<hack_ip_stack>>

		(Don't switch IP-forwarding off, if you need it !)
		Install the hack as /sbin/init.d/hacl_ip_stack (adjust
	permissions !) and make a sequencing-symlink
	/sbin/rc2.d/S350hack_ip_stack pointing to this script.
		Now all this is done on every reboot.

According to Rick Jones, the global promiscuous switch also has to be
turned on for HP-UX 11.00, but not for 11i - and, in fact, the switch
doesn't even exist on 11i.

Here's the "hack_ip_stack" script:

-----------------------------------Cut Here-------------------------------------
#!/sbin/sh
#
# nettune:  hack kernel parms for safety

OKAY=0
ERROR=-1

# /usr/contrib/bin fuer nettune auf Pfad
PATH=/sbin:/usr/sbin:/usr/bin:/usr/contrib/bin
export PATH


##########
#  main  #
##########

case $1 in
   start_msg)
      print "Tune IP-Stack for security"
      exit $OKAY
      ;;

   stop_msg)
      print "This action is not applicable"
      exit $OKAY
      ;;

   stop)
      exit $OKAY
      ;;

   start)
      ;;  # fall through

   *)
      print "USAGE: $0 {start_msg | stop_msg | start | stop}" >&2
      exit $ERROR
      ;;
   esac

###########
#  start  #
###########

#
# tcp-Sequence-Numbers nicht mehr inkrementieren sondern random
# Syn-Flood-Protection an
# ip_forwarding aus
# Source-Routing aus
# Ausgehende Packets an ethereal/tcpdump etc.

/usr/contrib/bin/nettune -s tcp_random_seq 2 || exit $ERROR
/usr/contrib/bin/nettune -s hp_syn_protect 1 || exit $ERROR
/usr/contrib/bin/nettune -s ip_forwarding 0 || exit $ERROR
echo 'ip_block_source_routed/W1' | /usr/bin/adb -w /stand/vmunix /dev/kmem || exit $ERROR
echo 'lanc_outbound_promisc_flag/W 1' | adb -w /stand/vmunix /dev/mem  || exit $ERROR

exit $OKAY
-----------------------------------Cut Here-------------------------------------
Under Win32, libpcap is integrated in the WinPcap packet capture system.
WinPcap provides a framework that allows libpcap to capture the packets
under Windows 95, Windows 98, Windows ME, Windows NT 4, Windows 2000
and Windows XP.
WinPcap binaries and source code can be found at http://winpcap.polito.it:
they include also a developer's pack with all the necessary to compile
libpcap-based applications under Windows.

How to compile libpcap with Visual Studio
-----------------------------------------

In order to compile libpcap you will need:

- version 6 (or higher) of Microsoft Visual Studio
- The November 2001 (or later) edition of Microsoft Platform
Software Development Kit (SDK), that contains some necessary includes
for IPv6 support. You can download it from http://www.microsoft.com/sdk
- the latest WinPcap sources from http://winpcap.polito.it/install

The WinPcap source code already contains a recent (usually the latest
stable) version of libpcap. If you need to compile a different one,
simply download it from www.tcpdump.org and copy the sources in the
winpcap\wpcap\libpcap folder of the WinPcap distribution. If you want to
compile a libpcap source retrieved from the tcpdump.org Git, you will
have to create the scanner and the grammar by hand (with lex and yacc)
or with the cygnus makefile, since The Visual Studio project is not able
to build them.

Open the project file winpcap\wpcap\prj\wpcap.dsw with Visual Studio and
build wpcap.dll. wpcap.lib, the library file to link with the applications,
will be generated in winpcap\wpcap\lib\. wpcap.dll will be generated in
winpcap\wpcap\prj\release or winpcap\wpcap\prj\debug depending on the type
of binary that is being created.

How to compile libpcap with Cygnus
----------------------------------

To build wpcap.dll, cd to the directory WPCAP/PRJ of the WinPcap source code
distribution and type "make". libwpcap.a, the library file to link with the
applications, will be generated in winpcap\wpcap\lib\. wpcap.dll will be
generated in winpcap\wpcap\prj.

Remember, you CANNOT use the MSVC-generated .lib files with gcc, use
libwpcap.a instead.

"make install" installs wpcap.dll in the Windows system folder.
The following instructions are applicable to Tru64 UNIX
(formerly Digital UNIX (formerly DEC OSF/1)) version 4.0, and
probably to later versions as well; at least some options apply to
Digital UNIX 3.2 - perhaps all do.

In order to use kernel packet filtering on this system, you have
to configure it in such a way:

Kernel configuration
--------------------

The packet filtering kernel option must be enabled at kernel
installation.  If it was not the case, you can rebuild the kernel with
"doconfig -c" after adding the following line in the kernel
configuration file (/sys/conf/<HOSTNAME>):

	option PACKETFILTER

or use "doconfig" without any arguments to add the packet filter driver
option via the kernel option menu (see the system administration
documentation for information on how to do this).

Device configuration
--------------------

Devices used for packet filtering must be created thanks to
the following command (executed in the /dev directory):

	./MAKEDEV pfilt

Interface configuration
-----------------------

In order to capture all packets on a network, you may want to allow
applications to put the interface on that network into "local copy"
mode, so that tcpdump can see packets sent by the host on which it's
running as well as packets received by that host, and to put the
interface into "promiscuous" mode, so that tcpdump can see packets on
the network segment not sent to the host on which it's running, by using
the pfconfig(1) command:

	pfconfig +c +p <network_device>

or allow application to put any interface into "local copy" or
"promiscuous" mode by using the command:

	pfconfig +c +p -a

Note: all instructions given require root privileges.
LIBPCAP 1.x.y

www.tcpdump.org

Please send inquiries/comments/reports to:
	tcpdump-workers@lists.tcpdump.org

Anonymous Git is available via:
	git clone git://bpf.tcpdump.org/libpcap

Please submit patches by forking the branch on GitHub at

	http://github.com/the-tcpdump-group/libpcap/tree/master

and issuing a pull request.

formerly from 	Lawrence Berkeley National Laboratory
		Network Research Group <libpcap@ee.lbl.gov>
		ftp://ftp.ee.lbl.gov/old/libpcap-0.4a7.tar.Z

This directory contains source code for libpcap, a system-independent
interface for user-level packet capture.  libpcap provides a portable
framework for low-level network monitoring.  Applications include
network statistics collection, security monitoring, network debugging,
etc.  Since almost every system vendor provides a different interface
for packet capture, and since we've developed several tools that
require this functionality, we've created this system-independent API
to ease in porting and to alleviate the need for several
system-dependent packet capture modules in each application.

For some platforms there are README.{system} files that discuss issues
with the OS's interface for packet capture on those platforms, such as
how to enable support for that interface in the OS, if it's not built in
by default.

The libpcap interface supports a filtering mechanism based on the
architecture in the BSD packet filter.  BPF is described in the 1993
Winter Usenix paper ``The BSD Packet Filter: A New Architecture for
User-level Packet Capture''.  A compressed PostScript version can be
found at

	ftp://ftp.ee.lbl.gov/papers/bpf-usenix93.ps.Z

or

	http://www.tcpdump.org/papers/bpf-usenix93.ps.Z

and a gzipped version can be found at

	http://www.tcpdump.org/papers/bpf-usenix93.ps.gz

A PDF version can be found at

	http://www.tcpdump.org/papers/bpf-usenix93.pdf

Although most packet capture interfaces support in-kernel filtering,
libpcap utilizes in-kernel filtering only for the BPF interface.
On systems that don't have BPF, all packets are read into user-space
and the BPF filters are evaluated in the libpcap library, incurring
added overhead (especially, for selective filters).  Ideally, libpcap
would translate BPF filters into a filter program that is compatible
with the underlying kernel subsystem, but this is not yet implemented.

BPF is standard in 4.4BSD, BSD/OS, NetBSD, FreeBSD, OpenBSD, DragonFly
BSD, and Mac OS X; an older, modified and undocumented version is
standard in AIX.  {DEC OSF/1, Digital UNIX, Tru64 UNIX} uses the
packetfilter interface but has been extended to accept BPF filters
(which libpcap utilizes).  Also, you can add BPF filter support to
Ultrix using the kernel source and/or object patches available in:

	http://www.tcpdump.org/other/bpfext42.tar.Z

Linux, in the 2.2 kernel and later kernels, has a "Socket Filter"
mechanism that accepts BPF filters; see the README.linux file for
information on configuring that option.

Note to Linux distributions and *BSD systems that include libpcap:

There's now a rule to make a shared library, which should work on Linux
and *BSD, among other platforms.

It sets the soname of the library to "libpcap.so.1"; this is what it
should be, *NOT* libpcap.so.1.x or libpcap.so.1.x.y or something such as
that.

We've been maintaining binary compatibility between libpcap releases for
quite a while; there's no reason to tie a binary linked with libpcap to
a particular release of libpcap.

Problems, bugs, questions, desirable enhancements, etc. should be sent
to the address "tcpdump-workers@lists.tcpdump.org".  Bugs, support
requests, and feature requests may also be submitted on the GitHub issue
tracker for libpcap at

	https://github.com/the-tcpdump-group/libpcap/issues

Source code contributions, etc. should be sent to the email address
above or submitted by forking the branch on GitHub at

	http://github.com/the-tcpdump-group/libpcap/tree/master

and issuing a pull request.

Current versions can be found at www.tcpdump.org.

 - The TCPdump team
The following instructions apply if you have a Linux platform and want
libpcap to support the 'ACN' WAN/LAN router product from from SITA
(http://www.sita.aero)

This might also work on non-Linux Unix-compatible platforms, but that
has not been tested.

See also the libpcap INSTALL.txt file for further libpcap configuration
options.

These additions/extensions have been made to PCAP to allow it to
capture packets from a SITA ACN device (and potentially others).

To enable its support you need to ensure that the distribution has
a correct configure.ac file; that can be created if neccessay by
using the normal autoconf procedure of:

aclocal
autoconf
autoheader
automake

Then run configure with the 'sita' option:

./configure --with-sita

Applications built with libpcap configured in this way will only detect SITA
ACN interfaces and will not capture from the native OS packet stream.

The SITA extension provides a remote datascope operation for capturing
both WAN and LAN protocols.  It effectively splits the operation of
PCAP into two halves.  The top layer performs the majority of the
work, but interfaces via a TCP session to remote agents that
provide the lower layer functionality of actual sniffing and
filtering. More detailed information regarding the functions and
inter-device protocol and naming conventions are described in detail
in 'pcap-sita.html'.

pcap_findalldevs() reads the local system's /etc/hosts file looking
for host names that match the format of IOP type devices.  ie.  aaa_I_x_y
and then queries each associated IP address for a list of its WAN and
LAN devices.  The local system the aggregates the lists obtained from
each IOP, sorts it, and provides it (to Wireshark et.al) as the
list of monitorable interfaces.

Once a valid interface has been selected, pcap_open() is called
which opens a TCP session (to a well known port) on the target IOP
and tells it to start monitoring.

All captured packets are then forwarded across that TCP session
back to the local 'top layer' for forwarding to the actual
sniffing program (wireshark...)

Note that the DLT_SITA link-layer type includes a proprietary header
that is documented as part of the SITA dissector of Wireshark and is
also described in 'pcap-sita.html' for posterity sake.

That header provides:
- Packet direction (in/out) (1 octet)
- Link layer hardware signal status (1 octet)
- Transmit/Receive error status (2 octets)
- Encapsulated WAN protocol ID (1 octet)



The following instructions apply if you have a Linux or FreeBSD platform and
want libpcap to support the DAG range of passive network monitoring cards from
Endace (http://www.endace.com, see below for further contact details).

1) Install and build the DAG software distribution by following the
instructions supplied with that package. Current Endace customers can download
the DAG software distibution from https://www.endace.com

2) Configure libcap. To allow the 'configure' script to locate the DAG
software distribution use the '--with-dag' option:

        ./configure --with-dag=DIR

Where DIR is the root of the DAG software distribution, for example
/var/src/dag. If the DAG software is correctly detected 'configure' will
report:

        checking whether we have DAG API... yes

If 'configure' reports that there is no DAG API, the directory may have been
incorrectly specified or the DAG software was not built before configuring
libpcap.

See also the libpcap INSTALL.txt file for further libpcap configuration
options.

Building libpcap at this stage will include support for both the native packet
capture stream (linux or bpf) and for capturing from DAG cards. To build
libpcap with only DAG support specify the capture type as 'dag' when
configuring libpcap:

        ./configure --with-dag=DIR --with-pcap=dag

Applications built with libpcap configured in this way will only detect DAG
cards and will not capture from the native OS packet stream.

----------------------------------------------------------------------

Libpcap when built for DAG cards against dag-2.5.1 or later releases:

Timeouts are supported. pcap_dispatch() will return after to_ms milliseconds
regardless of how many packets are received. If to_ms is zero pcap_dispatch()
will block waiting for data indefinitely.

pcap_dispatch() will block on and process a minimum of 64kB of data (before
filtering) for efficiency. This can introduce high latencies on quiet
interfaces unless a timeout value is set. The timeout expiring will override
the 64kB minimum causing pcap_dispatch() to process any available data and
return.

pcap_setnonblock is supported. When nonblock is set, pcap_dispatch() will
check once for available data, process any data available up to count, then
return immediately.

pcap_findalldevs() is supported, e.g. dag0, dag1...

Some DAG cards can provide more than one 'stream' of received data.
This can be data from different physical ports, or separated by filtering
or load balancing mechanisms. Receive streams have even numbers, e.g.
dag0:0, dag0:2 etc. Specifying transmit streams for capture is not supported.

pcap_setfilter() is supported, BPF programs run in userspace.

pcap_setdirection() is not supported. Only received traffic is captured.
DAG cards normally do not have IP or link layer addresses assigned as
they are used to passively monitor links.

pcap_breakloop() is supported.

pcap_datalink() and pcap_list_datalinks() are supported. The DAG card does
not attempt to set the correct datalink type automatically where more than
one type is possible.

pcap_stats() is supported. ps_drop is the number of packets dropped due to
RX stream buffer overflow, this count is before filters are applied (it will
include packets that would have been dropped by the filter). The RX stream
buffer size is user configurable outside libpcap, typically 16-512MB.

pcap_get_selectable_fd() is not supported, as DAG cards do not support
poll/select methods.

pcap_inject() and pcap_sendpacket() are not supported.

Some DAG cards now support capturing to multiple virtual interfaces, called
streams. Capture streams have even numbers. These are available via libpcap
as separate interfaces, e.g. dag0:0, dag0:2, dag0:4 etc. dag0:0 is the same
as dag0. These are visible via pcap_findalldevs().

libpcap now does NOT set the card's hardware snaplen (slen). This must now be
set using the appropriate DAG coniguration program, e.g. dagthree, dagfour,
dagsix, dagconfig. This is because the snaplen is currently shared between
all of the streams. In future this may change if per-stream slen is
implemented.

DAG cards by default capture entire packets including the L2
CRC/FCS. If the card is not configured to discard the CRC/FCS, this
can confuse applications that use libpcap if they're not prepared for
packets to have an FCS.

Libpcap now reads the environment variable ERF_FCS_BITS to determine
how many bits of CRC/FCS to strip from the end of the captured
frame. This defaults to 32 for use with Ethernet. If the card is
configured to strip the CRC/FCS, then set ERF_FCS_BITS=0. If used with
a HDLC/PoS/PPP/Frame Relay link with 16 bit CRC/FCS, then set
ERF_FCS_BITS=16.

If you wish to create a pcap file that DOES contain the Ethernet FCS,
specify the environment variable ERF_DONT_STRIP_FCS. This will cause
the existing FCS to be captured into the pcap file. Note some
applications may incorrectly report capture errors or oversize packets
when reading these files.

----------------------------------------------------------------------

Please submit bug reports via <support@endace.com>.

Please also visit our Web site at:

        http://www.endace.com/

For more information about Endace DAG cards contact <sales@endace.com>.
wpa_supplicant and hostapd
--------------------------

Copyright (c) 2002-2016, Jouni Malinen <j@w1.fi> and contributors
All Rights Reserved.

These programs are licensed under the BSD license (the one with
advertisement clause removed).

If you are submitting changes to the project, please see CONTRIBUTIONS
file for more instructions.


This package may include either wpa_supplicant, hostapd, or both. See
README file respective subdirectories (wpa_supplicant/README or
hostapd/README) for more details.

Source code files were moved around in v0.6.x releases and compared to
earlier releases, the programs are now built by first going to a
subdirectory (wpa_supplicant or hostapd) and creating build
configuration (.config) and running 'make' there (for Linux/BSD/cygwin
builds).


License
-------

This software may be distributed, used, and modified under the terms of
BSD license:

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

3. Neither the name(s) of the above-listed copyright holder(s) nor the
   names of its contributors may be used to endorse or promote products
   derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
wpa_supplicant and Wi-Fi P2P
============================

This document describes how the Wi-Fi P2P implementation in
wpa_supplicant can be configured and how an external component on the
client (e.g., management GUI) is used to enable WPS enrollment and
registrar registration.


Introduction to Wi-Fi P2P
-------------------------

TODO

More information about Wi-Fi P2P is available from Wi-Fi Alliance:
http://www.wi-fi.org/Wi-Fi_Direct.php


wpa_supplicant implementation
-----------------------------

TODO


wpa_supplicant configuration
----------------------------

Wi-Fi P2P is an optional component that needs to be enabled in the
wpa_supplicant build configuration (.config). Here is an example
configuration that includes Wi-Fi P2P support and Linux nl80211
-based driver interface:

CONFIG_DRIVER_NL80211=y
CONFIG_CTRL_IFACE=y
CONFIG_P2P=y
CONFIG_AP=y
CONFIG_WPS=y


In run-time configuration file (wpa_supplicant.conf), some parameters
for P2P may be set. In order to make the devices easier to recognize,
device_name and device_type should be specified. For example,
something like this should be included:

ctrl_interface=/var/run/wpa_supplicant
device_name=My P2P Device
device_type=1-0050F204-1


wpa_cli
-------

Actual Wi-Fi P2P operations are requested during runtime. These can be
done for example using wpa_cli (which is described below) or a GUI
like wpa_gui-qt4.


wpa_cli starts in interactive mode if no command string is included on
the command line. By default, it will select the first network interface
that it can find (and that wpa_supplicant controls). If more than one
interface is in use, it may be necessary to select one of the explicitly
by adding -i argument on the command line (e.g., 'wpa_cli -i wlan1').

Most of the P2P operations are done on the main interface (e.g., the
interface that is automatically added when the driver is loaded, e.g.,
wlan0). When using a separate virtual interface for group operations
(e.g., wlan1), the control interface for that group interface may need
to be used for some operations (mainly WPS activation in GO). This may
change in the future so that all the needed operations could be done
over the main control interface.

Device Discovery

p2p_find [timeout in seconds] [type=<social|progressive>] \
	[dev_id=<addr>] [dev_type=<device type>] \
	[delay=<search delay in ms>] [seek=<service name>] [freq=<MHz>]

The default behavior is to run a single full scan in the beginning and
then scan only social channels. type=social will scan only social
channels, i.e., it skips the initial full scan. type=progressive is
like the default behavior, but it will scan through all the channels
progressively one channel at the time in the Search state rounds. This
will help in finding new groups or groups missed during the initial
full scan. When the type parameter is not included (i.e., full scan), the
optional freq parameter can be used to override the first scan to use only
the specified channel after which only social channels are scanned.

The optional dev_id option can be used to specify a single P2P peer to
search for. The optional delay parameter can be used to request an extra
delay to be used between search iterations (e.g., to free up radio
resources for concurrent operations).

The optional dev_type option can be used to specify a single device type
(primary or secondary) to search for, e.g.,
"p2p_find dev_type=1-0050F204-1".


With one or more seek arguments, the command sends Probe Request frames
for a P2PS service. For example,
p2p_find 5 dev_id=11:22:33:44:55:66 seek=alt.example.chat seek=alt.example.video

Parameters description:
    Timeout - Optional ASCII base-10-encoded u16. If missing, request will not
	time out and must be canceled manually
    dev_id - Optional to request responses from a single known remote device
    Service Name - Mandatory UTF-8 string for ASP seeks
	Service name must match the remote service being advertised exactly
	(no prefix matching).
	Service name may be empty, in which case all ASP services will be
	returned, and may be filtered with p2p_serv_disc_req settings, and
	p2p_serv_asp_resp results.
	Multiple service names may be requested, but if it exceeds internal
	limit, it will automatically revert to requesting all ASP services.

p2p_listen [timeout in seconds]

Start Listen-only state (become discoverable without searching for
other devices). Optional parameter can be used to specify the duration
for the Listen operation in seconds. This command may not be of that
much use during normal operations and is mainly designed for
testing. It can also be used to keep the device discoverable without
having to maintain a group.

p2p_stop_find

Stop ongoing P2P device discovery or other operation (connect, listen
mode).

p2p_flush

Flush P2P peer table and state.

Group Formation

p2p_prov_disc <peer device address> <display|keypad|pbc> [join|auto]

Send P2P provision discovery request to the specified peer. The
parameters for this command are the P2P device address of the peer and
the desired configuration method. For example, "p2p_prov_disc
02:01:02:03:04:05 display" would request the peer to display a PIN for
us and "p2p_prov_disc 02:01:02:03:04:05 keypad" would request the peer
to enter a PIN that we display.

The optional "join" parameter can be used to indicate that this command
is requesting an already running GO to prepare for a new client. This is
mainly used with "display" to request it to display a PIN. The "auto"
parameter can be used to request wpa_supplicant to automatically figure
out whether the peer device is operating as a GO and if so, use
join-a-group style PD instead of GO Negotiation style PD.

p2p_connect <peer device address> <pbc|pin|PIN#|p2ps> [display|keypad|p2ps]
	[persistent|persistent=<network id>] [join|auth]
	[go_intent=<0..15>] [freq=<in MHz>] [ht40] [vht] [provdisc] [auto]
	[ssid=<hexdump>]

Start P2P group formation with a discovered P2P peer. This includes
optional group owner negotiation, group interface setup, provisioning,
and establishing data connection.

The <pbc|pin|PIN#> parameter specifies the WPS provisioning
method. "pbc" string starts pushbutton method, "pin" string start PIN
method using an automatically generated PIN (which will be returned as
the command return code), PIN# means that a pre-selected PIN can be
used (e.g., 12345670). [display|keypad] is used with PIN method
to specify which PIN is used (display=dynamically generated random PIN
from local display, keypad=PIN entered from peer display). "persistent"
parameter can be used to request a persistent group to be formed. The
"persistent=<network id>" alternative can be used to pre-populate
SSID/passphrase configuration based on a previously used persistent
group where this device was the GO. The previously used parameters will
then be used if the local end becomes the GO in GO Negotiation (which
can be forced with go_intent=15).

"join" indicates that this is a command to join an existing group as a
client. It skips the GO Negotiation part. This will send a Provision
Discovery Request message to the target GO before associating for WPS
provisioning.

"auth" indicates that the WPS parameters are authorized for the peer
device without actually starting GO Negotiation (i.e., the peer is
expected to initiate GO Negotiation). This is mainly for testing
purposes.

"go_intent" can be used to override the default GO Intent for this GO
Negotiation.

"freq" can be used to set a forced operating channel (e.g., freq=2412
to select 2.4 GHz channel 1).

"provdisc" can be used to request a Provision Discovery exchange to be
used prior to starting GO Negotiation as a workaround with some deployed
P2P implementations that require this to allow the user to accept the
connection.

"auto" can be used to request wpa_supplicant to automatically figure
out whether the peer device is operating as a GO and if so, use
join-a-group operation rather than GO Negotiation.

"ssid=<hexdump>" can be used to specify the Group SSID for join
operations. This allows the P2P Client interface to filter scan results
based on SSID to avoid selecting an incorrect BSS entry in case the same
P2P Device or Interface address have been used in multiple groups
recently.

P2PS attribute changes to p2p_connect command:

P2PS supports two WPS provisioning methods namely PIN method and P2PS default.
The remaining parameters hold same role as in legacy P2P. In case of P2PS
default config method "p2ps" keyword is added in p2p_connect command.

For example:
p2p_connect 02:0a:f5:85:11:00 12345670 p2ps persistent join
	(WPS Method = P2PS default)

p2p_connect 02:0a:f5:85:11:00 45629034 keypad persistent
	(WPS Method = PIN)

p2p_asp_provision <peer MAC address> <adv_id=peer adv id>
	<adv_mac=peer MAC address> [role=2|4|1] <session=session id>
	<session_mac=initiator mac address>
	[info='service info'] <method=Default|keypad|Display>

This command starts provision discovery with the P2PS enabled peer device.

For example,
p2p_asp_provision 00:11:22:33:44:55 adv_id=4d6fc7 adv_mac=00:55:44:33:22:11 role=1 session=12ab34 session_mac=00:11:22:33:44:55 info='name=john' method=1000

Parameter description:
    MAC address - Mandatory
    adv_id - Mandatory remote Advertising ID of service connection is being
	established for
    adv_mac - Mandatory MAC address that owns/registered the service
    role - Optional
	2 (group client only) or 4 (group owner only)
	if not present (or 1) role is negotiated by the two peers.
    session - Mandatory Session ID of the first session to be established
    session_mac - Mandatory MAC address that owns/initiated the session
    method - Optional method to request for provisioning (1000 - P2PS Default,
	100 - Keypad(PIN), 8 - Display(PIN))
    info - Optional UTF-8 string. Hint for service to indicate possible usage
	parameters - Escape single quote & backslash:
	with a backslash 0x27 == ' == \', and 0x5c == \ == \\

p2p_asp_provision_resp <peer mac address> <adv_id= local adv id>
	<adv_mac=local MAC address> <role=1|2|4> <status=0>
	<session=session id> <session_mac=peer MAC address>

This command sends a provision discovery response from responder side.

For example,
p2p_asp_provision_resp 00:55:44:33:22:11 adv_id=4d6fc7 adv_mac=00:55:44:33:22:11 role=1 status=0 session=12ab34 session_mac=00:11:22:33:44:55

Parameters definition:
    MAC address - Mandatory
    adv_id - Mandatory local Advertising ID of service connection is being
	established for
    adv_mac - Mandatory MAC address that owns/registered the service
    role -  Optional 2 (group client only) or 4 (group owner only)
	if not present (or 1) role is negotiated by the two peers.
    status - Mandatory Acceptance/Rejection code of Provisioning
    session - Mandatory Session ID of the first session to be established
    session_mac - Mandatory MAC address that owns/initiated the session

p2p_group_add [persistent|persistent=<network id>] [freq=<freq in MHz>]
	[ht40] [vht]

Set up a P2P group owner manually (i.e., without group owner
negotiation with a specific peer). This is also known as autonomous
GO. Optional persistent=<network id> can be used to specify restart of
a persistent group. Optional freq=<freq in MHz> can be used to force
the GO to be started on a specific frequency. Special freq=2 or freq=5
options can be used to request the best 2.4 GHz or 5 GHz band channel
to be selected automatically.

p2p_reject <peer device address>

Reject connection attempt from a peer (specified with a device
address). This is a mechanism to reject a pending GO Negotiation with
a peer and request to automatically block any further connection or
discovery of the peer.

p2p_group_remove <group interface>

Terminate a P2P group. If a new virtual network interface was used for
the group, it will also be removed. The network interface name of the
group interface is used as a parameter for this command.

p2p_cancel

Cancel an ongoing P2P group formation and joining-a-group related
operation. This operation unauthorizes the specific peer device (if any
had been authorized to start group formation), stops P2P find (if in
progress), stops pending operations for join-a-group, and removes the
P2P group interface (if one was used) that is in the WPS provisioning
step. If the WPS provisioning step has been completed, the group is not
terminated.

p2p_remove_client <peer's P2P Device Address|iface=<interface address>>

This command can be used to remove the specified client from all groups
(operating and persistent) from the local GO. Note that the peer device
can rejoin the group if it is in possession of a valid key. See p2p_set
per_sta_psk command below for more details on how the peer can be
removed securely.

Service Discovery

p2p_service_add asp <auto accept> <adv id> <status 0/1> <Config Methods>
	<Service name> [Service Information] [Response Info]

This command can be used to search for a P2PS service which includes
Play, Send, Display, and Print service. The parameters for this command
are "asp" to identify the command as P2PS one, auto accept value,
advertisement id which uniquely identifies the service requests, state
of the service whether the service is available or not, config methods
which can be either P2PS method or PIN method, service name followed by
two optional parameters service information, and response info.

For example,
p2p_service_add asp 1 4d6fc7 0 1108 alt.example.chat svc_info='name=john' rsp_info='enter PIN 1234'

Parameters definition:
    asp - Mandatory for ASP service registration
    auto accept - Mandatory ASCII hex-encoded boolean (0 == no auto-accept,
	1 == auto-accept ANY role, 2 == auto-accept CLIENT role,
	4 == auto-accept GO role)
    Advertisement ID - Mandatory non-zero ASCII hex-encoded u32
	(Must be unique/not yet exist in svc db)
    State - Mandatory ASCII hex-encoded u8 (0 -- Svc not available,
	1 -- Svc available, 2-0xff  Application defined)
    Config Methods - Mandatory ASCII hex-encoded u16 (bitmask of WSC config
	methods)
    Service Name - Mandatory UTF-8 string
    Service Information - Optional UTF-8 string
	Escape single quote & backslash with a backslash:
	0x27 == ' == \', and 0x5c == \ == \\
    Session response information -  Optional (used only if auto accept is TRUE)
	UTF-8 string
	Escape single quote & backslash with a backslash:
	0x27 == ' == \', and 0x5c == \ == \\

p2p_service_rep asp <auto accept> <adv id> <status 0/1> <Config Methods>
	<Service name> [Service Information] [Response Info]

This command can be used to replace the existing service request
attributes from the initiator side. The replacement is only allowed if
the advertisement id issued in the command matches with any one entry in
the list of existing SD queries. If advertisement id doesn't match the
command returns a failure.

For example,
p2p_service_rep asp 1 4d6fc7 1 1108 alt.example.chat svc_info='name=john' rsp_info='enter PIN 1234'

Parameters definition:
    asp - Mandatory for ASP service registration
    auto accept - Mandatory ASCII hex-encoded boolean (1 == true, 0 == false)
    Advertisement ID - Mandatory non-zero ASCII hex-encoded u32
	(Must already exist in svc db)
    State - Mandatory ASCII hex-encoded u8 (can be used to indicate svc
	available or not available for instance)
    Config Methods - Mandatory ASCII hex-encoded u16 (bitmask of WSC config
	methods)
    Service Name - Mandatory UTF-8 string (Must match existing string in svc db)
    Service Information - Optional UTF-8 string
	Escape single quote & backslash with a backslash:
	0x27 == ' == \', and 0x5c == \ == \\
    Session response information -  Optional (used only if auto accept is TRUE)
	UTF-8 string
	Escape single quote & backslash with a backslash:
	0x27 == ' == \', and 0x5c == \ == \\

p2p_serv_disc_req

Schedule a P2P service discovery request. The parameters for this
command are the device address of the peer device (or 00:00:00:00:00:00
for wildcard query that is sent to every discovered P2P peer that
supports service discovery) and P2P Service Query TLV(s) as hexdump. For
example,

p2p_serv_disc_req 00:00:00:00:00:00 02000001

schedules a request for listing all available services of all service
discovery protocols and requests this to be sent to all discovered
peers (note: this can result in long response frames). The pending
requests are sent during device discovery (see p2p_find).

There can be multiple pending peer device specific queries (each will be
sent in sequence whenever the peer is found).

This command returns an identifier for the pending query (e.g.,
"1f77628") that can be used to cancel the request. Directed requests
will be automatically removed when the specified peer has replied to
it.

Service Query TLV has following format:
Length (2 octets, little endian) - length of following data
Service Protocol Type (1 octet) - see the table below
Service Transaction ID (1 octet) - nonzero identifier for the TLV
Query Data (Length - 2 octets of data) - service protocol specific data

Service Protocol Types:
0 = All service protocols
1 = Bonjour
2 = UPnP
3 = WS-Discovery
4 = Wi-Fi Display

For UPnP, an alternative command format can be used to specify a
single query TLV (i.e., a service discovery for a specific UPnP
service):

p2p_serv_disc_req 00:00:00:00:00:00 upnp <version hex> <ST: from M-SEARCH>

For example:

p2p_serv_disc_req 00:00:00:00:00:00 upnp 10 urn:schemas-upnp-org:device:InternetGatewayDevice:1

Additional examples for queries:

# list of all Bonjour services
p2p_serv_disc_req 00:00:00:00:00:00 02000101

# list of all UPnP services
p2p_serv_disc_req 00:00:00:00:00:00 02000201

# list of all WS-Discovery services
p2p_serv_disc_req 00:00:00:00:00:00 02000301

# list of all Bonjour and UPnP services
p2p_serv_disc_req 00:00:00:00:00:00 0200010102000202

# Apple File Sharing over TCP
p2p_serv_disc_req 00:00:00:00:00:00 130001010b5f6166706f766572746370c00c000c01

# Bonjour SSTH (supported service type hash)
p2p_serv_disc_req 00:00:00:00:00:00 05000101000000

# UPnP examples
p2p_serv_disc_req 00:00:00:00:00:00 upnp 10 ssdp:all
p2p_serv_disc_req 00:00:00:00:00:00 upnp 10 upnp:rootdevice
p2p_serv_disc_req 00:00:00:00:00:00 upnp 10 urn:schemas-upnp-org:service:ContentDirectory:2
p2p_serv_disc_req 00:00:00:00:00:00 upnp 10 uuid:6859dede-8574-59ab-9332-123456789012
p2p_serv_disc_req 00:00:00:00:00:00 upnp 10 urn:schemas-upnp-org:device:InternetGatewayDevice:1

# Wi-Fi Display examples
# format: wifi-display <list of roles> <list of subelements>
p2p_serv_disc_req 00:00:00:00:00:00 wifi-display [source] 2,3,4,5
p2p_serv_disc_req 02:01:02:03:04:05 wifi-display [pri-sink] 3
p2p_serv_disc_req 00:00:00:00:00:00 wifi-display [sec-source] 2
p2p_serv_disc_req 00:00:00:00:00:00 wifi-display [source+sink] 2,3,4,5
p2p_serv_disc_req 00:00:00:00:00:00 wifi-display [source][pri-sink] 2,3,4,5

p2p_serv_disc_req <Unicast|Broadcast mac address> asp <Transaction ID>
	<Service Name> [Service Information]

The command can be used for service discovery for P2PS enabled devices.

For example: p2p_serv_disc_req 00:00:00:00:00:00 asp a1 alt.example 'john'

Parameters definition:
    MAC address - Mandatory Existing
    asp - Mandatory for ASP queries
    Transaction ID - Mandatory non-zero ASCII hex-encoded u8 for GAS
    Service Name Prefix - Mandatory UTF-8 string.
	Will match from beginning of remote Service Name
    Service Information Substring - Optional UTF-8 string
	If Service Information Substring is not included, all services matching
	Service Name Prefix will be returned.
	If Service Information Substring is included, both the Substring and the
	Service Name Prefix must match for service to be returned.
	If remote service has no Service Information, all Substring searches
	will fail.

p2p_serv_disc_cancel_req <query identifier>

Cancel a pending P2P service discovery request. This command takes a
single parameter: identifier for the pending query (the value returned
by p2p_serv_disc_req, e.g., "p2p_serv_disc_cancel_req 1f77628".

p2p_serv_disc_resp

Reply to a service discovery query. This command takes following
parameters: frequency in MHz, destination address, dialog token,
response TLV(s). The first three parameters are copied from the
request event. For example, "p2p_serv_disc_resp 2437 02:40:61:c2:f3:b7
1 0300000101". This command is used only if external program is used
to process the request (see p2p_serv_disc_external).

p2p_service_update

Indicate that local services have changed. This is used to increment
the P2P service indicator value so that peers know when previously
cached information may have changed. This is only needed when external
service discovery processing is enabled since the commands to
pre-configure services for internal processing will increment the
indicator automatically.

p2p_serv_disc_external <0|1>

Configure external processing of P2P service requests: 0 (default) =
no external processing of requests (i.e., internal code will process
each request based on pre-configured services), 1 = external
processing of requests (external program is responsible for replying
to service discovery requests with p2p_serv_disc_resp). Please note
that there is quite strict limit on how quickly the response needs to
be transmitted, so use of the internal processing is strongly
recommended.

p2p_service_add bonjour <query hexdump> <RDATA hexdump>

Add a local Bonjour service for internal SD query processing.

Examples:

# AFP Over TCP (PTR)
p2p_service_add bonjour 0b5f6166706f766572746370c00c000c01 074578616d706c65c027
# AFP Over TCP (TXT) (RDATA=null)
p2p_service_add bonjour 076578616d706c650b5f6166706f766572746370c00c001001 00

# IP Printing over TCP (PTR) (RDATA=MyPrinter._ipp._tcp.local.)
p2p_service_add bonjour 045f697070c00c000c01 094d795072696e746572c027
# IP Printing over TCP (TXT) (RDATA=txtvers=1,pdl=application/postscript)
p2p_service_add bonjour 096d797072696e746572045f697070c00c001001 09747874766572733d311a70646c3d6170706c69636174696f6e2f706f7374736372797074

# Supported Service Type Hash (SSTH)
p2p_service_add bonjour 000000 <32-byte bitfield as hexdump>
(note: see P2P spec Annex E.4 for information on how to construct the bitfield)

p2p_service_del bonjour <query hexdump>

Remove a local Bonjour service from internal SD query processing.

p2p_service_add upnp <version hex> <service>

Add a local UPnP service for internal SD query processing.

Examples:

p2p_service_add upnp 10 uuid:6859dede-8574-59ab-9332-123456789012::upnp:rootdevice
p2p_service_add upnp 10 uuid:5566d33e-9774-09ab-4822-333456785632::upnp:rootdevice
p2p_service_add upnp 10 uuid:1122de4e-8574-59ab-9322-333456789044::urn:schemas-upnp-org:service:ContentDirectory:2
p2p_service_add upnp 10 uuid:5566d33e-9774-09ab-4822-333456785632::urn:schemas-upnp-org:service:ContentDirectory:2
p2p_service_add upnp 10 uuid:6859dede-8574-59ab-9332-123456789012::urn:schemas-upnp-org:device:InternetGatewayDevice:1

p2p_service_del upnp <version hex> <service>

Remove a local UPnP service from internal SD query processing.

p2p_service_del asp <adv id>

Removes the local asp service from internal SD query list.
For example: p2p_service_del asp 4d6fc7

p2p_service_flush

Remove all local services from internal SD query processing.

Invitation

p2p_invite [persistent=<network id>|group=<group ifname>] [peer=address]
	[go_dev_addr=address] [freq=<freq in MHz>] [ht40] [vht]
	[pref=<MHz>]

Invite a peer to join a group (e.g., group=wlan1) or to reinvoke a
persistent group (e.g., persistent=4). If the peer device is the GO of
the persistent group, the peer parameter is not needed. Otherwise it is
used to specify which device to invite. go_dev_addr parameter can be
used to override the GO device address for Invitation Request should
it be not known for some reason (this should not be needed in most
cases). When reinvoking a persistent group, the GO device can specify
the frequency for the group with the freq parameter. When reinvoking a
persistent group, the P2P client device can use freq parameter to force
a specific operating channel (or invitation failure if GO rejects that)
or pref parameter to request a specific channel (while allowing GO to
select to use another channel, if needed).

Group Operations

(These are used on the group interface.)

wps_pin <any|address> <PIN>

Start WPS PIN method. This allows a single WPS Enrollee to connect to
the AP/GO. This is used on the GO when a P2P client joins an existing
group. The second parameter is the address of the Enrollee or a string
"any" to allow any station to use the entered PIN (which will restrict
the PIN for one-time-use). PIN is the Enrollee PIN read either from a
label or display on the P2P Client/WPS Enrollee.

wps_pbc

Start WPS PBC method (i.e., push the button). This allows a single WPS
Enrollee to connect to the AP/GO. This is used on the GO when a P2P
client joins an existing group.

p2p_get_passphrase

Get the passphrase for a group (only available when acting as a GO).

p2p_presence_req [<duration> <interval>] [<duration> <interval>]

Send a P2P Presence Request to the GO (this is only available when
acting as a P2P client). If no duration/interval pairs are given, the
request indicates that this client has no special needs for GO
presence. The first parameter pair gives the preferred duration and
interval values in microseconds. If the second pair is included, that
indicates which value would be acceptable. This command returns OK
immediately and the response from the GO is indicated in a
P2P-PRESENCE-RESPONSE event message.

Parameters

p2p_ext_listen [<period> <interval>]

Configure Extended Listen Timing. If the parameters are omitted, this
feature is disabled. If the parameters are included, Listen State will
be entered every interval msec for at least period msec. Both values
have acceptable range of 1-65535 (with interval obviously having to be
larger than or equal to duration). If the P2P module is not idle at
the time the Extended Listen Timing timeout occurs, the Listen State
operation will be skipped.

The configured values will also be advertised to other P2P Devices. The
received values are available in the p2p_peer command output:

ext_listen_period=100 ext_listen_interval=5000

p2p_set <field> <value>

Change dynamic P2P parameters

p2p_set discoverability <0/1>

Disable/enable advertisement of client discoverability. This is
enabled by default and this parameter is mainly used to allow testing
of device discoverability.

p2p_set managed <0/1>

Disable/enable managed P2P Device operations. This is disabled by
default.

p2p_set listen_channel <channel> [<op_class>]

Set P2P Listen channel. This is mainly meant for testing purposes and
changing the Listen channel during normal operations can result in
protocol failures.

When specifying a social channel on the 2.4 GHz band (1/6/11) there is
no need to specify the operating class since it defaults to 81.  When
specifying a social channel on the 60 GHz band (2), specify the 60 GHz
operating class (180).

p2p_set ssid_postfix <postfix>

Set postfix string to be added to the automatically generated P2P SSID
(DIRECT-<two random characters>). For example, postfix of "-testing"
could result in the SSID becoming DIRECT-ab-testing.

p2p_set per_sta_psk <0/1>

Disabled(default)/enables use of per-client PSK in the P2P groups. This
can be used to request GO to assign a unique PSK for each client during
WPS provisioning. When enabled, this allow clients to be removed from
the group securely with p2p_remove_client command since that client's
PSK is removed at the same time to prevent it from connecting back using
the old PSK. When per-client PSK is not used, the client can still be
disconnected, but it will be able to re-join the group since the PSK it
learned previously is still valid. It should be noted that the default
passphrase on the GO that is normally used to allow legacy stations to
connect through manual configuration does not change here, so if that is
shared, devices with knowledge of that passphrase can still connect.

set <field> <value>

Set global configuration parameters which may also affect P2P
operations. The format on these parameters is same as is used in
wpa_supplicant.conf. Only the parameters listen here should be
changed. Modifying other parameters may result in incorrect behavior
since not all existing users of the parameters are updated.

set uuid <UUID>

Set WPS UUID (by default, this is generated based on the MAC address).

set device_name <device name>

Set WPS Device Name (also included in some P2P messages).

set manufacturer <manufacturer>

Set WPS Manufacturer.

set model_name <model name>

Set WPS Model Name.

set model_number <model number>

Set WPS Model Number.

set serial_number <serial number>

Set WPS Serial Number.

set device_type <device type>

Set WPS Device Type.

set os_version <OS version>

Set WPS OS Version.

set config_methods <config methods>

Set WPS Configuration Methods.

set sec_device_type <device type>

Add a new Secondary Device Type.

set p2p_go_intent <GO intent>

Set the default P2P GO Intent. Note: This value can be overridden in
p2p_connect command and as such, there should be no need to change the
default value here during normal operations.

set p2p_ssid_postfix <P2P SSID postfix>

Set P2P SSID postfix.

set persistent_reconnect <0/1>

Disable/enabled persistent reconnect for reinvocation of persistent
groups. If enabled, invitations to reinvoke a persistent group will be
accepted without separate authorization (e.g., user interaction).

set country <two character country code>

Set country code (this is included in some P2P messages).

set p2p_search_delay <delay>

Set p2p_search_delay which adds extra delay in milliseconds between
concurrent search iterations to make p2p_find friendlier to concurrent
operations by avoiding it from taking 100% of radio resources. The
default value is 500 ms.

Status

p2p_peers [discovered]

List P2P Device Addresses of all the P2P peers we know. The optional
"discovered" parameter filters out the peers that we have not fully
discovered, i.e., which we have only seen in a received Probe Request
frame.

p2p_peer <P2P Device Address>

Fetch information about a known P2P peer.

Group Status

(These are used on the group interface.)

status

Show status information (connection state, role, use encryption
parameters, IP address, etc.).

sta

Show information about an associated station (when acting in AP/GO role).

all_sta

Lists the currently associated stations.

Configuration data

list_networks

Lists the configured networks, including stored information for
persistent groups. The identifier in this list is used with
p2p_group_add and p2p_invite to indicate which persistent group is to
be reinvoked.

remove_network <network id>

Remove a network entry from configuration. 


P2PS Events/Responses:

P2PS-PROV-START: This events gets triggered when provisioning is issued for
either seeker or advertiser.

For example,
P2PS-PROV-START 00:55:44:33:22:11 adv_id=111 adv_mac=00:55:44:33:22:11 conncap=1 session=1234567 session_mac=00:11:22:33:44:55 info='xxxx'

Parameters definition:
    MAC address - always
    adv_id - always ASCII hex-encoded u32
    adv_mac - always MAC address that owns/registered the service
    conncap - always mask of 0x01 (new), 0x02 (group client), 0x04 (group owner)
	bits
    session - always Session ID of the first session to be established
    session_mac - always MAC address that owns/initiated the session
    info - if available, UTF-8 string
	Escaped single quote & backslash with a backslash:
	\' == 0x27 == ', and \\ == 0x5c == \

P2PS-PROV-DONE: When provisioning is completed then this event gets triggered.

For example,
P2PS-PROV-DONE 00:11:22:33:44:55 status=0 adv_id=111 adv_mac=00:55:44:33:22:11 conncap=1 session=1234567 session_mac=00:11:22:33:44:55 [dev_passwd_id=8 | go=p2p-wlan0-0 | join=11:22:33:44:55:66 | persist=0]

Parameters definition:
    MAC address - always main device address of peer. May be different from MAC
	ultimately connected to.
    status - always ascii hex-encoded u8 (0 == success, 12 == deferred success)
    adv_id - always ascii hex-encoded u32
    adv_mac - always MAC address that owns/registered the service
    conncap - always One of: 1 (new), 2 (group client), 4 (group owner) bits
    session - always Session ID of the first session to be established
    session_mac - always MAC address that owns/initiated the session
    dev_passwd_id - only if conncap value == 1 (New GO negotiation)
	8 - "p2ps" password must be passed in p2p_connect command
	1 - "display" password must be passed in p2p_connect command
	5 - "keypad" password must be passed in p2p_connect command
    join only - if conncap value == 2 (Client Only). Display password and "join"
	must be passed in p2p_connect and address must be the MAC specified
    go only - if conncap value == 4 (GO Only). Interface name must be set with a
	password
    persist - only if previous persistent group existed between peers and shall
	be re-used. Group is restarted by sending "p2p_group_add persistent=0"
	where value is taken from P2P-PROV-DONE

Extended Events/Response

P2P-DEVICE-FOUND 00:11:22:33:44:55 p2p_dev_addr=00:11:22:33:44:55 pri_dev_type=0-00000000-0 name='' config_methods=0x108 dev_capab=0x21 group_capab=0x0 adv_id=111 asp_svc=alt.example.chat

Parameters definition:
    adv_id - if ASP ASCII hex-encoded u32. If it is reporting the
	"wildcard service", this value will be 0
    asp_svc - if ASP this is the service string. If it is reporting the
	"wildcard service", this value will be org.wi-fi.wfds


wpa_cli action script
---------------------

See examples/p2p-action.sh

TODO: describe DHCP/DNS setup
TODO: cross-connection
wpa_supplicant and Wi-Fi Protected Setup (WPS)
==============================================

This document describes how the WPS implementation in wpa_supplicant
can be configured and how an external component on the client (e.g.,
management GUI) is used to enable WPS enrollment and registrar
registration.


Introduction to WPS
-------------------

Wi-Fi Protected Setup (WPS) is a mechanism for easy configuration of a
wireless network. It allows automated generation of random keys (WPA
passphrase/PSK) and configuration of an access point and client
devices. WPS includes number of methods for setting up connections
with PIN method and push-button configuration (PBC) being the most
commonly deployed options.

While WPS can enable more home networks to use encryption in the
wireless network, it should be noted that the use of the PIN and
especially PBC mechanisms for authenticating the initial key setup is
not very secure. As such, use of WPS may not be suitable for
environments that require secure network access without chance for
allowing outsiders to gain access during the setup phase.

WPS uses following terms to describe the entities participating in the
network setup:
- access point: the WLAN access point
- Registrar: a device that control a network and can authorize
  addition of new devices); this may be either in the AP ("internal
  Registrar") or in an external device, e.g., a laptop, ("external
  Registrar")
- Enrollee: a device that is being authorized to use the network

It should also be noted that the AP and a client device may change
roles (i.e., AP acts as an Enrollee and client device as a Registrar)
when WPS is used to configure the access point.


More information about WPS is available from Wi-Fi Alliance:
http://www.wi-fi.org/wifi-protected-setup


wpa_supplicant implementation
-----------------------------

wpa_supplicant includes an optional WPS component that can be used as
an Enrollee to enroll new network credential or as a Registrar to
configure an AP.


wpa_supplicant configuration
----------------------------

WPS is an optional component that needs to be enabled in
wpa_supplicant build configuration (.config). Here is an example
configuration that includes WPS support and Linux nl80211 -based
driver interface:

CONFIG_DRIVER_NL80211=y
CONFIG_WPS=y

If you want to enable WPS external registrar (ER) functionality, you
will also need to add following line:

CONFIG_WPS_ER=y

Following parameter can be used to enable support for NFC config method:

CONFIG_WPS_NFC=y


WPS needs the Universally Unique IDentifier (UUID; see RFC 4122) for
the device. This is configured in the runtime configuration for
wpa_supplicant (if not set, UUID will be generated based on local MAC
address):

# example UUID for WPS
uuid=12345678-9abc-def0-1234-56789abcdef0

The network configuration blocks needed for WPS are added
automatically based on control interface commands, so they do not need
to be added explicitly in the configuration file.

WPS registration will generate new network blocks for the acquired
credentials. If these are to be stored for future use (after
restarting wpa_supplicant), wpa_supplicant will need to be configured
to allow configuration file updates:

update_config=1



External operations
-------------------

WPS requires either a device PIN code (usually, 8-digit number) or a
pushbutton event (for PBC) to allow a new WPS Enrollee to join the
network. wpa_supplicant uses the control interface as an input channel
for these events.

The PIN value used in the commands must be processed by an UI to
remove non-digit characters and potentially, to verify the checksum
digit. "wpa_cli wps_check_pin <PIN>" can be used to do such processing.
It returns FAIL if the PIN is invalid, or FAIL-CHECKSUM if the checksum
digit is incorrect, or the processed PIN (non-digit characters removed)
if the PIN is valid.

If the client device has a display, a random PIN has to be generated
for each WPS registration session. wpa_supplicant can do this with a
control interface request, e.g., by calling wpa_cli:

wpa_cli wps_pin any

This will return the generated 8-digit PIN which will then need to be
entered at the Registrar to complete WPS registration. At that point,
the client will be enrolled with credentials needed to connect to the
AP to access the network.


If the client device does not have a display that could show the
random PIN, a hardcoded PIN that is printed on a label can be
used. wpa_supplicant is notified this with a control interface
request, e.g., by calling wpa_cli:

wpa_cli wps_pin any 12345670

This starts the WPS negotiation in the same way as above with the
generated PIN.

When the wps_pin command is issued for an AP (including P2P GO) mode
interface, an optional timeout parameter can be used to specify
expiration timeout for the PIN in seconds. For example:

wpa_cli wps_pin any 12345670 300


If a random PIN is needed for a user interface, "wpa_cli wps_pin get"
can be used to generate a new PIN without starting WPS negotiation.
This random PIN can then be passed as an argument to another wps_pin
call when the actual operation should be started.

If the client design wants to support optional WPS PBC mode, this can
be enabled by either a physical button in the client device or a
virtual button in the user interface. The PBC operation requires that
a button is also pressed at the AP/Registrar at about the same time (2
minute window). wpa_supplicant is notified of the local button event
over the control interface, e.g., by calling wpa_cli:

wpa_cli wps_pbc

At this point, the AP/Registrar has two minutes to complete WPS
negotiation which will generate a new WPA PSK in the same way as the
PIN method described above.


If the client wants to operate in the Registrar role to learn the
current AP configuration and optionally, to configure an AP,
wpa_supplicant is notified over the control interface, e.g., with
wpa_cli:

wpa_cli wps_reg <AP BSSID> <AP PIN>
(example: wpa_cli wps_reg 02:34:56:78:9a:bc 12345670)

This is used to fetch the current AP settings instead of actually
changing them. The main difference with the wps_pin command is that
wps_reg uses the AP PIN (e.g., from a label on the AP) instead of a
PIN generated at the client.

In order to change the AP configuration, the new configuration
parameters are given to the wps_reg command:

wpa_cli wps_reg <AP BSSID> <AP PIN> <new SSID> <auth> <encr> <new key>
examples:
  wpa_cli wps_reg 02:34:56:78:9a:bc 12345670 testing WPA2PSK CCMP 12345678
  wpa_cli wps_reg 02:34:56:78:9a:bc 12345670 clear OPEN NONE ""

<auth> must be one of the following: OPEN WPAPSK WPA2PSK
<encr> must be one of the following: NONE WEP TKIP CCMP


Scanning
--------

Scan results ('wpa_cli scan_results' or 'wpa_cli bss <idx>') include a
flags field that is used to indicate whether the BSS support WPS. If
the AP support WPS, but has not recently activated a Registrar, [WPS]
flag will be included. If PIN method has been recently selected,
[WPS-PIN] is shown instead. Similarly, [WPS-PBC] is shown if PBC mode
is in progress. GUI programs can use these as triggers for suggesting
a guided WPS configuration to the user. In addition, control interface
monitor events WPS-AP-AVAILABLE{,-PBC,-PIN} can be used to find out if
there are WPS enabled APs in scan results without having to go through
all the details in the GUI. These notification could be used, e.g., to
suggest possible WPS connection to the user.


wpa_gui
-------

wpa_gui-qt4 directory contains a sample GUI that shows an example of
how WPS support can be integrated into the GUI. Its main window has a
WPS tab that guides user through WPS registration with automatic AP
selection. In addition, it shows how WPS can be started manually by
selecting an AP from scan results.


Credential processing
---------------------

By default, wpa_supplicant processes received credentials and updates
its configuration internally. However, it is possible to
control these operations from external programs, if desired.

This internal processing can be disabled with wps_cred_processing=1
option. When this is used, an external program is responsible for
processing the credential attributes and updating wpa_supplicant
configuration based on them.

Following control interface messages are sent out for external programs:

WPS-CRED-RECEIVED  <hexdump of Credential attribute(s)>
For example:
<2>WPS-CRED-RECEIVED 100e006f10260001011045000c6a6b6d2d7770732d74657374100300020020100f000200081027004030653462303435366332363666653064333961643135353461316634626637313234333761636664623766333939653534663166316230323061643434386235102000060266a0ee1727


wpa_supplicant as WPS External Registrar (ER)
---------------------------------------------

wpa_supplicant can be used as a WPS ER to configure an AP or enroll
new Enrollee to join the network. This functionality uses UPnP and
requires that a working IP connectivity is available with the AP (this
can be either over a wired or wireless connection).

Separate wpa_supplicant process can be started for WPS ER
operations. A special "none" driver can be used in such a case to
indicate that no local network interface is actually controlled. For
example, following command could be used to start the ER:

wpa_supplicant -Dnone -c er.conf -ieth0

Sample er.conf:

ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=admin
device_name=WPS External Registrar


wpa_cli commands for ER functionality:

wps_er_start [IP address]
- start WPS ER functionality
- the optional IP address parameter can be used to filter operations only
  to include a single AP
- if run again while ER is active, the stored information (discovered APs
  and Enrollees) are shown again

wps_er_stop
- stop WPS ER functionality

wps_er_learn <UUID|BSSID> <AP PIN>
- learn AP configuration

wps_er_set_config <UUID|BSSID> <network id>
- use AP configuration from a locally configured network (e.g., from
  wps_reg command); this does not change the AP's configuration, but
  only prepares a configuration to be used when enrolling a new device
  to the AP

wps_er_config <UUID|BSSID> <AP PIN> <new SSID> <auth> <encr> <new key>
- examples:
  wps_er_config 87654321-9abc-def0-1234-56789abc0002 12345670 testing WPA2PSK CCMP 12345678
  wpa_er_config 87654321-9abc-def0-1234-56789abc0002 12345670 clear OPEN NONE ""

<auth> must be one of the following: OPEN WPAPSK WPA2PSK
<encr> must be one of the following: NONE WEP TKIP CCMP


wps_er_pbc <Enrollee UUID|MAC address>
- accept an Enrollee PBC using External Registrar

wps_er_pin <Enrollee UUID|"any"|MAC address> <PIN> [Enrollee MAC address]
- add an Enrollee PIN to External Registrar
- if Enrollee UUID is not known, "any" can be used to add a wildcard PIN
- if the MAC address of the enrollee is known, it should be configured
  to allow the AP to advertise list of authorized enrollees


WPS ER events:

WPS_EVENT_ER_AP_ADD
- WPS ER discovered an AP

WPS-ER-AP-ADD 87654321-9abc-def0-1234-56789abc0002 02:11:22:33:44:55 pri_dev_type=6-0050F204-1 wps_state=1 |Very friendly name|Company|Long description of the model|WAP|http://w1.fi/|http://w1.fi/hostapd/

WPS_EVENT_ER_AP_REMOVE
- WPS ER removed an AP entry

WPS-ER-AP-REMOVE 87654321-9abc-def0-1234-56789abc0002

WPS_EVENT_ER_ENROLLEE_ADD
- WPS ER discovered a new Enrollee

WPS-ER-ENROLLEE-ADD 2b7093f1-d6fb-5108-adbb-bea66bb87333 02:66:a0:ee:17:27 M1=1 config_methods=0x14d dev_passwd_id=0 pri_dev_type=1-0050F204-1 |Wireless Client|Company|cmodel|123|12345|

WPS_EVENT_ER_ENROLLEE_REMOVE
- WPS ER removed an Enrollee entry

WPS-ER-ENROLLEE-REMOVE 2b7093f1-d6fb-5108-adbb-bea66bb87333 02:66:a0:ee:17:27

WPS-ER-AP-SETTINGS
- WPS ER learned AP settings

WPS-ER-AP-SETTINGS uuid=fd91b4ec-e3fa-5891-a57d-8c59efeed1d2 ssid=test-wps auth_type=0x0020 encr_type=0x0008 key=12345678


WPS with NFC
------------

WPS can be used with NFC-based configuration method. An NFC tag
containing a password token from the Enrollee can be used to
authenticate the connection instead of the PIN. In addition, an NFC tag
with a configuration token can be used to transfer AP settings without
going through the WPS protocol.

When the station acts as an Enrollee, a local NFC tag with a password
token can be used by touching the NFC interface of a Registrar.

"wps_nfc [BSSID]" command starts WPS protocol run with the local end as
the Enrollee using the NFC password token that is either pre-configured
in the configuration file (wps_nfc_dev_pw_id, wps_nfc_dh_pubkey,
wps_nfc_dh_privkey, wps_nfc_dev_pw) or generated dynamically with
"wps_nfc_token <WPS|NDEF>" command. The included nfc_pw_token tool
(build with "make nfc_pw_token") can be used to generate NFC password
tokens during manufacturing (each station needs to have its own random
keys).

The "wps_nfc_config_token <WPS/NDEF>" command can be used to build an
NFC configuration token when wpa_supplicant is controlling an AP
interface (AP or P2P GO). The output value from this command is a
hexdump of the current AP configuration (WPS parameter requests this to
include only the WPS attributes; NDEF parameter requests additional NDEF
encapsulation to be included). This data needs to be written to an NFC
tag with an external program. Once written, the NFC configuration token
can be used to touch an NFC interface on a station to provision the
credentials needed to access the network.

The "wps_nfc_config_token <WPS/NDEF> <network id>" command can be used
to build an NFC configuration token based on a locally configured
network.

If the station includes NFC interface and reads an NFC tag with a MIME
media type "application/vnd.wfa.wsc", the NDEF message payload (with or
without NDEF encapsulation) can be delivered to wpa_supplicant using the
following wpa_cli command:

wps_nfc_tag_read <hexdump of payload>

If the NFC tag contains a configuration token, the network is added to
wpa_supplicant configuration. If the NFC tag contains a password token,
the token is added to the WPS Registrar component. This information can
then be used with wps_reg command (when the NFC password token was from
an AP) using a special value "nfc-pw" in place of the PIN parameter. If
the ER functionality has been started (wps_er_start), the NFC password
token is used to enable enrollment of a new station (that was the source
of the NFC password token).

"nfc_get_handover_req <NDEF> <WPS-CR>" command can be used to build the
WPS carrier record for a Handover Request Message for connection
handover. The first argument selects the format of the output data and
the second argument selects which type of connection handover is
requested (WPS-CR = Wi-Fi handover as specified in WSC 2.0).

"nfc_get_handover_sel <NDEF> <WPS> [UUID|BSSID]" command can be used to
build the contents of a Handover Select Message for connection handover
when this does not depend on the contents of the Handover Request
Message. The first argument selects the format of the output data and
the second argument selects which type of connection handover is
requested (WPS = Wi-Fi handover as specified in WSC 2.0). If the options
UUID|BSSID argument is included, this is a request to build the handover
message for the specified AP when wpa_supplicant is operating as a WPS
ER.

"nfc_report_handover <INIT/RESP> WPS <carrier from handover request>
<carrier from handover select>" can be used as an alternative way for
reporting completed NFC connection handover. The first parameter
indicates whether the local device initiated or responded to the
connection handover and the carrier records are the selected carrier
from the handover request and select messages as a hexdump.

The "wps_er_nfc_config_token <WPS/NDEF> <UUID|BSSID>" command can be
used to build an NFC configuration token for the specified AP when
wpa_supplicant is operating as a WPS ER. The output value from this
command is a hexdump of the selected AP configuration (WPS parameter
requests this to include only the WPS attributes; NDEF parameter
requests additional NDEF encapsulation to be included). This data needs
to be written to an NFC tag with an external program. Once written, the
NFC configuration token can be used to touch an NFC interface on a
station to provision the credentials needed to access the network.
wpa_supplicant for Windows
==========================

Copyright (c) 2003-2009, Jouni Malinen <j@w1.fi> and contributors
All Rights Reserved.

This program is licensed under the BSD license (the one with
advertisement clause removed).


wpa_supplicant has support for being used as a WPA/WPA2/IEEE 802.1X
Supplicant on Windows. The current port requires that WinPcap
(http://winpcap.polito.it/) is installed for accessing packets and the
driver interface. Both release versions 3.0 and 3.1 are supported.

The current port is still somewhat experimental. It has been tested
mainly on Windows XP (SP2) with limited set of NDIS drivers. In
addition, the current version has been reported to work with Windows
2000.

All security modes have been verified to work (at least complete
authentication and successfully ping a wired host):
- plaintext
- static WEP / open system authentication
- static WEP / shared key authentication
- IEEE 802.1X with dynamic WEP keys
- WPA-PSK, TKIP, CCMP, TKIP+CCMP
- WPA-EAP, TKIP, CCMP, TKIP+CCMP
- WPA2-PSK, TKIP, CCMP, TKIP+CCMP
- WPA2-EAP, TKIP, CCMP, TKIP+CCMP


Building wpa_supplicant with mingw
----------------------------------

The default build setup for wpa_supplicant is to use MinGW and
cross-compiling from Linux to MinGW/Windows. It should also be
possible to build this under Windows using the MinGW tools, but that
is not tested nor supported and is likely to require some changes to
the Makefile unless cygwin is used.


Building wpa_supplicant with MSVC
---------------------------------

wpa_supplicant can be built with Microsoft Visual C++ compiler. This
has been tested with Microsoft Visual C++ Toolkit 2003 and Visual
Studio 2005 using the included nmake.mak as a Makefile for nmake. IDE
can also be used by creating a project that includes the files and
defines mentioned in nmake.mak. Example VS2005 solution and project
files are included in vs2005 subdirectory. This can be used as a
starting point for building the programs with VS2005 IDE. Visual Studio
2008 Express Edition is also able to use these project files.

WinPcap development package is needed for the build and this can be
downloaded from http://www.winpcap.org/install/bin/WpdPack_4_0_2.zip. The
default nmake.mak expects this to be unpacked into C:\dev\WpdPack so
that Include and Lib directories are in this directory. The files can be
stored elsewhere as long as the WINPCAPDIR in nmake.mak is updated to
match with the selected directory. In case a project file in the IDE is
used, these Include and Lib directories need to be added to project
properties as additional include/library directories.

OpenSSL source package can be downloaded from
http://www.openssl.org/source/openssl-0.9.8i.tar.gz and built and
installed following instructions in INSTALL.W32. Note that if EAP-FAST
support will be included in the wpa_supplicant, OpenSSL needs to be
patched to# support it openssl-0.9.8i-tls-extensions.patch. The example
nmake.mak file expects OpenSSL to be installed into C:\dev\openssl, but
this directory can be modified by changing OPENSSLDIR variable in
nmake.mak.

If you do not need EAP-FAST support, you may also be able to use Win32
binary installation package of OpenSSL from
http://www.slproweb.com/products/Win32OpenSSL.html instead of building
the library yourself. In this case, you will need to copy Include and
Lib directories in suitable directory, e.g., C:\dev\openssl for the
default nmake.mak. Copy {Win32OpenSSLRoot}\include into
C:\dev\openssl\include and make C:\dev\openssl\lib subdirectory with
files from {Win32OpenSSLRoot}\VC (i.e., libeay*.lib and ssleay*.lib).
This will end up using dynamically linked OpenSSL (i.e., .dll files are
needed) for it. Alternative, you can copy files from
{Win32OpenSSLRoot}\VC\static to create a static build (no OpenSSL .dll
files needed).


Building wpa_supplicant for cygwin
----------------------------------

wpa_supplicant can be built for cygwin by installing the needed
development packages for cygwin. This includes things like compiler,
make, openssl development package, etc. In addition, developer's pack
for WinPcap (WPdpack.zip) from
http://winpcap.polito.it/install/default.htm is needed.

.config file should enable only one driver interface,
CONFIG_DRIVER_NDIS. In addition, include directories may need to be
added to match the system. An example configuration is available in
defconfig. The library and include files for WinPcap will either need
to be installed in compiler/linker default directories or their
location will need to be adding to .config when building
wpa_supplicant.

Othen than this, the build should be more or less identical to Linux
version, i.e., just run make after having created .config file. An
additional tool, win_if_list.exe, can be built by running "make
win_if_list".


Building wpa_gui
----------------

wpa_gui uses Qt application framework from Trolltech. It can be built
with the open source version of Qt4 and MinGW. Following commands can
be used to build the binary in the Qt 4 Command Prompt:

# go to the root directory of wpa_supplicant source code
cd wpa_gui-qt4
qmake -o Makefile wpa_gui.pro
make
# the wpa_gui.exe binary is created into 'release' subdirectory


Using wpa_supplicant for Windows
--------------------------------

wpa_supplicant, wpa_cli, and wpa_gui behave more or less identically to
Linux version, so instructions in README and example wpa_supplicant.conf
should be applicable for most parts. In addition, there is another
version of wpa_supplicant, wpasvc.exe, which can be used as a Windows
service and which reads its configuration from registry instead of
text file.

When using access points in "hidden SSID" mode, ap_scan=2 mode need to
be used (see wpa_supplicant.conf for more information).

Windows NDIS/WinPcap uses quite long interface names, so some care
will be needed when starting wpa_supplicant. Alternatively, the
adapter description can be used as the interface name which may be
easier since it is usually in more human-readable
format. win_if_list.exe can be used to find out the proper interface
name.

Example steps in starting up wpa_supplicant:

# win_if_list.exe
ifname: \Device\NPF_GenericNdisWanAdapter
description: Generic NdisWan adapter

ifname: \Device\NPF_{769E012B-FD17-4935-A5E3-8090C38E25D2}
description: Atheros Wireless Network Adapter (Microsoft's Packet Scheduler)

ifname: \Device\NPF_{732546E7-E26C-48E3-9871-7537B020A211}
description: Intel 8255x-based Integrated Fast Ethernet (Microsoft's Packet Scheduler)


Since the example configuration used Atheros WLAN card, the middle one
is the correct interface in this case. The interface name for -i
command line option is the full string following "ifname:" (the
"\Device\NPF_" prefix can be removed). In other words, wpa_supplicant
would be started with the following command:

# wpa_supplicant.exe -i'{769E012B-FD17-4935-A5E3-8090C38E25D2}' -c wpa_supplicant.conf -d

-d optional enables some more debugging (use -dd for even more, if
needed). It can be left out if debugging information is not needed.

With the alternative mechanism for selecting the interface, this
command has identical results in this case:

# wpa_supplicant.exe -iAtheros -c wpa_supplicant.conf -d


Simple configuration example for WPA-PSK:

#ap_scan=2
ctrl_interface=
network={
	ssid="test"
	key_mgmt=WPA-PSK
	proto=WPA
	pairwise=TKIP
	psk="secret passphrase"
}

(remove '#' from the comment out ap_scan line to enable mode in which
wpa_supplicant tries to associate with the SSID without doing
scanning; this allows APs with hidden SSIDs to be used)


wpa_cli.exe and wpa_gui.exe can be used to interact with the
wpa_supplicant.exe program in the same way as with Linux. Note that
ctrl_interface is using UNIX domain sockets when built for cygwin, but
the native build for Windows uses named pipes and the contents of the
ctrl_interface configuration item is used to control access to the
interface. Anyway, this variable has to be included in the configuration
to enable the control interface.


Example SDDL string formats:

(local admins group has permission, but nobody else):

ctrl_interface=SDDL=D:(A;;GA;;;BA)

("A" == "access allowed", "GA" == GENERIC_ALL == all permissions, and
"BA" == "builtin administrators" == the local admins.  The empty fields
are for flags and object GUIDs, none of which should be required in this
case.)

(local admins and the local "power users" group have permissions,
but nobody else):

ctrl_interface=SDDL=D:(A;;GA;;;BA)(A;;GA;;;PU)

(One ACCESS_ALLOWED ACE for GENERIC_ALL for builtin administrators, and
one ACCESS_ALLOWED ACE for GENERIC_ALL for power users.)

(close to wide open, but you have to be a valid user on
the machine):

ctrl_interface=SDDL=D:(A;;GA;;;AU)

(One ACCESS_ALLOWED ACE for GENERIC_ALL for the "authenticated users"
group.)

This one would allow absolutely everyone (including anonymous
users) -- this is *not* recommended, since named pipes can be attached
to from anywhere on the network (i.e. there's no "this machine only"
like there is with 127.0.0.1 sockets):

ctrl_interface=SDDL=D:(A;;GA;;;BU)(A;;GA;;;AN)

(BU == "builtin users", "AN" == "anonymous")

See also [1] for the format of ACEs, and [2] for the possible strings
that can be used for principal names.

[1]
http://msdn.microsoft.com/library/default.asp?url=/library/en-us/secauthz/security/ace_strings.asp
[2]
http://msdn.microsoft.com/library/default.asp?url=/library/en-us/secauthz/security/sid_strings.asp


Starting wpa_supplicant as a Windows service (wpasvc.exe)
---------------------------------------------------------

wpa_supplicant can be started as a Windows service by using wpasvc.exe
program that is alternative build of wpa_supplicant.exe. Most of the
core functionality of wpasvc.exe is identical to wpa_supplicant.exe,
but it is using Windows registry for configuration information instead
of a text file and command line parameters. In addition, it can be
registered as a service that can be started automatically or manually
like any other Windows service.

The root of wpa_supplicant configuration in registry is
HKEY_LOCAL_MACHINE\SOFTWARE\wpa_supplicant. This level includes global
parameters and a 'interfaces' subkey with all the interface configuration
(adapter to confname mapping). Each such mapping is a subkey that has
'adapter', 'config', and 'ctrl_interface' values.

This program can be run either as a normal command line application,
e.g., for debugging, with 'wpasvc.exe app' or as a Windows service.
Service need to be registered with 'wpasvc.exe reg <full path to
wpasvc.exe>'. Alternatively, 'wpasvc.exe reg' can be used to register
the service with the current location of wpasvc.exe. After this, wpasvc
can be started like any other Windows service (e.g., 'net start wpasvc')
or it can be configured to start automatically through the Services tool
in administrative tasks. The service can be unregistered with
'wpasvc.exe unreg'.

If the service is set to start during system bootup to make the
network connection available before any user has logged in, there may
be a long (half a minute or so) delay in starting up wpa_supplicant
due to WinPcap needing a driver called "Network Monitor Driver" which
is started by default on demand.

To speed up wpa_supplicant start during system bootup, "Network
Monitor Driver" can be configured to be started sooner by setting its
startup type to System instead of the default Demand. To do this, open
up Device Manager, select Show Hidden Devices, expand the "Non
Plug-and-Play devices" branch, double click "Network Monitor Driver",
go to the Driver tab, and change the Demand setting to System instead.

Configuration data is in HKEY_LOCAL_MACHINE\SOFTWARE\wpa_supplicant\configs
key. Each configuration profile has its own key under this. In terms of text
files, each profile would map to a separate text file with possibly multiple
networks. Under each profile, there is a networks key that lists all
networks as a subkey. Each network has set of values in the same way as
network block in the configuration file. In addition, blobs subkey has
possible blobs as values.

HKEY_LOCAL_MACHINE\SOFTWARE\wpa_supplicant\configs\test\networks\0000
   ssid="example"
   key_mgmt=WPA-PSK

See win_example.reg for an example on how to setup wpasvc.exe
parameters in registry. It can also be imported to registry as a
starting point for the configuration.
wpa_supplicant and Hotspot 2.0
==============================

This document describe how the IEEE 802.11u Interworking and Wi-Fi
Hotspot 2.0 (Release 1) implementation in wpa_supplicant can be
configured and how an external component on the client e.g., management
GUI or Wi-Fi framework) is used to manage this functionality.


Introduction to Wi-Fi Hotspot 2.0
---------------------------------

Hotspot 2.0 is the name of the Wi-Fi Alliance specification that is used
in the Wi-Fi CERTIFIED Passpoint<TM> program. More information about
this is available in this white paper:

http://www.wi-fi.org/knowledge-center/white-papers/wi-fi-certified-passpoint%E2%84%A2-new-program-wi-fi-alliance%C2%AE-enable-seamless

The Hotspot 2.0 specification is also available from WFA:
https://www.wi-fi.org/knowledge-center/published-specifications

The core Interworking functionality (network selection, GAS/ANQP) were
standardized in IEEE Std 802.11u-2011 which is now part of the IEEE Std
802.11-2012.


wpa_supplicant network selection
--------------------------------

Interworking support added option for configuring credentials that can
work with multiple networks as an alternative to configuration of
network blocks (e.g., per-SSID parameters). When requested to perform
network selection, wpa_supplicant picks the highest priority enabled
network block or credential. If a credential is picked (based on ANQP
information from APs), a temporary network block is created
automatically for the matching network. This temporary network block is
used similarly to the network blocks that can be configured by the user,
but it is not stored into the configuration file and is meant to be used
only for temporary period of time since a new one can be created
whenever needed based on ANQP information and the credential.

By default, wpa_supplicant is not using automatic network selection
unless requested explicitly with the interworking_select command. This
can be changed with the auto_interworking=1 parameter to perform network
selection automatically whenever trying to find a network for connection
and none of the enabled network blocks match with the scan results. This
case works similarly to "interworking_select auto", i.e., wpa_supplicant
will internally determine which network or credential is going to be
used based on configured priorities, scan results, and ANQP information.


wpa_supplicant configuration
----------------------------

Interworking and Hotspot 2.0 functionality are optional components that
need to be enabled in the wpa_supplicant build configuration
(.config). This is done by adding following parameters into that file:

CONFIG_INTERWORKING=y
CONFIG_HS20=y

It should be noted that this functionality requires a driver that
supports GAS/ANQP operations. This uses the same design as P2P, i.e.,
Action frame processing and building in user space within
wpa_supplicant. The Linux nl80211 driver interface provides the needed
functionality for this.


There are number of run-time configuration parameters (e.g., in
wpa_supplicant.conf when using the configuration file) that can be used
to control Hotspot 2.0 operations.

# Enable Interworking
interworking=1

# Enable Hotspot 2.0
hs20=1

# Parameters for controlling scanning

# Homogenous ESS identifier
# If this is set, scans will be used to request response only from BSSes
# belonging to the specified Homogeneous ESS. This is used only if interworking
# is enabled.
#hessid=00:11:22:33:44:55

# Access Network Type
# When Interworking is enabled, scans can be limited to APs that advertise the
# specified Access Network Type (0..15; with 15 indicating wildcard match).
# This value controls the Access Network Type value in Probe Request frames.
#access_network_type=15

# Automatic network selection behavior
# 0 = do not automatically go through Interworking network selection
#     (i.e., require explicit interworking_select command for this; default)
# 1 = perform Interworking network selection if one or more
#     credentials have been configured and scan did not find a
#     matching network block
#auto_interworking=0


Credentials can be pre-configured for automatic network selection:

# credential block
#
# Each credential used for automatic network selection is configured as a set
# of parameters that are compared to the information advertised by the APs when
# interworking_select and interworking_connect commands are used.
#
# credential fields:
#
# temporary: Whether this credential is temporary and not to be saved
#
# priority: Priority group
#	By default, all networks and credentials get the same priority group
#	(0). This field can be used to give higher priority for credentials
#	(and similarly in struct wpa_ssid for network blocks) to change the
#	Interworking automatic networking selection behavior. The matching
#	network (based on either an enabled network block or a credential)
#	with the highest priority value will be selected.
#
# pcsc: Use PC/SC and SIM/USIM card
#
# realm: Home Realm for Interworking
#
# username: Username for Interworking network selection
#
# password: Password for Interworking network selection
#
# ca_cert: CA certificate for Interworking network selection
#
# client_cert: File path to client certificate file (PEM/DER)
#	This field is used with Interworking networking selection for a case
#	where client certificate/private key is used for authentication
#	(EAP-TLS). Full path to the file should be used since working
#	directory may change when wpa_supplicant is run in the background.
#
#	Alternatively, a named configuration blob can be used by setting
#	this to blob://blob_name.
#
# private_key: File path to client private key file (PEM/DER/PFX)
#	When PKCS#12/PFX file (.p12/.pfx) is used, client_cert should be
#	commented out. Both the private key and certificate will be read
#	from the PKCS#12 file in this case. Full path to the file should be
#	used since working directory may change when wpa_supplicant is run
#	in the background.
#
#	Windows certificate store can be used by leaving client_cert out and
#	configuring private_key in one of the following formats:
#
#	cert://substring_to_match
#
#	hash://certificate_thumbprint_in_hex
#
#	For example: private_key="hash://63093aa9c47f56ae88334c7b65a4"
#
#	Note that when running wpa_supplicant as an application, the user
#	certificate store (My user account) is used, whereas computer store
#	(Computer account) is used when running wpasvc as a service.
#
#	Alternatively, a named configuration blob can be used by setting
#	this to blob://blob_name.
#
# private_key_passwd: Password for private key file
#
# imsi: IMSI in <MCC> | <MNC> | '-' | <MSIN> format
#
# milenage: Milenage parameters for SIM/USIM simulator in <Ki>:<OPc>:<SQN>
#	format
#
# domain_suffix_match: Constraint for server domain name
#	If set, this FQDN is used as a suffix match requirement for the AAA
#	server certificate in SubjectAltName dNSName element(s). If a
#	matching dNSName is found, this constraint is met. If no dNSName
#	values are present, this constraint is matched against SubjectName CN
#	using same suffix match comparison. Suffix match here means that the
#	host/domain name is compared one label at a time starting from the
#	top-level domain and all the labels in @domain_suffix_match shall be
#	included in the certificate. The certificate may include additional
#	sub-level labels in addition to the required labels.
#
#	For example, domain_suffix_match=example.com would match
#	test.example.com but would not match test-example.com.
#
# domain: Home service provider FQDN(s)
#	This is used to compare against the Domain Name List to figure out
#	whether the AP is operated by the Home SP. Multiple domain entries can
#	be used to configure alternative FQDNs that will be considered home
#	networks.
#
# roaming_consortium: Roaming Consortium OI
#	If roaming_consortium_len is non-zero, this field contains the
#	Roaming Consortium OI that can be used to determine which access
#	points support authentication with this credential. This is an
#	alternative to the use of the realm parameter. When using Roaming
#	Consortium to match the network, the EAP parameters need to be
#	pre-configured with the credential since the NAI Realm information
#	may not be available or fetched.
#
# eap: Pre-configured EAP method
#	This optional field can be used to specify which EAP method will be
#	used with this credential. If not set, the EAP method is selected
#	automatically based on ANQP information (e.g., NAI Realm).
#
# phase1: Pre-configure Phase 1 (outer authentication) parameters
#	This optional field is used with like the 'eap' parameter.
#
# phase2: Pre-configure Phase 2 (inner authentication) parameters
#	This optional field is used with like the 'eap' parameter.
#
# excluded_ssid: Excluded SSID
#	This optional field can be used to excluded specific SSID(s) from
#	matching with the network. Multiple entries can be used to specify more
#	than one SSID.
#
# roaming_partner: Roaming partner information
#	This optional field can be used to configure preferences between roaming
#	partners. The field is a string in following format:
#	<FQDN>,<0/1 exact match>,<priority>,<* or country code>
#	(non-exact match means any subdomain matches the entry; priority is in
#	0..255 range with 0 being the highest priority)
#
# update_identifier: PPS MO ID
#	(Hotspot 2.0 PerProviderSubscription/UpdateIdentifier)
#
# provisioning_sp: FQDN of the SP that provisioned the credential
#	This optional field can be used to keep track of the SP that provisioned
#	the credential to find the PPS MO (./Wi-Fi/<provisioning_sp>).
#
# sp_priority: Credential priority within a provisioning SP
#	This is the priority of the credential among all credentials
#	provisioned by the same SP (i.e., for entries that have identical
#	provisioning_sp value). The range of this priority is 0-255 with 0
#	being the highest and 255 the lower priority.
#
# Minimum backhaul threshold (PPS/<X+>/Policy/MinBackhauldThreshold/*)
#	These fields can be used to specify minimum download/upload backhaul
#	bandwidth that is preferred for the credential. This constraint is
#	ignored if the AP does not advertise WAN Metrics information or if the
#	limit would prevent any connection. Values are in kilobits per second.
# min_dl_bandwidth_home
# min_ul_bandwidth_home
# min_dl_bandwidth_roaming
# min_ul_bandwidth_roaming
#
# max_bss_load: Maximum BSS Load Channel Utilization (1..255)
#	(PPS/<X+>/Policy/MaximumBSSLoadValue)
#	This value is used as the maximum channel utilization for network
#	selection purposes for home networks. If the AP does not advertise
#	BSS Load or if the limit would prevent any connection, this constraint
#	will be ignored.
#
# req_conn_capab: Required connection capability
#	(PPS/<X+>/Policy/RequiredProtoPortTuple)
#	This value is used to configure set of required protocol/port pairs that
#	a roaming network shall support (include explicitly in Connection
#	Capability ANQP element). This constraint is ignored if the AP does not
#	advertise Connection Capability or if this constraint would prevent any
#	network connection. This policy is not used in home networks.
#	Format: <protocol>[:<comma-separated list of ports]
#	Multiple entries can be used to list multiple requirements.
#	For example, number of common TCP protocols:
#	req_conn_capab=6:22,80,443
#	For example, IPSec/IKE:
#	req_conn_capab=17:500
#	req_conn_capab=50
#
# ocsp: Whether to use/require OCSP to check server certificate
#	0 = do not use OCSP stapling (TLS certificate status extension)
#	1 = try to use OCSP stapling, but not require response
#	2 = require valid OCSP stapling response
#
# sim_num: Identifier for which SIM to use in multi-SIM devices
#
# for example:
#
#cred={
#	realm="example.com"
#	username="user@example.com"
#	password="password"
#	ca_cert="/etc/wpa_supplicant/ca.pem"
#	domain="example.com"
#	domain_suffix_match="example.com"
#}
#
#cred={
#	imsi="310026-000000000"
#	milenage="90dca4eda45b53cf0f12d7c9c3bc6a89:cb9cccc4b9258e6dca4760379fb82"
#}
#
#cred={
#	realm="example.com"
#	username="user"
#	password="password"
#	ca_cert="/etc/wpa_supplicant/ca.pem"
#	domain="example.com"
#	roaming_consortium=223344
#	eap=TTLS
#	phase2="auth=MSCHAPV2"
#}


Control interface
-----------------

wpa_supplicant provides a control interface that can be used from
external programs to manage various operations. The included command
line tool, wpa_cli, can be used for manual testing with this interface.

Following wpa_cli interactive mode commands show some examples of manual
operations related to Hotspot 2.0:

Remove configured networks and credentials:

> remove_network all
OK
> remove_cred all
OK


Add a username/password credential:

> add_cred
0
> set_cred 0 realm "mail.example.com"
OK
> set_cred 0 username "username"
OK
> set_cred 0 password "password"
OK
> set_cred 0 priority 1
OK
> set_cred 0 temporary 1
OK

Add a SIM credential using a simulated SIM/USIM card for testing:

> add_cred
1
> set_cred 1 imsi "23456-0000000000"
OK
> set_cred 1 milenage "90dca4eda45b53cf0f12d7c9c3bc6a89:cb9cccc4b9258e6dca4760379fb82581:000000000123"
OK
> set_cred 1 priority 1
OK

Note: the return value of add_cred is used as the first argument to
the following set_cred commands.

Add a SIM credential using a external SIM/USIM processing:

> set external_sim 1
OK
> add_cred
1
> set_cred 1 imsi "23456-0000000000"
OK
> set_cred 1 eap SIM
OK


Add a WPA2-Enterprise network:

> add_network
0
> set_network 0 key_mgmt WPA-EAP
OK
> set_network 0 ssid "enterprise"
OK
> set_network 0 eap TTLS
OK
> set_network 0 anonymous_identity "anonymous"
OK
> set_network 0 identity "user"
OK
> set_network 0 password "password"
OK
> set_network 0 priority 0
OK
> enable_network 0 no-connect
OK


Add an open network:

> add_network
3
> set_network 3 key_mgmt NONE
OK
> set_network 3 ssid "coffee-shop"
OK
> select_network 3
OK

Note: the return value of add_network is used as the first argument to
the following set_network commands.

The preferred credentials/networks can be indicated with the priority
parameter (1 is higher priority than 0).


Interworking network selection can be started with interworking_select
command. This instructs wpa_supplicant to run a network scan and iterate
through the discovered APs to request ANQP information from the APs that
advertise support for Interworking/Hotspot 2.0:

> interworking_select
OK
<3>Starting ANQP fetch for 02:00:00:00:01:00
<3>RX-ANQP 02:00:00:00:01:00 ANQP Capability list
<3>RX-ANQP 02:00:00:00:01:00 Roaming Consortium list
<3>RX-HS20-ANQP 02:00:00:00:01:00 HS Capability List
<3>ANQP fetch completed
<3>INTERWORKING-AP 02:00:00:00:01:00 type=unknown


INTERWORKING-AP event messages indicate the APs that support network
selection and for which there is a matching
credential. interworking_connect command can be used to select a network
to connect with:


> interworking_connect 02:00:00:00:01:00
OK
<3>CTRL-EVENT-SCAN-RESULTS
<3>SME: Trying to authenticate with 02:00:00:00:01:00 (SSID='Example Network' freq=2412 MHz)
<3>Trying to associate with 02:00:00:00:01:00 (SSID='Example Network' freq=2412 MHz)
<3>Associated with 02:00:00:00:01:00
<3>CTRL-EVENT-EAP-STARTED EAP authentication started
<3>CTRL-EVENT-EAP-PROPOSED-METHOD vendor=0 method=21
<3>CTRL-EVENT-EAP-METHOD EAP vendor 0 method 21 (TTLS) selected
<3>CTRL-EVENT-EAP-SUCCESS EAP authentication completed successfully
<3>WPA: Key negotiation completed with 02:00:00:00:01:00 [PTK=CCMP GTK=CCMP]
<3>CTRL-EVENT-CONNECTED - Connection to 02:00:00:00:01:00 completed (auth) [id=0 id_str=]


wpa_supplicant creates a temporary network block for the selected
network based on the configured credential and ANQP information from the
AP:

> list_networks
network id / ssid / bssid / flags
0	Example Network	any	[CURRENT]
> get_network 0 key_mgmt
WPA-EAP
> get_network 0 eap
TTLS


Alternatively to using an external program to select the network,
"interworking_select auto" command can be used to request wpa_supplicant
to select which network to use based on configured priorities:


> remove_network all
OK
<3>CTRL-EVENT-DISCONNECTED bssid=02:00:00:00:01:00 reason=1 locally_generated=1
> interworking_select auto
OK
<3>Starting ANQP fetch for 02:00:00:00:01:00
<3>RX-ANQP 02:00:00:00:01:00 ANQP Capability list
<3>RX-ANQP 02:00:00:00:01:00 Roaming Consortium list
<3>RX-HS20-ANQP 02:00:00:00:01:00 HS Capability List
<3>ANQP fetch completed
<3>INTERWORKING-AP 02:00:00:00:01:00 type=unknown
<3>CTRL-EVENT-SCAN-RESULTS
<3>SME: Trying to authenticate with 02:00:00:00:01:00 (SSID='Example Network' freq=2412 MHz)
<3>Trying to associate with 02:00:00:00:01:00 (SSID='Example Network' freq=2412 MHz)
<3>Associated with 02:00:00:00:01:00
<3>CTRL-EVENT-EAP-STARTED EAP authentication started
<3>CTRL-EVENT-EAP-PROPOSED-METHOD vendor=0 method=21
<3>CTRL-EVENT-EAP-METHOD EAP vendor 0 method 21 (TTLS) selected
<3>CTRL-EVENT-EAP-SUCCESS EAP authentication completed successfully
<3>WPA: Key negotiation completed with 02:00:00:00:01:00 [PTK=CCMP GTK=CCMP]
<3>CTRL-EVENT-CONNECTED - Connection to 02:00:00:00:01:00 completed (reauth) [id=0 id_str=]


The connection status can be shown with the status command:

> status
bssid=02:00:00:00:01:00
ssid=Example Network
id=0
mode=station
pairwise_cipher=CCMP       <--- link layer security indication
group_cipher=CCMP
key_mgmt=WPA2/IEEE 802.1X/EAP
wpa_state=COMPLETED
p2p_device_address=02:00:00:00:00:00
address=02:00:00:00:00:00
hs20=1      <--- HS 2.0 indication
Supplicant PAE state=AUTHENTICATED
suppPortStatus=Authorized
EAP state=SUCCESS
selectedMethod=21 (EAP-TTLS)
EAP TLS cipher=AES-128-SHA
EAP-TTLSv0 Phase2 method=PAP


> status
bssid=02:00:00:00:02:00
ssid=coffee-shop
id=3
mode=station
pairwise_cipher=NONE
group_cipher=NONE
key_mgmt=NONE
wpa_state=COMPLETED
p2p_device_address=02:00:00:00:00:00
address=02:00:00:00:00:00


Note: The Hotspot 2.0 indication is shown as "hs20=1" in the status
command output. Link layer security is indicated with the
pairwise_cipher (CCMP = secure, NONE = no encryption used).


Also the scan results include the Hotspot 2.0 indication:

> scan_results
bssid / frequency / signal level / flags / ssid
02:00:00:00:01:00	2412	-30	[WPA2-EAP-CCMP][ESS][HS20]	Example Network


ANQP information for the BSS can be fetched using the BSS command:

> bss 02:00:00:00:01:00
id=1
bssid=02:00:00:00:01:00
freq=2412
beacon_int=100
capabilities=0x0411
qual=0
noise=-92
level=-30
tsf=1345573286517276
age=105
ie=000f4578616d706c65204e6574776f726b010882848b960c1218240301012a010432043048606c30140100000fac040100000fac040100000fac0100007f04000000806b091e07010203040506076c027f006f1001531122331020304050010203040506dd05506f9a1000
flags=[WPA2-EAP-CCMP][ESS][HS20]
ssid=Example Network
anqp_roaming_consortium=031122330510203040500601020304050603fedcba


ANQP queries can also be requested with the anqp_get and hs20_anqp_get
commands:

> anqp_get 02:00:00:00:01:00 261
OK
<3>RX-ANQP 02:00:00:00:01:00 Roaming Consortium list
> hs20_anqp_get 02:00:00:00:01:00 2
OK
<3>RX-HS20-ANQP 02:00:00:00:01:00 HS Capability List

In addition, fetch_anqp command can be used to request similar set of
ANQP queries to be done as is run as part of interworking_select:

> scan
OK
<3>CTRL-EVENT-SCAN-RESULTS
> fetch_anqp
OK
<3>Starting ANQP fetch for 02:00:00:00:01:00
<3>RX-ANQP 02:00:00:00:01:00 ANQP Capability list
<3>RX-ANQP 02:00:00:00:01:00 Roaming Consortium list
<3>RX-HS20-ANQP 02:00:00:00:01:00 HS Capability List
<3>ANQP fetch completed


Hotspot 2.0 Rel 2 online signup and OSEN
----------------------------------------

Following parameters can be used to create a network profile for
link-layer protected Hotspot 2.0 online signup connection with
OSEN. Note that ssid and identify (NAI) values need to be set based on
the information for the selected provider in the OSU Providers list
ANQP-element.

network={
    ssid="HS 2.0 OSU"
    proto=OSEN
    key_mgmt=OSEN
    pairwise=CCMP
    group=GTK_NOT_USED
    eap=WFA-UNAUTH-TLS
    identity="anonymous@example.com"
    ca_cert="osu-ca.pem"
    ocsp=2
}


Hotspot 2.0 connection with external network selection
------------------------------------------------------

When an component controlling wpa_supplicant takes care of Interworking
network selection, following configuration and network profile
parameters can be used to configure a temporary network profile for a
Hotspot 2.0 connection (e.g., with SET, ADD_NETWORK, SET_NETWORK, and
SELECT_NETWORK control interface commands):

interworking=1
hs20=1
auto_interworking=0

network={
    ssid="test-hs20"
    proto=RSN
    key_mgmt=WPA-EAP
    pairwise=CCMP
    anonymous_identity="anonymous@example.com"
    identity="hs20-test@example.com"
    password="password"
    ca_cert="ca.pem"
    eap=TTLS
    phase2="auth=MSCHAPV2"
    update_identifier=54321
    #ocsp=2
}


These parameters are set based on the PPS MO credential and/or NAI Realm
list ANQP-element:

anonymous_identity: Credential/UsernamePassword/Username with username part
		    replaced with "anonymous"
identity: Credential/UsernamePassword/Username
password: Credential/UsernamePassword/Password
update_identifier: PPS/UpdateIdentifier
ca_cert: from the downloaded trust root based on PPS information
eap: Credential/UsernamePassword/EAPMethod or NAI Realm list
phase2: Credential/UsernamePassword/EAPMethod or NAI Realm list
ocsp: Credential/CheckAAAServerCertStatus
WPA Supplicant
==============

Copyright (c) 2003-2016, Jouni Malinen <j@w1.fi> and contributors
All Rights Reserved.

This program is licensed under the BSD license (the one with
advertisement clause removed).

If you are submitting changes to the project, please see CONTRIBUTIONS
file for more instructions.



License
-------

This software may be distributed, used, and modified under the terms of
BSD license:

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

3. Neither the name(s) of the above-listed copyright holder(s) nor the
   names of its contributors may be used to endorse or promote products
   derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



Features
--------

Supported WPA/IEEE 802.11i features:
- WPA-PSK ("WPA-Personal")
- WPA with EAP (e.g., with RADIUS authentication server) ("WPA-Enterprise")
  Following authentication methods are supported with an integrate IEEE 802.1X
  Supplicant:
  * EAP-TLS
  * EAP-PEAP/MSCHAPv2 (both PEAPv0 and PEAPv1)
  * EAP-PEAP/TLS (both PEAPv0 and PEAPv1)
  * EAP-PEAP/GTC (both PEAPv0 and PEAPv1)
  * EAP-PEAP/OTP (both PEAPv0 and PEAPv1)
  * EAP-PEAP/MD5-Challenge (both PEAPv0 and PEAPv1)
  * EAP-TTLS/EAP-MD5-Challenge
  * EAP-TTLS/EAP-GTC
  * EAP-TTLS/EAP-OTP
  * EAP-TTLS/EAP-MSCHAPv2
  * EAP-TTLS/EAP-TLS
  * EAP-TTLS/MSCHAPv2
  * EAP-TTLS/MSCHAP
  * EAP-TTLS/PAP
  * EAP-TTLS/CHAP
  * EAP-SIM
  * EAP-AKA
  * EAP-AKA'
  * EAP-PSK
  * EAP-PAX
  * EAP-SAKE
  * EAP-IKEv2
  * EAP-GPSK
  * EAP-pwd
  * LEAP (note: requires special support from the driver for IEEE 802.11
	  authentication)
  (following methods are supported, but since they do not generate keying
   material, they cannot be used with WPA or IEEE 802.1X WEP keying)
  * EAP-MD5-Challenge 
  * EAP-MSCHAPv2
  * EAP-GTC
  * EAP-OTP
- key management for CCMP, TKIP, WEP104, WEP40
- RSN/WPA2 (IEEE 802.11i)
  * pre-authentication
  * PMKSA caching

Supported TLS/crypto libraries:
- OpenSSL (default)
- GnuTLS

Internal TLS/crypto implementation (optional):
- can be used in place of an external TLS/crypto library
- TLSv1
- X.509 certificate processing
- PKCS #1
- ASN.1
- RSA
- bignum
- minimal size (ca. 50 kB binary, parts of which are already needed for WPA;
  TLSv1/X.509/ASN.1/RSA/bignum parts are about 25 kB on x86)


Requirements
------------

Current hardware/software requirements:
- Linux kernel 2.4.x or 2.6.x with Linux Wireless Extensions v15 or newer
- FreeBSD 6-CURRENT
- NetBSD-current
- Microsoft Windows with WinPcap (at least WinXP, may work with other versions)
- drivers:
	Linux drivers that support cfg80211/nl80211. Even though there are
	number of driver specific interface included in wpa_supplicant, please
	note that Linux drivers are moving to use generic wireless configuration
	interface driver_nl80211 (-Dnl80211 on wpa_supplicant command line)
	should be the default option to start with before falling back to driver
	specific interface.

	Linux drivers that support WPA/WPA2 configuration with the generic
	Linux wireless extensions (WE-18 or newer). Obsoleted by nl80211.

	In theory, any driver that supports Linux wireless extensions can be
	used with IEEE 802.1X (i.e., not WPA) when using ap_scan=0 option in
	configuration file.

	Wired Ethernet drivers (with ap_scan=0)

	BSD net80211 layer (e.g., Atheros driver)
	At the moment, this is for FreeBSD 6-CURRENT branch and NetBSD-current.

	Windows NDIS
	The current Windows port requires WinPcap (http://winpcap.polito.it/).
	See README-Windows.txt for more information.

wpa_supplicant was designed to be portable for different drivers and
operating systems. Hopefully, support for more wlan cards and OSes will be
added in the future. See developer's documentation
(http://hostap.epitest.fi/wpa_supplicant/devel/) for more information about the
design of wpa_supplicant and porting to other drivers. One main goal
is to add full WPA/WPA2 support to Linux wireless extensions to allow
new drivers to be supported without having to implement new
driver-specific interface code in wpa_supplicant.

Optional libraries for layer2 packet processing:
- libpcap (tested with 0.7.2, most relatively recent versions assumed to work,
	this is likely to be available with most distributions,
	http://tcpdump.org/)
- libdnet (tested with v1.4, most versions assumed to work,
	http://libdnet.sourceforge.net/)

These libraries are _not_ used in the default Linux build. Instead,
internal Linux specific implementation is used. libpcap/libdnet are
more portable and they can be used by adding CONFIG_L2_PACKET=pcap into
.config. They may also be selected automatically for other operating
systems. In case of Windows builds, WinPcap is used by default
(CONFIG_L2_PACKET=winpcap).


Optional libraries for EAP-TLS, EAP-PEAP, and EAP-TTLS:
- OpenSSL (tested with 1.0.1 and 1.0.2 versions; assumed to
  work with most relatively recent versions; this is likely to be
  available with most distributions, http://www.openssl.org/)
- GnuTLS
- internal TLSv1 implementation

One of these libraries is needed when EAP-TLS, EAP-PEAP, EAP-TTLS, or
EAP-FAST support is enabled. WPA-PSK mode does not require this or EAPOL/EAP
implementation. A configuration file, .config, for compilation is
needed to enable IEEE 802.1X/EAPOL and EAP methods. Note that EAP-MD5,
EAP-GTC, EAP-OTP, and EAP-MSCHAPV2 cannot be used alone with WPA, so
they should only be enabled if testing the EAPOL/EAP state
machines. However, there can be used as inner authentication
algorithms with EAP-PEAP and EAP-TTLS.

See Building and installing section below for more detailed
information about the wpa_supplicant build time configuration.



WPA
---

The original security mechanism of IEEE 802.11 standard was not
designed to be strong and has proven to be insufficient for most
networks that require some kind of security. Task group I (Security)
of IEEE 802.11 working group (http://www.ieee802.org/11/) has worked
to address the flaws of the base standard and has in practice
completed its work in May 2004. The IEEE 802.11i amendment to the IEEE
802.11 standard was approved in June 2004 and published in July 2004.

Wi-Fi Alliance (http://www.wi-fi.org/) used a draft version of the
IEEE 802.11i work (draft 3.0) to define a subset of the security
enhancements that can be implemented with existing wlan hardware. This
is called Wi-Fi Protected Access<TM> (WPA). This has now become a
mandatory component of interoperability testing and certification done
by Wi-Fi Alliance. Wi-Fi provides information about WPA at its web
site (http://www.wi-fi.org/OpenSection/protected_access.asp).

IEEE 802.11 standard defined wired equivalent privacy (WEP) algorithm
for protecting wireless networks. WEP uses RC4 with 40-bit keys,
24-bit initialization vector (IV), and CRC32 to protect against packet
forgery. All these choices have proven to be insufficient: key space is
too small against current attacks, RC4 key scheduling is insufficient
(beginning of the pseudorandom stream should be skipped), IV space is
too small and IV reuse makes attacks easier, there is no replay
protection, and non-keyed authentication does not protect against bit
flipping packet data.

WPA is an intermediate solution for the security issues. It uses
Temporal Key Integrity Protocol (TKIP) to replace WEP. TKIP is a
compromise on strong security and possibility to use existing
hardware. It still uses RC4 for the encryption like WEP, but with
per-packet RC4 keys. In addition, it implements replay protection,
keyed packet authentication mechanism (Michael MIC).

Keys can be managed using two different mechanisms. WPA can either use
an external authentication server (e.g., RADIUS) and EAP just like
IEEE 802.1X is using or pre-shared keys without need for additional
servers. Wi-Fi calls these "WPA-Enterprise" and "WPA-Personal",
respectively. Both mechanisms will generate a master session key for
the Authenticator (AP) and Supplicant (client station).

WPA implements a new key handshake (4-Way Handshake and Group Key
Handshake) for generating and exchanging data encryption keys between
the Authenticator and Supplicant. This handshake is also used to
verify that both Authenticator and Supplicant know the master session
key. These handshakes are identical regardless of the selected key
management mechanism (only the method for generating master session
key changes).



IEEE 802.11i / WPA2
-------------------

The design for parts of IEEE 802.11i that were not included in WPA has
finished (May 2004) and this amendment to IEEE 802.11 was approved in
June 2004. Wi-Fi Alliance is using the final IEEE 802.11i as a new
version of WPA called WPA2. This includes, e.g., support for more
robust encryption algorithm (CCMP: AES in Counter mode with CBC-MAC)
to replace TKIP and optimizations for handoff (reduced number of
messages in initial key handshake, pre-authentication, and PMKSA caching).



wpa_supplicant
--------------

wpa_supplicant is an implementation of the WPA Supplicant component,
i.e., the part that runs in the client stations. It implements WPA key
negotiation with a WPA Authenticator and EAP authentication with
Authentication Server. In addition, it controls the roaming and IEEE
802.11 authentication/association of the wlan driver.

wpa_supplicant is designed to be a "daemon" program that runs in the
background and acts as the backend component controlling the wireless
connection. wpa_supplicant supports separate frontend programs and an
example text-based frontend, wpa_cli, is included with wpa_supplicant.

Following steps are used when associating with an AP using WPA:

- wpa_supplicant requests the kernel driver to scan neighboring BSSes
- wpa_supplicant selects a BSS based on its configuration
- wpa_supplicant requests the kernel driver to associate with the chosen
  BSS
- If WPA-EAP: integrated IEEE 802.1X Supplicant completes EAP
  authentication with the authentication server (proxied by the
  Authenticator in the AP)
- If WPA-EAP: master key is received from the IEEE 802.1X Supplicant
- If WPA-PSK: wpa_supplicant uses PSK as the master session key
- wpa_supplicant completes WPA 4-Way Handshake and Group Key Handshake
  with the Authenticator (AP)
- wpa_supplicant configures encryption keys for unicast and broadcast
- normal data packets can be transmitted and received



Building and installing
-----------------------

In order to be able to build wpa_supplicant, you will first need to
select which parts of it will be included. This is done by creating a
build time configuration file, .config, in the wpa_supplicant root
directory. Configuration options are text lines using following
format: CONFIG_<option>=y. Lines starting with # are considered
comments and are ignored. See defconfig file for an example configuration
and a list of available options and additional notes.

The build time configuration can be used to select only the needed
features and limit the binary size and requirements for external
libraries. The main configuration parts are the selection of which
driver interfaces (e.g., nl80211, wext, ..) and which authentication
methods (e.g., EAP-TLS, EAP-PEAP, ..) are included.

Following build time configuration options are used to control IEEE
802.1X/EAPOL and EAP state machines and all EAP methods. Including
TLS, PEAP, or TTLS will require linking wpa_supplicant with OpenSSL
library for TLS implementation. Alternatively, GnuTLS or the internal
TLSv1 implementation can be used for TLS functionality.

CONFIG_IEEE8021X_EAPOL=y
CONFIG_EAP_MD5=y
CONFIG_EAP_MSCHAPV2=y
CONFIG_EAP_TLS=y
CONFIG_EAP_PEAP=y
CONFIG_EAP_TTLS=y
CONFIG_EAP_GTC=y
CONFIG_EAP_OTP=y
CONFIG_EAP_SIM=y
CONFIG_EAP_AKA=y
CONFIG_EAP_AKA_PRIME=y
CONFIG_EAP_PSK=y
CONFIG_EAP_SAKE=y
CONFIG_EAP_GPSK=y
CONFIG_EAP_PAX=y
CONFIG_EAP_LEAP=y
CONFIG_EAP_IKEV2=y
CONFIG_EAP_PWD=y

Following option can be used to include GSM SIM/USIM interface for GSM/UMTS
authentication algorithm (for EAP-SIM/EAP-AKA/EAP-AKA'). This requires pcsc-lite
(http://www.linuxnet.com/) for smart card access.

CONFIG_PCSC=y

Following options can be added to .config to select which driver
interfaces are included.

CONFIG_DRIVER_NL80211=y
CONFIG_DRIVER_WEXT=y
CONFIG_DRIVER_BSD=y
CONFIG_DRIVER_NDIS=y

Following example includes some more features and driver interfaces that
are included in the wpa_supplicant package:

CONFIG_DRIVER_NL80211=y
CONFIG_DRIVER_WEXT=y
CONFIG_DRIVER_BSD=y
CONFIG_DRIVER_NDIS=y
CONFIG_IEEE8021X_EAPOL=y
CONFIG_EAP_MD5=y
CONFIG_EAP_MSCHAPV2=y
CONFIG_EAP_TLS=y
CONFIG_EAP_PEAP=y
CONFIG_EAP_TTLS=y
CONFIG_EAP_GTC=y
CONFIG_EAP_OTP=y
CONFIG_EAP_SIM=y
CONFIG_EAP_AKA=y
CONFIG_EAP_PSK=y
CONFIG_EAP_SAKE=y
CONFIG_EAP_GPSK=y
CONFIG_EAP_PAX=y
CONFIG_EAP_LEAP=y
CONFIG_EAP_IKEV2=y
CONFIG_PCSC=y

EAP-PEAP and EAP-TTLS will automatically include configured EAP
methods (MD5, OTP, GTC, MSCHAPV2) for inner authentication selection.


After you have created a configuration file, you can build
wpa_supplicant and wpa_cli with 'make' command. You may then install
the binaries to a suitable system directory, e.g., /usr/local/bin.

Example commands:

# build wpa_supplicant and wpa_cli
make
# install binaries (this may need root privileges)
cp wpa_cli wpa_supplicant /usr/local/bin


You will need to make a configuration file, e.g.,
/etc/wpa_supplicant.conf, with network configuration for the networks
you are going to use. Configuration file section below includes
explanation fo the configuration file format and includes various
examples. Once the configuration is ready, you can test whether the
configuration work by first running wpa_supplicant with following
command to start it on foreground with debugging enabled:

wpa_supplicant -iwlan0 -c/etc/wpa_supplicant.conf -d

Assuming everything goes fine, you can start using following command
to start wpa_supplicant on background without debugging:

wpa_supplicant -iwlan0 -c/etc/wpa_supplicant.conf -B

Please note that if you included more than one driver interface in the
build time configuration (.config), you may need to specify which
interface to use by including -D<driver name> option on the command
line. See following section for more details on command line options
for wpa_supplicant.



Command line options
--------------------

usage:
  wpa_supplicant [-BddfhKLqqtuvW] [-P<pid file>] [-g<global ctrl>] \
        [-G<group>] \
        -i<ifname> -c<config file> [-C<ctrl>] [-D<driver>] [-p<driver_param>] \
        [-b<br_ifname> [-MN -i<ifname> -c<conf> [-C<ctrl>] [-D<driver>] \
        [-p<driver_param>] [-b<br_ifname>] [-m<P2P Device config file>] ...

options:
  -b = optional bridge interface name
  -B = run daemon in the background
  -c = Configuration file
  -C = ctrl_interface parameter (only used if -c is not)
  -i = interface name
  -d = increase debugging verbosity (-dd even more)
  -D = driver name (can be multiple drivers: nl80211,wext)
  -f = Log output to default log location (normally /tmp)
  -g = global ctrl_interface
  -G = global ctrl_interface group
  -K = include keys (passwords, etc.) in debug output
  -t = include timestamp in debug messages
  -h = show this help text
  -L = show license (BSD)
  -p = driver parameters
  -P = PID file
  -q = decrease debugging verbosity (-qq even less)
  -u = enable DBus control interface
  -v = show version
  -W = wait for a control interface monitor before starting
  -M = start describing matching interface
  -N = start describing new interface
  -m = Configuration file for the P2P Device

drivers:
  nl80211 = Linux nl80211/cfg80211
  wext = Linux wireless extensions (generic)
  wired = wpa_supplicant wired Ethernet driver
  roboswitch = wpa_supplicant Broadcom switch driver
  bsd = BSD 802.11 support (Atheros, etc.)
  ndis = Windows NDIS driver

In most common cases, wpa_supplicant is started with

wpa_supplicant -B -c/etc/wpa_supplicant.conf -iwlan0

This makes the process fork into background.

The easiest way to debug problems, and to get debug log for bug
reports, is to start wpa_supplicant on foreground with debugging
enabled:

wpa_supplicant -c/etc/wpa_supplicant.conf -iwlan0 -d

If the specific driver wrapper is not known beforehand, it is possible
to specify multiple comma separated driver wrappers on the command
line. wpa_supplicant will use the first driver wrapper that is able to
initialize the interface.

wpa_supplicant -Dnl80211,wext -c/etc/wpa_supplicant.conf -iwlan0


wpa_supplicant can control multiple interfaces (radios) either by
running one process for each interface separately or by running just
one process and list of options at command line. Each interface is
separated with -N argument. As an example, following command would
start wpa_supplicant for two interfaces:

wpa_supplicant \
	-c wpa1.conf -i wlan0 -D nl80211 -N \
	-c wpa2.conf -i wlan1 -D wext


If the interfaces on which wpa_supplicant is to run are not known or do
not exist, wpa_supplicant can match an interface when it arrives. Each
matched interface is separated with -M argument and the -i argument now
allows for pattern matching.

As an example, the following command would start wpa_supplicant for a
specific wired interface called lan0, any interface starting with wlan
and lastly any other interface. Each match has its own configuration
file, and for the wired interface a specific driver has also been given.

wpa_supplicant \
	-M -c wpa_wired.conf -ilan0 -D wired \
	-M -c wpa1.conf -iwlan* \
	-M -c wpa2.conf


If the interface is added in a Linux bridge (e.g., br0), the bridge
interface needs to be configured to wpa_supplicant in addition to the
main interface:

wpa_supplicant -cw.conf -Dnl80211 -iwlan0 -bbr0


Configuration file
------------------

wpa_supplicant is configured using a text file that lists all accepted
networks and security policies, including pre-shared keys. See
example configuration file, wpa_supplicant.conf, for detailed
information about the configuration format and supported fields.

Changes to configuration file can be reloaded be sending SIGHUP signal
to wpa_supplicant ('killall -HUP wpa_supplicant'). Similarly,
reloading can be triggered with 'wpa_cli reconfigure' command.

Configuration file can include one or more network blocks, e.g., one
for each used SSID. wpa_supplicant will automatically select the best
network based on the order of network blocks in the configuration
file, network security level (WPA/WPA2 is preferred), and signal
strength.

Example configuration files for some common configurations:

1) WPA-Personal (PSK) as home network and WPA-Enterprise with EAP-TLS as work
   network

# allow frontend (e.g., wpa_cli) to be used by all users in 'wheel' group
ctrl_interface=/var/run/wpa_supplicant
ctrl_interface_group=wheel
#
# home network; allow all valid ciphers
network={
	ssid="home"
	scan_ssid=1
	key_mgmt=WPA-PSK
	psk="very secret passphrase"
}
#
# work network; use EAP-TLS with WPA; allow only CCMP and TKIP ciphers
network={
	ssid="work"
	scan_ssid=1
	key_mgmt=WPA-EAP
	pairwise=CCMP TKIP
	group=CCMP TKIP
	eap=TLS
	identity="user@example.com"
	ca_cert="/etc/cert/ca.pem"
	client_cert="/etc/cert/user.pem"
	private_key="/etc/cert/user.prv"
	private_key_passwd="password"
}


2) WPA-RADIUS/EAP-PEAP/MSCHAPv2 with RADIUS servers that use old peaplabel
   (e.g., Funk Odyssey and SBR, Meetinghouse Aegis, Interlink RAD-Series)

ctrl_interface=/var/run/wpa_supplicant
ctrl_interface_group=wheel
network={
	ssid="example"
	scan_ssid=1
	key_mgmt=WPA-EAP
	eap=PEAP
	identity="user@example.com"
	password="foobar"
	ca_cert="/etc/cert/ca.pem"
	phase1="peaplabel=0"
	phase2="auth=MSCHAPV2"
}


3) EAP-TTLS/EAP-MD5-Challenge configuration with anonymous identity for the
   unencrypted use. Real identity is sent only within an encrypted TLS tunnel.

ctrl_interface=/var/run/wpa_supplicant
ctrl_interface_group=wheel
network={
	ssid="example"
	scan_ssid=1
	key_mgmt=WPA-EAP
	eap=TTLS
	identity="user@example.com"
	anonymous_identity="anonymous@example.com"
	password="foobar"
	ca_cert="/etc/cert/ca.pem"
	phase2="auth=MD5"
}


4) IEEE 802.1X (i.e., no WPA) with dynamic WEP keys (require both unicast and
   broadcast); use EAP-TLS for authentication

ctrl_interface=/var/run/wpa_supplicant
ctrl_interface_group=wheel
network={
	ssid="1x-test"
	scan_ssid=1
	key_mgmt=IEEE8021X
	eap=TLS
	identity="user@example.com"
	ca_cert="/etc/cert/ca.pem"
	client_cert="/etc/cert/user.pem"
	private_key="/etc/cert/user.prv"
	private_key_passwd="password"
	eapol_flags=3
}


5) Catch all example that allows more or less all configuration modes. The
   configuration options are used based on what security policy is used in the
   selected SSID. This is mostly for testing and is not recommended for normal
   use.

ctrl_interface=/var/run/wpa_supplicant
ctrl_interface_group=wheel
network={
	ssid="example"
	scan_ssid=1
	key_mgmt=WPA-EAP WPA-PSK IEEE8021X NONE
	pairwise=CCMP TKIP
	group=CCMP TKIP WEP104 WEP40
	psk="very secret passphrase"
	eap=TTLS PEAP TLS
	identity="user@example.com"
	password="foobar"
	ca_cert="/etc/cert/ca.pem"
	client_cert="/etc/cert/user.pem"
	private_key="/etc/cert/user.prv"
	private_key_passwd="password"
	phase1="peaplabel=0"
	ca_cert2="/etc/cert/ca2.pem"
	client_cert2="/etc/cer/user.pem"
	private_key2="/etc/cer/user.prv"
	private_key2_passwd="password"
}


6) Authentication for wired Ethernet. This can be used with 'wired' or
   'roboswitch' interface (-Dwired or -Droboswitch on command line).

ctrl_interface=/var/run/wpa_supplicant
ctrl_interface_group=wheel
ap_scan=0
network={
	key_mgmt=IEEE8021X
	eap=MD5
	identity="user"
	password="password"
	eapol_flags=0
}



Certificates
------------

Some EAP authentication methods require use of certificates. EAP-TLS
uses both server side and client certificates whereas EAP-PEAP and
EAP-TTLS only require the server side certificate. When client
certificate is used, a matching private key file has to also be
included in configuration. If the private key uses a passphrase, this
has to be configured in wpa_supplicant.conf ("private_key_passwd").

wpa_supplicant supports X.509 certificates in PEM and DER
formats. User certificate and private key can be included in the same
file.

If the user certificate and private key is received in PKCS#12/PFX
format, they need to be converted to suitable PEM/DER format for
wpa_supplicant. This can be done, e.g., with following commands:

# convert client certificate and private key to PEM format
openssl pkcs12 -in example.pfx -out user.pem -clcerts
# convert CA certificate (if included in PFX file) to PEM format
openssl pkcs12 -in example.pfx -out ca.pem -cacerts -nokeys



wpa_cli
-------

wpa_cli is a text-based frontend program for interacting with
wpa_supplicant. It is used to query current status, change
configuration, trigger events, and request interactive user input.

wpa_cli can show the current authentication status, selected security
mode, dot11 and dot1x MIBs, etc. In addition, it can configure some
variables like EAPOL state machine parameters and trigger events like
reassociation and IEEE 802.1X logoff/logon. wpa_cli provides a user
interface to request authentication information, like username and
password, if these are not included in the configuration. This can be
used to implement, e.g., one-time-passwords or generic token card
authentication where the authentication is based on a
challenge-response that uses an external device for generating the
response.

The control interface of wpa_supplicant can be configured to allow
non-root user access (ctrl_interface_group in the configuration
file). This makes it possible to run wpa_cli with a normal user
account.

wpa_cli supports two modes: interactive and command line. Both modes
share the same command set and the main difference is in interactive
mode providing access to unsolicited messages (event messages,
username/password requests).

Interactive mode is started when wpa_cli is executed without including
the command as a command line parameter. Commands are then entered on
the wpa_cli prompt. In command line mode, the same commands are
entered as command line arguments for wpa_cli.


Interactive authentication parameters request

When wpa_supplicant need authentication parameters, like username and
password, which are not present in the configuration file, it sends a
request message to all attached frontend programs, e.g., wpa_cli in
interactive mode. wpa_cli shows these requests with
"CTRL-REQ-<type>-<id>:<text>" prefix. <type> is IDENTITY, PASSWORD, or
OTP (one-time-password). <id> is a unique identifier for the current
network. <text> is description of the request. In case of OTP request,
it includes the challenge from the authentication server.

The reply to these requests can be given with 'identity', 'password',
and 'otp' commands. <id> needs to be copied from the the matching
request. 'password' and 'otp' commands can be used regardless of
whether the request was for PASSWORD or OTP. The main difference
between these two commands is that values given with 'password' are
remembered as long as wpa_supplicant is running whereas values given
with 'otp' are used only once and then forgotten, i.e., wpa_supplicant
will ask frontend for a new value for every use. This can be used to
implement one-time-password lists and generic token card -based
authentication.

Example request for password and a matching reply:

CTRL-REQ-PASSWORD-1:Password needed for SSID foobar
> password 1 mysecretpassword

Example request for generic token card challenge-response:

CTRL-REQ-OTP-2:Challenge 1235663 needed for SSID foobar
> otp 2 9876


wpa_cli commands

  status = get current WPA/EAPOL/EAP status
  mib = get MIB variables (dot1x, dot11)
  help = show this usage help
  interface [ifname] = show interfaces/select interface
  level <debug level> = change debug level
  license = show full wpa_cli license
  logoff = IEEE 802.1X EAPOL state machine logoff
  logon = IEEE 802.1X EAPOL state machine logon
  set = set variables (shows list of variables when run without arguments)
  pmksa = show PMKSA cache
  reassociate = force reassociation
  reconfigure = force wpa_supplicant to re-read its configuration file
  preauthenticate <BSSID> = force preauthentication
  identity <network id> <identity> = configure identity for an SSID
  password <network id> <password> = configure password for an SSID
  pin <network id> <pin> = configure pin for an SSID
  otp <network id> <password> = configure one-time-password for an SSID
  passphrase <network id> <passphrase> = configure private key passphrase
    for an SSID
  bssid <network id> <BSSID> = set preferred BSSID for an SSID
  list_networks = list configured networks
  select_network <network id> = select a network (disable others)
  enable_network <network id> = enable a network
  disable_network <network id> = disable a network
  add_network = add a network
  remove_network <network id> = remove a network
  set_network <network id> <variable> <value> = set network variables (shows
    list of variables when run without arguments)
  get_network <network id> <variable> = get network variables
  save_config = save the current configuration
  disconnect = disconnect and wait for reassociate command before connecting
  scan = request new BSS scan
  scan_results = get latest scan results
  get_capability <eap/pairwise/group/key_mgmt/proto/auth_alg> = get capabilies
  terminate = terminate wpa_supplicant
  quit = exit wpa_cli


wpa_cli command line options

wpa_cli [-p<path to ctrl sockets>] [-i<ifname>] [-hvB] [-a<action file>] \
        [-P<pid file>] [-g<global ctrl>]  [command..]
  -h = help (show this usage text)
  -v = shown version information
  -a = run in daemon mode executing the action file based on events from
       wpa_supplicant
  -B = run a daemon in the background
  default path: /var/run/wpa_supplicant
  default interface: first interface found in socket path


Using wpa_cli to run external program on connect/disconnect
-----------------------------------------------------------

wpa_cli can used to run external programs whenever wpa_supplicant
connects or disconnects from a network. This can be used, e.g., to
update network configuration and/or trigget DHCP client to update IP
addresses, etc.

One wpa_cli process in "action" mode needs to be started for each
interface. For example, the following command starts wpa_cli for the
default interface (-i can be used to select the interface in case of
more than one interface being used at the same time):

wpa_cli -a/sbin/wpa_action.sh -B

The action file (-a option, /sbin/wpa_action.sh in this example) will
be executed whenever wpa_supplicant completes authentication (connect
event) or detects disconnection). The action script will be called
with two command line arguments: interface name and event (CONNECTED
or DISCONNECTED). If the action script needs to get more information
about the current network, it can use 'wpa_cli status' to query
wpa_supplicant for more information.

Following example can be used as a simple template for an action
script:

#!/bin/sh

IFNAME=$1
CMD=$2

if [ "$CMD" = "CONNECTED" ]; then
    SSID=`wpa_cli -i$IFNAME status | grep ^ssid= | cut -f2- -d=`
    # configure network, signal DHCP client, etc.
fi

if [ "$CMD" = "DISCONNECTED" ]; then
    # remove network configuration, if needed
    SSID=
fi



Integrating with pcmcia-cs/cardmgr scripts
------------------------------------------

wpa_supplicant needs to be running when using a wireless network with
WPA. It can be started either from system startup scripts or from
pcmcia-cs/cardmgr scripts (when using PC Cards). WPA handshake must be
completed before data frames can be exchanged, so wpa_supplicant
should be started before DHCP client.

For example, following small changes to pcmcia-cs scripts can be used
to enable WPA support:

Add MODE="Managed" and WPA="y" to the network scheme in
/etc/pcmcia/wireless.opts.

Add the following block to the end of 'start' action handler in
/etc/pcmcia/wireless:

    if [ "$WPA" = "y" -a -x /usr/local/bin/wpa_supplicant ]; then
	/usr/local/bin/wpa_supplicant -B -c/etc/wpa_supplicant.conf \
		-i$DEVICE
    fi

Add the following block to the end of 'stop' action handler (may need
to be separated from other actions) in /etc/pcmcia/wireless:

    if [ "$WPA" = "y" -a -x /usr/local/bin/wpa_supplicant ]; then
	killall wpa_supplicant
    fi

This will make cardmgr start wpa_supplicant when the card is plugged
in.



Dynamic interface add and operation without configuration files
---------------------------------------------------------------

wpa_supplicant can be started without any configuration files or
network interfaces. When used in this way, a global (i.e., per
wpa_supplicant process) control interface is used to add and remove
network interfaces. Each network interface can then be configured
through a per-network interface control interface. For example,
following commands show how to start wpa_supplicant without any
network interfaces and then add a network interface and configure a
network (SSID):

# Start wpa_supplicant in the background
wpa_supplicant -g/var/run/wpa_supplicant-global -B

# Add a new interface (wlan0, no configuration file, driver=nl80211, and
# enable control interface)
wpa_cli -g/var/run/wpa_supplicant-global interface_add wlan0 \
	"" nl80211 /var/run/wpa_supplicant

# Configure a network using the newly added network interface:
wpa_cli -iwlan0 add_network
wpa_cli -iwlan0 set_network 0 ssid '"test"'
wpa_cli -iwlan0 set_network 0 key_mgmt WPA-PSK
wpa_cli -iwlan0 set_network 0 psk '"12345678"'
wpa_cli -iwlan0 set_network 0 pairwise TKIP
wpa_cli -iwlan0 set_network 0 group TKIP
wpa_cli -iwlan0 set_network 0 proto WPA
wpa_cli -iwlan0 enable_network 0

# At this point, the new network interface should start trying to associate
# with the WPA-PSK network using SSID test.

# Remove network interface
wpa_cli -g/var/run/wpa_supplicant-global interface_remove wlan0


Privilege separation
--------------------

To minimize the size of code that needs to be run with root privileges
(e.g., to control wireless interface operation), wpa_supplicant
supports optional privilege separation. If enabled, this separates the
privileged operations into a separate process (wpa_priv) while leaving
rest of the code (e.g., EAP authentication and WPA handshakes) into an
unprivileged process (wpa_supplicant) that can be run as non-root
user. Privilege separation restricts the effects of potential software
errors by containing the majority of the code in an unprivileged
process to avoid full system compromise.

Privilege separation is not enabled by default and it can be enabled
by adding CONFIG_PRIVSEP=y to the build configuration (.config). When
enabled, the privileged operations (driver wrapper and l2_packet) are
linked into a separate daemon program, wpa_priv. The unprivileged
program, wpa_supplicant, will be built with a special driver/l2_packet
wrappers that communicate with the privileged wpa_priv process to
perform the needed operations. wpa_priv can control what privileged
are allowed.

wpa_priv needs to be run with network admin privileges (usually, root
user). It opens a UNIX domain socket for each interface that is
included on the command line; any other interface will be off limits
for wpa_supplicant in this kind of configuration. After this,
wpa_supplicant can be run as a non-root user (e.g., all standard users
on a laptop or as a special non-privileged user account created just
for this purpose to limit access to user files even further).


Example configuration:
- create user group for users that are allowed to use wpa_supplicant
  ('wpapriv' in this example) and assign users that should be able to
  use wpa_supplicant into that group
- create /var/run/wpa_priv directory for UNIX domain sockets and control
  user access by setting it accessible only for the wpapriv group:
  mkdir /var/run/wpa_priv
  chown root:wpapriv /var/run/wpa_priv
  chmod 0750 /var/run/wpa_priv
- start wpa_priv as root (e.g., from system startup scripts) with the
  enabled interfaces configured on the command line:
  wpa_priv -B -P /var/run/wpa_priv.pid nl80211:wlan0
- run wpa_supplicant as non-root with a user that is in wpapriv group:
  wpa_supplicant -i ath0 -c wpa_supplicant.conf

wpa_priv does not use the network interface before wpa_supplicant is
started, so it is fine to include network interfaces that are not
available at the time wpa_priv is started. As an alternative, wpa_priv
can be started when an interface is added (hotplug/udev/etc. scripts).
wpa_priv can control multiple interface with one process, but it is
also possible to run multiple wpa_priv processes at the same time, if
desired.


Linux capabilities instead of privileged process
------------------------------------------------

wpa_supplicant performs operations that need special permissions, e.g.,
to control the network connection. Traditionally this has been achieved
by running wpa_supplicant as a privileged process with effective user id
0 (root). Linux capabilities can be used to provide restricted set of
capabilities to match the functions needed by wpa_supplicant. The
minimum set of capabilities needed for the operations is CAP_NET_ADMIN
and CAP_NET_RAW.

setcap(8) can be used to set file capabilities. For example:

sudo setcap cap_net_raw,cap_net_admin+ep wpa_supplicant

Please note that this would give anyone being able to run that
wpa_supplicant binary access to the additional capabilities. This can
further be limited by file owner/group and mode bits. For example:

sudo chown wpas wpa_supplicant
sudo chmod 0100 wpa_supplicant

This combination of setcap, chown, and chmod commands would allow wpas
user to execute wpa_supplicant with additional network admin/raw
capabilities.

Common way style of creating a control interface socket in
/var/run/wpa_supplicant could not be done by this user, but this
directory could be created before starting the wpa_supplicant and set to
suitable mode to allow wpa_supplicant to create sockets
there. Alternatively, other directory or abstract socket namespace could
be used for the control interface.


External requests for radio control
-----------------------------------

External programs can request wpa_supplicant to not start offchannel
operations during other tasks that may need exclusive control of the
radio. The RADIO_WORK control interface command can be used for this.

"RADIO_WORK add <name> [freq=<MHz>] [timeout=<seconds>]" command can be
used to reserve a slot for radio access. If freq is specified, other
radio work items on the same channel may be completed in
parallel. Otherwise, all other radio work items are blocked during
execution. Timeout is set to 10 seconds by default to avoid blocking
wpa_supplicant operations for excessive time. If a longer (or shorter)
safety timeout is needed, that can be specified with the optional
timeout parameter. This command returns an identifier for the radio work
item.

Once the radio work item has been started, "EXT-RADIO-WORK-START <id>"
event message is indicated that the external processing can start. Once
the operation has been completed, "RADIO_WORK done <id>" is used to
indicate that to wpa_supplicant. This allows other radio works to be
performed. If this command is forgotten (e.g., due to the external
program terminating), wpa_supplicant will time out the radio work item
and send "EXT-RADIO-WORK-TIMEOUT <id>" event to indicate that this has
happened. "RADIO_WORK done <id>" can also be used to cancel items that
have not yet been started.

For example, in wpa_cli interactive mode:

> radio_work add test
1
<3>EXT-RADIO-WORK-START 1
> radio_work show
ext:test@wlan0:0:1:2.487797
> radio_work done 1
OK
> radio_work show


> radio_work done 3
OK
> radio_work show
ext:test freq=2412 timeout=30@wlan0:2412:1:28.583483
<3>EXT-RADIO-WORK-TIMEOUT 2


> radio_work add test2 freq=2412 timeout=60
5
<3>EXT-RADIO-WORK-START 5
> radio_work add test3
6
> radio_work add test4
7
> radio_work show
ext:test2 freq=2412 timeout=60@wlan0:2412:1:9.751844
ext:test3@wlan0:0:0:5.071812
ext:test4@wlan0:0:0:3.143870
> radio_work done 6
OK
> radio_work show
ext:test2 freq=2412 timeout=60@wlan0:2412:1:16.287869
ext:test4@wlan0:0:0:9.679895
> radio_work done 5
OK
<3>EXT-RADIO-WORK-START 7
<3>EXT-RADIO-WORK-TIMEOUT 7
wpa_gui icon files

To convert the svg icons to other formats, make sure inkscape and imagemagick
are installed and use `make' to create various sized png and xpm icons.


wpa_gui.svg
-----------

Copyright (c) 2008 Bernard Gray <bernard.gray@gmail.com>

The wpa_gui icon is licensed under the GPL version 2. Alternatively, the icon
may be distributed under the terms of BSD license.


ap.svg
------

mystica_Wireless_Router.svg

http://openclipart.org/media/files/mystica/8390
Wireless Router
by:     mystica
last change:    April 20, 2008 10:32 pm (File added)
date:   April 20, 2008 10:31 pm
license: PD


laptop.svg
----------

metalmarious_Laptop.svg

http://openclipart.org/media/files/metalmarious/4056
Laptop
by:      metalmarious
last change:    May 18, 2008 07:04 pm (File added)
date:   August 27, 2007 04:44 am
license: PD


group.svg
---------

http://www.openclipart.org/detail/25428
http://www.openclipart.org/people/Anonymous/Anonymous_Network.svg
Uploader:
    Anonymous
Drawn by:
    Andrew Fitzsimon / Anonymous
Created:
    2009-04-29 04:07:37
Description:
    A network icon by Andrew Fitzsimon. Etiquette Icon set.
    From 0.18 OCAL database.

Public Domain



invitation.svg
--------------

http://www.openclipart.org/detail/974
http://www.openclipart.org/people/jean_victor_balin/jean_victor_balin_unknown_green.svg
Uploader:
    jean_victor_balin
Drawn by:
    jean_victor_balin
Created:
    2006-10-27 02:12:13
Description:

Public Domain
hostapd and Wi-Fi Protected Setup (WPS)
=======================================

This document describes how the WPS implementation in hostapd can be
configured and how an external component on an AP (e.g., web UI) is
used to enable enrollment of client devices.


Introduction to WPS
-------------------

Wi-Fi Protected Setup (WPS) is a mechanism for easy configuration of a
wireless network. It allows automated generation of random keys (WPA
passphrase/PSK) and configuration of an access point and client
devices. WPS includes number of methods for setting up connections
with PIN method and push-button configuration (PBC) being the most
commonly deployed options.

While WPS can enable more home networks to use encryption in the
wireless network, it should be noted that the use of the PIN and
especially PBC mechanisms for authenticating the initial key setup is
not very secure. As such, use of WPS may not be suitable for
environments that require secure network access without chance for
allowing outsiders to gain access during the setup phase.

WPS uses following terms to describe the entities participating in the
network setup:
- access point: the WLAN access point
- Registrar: a device that control a network and can authorize
  addition of new devices); this may be either in the AP ("internal
  Registrar") or in an external device, e.g., a laptop, ("external
  Registrar")
- Enrollee: a device that is being authorized to use the network

It should also be noted that the AP and a client device may change
roles (i.e., AP acts as an Enrollee and client device as a Registrar)
when WPS is used to configure the access point.


More information about WPS is available from Wi-Fi Alliance:
http://www.wi-fi.org/wifi-protected-setup


hostapd implementation
----------------------

hostapd includes an optional WPS component that can be used as an
internal WPS Registrar to manage addition of new WPS enabled clients
to the network. In addition, WPS Enrollee functionality in hostapd can
be used to allow external WPS Registrars to configure the access
point, e.g., for initial network setup. In addition, hostapd can proxy a
WPS registration between a wireless Enrollee and an external Registrar
(e.g., Microsoft Vista or Atheros JumpStart) with UPnP.


hostapd configuration
---------------------

WPS is an optional component that needs to be enabled in hostapd build
configuration (.config). Here is an example configuration that
includes WPS support and uses nl80211 driver interface:

CONFIG_DRIVER_NL80211=y
CONFIG_WPS=y
CONFIG_WPS_UPNP=y

Following parameter can be used to enable support for NFC config method:

CONFIG_WPS_NFC=y


Following section shows an example runtime configuration
(hostapd.conf) that enables WPS:

# Configure the driver and network interface
driver=nl80211
interface=wlan0

# WPA2-Personal configuration for the AP
ssid=wps-test
wpa=2
wpa_key_mgmt=WPA-PSK
wpa_pairwise=CCMP
# Default WPA passphrase for legacy (non-WPS) clients
wpa_passphrase=12345678
# Enable random per-device PSK generation for WPS clients
# Please note that the file has to exists for hostapd to start (i.e., create an
# empty file as a starting point).
wpa_psk_file=/etc/hostapd.psk

# Enable control interface for PBC/PIN entry
ctrl_interface=/var/run/hostapd

# Enable internal EAP server for EAP-WSC (part of Wi-Fi Protected Setup)
eap_server=1

# WPS configuration (AP configured, do not allow external WPS Registrars)
wps_state=2
ap_setup_locked=1
# If UUID is not configured, it will be generated based on local MAC address.
uuid=87654321-9abc-def0-1234-56789abc0000
wps_pin_requests=/var/run/hostapd.pin-req
device_name=Wireless AP
manufacturer=Company
model_name=WAP
model_number=123
serial_number=12345
device_type=6-0050F204-1
os_version=01020300
config_methods=label display push_button keypad

# if external Registrars are allowed, UPnP support could be added:
#upnp_iface=br0
#friendly_name=WPS Access Point


External operations
-------------------

WPS requires either a device PIN code (usually, 8-digit number) or a
pushbutton event (for PBC) to allow a new WPS Enrollee to join the
network. hostapd uses the control interface as an input channel for
these events.

The PIN value used in the commands must be processed by an UI to
remove non-digit characters and potentially, to verify the checksum
digit. "hostapd_cli wps_check_pin <PIN>" can be used to do such
processing. It returns FAIL if the PIN is invalid, or FAIL-CHECKSUM if
the checksum digit is incorrect, or the processed PIN (non-digit
characters removed) if the PIN is valid.

When a client device (WPS Enrollee) connects to hostapd (WPS
Registrar) in order to start PIN mode negotiation for WPS, an
identifier (Enrollee UUID) is sent. hostapd will need to be configured
with a device password (PIN) for this Enrollee. This is an operation
that requires user interaction (assuming there are no pre-configured
PINs on the AP for a set of Enrollee).

The PIN request with information about the device is appended to the
wps_pin_requests file (/var/run/hostapd.pin-req in this example). In
addition, hostapd control interface event is sent as a notification of
a new device. The AP could use, e.g., a web UI for showing active
Enrollees to the user and request a PIN for an Enrollee.

The PIN request file has one line for every Enrollee that connected to
the AP, but for which there was no PIN. Following information is
provided for each Enrollee (separated with tabulators):
- timestamp (seconds from 1970-01-01)
- Enrollee UUID
- MAC address
- Device name
- Manufacturer
- Model Name
- Model Number
- Serial Number
- Device category

Example line in the /var/run/hostapd.pin-req file:
1200188391	53b63a98-d29e-4457-a2ed-094d7e6a669c	Intel(R) Centrino(R)	Intel Corporation	Intel(R) Centrino(R)	-	-	1-0050F204-1

Control interface data:
WPS-PIN-NEEDED [UUID-E|MAC Address|Device Name|Manufacturer|Model Name|Model Number|Serial Number|Device Category]
For example:
<2>WPS-PIN-NEEDED [53b63a98-d29e-4457-a2ed-094d7e6a669c|02:12:34:56:78:9a|Device|Manuf|Model|Model Number|Serial Number|1-0050F204-1]

When the user enters a PIN for a pending Enrollee, e.g., on the web
UI), hostapd needs to be notified of the new PIN over the control
interface. This can be done either by using the UNIX domain socket
-based control interface directly (src/common/wpa_ctrl.c provides
helper functions for using the interface) or by calling hostapd_cli.

Example command to add a PIN (12345670) for an Enrollee:

hostapd_cli wps_pin 53b63a98-d29e-4457-a2ed-094d7e6a669c 12345670

If the UUID-E is not available (e.g., Enrollee waits for the Registrar
to be selected before connecting), wildcard UUID may be used to allow
the PIN to be used once with any UUID:

hostapd_cli wps_pin any 12345670

To reduce likelihood of PIN being used with other devices or of
forgetting an active PIN available for potential attackers, expiration
time in seconds can be set for the new PIN (value 0 indicates no
expiration):

hostapd_cli wps_pin any 12345670 300

If the MAC address of the enrollee is known, it should be configured
to allow the AP to advertise list of authorized enrollees:

hostapd_cli wps_pin 53b63a98-d29e-4457-a2ed-094d7e6a669c \
	12345670 300 00:11:22:33:44:55


After this, the Enrollee can connect to the AP again and complete WPS
negotiation. At that point, a new, random WPA PSK is generated for the
client device and the client can then use that key to connect to the
AP to access the network.


If the AP includes a pushbutton, WPS PBC mode can be used. It is
enabled by pushing a button on both the AP and the client at about the
same time (2 minute window). hostapd needs to be notified about the AP
button pushed event over the control interface, e.g., by calling
hostapd_cli:

hostapd_cli wps_pbc

At this point, the client has two minutes to complete WPS negotiation
which will generate a new WPA PSK in the same way as the PIN method
described above.


When an external Registrar is used, the AP can act as an Enrollee and
use its AP PIN. A static AP PIN (e.g., one one a label in the AP
device) can be configured in hostapd.conf (ap_pin parameter). A more
secure option is to use hostapd_cli wps_ap_pin command to enable the
AP PIN only based on user action (and even better security by using a
random AP PIN for each session, i.e., by using "wps_ap_pin random"
command with a timeout value). Following commands are available for
managing the dynamic AP PIN operations:

hostapd_cli wps_ap_pin disable
- disable AP PIN (i.e., do not allow external Registrars to use it to
  learn the current AP settings or to reconfigure the AP)

hostapd_cli wps_ap_pin random [timeout]
- generate a random AP PIN and enable it
- if the optional timeout parameter is given, the AP PIN will be enabled
  for the specified number of seconds

hostapd_cli wps_ap_pin get
- fetch the current AP PIN

hostapd_cli wps_ap_pin set <PIN> [timeout]
- set the AP PIN and enable it
- if the optional timeout parameter is given, the AP PIN will be enabled
  for the specified number of seconds

hostapd_cli get_config
- display the current configuration

hostapd_cli wps_config <new SSID> <auth> <encr> <new key>
examples:
  hostapd_cli wps_config testing WPA2PSK CCMP 12345678
  hostapd_cli wps_config "no security" OPEN NONE ""

<auth> must be one of the following: OPEN WPAPSK WPA2PSK
<encr> must be one of the following: NONE WEP TKIP CCMP


Credential generation and configuration changes
-----------------------------------------------

By default, hostapd generates credentials for Enrollees and processing
AP configuration updates internally. However, it is possible to
control these operations from external programs, if desired.

The internal credential generation can be disabled with
skip_cred_build=1 option in the configuration. extra_cred option will
then need to be used to provide pre-configured Credential attribute(s)
for hostapd to use. The exact data from this binary file will be sent,
i.e., it will have to include valid WPS attributes. extra_cred can
also be used to add additional networks if the Registrar is used to
configure credentials for multiple networks.

Processing of received configuration updates can be disabled with
wps_cred_processing=1 option. When this is used, an external program
is responsible for creating hostapd configuration files and processing
configuration updates based on messages received from hostapd over
control interface. This will also include the initial configuration on
first successful registration if the AP is initially set in
unconfigured state.

Following control interface messages are sent out for external programs:

WPS-REG-SUCCESS <Enrollee MAC address <UUID-E>
For example:
<2>WPS-REG-SUCCESS 02:66:a0:ee:17:27 2b7093f1-d6fb-5108-adbb-bea66bb87333

This can be used to trigger change from unconfigured to configured
state (random configuration based on the first successful WPS
registration). In addition, this can be used to update AP UI about the
status of WPS registration progress.


WPS-NEW-AP-SETTINGS <hexdump of AP Setup attributes>
For example:
<2>WPS-NEW-AP-SETTINGS 10260001011045000c6a6b6d2d7770732d74657374100300020020100f00020008102700403065346230343536633236366665306433396164313535346131663462663731323433376163666462376633393965353466316631623032306164343438623510200006024231cede15101e000844

This can be used to update the externally stored AP configuration and
then update hostapd configuration (followed by restarting of hostapd).


WPS with NFC
------------

WPS can be used with NFC-based configuration method. An NFC tag
containing a password token from the Enrollee can be used to
authenticate the connection instead of the PIN. In addition, an NFC tag
with a configuration token can be used to transfer AP settings without
going through the WPS protocol.

When the AP acts as an Enrollee, a local NFC tag with a password token
can be used by touching the NFC interface of an external Registrar. The
wps_nfc_token command is used to manage use of the NFC password token
from the AP. "wps_nfc_token enable" enables the use of the AP's NFC
password token (in place of AP PIN) and "wps_nfc_token disable" disables
the NFC password token.

The NFC password token that is either pre-configured in the
configuration file (wps_nfc_dev_pw_id, wps_nfc_dh_pubkey,
wps_nfc_dh_privkey, wps_nfc_dev_pw) or generated dynamically with
"wps_nfc_token <WPS|NDEF>" command. The nfc_pw_token tool from
wpa_supplicant can be used to generate NFC password tokens during
manufacturing (each AP needs to have its own random keys).

The "wps_nfc_config_token <WPS/NDEF>" command can be used to build an
NFC configuration token. The output value from this command is a hexdump
of the current AP configuration (WPS parameter requests this to include
only the WPS attributes; NDEF parameter requests additional NDEF
encapsulation to be included). This data needs to be written to an NFC
tag with an external program. Once written, the NFC configuration token
can be used to touch an NFC interface on a station to provision the
credentials needed to access the network.

When the NFC device on the AP reads an NFC tag with a MIME media type
"application/vnd.wfa.wsc", the NDEF message payload (with or without
NDEF encapsulation) can be delivered to hostapd using the
following hostapd_cli command:

wps_nfc_tag_read <hexdump of payload>

If the NFC tag contains a password token, the token is added to the
internal Registrar. This allows station Enrollee from which the password
token was received to run through WPS protocol to provision the
credential.

"nfc_get_handover_sel <NDEF> <WPS>" command can be used to build the
contents of a Handover Select Message for connection handover when this
does not depend on the contents of the Handover Request Message. The
first argument selects the format of the output data and the second
argument selects which type of connection handover is requested (WPS =
Wi-Fi handover as specified in WSC 2.0).

"nfc_report_handover <INIT/RESP> WPS <carrier from handover request>
<carrier from handover select>" is used to report completed NFC
connection handover. The first parameter indicates whether the local
device initiated or responded to the connection handover and the carrier
records are the selected carrier from the handover request and select
messages as a hexdump.
hostapd - user space IEEE 802.11 AP and IEEE 802.1X/WPA/WPA2/EAP
	  Authenticator and RADIUS authentication server
================================================================

Copyright (c) 2002-2016, Jouni Malinen <j@w1.fi> and contributors
All Rights Reserved.

This program is licensed under the BSD license (the one with
advertisement clause removed).

If you are submitting changes to the project, please see CONTRIBUTIONS
file for more instructions.



License
-------

This software may be distributed, used, and modified under the terms of
BSD license:

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

3. Neither the name(s) of the above-listed copyright holder(s) nor the
   names of its contributors may be used to endorse or promote products
   derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



Introduction
============

Originally, hostapd was an optional user space component for Host AP
driver. It adds more features to the basic IEEE 802.11 management
included in the kernel driver: using external RADIUS authentication
server for MAC address based access control, IEEE 802.1X Authenticator
and dynamic WEP keying, RADIUS accounting, WPA/WPA2 (IEEE 802.11i/RSN)
Authenticator and dynamic TKIP/CCMP keying.

The current version includes support for other drivers, an integrated
EAP server (i.e., allow full authentication without requiring
an external RADIUS authentication server), and RADIUS authentication
server for EAP authentication.


Requirements
------------

Current hardware/software requirements:
- drivers:
	Host AP driver for Prism2/2.5/3.
	(http://hostap.epitest.fi/)
	Please note that station firmware version needs to be 1.7.0 or newer
	to work in WPA mode.

	mac80211-based drivers that support AP mode (with driver=nl80211).
	This includes drivers for Atheros (ath9k) and Broadcom (b43)
	chipsets.

	Any wired Ethernet driver for wired IEEE 802.1X authentication
	(experimental code)

	FreeBSD -current (with some kernel mods that have not yet been
	committed when hostapd v0.3.0 was released)
	BSD net80211 layer (e.g., Atheros driver)


Build configuration
-------------------

In order to be able to build hostapd, you will need to create a build
time configuration file, .config that selects which optional
components are included. See defconfig file for example configuration
and list of available options.



IEEE 802.1X
===========

IEEE Std 802.1X-2001 is a standard for port-based network access
control. In case of IEEE 802.11 networks, a "virtual port" is used
between each associated station and the AP. IEEE 802.11 specifies
minimal authentication mechanism for stations, whereas IEEE 802.1X
introduces a extensible mechanism for authenticating and authorizing
users.

IEEE 802.1X uses elements called Supplicant, Authenticator, Port
Access Entity, and Authentication Server. Supplicant is a component in
a station and it performs the authentication with the Authentication
Server. An access point includes an Authenticator that relays the packets
between a Supplicant and an Authentication Server. In addition, it has a
Port Access Entity (PAE) with Authenticator functionality for
controlling the virtual port authorization, i.e., whether to accept
packets from or to the station.

IEEE 802.1X uses Extensible Authentication Protocol (EAP). The frames
between a Supplicant and an Authenticator are sent using EAP over LAN
(EAPOL) and the Authenticator relays these frames to the Authentication
Server (and similarly, relays the messages from the Authentication
Server to the Supplicant). The Authentication Server can be colocated with the
Authenticator, in which case there is no need for additional protocol
for EAP frame transmission. However, a more common configuration is to
use an external Authentication Server and encapsulate EAP frame in the
frames used by that server. RADIUS is suitable for this, but IEEE
802.1X would also allow other mechanisms.

Host AP driver includes PAE functionality in the kernel driver. It
is a relatively simple mechanism for denying normal frames going to
or coming from an unauthorized port. PAE allows IEEE 802.1X related
frames to be passed between the Supplicant and the Authenticator even
on an unauthorized port.

User space daemon, hostapd, includes Authenticator functionality. It
receives 802.1X (EAPOL) frames from the Supplicant using the wlan#ap
device that is also used with IEEE 802.11 management frames. The
frames to the Supplicant are sent using the same device.

The normal configuration of the Authenticator would use an external
Authentication Server. hostapd supports RADIUS encapsulation of EAP
packets, so the Authentication Server should be a RADIUS server, like
FreeRADIUS (http://www.freeradius.org/). The Authenticator in hostapd
relays the frames between the Supplicant and the Authentication
Server. It also controls the PAE functionality in the kernel driver by
controlling virtual port authorization, i.e., station-AP
connection, based on the IEEE 802.1X state.

When a station would like to use the services of an access point, it
will first perform IEEE 802.11 authentication. This is normally done
with open systems authentication, so there is no security. After
this, IEEE 802.11 association is performed. If IEEE 802.1X is
configured to be used, the virtual port for the station is set in
Unauthorized state and only IEEE 802.1X frames are accepted at this
point. The Authenticator will then ask the Supplicant to authenticate
with the Authentication Server. After this is completed successfully,
the virtual port is set to Authorized state and frames from and to the
station are accepted.

Host AP configuration for IEEE 802.1X
-------------------------------------

The user space daemon has its own configuration file that can be used to
define AP options. Distribution package contains an example
configuration file (hostapd/hostapd.conf) that can be used as a basis
for configuration. It includes examples of all supported configuration
options and short description of each option. hostapd should be started
with full path to the configuration file as the command line argument,
e.g., './hostapd /etc/hostapd.conf'. If you have more that one wireless
LAN card, you can use one hostapd process for multiple interfaces by
giving a list of configuration files (one per interface) in the command
line.

hostapd includes a minimal co-located IEEE 802.1X server which can be
used to test IEEE 802.1X authentication. However, it should not be
used in normal use since it does not provide any security. This can be
configured by setting ieee8021x and minimal_eap options in the
configuration file.

An external Authentication Server (RADIUS) is configured with
auth_server_{addr,port,shared_secret} options. In addition,
ieee8021x and own_ip_addr must be set for this mode. With such
configuration, the co-located Authentication Server is not used and EAP
frames will be relayed using EAPOL between the Supplicant and the
Authenticator and RADIUS encapsulation between the Authenticator and
the Authentication Server. Other than this, the functionality is similar
to the case with the co-located Authentication Server.

Authentication Server and Supplicant
------------------------------------

Any RADIUS server supporting EAP should be usable as an IEEE 802.1X
Authentication Server with hostapd Authenticator. FreeRADIUS
(http://www.freeradius.org/) has been successfully tested with hostapd
Authenticator and both Xsupplicant (http://www.open1x.org) and Windows
XP Supplicants. EAP/TLS was used with Xsupplicant and
EAP/MD5-Challenge with Windows XP.

http://www.missl.cs.umd.edu/wireless/eaptls/ has useful information
about using EAP/TLS with FreeRADIUS and Xsupplicant (just replace
Cisco access point with Host AP driver, hostapd daemon, and a Prism2
card ;-). http://www.freeradius.org/doc/EAP-MD5.html has information
about using EAP/MD5 with FreeRADIUS, including instructions for WinXP
configuration. http://www.denobula.com/EAPTLS.pdf has a HOWTO on
EAP/TLS use with WinXP Supplicant.

Automatic WEP key configuration
-------------------------------

EAP/TLS generates a session key that can be used to send WEP keys from
an AP to authenticated stations. The Authenticator in hostapd can be
configured to automatically select a random default/broadcast key
(shared by all authenticated stations) with wep_key_len_broadcast
option (5 for 40-bit WEP or 13 for 104-bit WEP). In addition,
wep_key_len_unicast option can be used to configure individual unicast
keys for stations. This requires support for individual keys in the
station driver.

WEP keys can be automatically updated by configuring rekeying. This
will improve security of the network since same WEP key will only be
used for a limited period of time. wep_rekey_period option sets the
interval for rekeying in seconds.


WPA/WPA2
========

Features
--------

Supported WPA/IEEE 802.11i features:
- WPA-PSK ("WPA-Personal")
- WPA with EAP (e.g., with RADIUS authentication server) ("WPA-Enterprise")
- key management for CCMP, TKIP, WEP104, WEP40
- RSN/WPA2 (IEEE 802.11i), including PMKSA caching and pre-authentication

WPA
---

The original security mechanism of IEEE 802.11 standard was not
designed to be strong and has proved to be insufficient for most
networks that require some kind of security. Task group I (Security)
of IEEE 802.11 working group (http://www.ieee802.org/11/) has worked
to address the flaws of the base standard and has in practice
completed its work in May 2004. The IEEE 802.11i amendment to the IEEE
802.11 standard was approved in June 2004 and this amendment is likely
to be published in July 2004.

Wi-Fi Alliance (http://www.wi-fi.org/) used a draft version of the
IEEE 802.11i work (draft 3.0) to define a subset of the security
enhancements that can be implemented with existing wlan hardware. This
is called Wi-Fi Protected Access<TM> (WPA). This has now become a
mandatory component of interoperability testing and certification done
by Wi-Fi Alliance. Wi-Fi provides information about WPA at its web
site (http://www.wi-fi.org/OpenSection/protected_access.asp).

IEEE 802.11 standard defined wired equivalent privacy (WEP) algorithm
for protecting wireless networks. WEP uses RC4 with 40-bit keys,
24-bit initialization vector (IV), and CRC32 to protect against packet
forgery. All these choices have proven to be insufficient: key space is
too small against current attacks, RC4 key scheduling is insufficient
(beginning of the pseudorandom stream should be skipped), IV space is
too small and IV reuse makes attacks easier, there is no replay
protection, and non-keyed authentication does not protect against bit
flipping packet data.

WPA is an intermediate solution for the security issues. It uses
Temporal Key Integrity Protocol (TKIP) to replace WEP. TKIP is a
compromise on strong security and possibility to use existing
hardware. It still uses RC4 for the encryption like WEP, but with
per-packet RC4 keys. In addition, it implements replay protection,
keyed packet authentication mechanism (Michael MIC).

Keys can be managed using two different mechanisms. WPA can either use
an external authentication server (e.g., RADIUS) and EAP just like
IEEE 802.1X is using or pre-shared keys without need for additional
servers. Wi-Fi calls these "WPA-Enterprise" and "WPA-Personal",
respectively. Both mechanisms will generate a master session key for
the Authenticator (AP) and Supplicant (client station).

WPA implements a new key handshake (4-Way Handshake and Group Key
Handshake) for generating and exchanging data encryption keys between
the Authenticator and Supplicant. This handshake is also used to
verify that both Authenticator and Supplicant know the master session
key. These handshakes are identical regardless of the selected key
management mechanism (only the method for generating master session
key changes).


IEEE 802.11i / WPA2
-------------------

The design for parts of IEEE 802.11i that were not included in WPA has
finished (May 2004) and this amendment to IEEE 802.11 was approved in
June 2004. Wi-Fi Alliance is using the final IEEE 802.11i as a new
version of WPA called WPA2. This includes, e.g., support for more
robust encryption algorithm (CCMP: AES in Counter mode with CBC-MAC)
to replace TKIP and optimizations for handoff (reduced number of
messages in initial key handshake, pre-authentication, and PMKSA caching).

Some wireless LAN vendors are already providing support for CCMP in
their WPA products. There is no "official" interoperability
certification for CCMP and/or mixed modes using both TKIP and CCMP, so
some interoperability issues can be expected even though many
combinations seem to be working with equipment from different vendors.
Testing for WPA2 is likely to start during the second half of 2004.

hostapd configuration for WPA/WPA2
----------------------------------

TODO

# Enable WPA. Setting this variable configures the AP to require WPA (either
# WPA-PSK or WPA-RADIUS/EAP based on other configuration). For WPA-PSK, either
# wpa_psk or wpa_passphrase must be set and wpa_key_mgmt must include WPA-PSK.
# For WPA-RADIUS/EAP, ieee8021x must be set (but without dynamic WEP keys),
# RADIUS authentication server must be configured, and WPA-EAP must be included
# in wpa_key_mgmt.
# This field is a bit field that can be used to enable WPA (IEEE 802.11i/D3.0)
# and/or WPA2 (full IEEE 802.11i/RSN):
# bit0 = WPA
# bit1 = IEEE 802.11i/RSN (WPA2)
#wpa=1

# WPA pre-shared keys for WPA-PSK. This can be either entered as a 256-bit
# secret in hex format (64 hex digits), wpa_psk, or as an ASCII passphrase
# (8..63 characters) that will be converted to PSK. This conversion uses SSID
# so the PSK changes when ASCII passphrase is used and the SSID is changed.
#wpa_psk=0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
#wpa_passphrase=secret passphrase

# Set of accepted key management algorithms (WPA-PSK, WPA-EAP, or both). The
# entries are separated with a space.
#wpa_key_mgmt=WPA-PSK WPA-EAP

# Set of accepted cipher suites (encryption algorithms) for pairwise keys
# (unicast packets). This is a space separated list of algorithms:
# CCMP = AES in Counter mode with CBC-MAC [RFC 3610, IEEE 802.11i]
# TKIP = Temporal Key Integrity Protocol [IEEE 802.11i]
# Group cipher suite (encryption algorithm for broadcast and multicast frames)
# is automatically selected based on this configuration. If only CCMP is
# allowed as the pairwise cipher, group cipher will also be CCMP. Otherwise,
# TKIP will be used as the group cipher.
#wpa_pairwise=TKIP CCMP

# Time interval for rekeying GTK (broadcast/multicast encryption keys) in
# seconds.
#wpa_group_rekey=600

# Time interval for rekeying GMK (master key used internally to generate GTKs
# (in seconds).
#wpa_gmk_rekey=86400

# Enable IEEE 802.11i/RSN/WPA2 pre-authentication. This is used to speed up
# roaming be pre-authenticating IEEE 802.1X/EAP part of the full RSN
# authentication and key handshake before actually associating with a new AP.
#rsn_preauth=1
#
# Space separated list of interfaces from which pre-authentication frames are
# accepted (e.g., 'eth0' or 'eth0 wlan0wds0'. This list should include all
# interface that are used for connections to other APs. This could include
# wired interfaces and WDS links. The normal wireless data interface towards
# associated stations (e.g., wlan0) should not be added, since
# pre-authentication is only used with APs other than the currently associated
# one.
#rsn_preauth_interfaces=eth0
Logwatch is a utility for analyzing system logs and provide a human
readable summary. This directory has a configuration file and a log
analyzer script for parsing hostapd system log entries for logwatch.
These files can be installed by copying them to following locations:

/etc/log.d/conf/services/hostapd.conf
/etc/log.d/scripts/services/hostapd

More information about logwatch is available from http://www.logwatch.org/
Unbound README
* ./configure && make && make install
* You can use libevent if you want. libevent is useful when using 
  many (10000) outgoing ports. By default max 256 ports are opened at
  the same time and the builtin alternative is equally capable and a 
  little faster.
* More detailed README, README.svn, README.tests in doc directory
* manual pages can be found in doc directory, and are installed, unbound(8).
* example configuration file doc/example.conf

These files are contributed to unbound, and are not part of the official
distribution but may be helpful.

* rc_d_unbound: FreeBSD compatible /etc/rc.d script.
* parseunbound.pl: perl script to run from cron that parses statistics from
	the log file and stores them.
* unbound.spec and unbound.init: RPM specfile and Linux rc.d initfile.
* update-anchor.sh: shell script that uses unbound-host to update a set
	of trust anchor files. Run from cron twice a month.
* unbound_munin_ : plugin for munin statistics report
* unbound_cacti.tar.gz : setup files for cacti statistics report
* selinux: the .fc and .te files for SElinux protection of the unbound daemon
* unbound.plist: launchd configuration file for MacOSX.
* build-unbound-localzone-from-hosts.pl: perl script to turn /etc/hosts into
	a local-zone and local-data include file for unbound.conf.
* unbound-host.nagios.patch: makes unbound-host return status that fits right
	in with the nagios monitoring framework.  Contributed by Migiel de Vos.
* patch_rsamd5_enable.diff: this patch enables RSAMD5 validation (otherwise
  it is treated as insecure).  The RSAMD5 algorithm is deprecated (RFC6725).
* create_unbound_ad_servers.sh: shell script to enter anti-ad server lists.
* create_unbound_ad_servers.cmd: windows script to enter anti-ad server lists.
* unbound_cache.sh: shell script to save and load the cache.
* unbound_cache.cmd: windows script to save and load the cache.
* warmup.sh: shell script to warm up DNS cache by your own MRU domains.
* warmup.cmd: windows script to warm up DNS cache by your own MRU domains.
* aaaa-filter-iterator.patch: adds config option aaaa-filter: yes that
  works like the BIND feature (removes AAAA records unless AAAA-only domain).
  Useful for certain 'broken IPv6 default route' scenarios.
  Patch from Stephane Lapie for ASAHI Net.
* unbound_smf22.tar.gz: Solaris SMF installation/removal scripts.
  Contributed by Yuri Voinov.
README for Unbound on Windows.

(C) 2009, W.C.A. Wijngaards, NLnet Labs.

See LICENSE for the license text file.


+++ Introduction

Unbound is a recursive DNS server.  It does caching, full recursion, stub
recursion, DNSSEC validation, NSEC3, IPv6.  More information can be found 
at the http://unbound.net site.  Unbound has been built and tested on 
Windows XP, Vista, 7 and 8.

At http://unbound.net/documentation is an install and configuration manual
for windows.

email: unbound-bugs@nlnetlabs.nl


+++ How to use it

In ControlPanels\SystemTasks\Services you can start/stop the daemon.
In ControlPanels\SystemTasks\Logbooks you can see log entries (unless you
configured unbound to log to file).

By default the daemon provides service only to localhost.  See the manual
on how to change that (you need to edit the config file).

To change options, edit the service.conf file.  The example.conf file 
contains information on the various configuration options.  The config
file is the same as on Unix.  The options log-time-ascii, chroot, username
and pidfile are not supported on windows.


+++ How to compile

Unbound is open source under the BSD license.  You can compile it yourself.

1. Install MinGW and MSYS.  http://www.mingw.org
This is a free, open source, compiler and build environment.
Note, if your username contains a space, create a directory
C:\msys\...\home\user to work in (click on MSYS; type: mkdir /home/user ).

2. Install openssl, or compile it yourself.  http://www.openssl.org
Unbounds need the header files and libraries.  Static linking makes
things easier.  This is an open source library for cryptographic functions.
And libexpat is needed.

3. Compile Unbound
Get the source code tarball  http://unbound.net
Move it into the C:\msys\...\home\user directory.
Double click on the MSYS icon and give these commands
$ cd /home/user
$ tar xzvf unbound-xxx.tar.gz
$ cd unbound-xxx
$ ./configure --enable-static-exe
If you compiled openssl yourself, pass --with-ssl=../openssl-xxx too.
If you compiled libexpat yourself, pass --with-libexpat=../expat-install too.
The configure options for libevent or threads are not applicable for 
windows, because builtin alternatives for the windows platform are used.
$ make
And you have unbound.exe

If you run unbound-service-install.exe (double click in the explorer),
unbound is installed as a service in the controlpanels\systemtasks\services,
from the current directory. unbound-service-remove.exe uninstalls the service.

Unbound and its utilities also work from the commandline (like on unix) if 
you prefer.


+++ Cross compile

You can crosscompile unbound.  This results in .exe files.
Install the packages: mingw32-binutils mingw32-cpp mingw32-filesystem 
mingw32-gcc mingw32-openssl mingw32-openssl-static mingw32-runtime zip
mingw32-termcap mingw32-w32api mingw32-zlib mingw32-zlib-static mingw32-nsis
(package names for fedora 11).

For dynamic linked executables
$ mingw32-configure
$ make
$ mkdir /home/user/installdir
$ make install DESTDIR=/home/user/installdir
Find the dlls and exes in /home/user/installdir and
crypto in /usr/i686-pc-mingw32/sys-root/mingw/bin

For static linked executables
Use --enable-staticexe for mingw32-configure, see above. Or use makedist.sh,
copy System.dll from the windows dist of NSIS to /usr/share/nsis/Plugins/
Then do ./makedist.sh -w and the setup.exe is created using nsis.


+++ CREDITS

Unbound was written in portable C by Wouter Wijngaards (NLnet Labs).
See the CREDITS file in the source package for more contributor information.
Email unbound-bugs@nlnetlabs.nl

The DNS64 code was written by Viagenie, 2009, by Simon Perrault as part
of the Ecdysis project.  The code is copyright by them, and has the BSD
license (see the dns64/dns64.c file).

To enable DNS64 functionality in Unbound, two directives in unbound.conf must
be edited:

1. The "module-config" directive must start with "dns64". For example:

    module-config: "dns64 validator iterator"

If you're not using DNSSEC then you may remove "validator".

2. The "dns64-prefix" directive indicates your DNS64 prefix. For example:

    dns64-prefix: 64:FF9B::/96

The prefix must be a /96 or shorter.

To test that things are working right, perform a query against Unbound for a
domain name for which no AAAA record exists. You should see a AAAA record in
the answer section. The corresponding IPv6 address will be inside the DNS64
prefix. For example:

    $ unbound -c unbound.conf
    $ dig @localhost jazz-v4.viagenie.ca aaaa
    [...]
    ;; ANSWER SECTION:
    jazz-v4.viagenie.ca.        86400   IN      AAAA    64:ff9b::ce7b:1f02

README for Unbound 1.5.9
Copyright 2007 NLnet Labs
http://unbound.net

This software is under BSD license, see LICENSE for details.
The DNS64 module has BSD license in dns64/dns64.c.
The DNSTAP code has BSD license in dnstap/dnstap.c.

* Download the latest release version of this software from 
  	http://unbound.net 
  or get a beta version from the svn repository at 
  	http://unbound.net/svn/

* Uses the following libraries; 
  * libevent	http://www.monkey.org/~provos/libevent/		(BSD license)
    (optional) can use builtin alternative instead.
  * libexpat	(for the unbound-anchor helper program)		(MIT license)

* Make and install: ./configure; make; make install
  * --with-libevent=/path/to/libevent
  	Can be set to either the system install or the build directory.
	--with-libevent=no (default) gives a builtin alternative 
	implementation. libevent is useful when having many (thousands) 
	of outgoing ports. This improves randomization and spoof 
	resistance. For the default of 16 ports the builtin alternative 
	works well and is a little faster.
  * --with-libexpat=/path/to/libexpat
  	Can be set to the install directory of libexpat.
  * --without-pthreads 
	This disables pthreads. Without this option the pthreads library 
	is detected automatically. Use this option to disable threading
	altogether, or, on Solaris, also use --with(out)-solaris-threads.
  * --enable-checking
  	This enables assertions in the code that guard against a variety of
	programming errors, among which buffer overflows.  The program exits
	with an error if an assertion fails (but the buffer did not overflow).
  * --enable-static-exe
	This enables a debug option to statically link against the
	libevent library.
  * --enable-lock-checks
  	This enables a debug option to check lock and unlock calls. It needs
	a recent pthreads library to work.
  * --enable-alloc-checks
	This enables a debug option to check malloc (calloc, realloc, free).
	The server periodically checks if the amount of memory used fits with
	the amount of memory it thinks it should be using, and reports 
	memory usage in detail.
  * --with-conf-file=filename
  	Set default location of config file, 
	the default is /usr/local/etc/unbound/unbound.conf.
  * --with-pidfile=filename
  	Set default location of pidfile,
	the default is /usr/local/etc/unbound/unbound.pid.
  * --with-run-dir=path
  	Set default working directory,
	the default is /usr/local/etc/unbound.
  * --with-chroot-dir=path
  	Set default chroot directory,
	the default is /usr/local/etc/unbound.
  * --with-rootkey-file=path
  	Set the default root.key path.  This file is read and written.
	the default is /usr/local/etc/unbound/root.key
  * --with-rootcert-file=path
  	Set the default root update certificate path.  A builtin certificate
	is used if this file is empty or does not exist.
	the default is /usr/local/etc/unbound/icannbundle.pem
  * --with-username=user
  	Set default user name to change to,
	the default is the "unbound" user.
  * --with-pyunbound
  	Create libunbound wrapper usable from python.
	Needs python-devel and swig development tools.
  * --with-pythonmodule
  	Compile the python module that processes responses in the server.
  * --disable-sha2
  	Disable support for RSASHA256 and RSASHA512 crypto.
  * --disable-gost
  	Disable support for GOST crypto, RFC 5933.

* 'make test' runs a series of self checks.

Known issues
------------
o If there are no replies for a forward or stub zone, for a reverse zone,
  you may need to add a local-zone: name transparent or nodefault to the
  server: section of the config file to unblock the reverse zone.
  Only happens for (sub)zones that are blocked by default; e.g. 10.in-addr.arpa
o If libevent is older (before 1.3c), unbound will exit instead of reload
  on sighup. On a restart 'did not exit gracefully last time' warning is 
  printed. Perform ./configure --with-libevent=no or update libevent, rerun 
  configure and recompile unbound to make sighup work correctly.
  It is strongly suggested to use a recent version of libevent.
o If you are not receiving the correct source IP address on replies (e.g.
  you are running a multihomed, anycast server), the interface-automatic
  option can be enabled to set socket options to achieve the correct
  source IP address on UDP replies. Listing all IP addresses explicitly in
  the config file is an alternative. The interface-automatic option uses
  non portable socket options, Linux and FreeBSD should work fine.
o The warning 'openssl has no entropy, seeding with time', with chroot 
  enabled, may be solved with a symbolic link to /dev/random from <chrootdir>.
o On Solaris 5.10 some libtool packages from repositories do not work with
  gcc, showing errors gcc: unrecognized option `-KPIC'
  To solve this do ./configure libtool=./libtool [your options...].
  On Solaris you may pass CFLAGS="-xO4 -xtarget=generic" if you use sun-cc.
o If unbound-control (or munin graphs) do not work, this can often be because
  the unbound-control-setup script creates the keys with restricted 
  permissions, and the files need to be made readable or ownered by both the
  unbound daemon and unbound-control.
o Crosscompile seems to hang.  You tried to install unbound under wine.
  wine regedit and remove all the unbound entries from the registry or
  delete .wine/drive_c.

Acknowledgements
----------------
o Unbound was written in portable C by Wouter Wijngaards (NLnet Labs).
o Thanks to David Blacka and Matt Larson (Verisign) for the unbound-java
  prototype. Design and code from that prototype has been used to create
  this program. Such as the iterator state machine and the cache design.
o Other code origins are from the NSD (NLnet Labs) and LDNS (NLnet Labs)
  projects. Such as buffer, region-allocator and red-black tree code.
o See Credits file for contributors.


Your Support
------------
NLnet Labs offers all of its software products as open source, most are
published under a BSD license. You can download them, not only from the
NLnet Labs website but also through the various OS distributions for
which NSD, ldns, and Unbound are packaged. We therefore have little idea
who uses our software in production environments and have no direct ties
with 'our customers'.

Therefore, we ask you to contact us at users@NLnetLabs.nl and tell us
whether you use one of our products in your production environment,
what that environment looks like, and maybe even share some praise.
We would like to refer to the fact that your organization is using our
products. We will only do that if you explicitly allow us. In all other
cases we will keep the information you share with us to ourselves.

In addition to the moral support you can also support us
financially. NLnet Labs is a recognized not-for-profit charity foundation
that is chartered to develop open-source software and open-standards
for the Internet. If you use our software to satisfaction please express
that by giving us a donation. For small donations PayPal can be used. For
larger and regular donations please contact us at users@NLnetLabs.nl. Also
see http://www.nlnetlabs.nl/labs/contributors/.


* mailto:unbound-bugs@nlnetlabs.nl
README unbound tests

For a quick test that runs unit tests and state machine tests, use
	make test

There is a long test setup for unbound that needs tools installed. Use
	make longtest
To make and run the long tests. The results are summarized at the end.

You need to have the following programs installed and in your PATH.
* dig - from the bind-tools package. Used to send DNS queries.
* splint (optional) - for lint test
* doxygen (optional) - for doc completeness test
* ldns-testns - from ldns examples. Used as DNS auth server.
* xxd and nc (optional) - for (malformed) packet transmission.
The optional programs are detected and can be omitted.

testdata/ contains the data for tests. 
testcode/ contains scripts and c code for the tests.

do-tests.sh : runs all the tests in the testdata directory.
testbed.sh : compiles on a set of (user specific) hosts and runs do-tests.

Tests are run using testcode/mini_tpkg.sh.
README.svn

For a svn checkout:
* configure script, aclocal.m4, as well as yacc/lex output files are
  committed to the repository.
* use --enable-debug flag for configure to enable dependency tracking and
  assertions, otherwise, use make clean; make after svn update.

* Note changes in the Changelog.
* Every check-in a postcommit hook is run
	(the postcommit hook is in the svn/unbound/hooks directory).
	* generates commit email with your changes and comment.
	* compiles and runs the tests (with testcode/do-tests.sh).
	* If build errors or test errors happen
		* Please fix your errors and commit again.

* Use gnu make to compile, make or 'gmake'.
LDAP support for am-utils was originally done by Leif Johansson
<leifj AT it.su.se>.  He no longer maintains it.

The current LDAP support for am-utils is for LDAPv2 only.  Reportedly,
LDAPv3 mostly works.  Volunteers and patches are welcome.

The IANA has assigned the following Private Enterprise Number to:

	10180  Am-utils Organization   Erez Zadok	ezk AT am-utils.org

There are three files in this directory that relate to LDAP:

ldap.schema:

	This is the most current schema.

ldap-id.txt:

	This is an experimental schema for amd mount maps in LDAP.  Since
	LDAP isn't maintained now, this code may not match with either the
	current ldap_info.c code or with Amd mount maps in general.

	The schema is written in the form of an internet-draft but it has
	not been published as such.  We need volunteers who know and use
	LDAP to clean it up and further its process of submission.

ldap-id.ms:

	This is the NROFF source of the draft.  To generate the text from
	it, run "nroff -ms ldap-id.ms > ldap-id.txt"

Erez.

------- Forwarded Message

From: "IANA Private Enterprise Number" <iana-pen AT icann.org>
To: "Erez Zadok" <ezk AT cs.columbia.edu>
Subject: RE: Application for Enterprise-number (10180)
Date: Sun, 15 Jul 2001 14:43:45 -0700

Dear Erez,

The IANA has assigned the following Private Enterprise
Number to:

10180  Am-utils Organization   Erez Zadok                 ezk AT am-utils.org

Please notify the IANA if there is a change in your contact
or company information.

Thank you,

Bill Huang
IANA - Private Enterprise Numbers

------- End of Forwarded Message

		 NFS Attribute Caching OS Problems and Amd
		      Last updated September 18, 2005

* Summary:

Some OSs don't seem to have a way to turn off the NFS attribute cache, which
breaks the Amd automounter so badly that it is not recommend using Amd on
such OS for heavy use, not until this is fixed.


* Details:

Amd is a user-level NFSv2 server that manages automounts of all other file
systems.  The kernel contacts Amd via RPCs, and Amd in turn performs the
actual mounts, and then responds back to the kernel's RPCs.  Every kernel
caches attributes of files, in a cache called the Directory Name Lookup
Cache (DNLC), or a Directory Cache (dcache).

Amd manages its namespace in the user level, but the kernel caches names
itself.  So the two must coordinate to ensure that both namespaces are in
sync.  If the kernel uses a cached entry from the DNLC, without consulting
Amd, users may see corruption of the automounter namespace (symlinks
pointing to the wrong places, ESTALE errors, and more).  For example,
suppose Amd timed out an entry and removed the entry from Amd's namespace.
Amd has to tell the kernel to purge its corresponding DNLC entry too.  The
way Amd often does that is by incrementing the last modification time
(mtime) of the parent directory.  This is the most common method for kernels
to check if their DNLC entries are stale: if the parent directory mtime is
newer, the kernel will discard all cached entries for that directory, and
will re-issue lookup methods.  Those lookups will result in
NFS_GETATTR/NFS_LOOKUP calls sent from the kernel down to Amd, and Amd can
then properly inform the kernel of the new state of automounted entries.

In order to ensure that Amd is "in charge" of its namespace without
interference from the kernel, Amd will try to turn off the NFS attribute
cache.  It does so by using the NFSMNT_NOAC flag, if it exists, or by
setting various "cache timeout" fields in struct nfs_args to 0 (acregmin,
acregmax, acdirmin, or acdirmax).

We have released a major new version of am-utils, version 6.1, in June 2005.
Since then, a lot of people have experimented with Amd, in anticipation of
migrating from the very old am-utils 6.0 to the new 6.1.  For a couple of
months since the release of 6.1, we have received reports of problems with
Amd, especially under heavy use.  Users reported getting ESTALE errors from
time to time, or seeing automounted entries whose symlinks don't point to
where it should be.  After much debugging, we traced it to a few places in
Amd where it wasn't updating the parent directory mtime as it should have;
in some places where Amd was indeed updating the mtime, it was using a
resolution of only 1 second, which was not fine enough under heavy load.  We
fixed this problem and switched to using a microsecond resolution mtime.

After fixing this in Amd, we went on to verify that things work for other
OSs.  When we got to test certain BSDs, we found out that they always cache
directory entries, and there is no way to turn it off completely.
Specifically, if we set the ac{reg,dir}{min,max} fields in struct nfs_args
all to zero, the kernel seems to cache the entries for a default number of
seconds (something like 5-30 seconds).  On some OSs, setting these four
fields to 0 turns off the attribute cache, but not on some BSDs.  We were
able to verify this using Amd and a script that exercises the interaction of
the kernel's attrcache and Amd.  (If you're interested, the script can be
made available.)

We then experimented by setting the ac{reg,dir}{min,max} fields in struct
nfs_args all to 1, the smallest non-zero value we could.  When we ran the
Amd exercising script, we found that the value of 1 reduced the race between
the DNLC and Amd, and the script took a little longer to run before it
detected an incoherency.  That makes sense: the smaller the DNLC cache
interval is, the shorter the window of vulnerability is.  (BTW, the man
pages on some OSs say that the ac{reg,dir}{min,max} fields use a 1 second
resolution, but experimentation indicated it was in 0.1 second units.)

Clearly, setting the ac{reg,dir}{min,max} fields to 0 is worse than setting
it to 1 on those OSs that don't have a way to turn off the attribute cache.
So the current workaround I've implemented in am-utils is to create a
configuration parameter called "broken_attrcache" which, if turned on, will
set these nfs_args fields to 1 instead of 0.  I wish I didn't have to create
such ugly workaround features in Amd, but I've got no choice.

The near term solution is for every OS to support a true 'noac' flag, which
can be added fairly easily.  This'd make Amd work reliably.

The long term solution is to implement Autofs support for all OSs and to
support it in Amd.  Currently, Amd supports autofs on Solaris and Linux;
FreeBSD is next.  Still, we found that even with autofs support, many
sysadmins still prefer to use the good 'ol non-autofs mode.


* Confirmed Status

This is the confirmed status of various OSs' vulnerability to this attribute
cache bug.  We are slowly checking the status of other OSs.  The status of
any OS not listed is unknown as of the date at the top of this file.

** Not Vulnerable (support a proper "noac" flag):

Sun Solaris 8 and 9 (10 probably works fine)
Linux: 2.6.11 kernel (2.4.latest probably works fine)
FreeBSD 5.4 and 6.0-SNAP001 (older versions probably work fine)
OpenBSD 3.7 (older versions probably work fine)

** Vulnerable (don't support a proper "noac" flag natively):

NetBSD 2.0.2 (older versions are also probably affected)

Note: NetBSD has promised to support a noac flag hopefully after 2.1.0 is
released (maybe in 3.0 or 2.2).  In the mean time, you can apply one of
these two kernel patchs to support a 'noac' flag in NetBSD 2.x or 3.x:
	ftp://ftp.netbsd.org/pub/NetBSD/misc/christos/2x.nfs.noac.diff
	ftp://ftp.netbsd.org/pub/NetBSD/misc/christos/3x.nfs.noac.diff
After applying this patch and rebuilding your kernel, reboot with the new
kernel.  Then copy the new nfs.h and nfsmount.h from /sys/nfs/ to
/usr/include/nfs/, and finally rebuild am-utils from scratch.

** Testing

When you build am-utils, a script named scripts/test-attrcache is built,
which can be used to test the NFS attribute cache behavior of the current
OS.  You can run this script as root as follows:

# make install
# cd scripts
# sh test-attrcache

If you run this script on an OS whose status is known (and not listed
above), please report it to us via Bugzilla or the am-utils mailing list
(see www.am-utils.org), so we can record it in this file.

Sincerely,
Erez.
Here some unconfirmed instructions for how to setup Amd on a MAC OS-X
machine.  Please direct all comments about this information, as well fixes,
updates, and corrections to the am-utils mailing list (see
www.am-utils.org).

Thanks,
Erez.

--cut--here----cut--here----cut--here----cut--here----cut--here----cut--here--


Date: Fri, 28 Jan 2005 06:53:50 -0800
From: Kevin Layer

The stock amd didn't work for us in Mac OS X 10.3.
Here's how we installed am-utils-6.1-20031025:

./configure --prefix=/usr
make
make install

Create /System/Library/StartupItems/AMD with the files:

AMD
Resources/	-- copy from other directories in ..
StartupParameters.plist

Then, make sure that AMDSERVER=-YES- is added to /etc/hostconfig.

******************* the file AMD:

#!/bin/sh

##
# Start AMD
##

. /etc/rc.common

if [ "${AMDSERVER:=-NO-}" = "-YES-" ]; then
    ConsoleMessage "Starting AMD server"

    if [ ! -f /etc/amd.conf -a -r /etc/amd.conf.template ]; then
        cp /etc/amd.conf.template /etc/amd.conf
    fi
    if [ ! -f /etc/amd.map -a -r /etc/amd.map.template ]; then
        cp /etc/amd.map.template /etc/amd.map
    fi
    /usr/sbin/amd
fi

******************* the file StartupParameters.plist:

{
  Description     = "AMD server";
  Provides        = ("AMD");
  Requires        = ("Resolver");
  OrderPreference = "None";
  Messages =
  {
    start = "Starting AMD server";
    stop  = "Stopping AMD server";
  };
}

*******************

With the beta's of 10.4 (64-bit) we're having issues with realpath()
and amd mounted directories, but hopefully this is just a bug that
they'll fix.

--
Kevin Layer                     http://www.franz.com/

		       AM-UTILS YEAR-2000 COMPLIANCE

Most likely am-utils is y2k compliant.

I do not know for sure because I have not certified am-utils myself, nor do
I have the time for it.  I do not think that amd will be affected by y2k at
all, because it does not do anything with dates other than print the date on
the log file, in whatever format is provided by your os/libc --- especially
the ctime(3) call.

However, on Friday, September 18th 1998, Matthew Crosby <mcrosby AT ms.com>
reported that they evaluated 6.0a16 and found it to be compliant.

On March 26, 1999, Paul Balyoz <pbalyoz AT sedona.ch.intel.com> submitted a
patch to lostaltmail which makes it print Y2K compliant dates.  He used a
code scanner and manually "eyeballed" the code and could not find any more
problems.  Paul's patch is included in am-utils-6.0.1s7 and newer versions.
Paul also said that other 2-digit years used in am-utils are "harmless."

NOTE: NONE OF THE PERSONS MENTIONED HERE, AUTHOR INCLUDED, ARE WILLING TO
CERTIFY AM-UTILS AS Y2K COMPLIANT.  USE AT YOUR OWN RISK.

---
Erez Zadok.
Maintainer, am-utils package and am-utils list.
WWW:   http://www.am-utils.org
- update vers.m4 to the new version you want to release: then rerun ./bootstrap
- update NEWS w/ appropriate items
- ./bootstrap && ./buildall && cd A.$ARCH.$VENDOR.$OSVERSION. && make check
- cd A.$ARCH.$VENDOR.$OSVERSION make distcheck
  That generates a tar.gz w/ version number to drop into the ftp/http server
- Make sure that all your files are committed at this point
- Now tag the current trunk:
	git tag -a am-utils-X_Y[abc]Z
  or
	git tag -a am-utils-X_Y_rcZ
  If you want to move the tag, use --force
- Push all commits and tags to the master repository
	git push --all
- email announcement to am-utils-announce@am-utils.org.  See
  README.release-announce as sample.
This is the official version of am-utils.

See the file NEWS for news on this and previous releases.

*** General Notes to alpha/beta testers:

[A] as alpha/beta testers, I expect you to be able to find certain things on
your own (especially look at the sources to figure out how things work).

[B] if you intend to modify any files, first find out if the file you want
to modify gets autogenerated from some other place.  If so, modify it at the
source.

You can adjust some of the configuration of am-utils after it has been
auto-configured by putting whatever definitions you wish in a file called
localconfig.h, located in the top build directory (the same one where
config.h is created for you).

[C] there are several ways you can build am-utils:

(1) run the buildall script as follows:

	./buildall

This would build all the applications inside a special directory relative to
the root of the source tree, called A.<cpu-company-system>, where the <>
part is filled in by GNU's config.guess script.  This is the preferred
method, for it will separate the build from the sources, and allow you to
run buildall for multiple architectures concurrently.

You can run "buildall -h" to see what options it takes.

(2) run the configure script such as:

	./configure

and then run

	make

This would configure amd in the directory you've run the configure script,
and build it there.  Run "make install" to install all the necessary files.

Note that this is good for building only one version of amd on one
architecture!  Don't try this for multiple architectures.  If you must, then
after doing one such build, run "make distclean" and then reconfigure for
another architecture.

(3) run the configure script for build in a different location.  Let's say
that /src/am-utils-6.0 is where you unpacked the sources.  So you could

	mkdir /src/build/sunos5
	cd /src/build/sunos5
	/src/am-utils-6.0/configure --srcdir=/src/am-utils-6.0
	make

This is a manual method that will let you build in any directory outside the
am-utils source tree.  It requires that your "make" program understand
VPATH.  This can be used multiple times to build am-utils concurrently in
multiple (but different) directories.  In fact, the buildall script
described above does precisely that, using the A.* subdirectories.

(4) If you need to configure am-utils with extra libraries and/or headers,
for example to add hesiod support, do so as follows:

	configure --enable-libs="-lhesiod -lresolv" \
		--enable-ldflags="-L/usr/local/hesiod/lib" \
		--enable-cppflags="-I/usr/local/hesiod/include"

[D] If you modify any of the *.[chyl] sources in the directories amd, amq,
hlfsd, lib, etc, all you need to do to get a new version of am-utils is run
make.

If you modify any of the files in the m4/ or conf/ directories, or any *.in
or *.am file, then you must rebuild the configure script, Makefile.in files,
aclocal.m4, etc.  The best way to do so is to run

	./bootstrap
or
	./buildall -K

To be a developer and be able to run "bootstrap", you must have
autoconf-2.68, automake-1.11.1, and libtool 2.2.6b installed on your system (or
later versions thereof).  You no longer need to get my special version of
automake.  Contact me if you'd like to be a maintainer and get access to the
CVS server.

After you've remade the basic configuration files you must rerun the
buildall script to rerun configure and then remake the binaries.

Modifying M4 macros may not be very intuitive to anyone that has not done so
before.  Let me know if you are having any problems with them.  I fully
expect, at least initially, to have to be the sole developer of the M4
macros and let others concentrate on C sources.

[E] Report all bugs via Bugzilla or the am-utils list (see
www.am-utils.org).  Avoid reporting to my personal email address.  It is
important to involve the whole list in bug fixes etc.

Good luck.

Erez Zadok,
Maintainer, am-utils.
# This file is README.autofs
# am-utils-6.1
# Erez Zadok <ezk AT cs.columbia.edu> a.k.a. "Darth Autoconf"
#
# modified by
# Ion Badulescu <ib42 At cs.columbia.edu> a.k.a. "The Autofs Master"

** General notes about the autofs support in am-utils

- The autofs code in am-utils is gamma quality for Linux, beta quality for
  Solaris 2.5+ and non-working for all the other systems.

- Link, lofs, ufs, nfs, nfsl, and auto mounts were tested and work properly;
  the others should work, but were not tested.

* Caveats:

- [this applies to Solaris/IRIX 6/HP-UX/AIX] Amd acts as *both* automountd and
automount.  There's no way to distinguish between the two.  When amd starts,
it first registers itself as an autofs server (automountd's job), then
parses its own maps, and decides which autofs-type mounts to make
(automount's job).  After the autofs mounts are made, amd listens for
requests from the kernel-based autofs, and acts upon them. Since there can
be only one autofs listener on a system, this means that automountd and amd
cannot run at the same time; nor can two amd's run at the same time if they
are both using autofs mounts.

- Linux support is available and fairly stable. Solaris 2.5+ support is
newer and less tested, but seems pretty stable as well. IRIX 6 and HP-UX
autofs support will probably be very easy once we get Solaris 2.5 to work,
as they use the same protocol and almost identical data structures. AIX
seems to be using the Solaris 2.5 protocol as well, but they don't provide
any headers or documentation so getting autofs to work will be tricky at best.

- Killing amd can become a problem if there are active mounts. Since mounts
are done "in place", we can't just unmount our mount points and go away. For
now, amd simply tells the kernel that it is dying; we need to think this
further.  It would be nice to "take over" the old mountpoints, there is
support for this in Solaris (due to is RPC nature) and is easy to add to
the Linux kernel (an ioctl on the mountpoint).

- The Solaris 2.5 implementation of the autofs v1 protocol is NON-REENTRANT,
and ignoring this limitation results in a DEADLOCK between the kernel and
the daemon. This is a serious problem, although only for the lofs and link
mount types. In other words, suppose the daemon is trying a lofs/link mount
whose destination crosses another autofs mountpoint. If that mountpoint is
not yet mounted, it will trigger another autofs lookup which will deadlock
inside the kernel -- because the kernel is waiting for the previous request
to return. This is not even something specific to amd, Sun's own automountd
has the exact same problem. Ctrl-C breaks the deadlock, so it's not fatal,
but the lofs/link entry is in effect unusable.

- Solaris 2.8 (at least) doesn't like getting to get out-of-order replies to
mount requests, and ignores the late ones. It's only a minor annoyance (read
delay), because the RPC is retried by the kernel and it succeeds the second
time around.

* Solaris:

- Amd w/ autofs mounts will fight over the listener port with Sun's
  automountd, so running both simultaneously is a really bad idea.

- Browsable_dirs is possible and implemented for Solaris 2.6+.

- Direct mounts are implemented and work correctly.

- Host maps are implemented and work correctly.

- On Solaris 2.6+, symlinks over autofs are inefficient. The kernel seems to
  time them out immediately after receiving them, so each access to a
  symlink causes a call to userspace. Needless to say, this negates the
  whole point of using autofs. Automountd seems to always use lofs mounts
  instead of symlinks, we do the same unless the admin requests
  "autofs_use_lofs = no" in amd.conf.

- Solaris 2.5/2.5.1 do not support symlinks over autofs, so links are always
  mounted as lofs mounts, subject to the limitation described in the caveats
  section above.

- Restarting autofs mounts is possible, but not yet implemented.

* Linux:

- Amd should work fine even when the Linux automounter is running, the
  mechanism being used prevents any kind of (evil) interaction between them.

- Browsing is not available if autofs support is used, due to limitations in
  the kernel<->daemon protocol used by Linux 2.2 and 2.4. Only already-mounted
  nodes will appear in the autofs directory, and this is implemented entirely
  in the kernel.

- Host maps are supported with all autofs versions.

- Direct maps cannot be supported since there is no kernel support for them;
  we might be able to get something eventually, but don't hold your breath.
  If anything, we may do it using a regular NFS mountpoint and bind-mount on
  top of it.

- Inherit doesn't make much sense because we can't restart a hung autofs
  mount point, due to kernel limitations. This needs to be fixed in the
  Linux kernel; it's not particularly difficult, and we might provide a
  patch at some point.

- Link (and lofs) mounts will use the new bind-mount support in Linux
  2.4+. No more symlinks! And /bin/pwd works great too.

- Auto maps are actually mounted as a separate autofs filesystem. Since each
  autofs filesystem consumes 2 file descriptors, and amd has at most 1024
  file descriptors available, there can be at most 512 of them mounted at
  the same time. Because of this, extensive use of auto maps is discouraged
  when using autofs on Linux.

* amd.conf requirements:

To tell amd to use an autofs-style mount point and mounts for a map, add

	mount_type = autofs

either to the global section, or to the sections of the individual maps you
selected. Mixing autofs and normal amd mount points in the same amd.conf
file *does* work.

* Map changes:

No changes are necessary. If a map is marked as autofs in amd.conf, mounts
are done "in place" and the "fs" parameter is ignored most of the time.

* Todo:

We are looking for volunteers to improve the autofs code!

(1) These fixes are needed:

- sublinks are broken

- when the mount type is 'link', transparently translate it into a loopback
file system mount (lofs), that would mount in place, rather than supply a
symlink, at least on systems whose autofs doesn't support symlinks. Linux
does support symlinks, Solaris 2.6+ does too, but Solaris 2.5/2.5.1 doesn't
and neither does IRIX 6. Moreover, Sun's automountd always uses lofs for
link mounts, even on 2.6+, because symlinks are not cached in the kernel and
thus are not particularly efficient. [done]

- complain if certain incompatible options (autofs and ...) are used. Direct
maps on Linux is one such case of incompatible options. Browsable_dirs on
Linux is another such case.

- if amd is killed or dies, the autofs mounts will remain intact as required
(your system is _not_ hung, yay!).  However, if you restart amd, it will not
correctly 'restart' the autofs mounts as the Sun automounter does.  Rather,
it might cause another mount to happen, which leaves your /etc/mnttab
cluttered with older mounts entries which cannot be unmounted. It might also
just pretend everything is ok, when in fact it isn't.

(2) Code expansion:

- [Solaris only] implement the sockets version of amu_get_autofs_address()
and create_autofs_service(), in conf/transp/transp_sockets.c. Not sure if
it's necessary, Solaris it still biased towards TLI/STREAMS in
userspace. [mostly done, untested, might be used on AIX 5.x+ and IRIX 6]

- Implement the restarting of autofs mount points. This is already doable on
Solaris; on Linux, the kernel needs to be patched to allow it.

(3) Testing and porting to other systems:

- nothing has been tested on IRIX 6, which reportedly has a similarly
functioning autofs to Solaris 2.5.  The code compiles, but has not been run
yet and is most likely broken.

- support for Linux autofs is stable, we need testers!

- support for Solaris 2.6+ is pretty stable, so we need testers for it, too!

- we did not test any version of Solaris on x86. It will probably work, but
you have been warned. Testers are welcome.
		Sun-style Automounter Syntax Support in Amd
		       Last updated: October 10, 2005


* Summary

The goal of this effort is to provide a drop in replacement for the Sun
automounter.  This is implemented in two ways.  (1) A new parser built into
Amd.  (2) a standalone sun2amd Unix filter tool that can convert Sun-style
maps to Amd maps.

Support for Sun-style maps in Amd (including this document) is a
work-in-progress.  This feature as a whole has not been throughly tested and
is "alpha" quality as of the date posted at the top of this document.

* Status

Currently sun2amd supports most of the basic syntax features of Sun maps.
However, support for Sun auto.master maps does not yet exists at any level.
Below is a list of supported and not-yet supported features.

1. Supported Sun map features:

- Simple map entry format: <key> [mount_options] location ...
	locations take the form of host:pathname

- Map Key Substitution: <key> <hostname>:/tmp/&
	the & expands to <key>

- Wildcard Key: * <hostname>:/tmp
	This is is a catch-all entry.

- Variable Substitution:
	$ARCH, $CPU, $HOST, $OSNAME, $OSREL, $OSVERS, $NATISA

- Multiple Mount format:
	<key> [mount_options] [mountpoint] [mount-options locations...]...

	ex.
		pluto -ro /tmp1 host1:/tmp1 /tmp2 host2:/tmp2

- HSFS file-system (cdrom): <key> -fstype=hsfs,ro :/dev/cdrom


2. Not-yet supported Sun map features:

- Replicated File Systems: <hostname>,<hostname>...:<pathname>
	This is a feature of Solaris that allows a user to specify a
	set of hosts to appear as one in a map entry.  When a host
	does not respond the kernel will switch to an alternate
	server.  Amd will supports the syntax for replicated file
	systems by creating multiple hosts, however, Amd will not
	perform any kind of fail over.

- CacheFS (although Amd supports type:=cachefs, sun2amd doesn't yet parse
  it).

- AutoFS (although Amd supports Autofs mounts and type:=auto, sun2amd
  may not parse it fully).

- Included maps: +<map_name>
	This will include the contents of a map into another map.

- Sun master maps: auto.master
	Amd still relies and amd.conf.  Any feature in auto.master
        must be duplicated using Amd equivalent features in amd.conf.

- /net mount point:
	The automounter is suppose to dynamically create map entries
        under this mount point corresponding to NFS server(s) exported
        file systems.  While this isn't parser by sun2amd, Amd does support
	host maps (type:=host).

- Federated Naming Service (FNS) (-xfn):
	A naming server that wraps a number of other naming service
        under one interface.

- "-null" map option: A way to cancel maps in the auto.master file.

- nsswitch.conf:
	This file provides a way to specify what lookup service to use
        for a number of systems including Sun's automounter.  In amd
        lookup services for map files are define in the amd.conf file.


* Setup

To enable Sun-style maps in Amd, set "sun_map_syntax = yes" in your amd.conf
file.  When this flag is set in [global], all maps read by Amd are assumed
to be Sun style maps.  You can set this on a per map basis, thus mixing
Sun-style and Amd-style maps.  For more information about amd.conf please
see the Amd documentation.

Example:

# file: amd.conf
################
[ global ]
sun_map_syntax = yes

[ /home ]
map_name = /etc/amd.sun_map
map_type = file


* sun2amd command line utility

In addition to build-in Amd support there also exists the sun2amd command
line utility that converts Sun maps to Amd maps.  This can be useful in
migrating one or more Sun maps to the Amd syntax in one step.

Example usage:

$ sun2amd -i sun_map -o amd_map

This line tells sun2amd to read the file sun_map and redirect its output to
a file called amd_map.  By default sun2amd reads from stdin and writes to
stdout (i.e., traditional Unix filter).


* FAQ

Q1: I know "/net" is not supported, but how can I achieve the same
    functionality with Amd?

A1: In short, you must create the '/net' entry as a Amd entry of type host
    by hand.  Below is an example of how to do this.  See the Amd
    documentation on type autofs for more information.

Example:

# file: amd.conf
################
[ /net ]
map_name = /etc/amd.net

# file: /etc/amd.net
###############
/defaults  fs:=${autodir}/${rhost}/root/${rfs}
*          rhost:=${key};type:=host;rfs:=/

(more FAQ entries to come...)
To: am-utils-announce@am-utils.org
Cc: am-utils@am-utils.org
Subject: am-utils-6.2-rc1 released

We've released am-utils-6.2-rc1, first and only release candidate in the
6.2 series (with nfsv4 support, autofs supports, and quite a few more
features).  This will be the first official release since 2006, so expect a
lot of bug fixes and improvements.

You can download am-utils-6.2-rc1 as well as other releases from:

	http://www.am-utils.org/

*** Notes specific to am-utils version 6.2-rc1

Filesystem Support:
    - Add support for NFSv4
    - Add support for Lustre
    - Add support for ext{2,3,4}
    - Add support for linux autofs version 5
    - Add support for TMPFS and UDF
New features:
    - Add amq -i (prints information about maps)
    - Add synchronous unmount code for amq -uu
    - Allow a comma-separated list of LDAP servers for failover
Changes in behavior:
    - Empty variable assignments, now unset the variable value.
    - Share LDAP connections between different maps to save resources
Portability fixes:
    - Changes to work with Linux-2.6.x, Linux-3.x and NetBSD-5.x, NetBSD-6.x,
      FreeBSD 7.x, Solaris
Bug fixes:
    - Many bug fixes, see ChangeLog

- minor new ports:
	* Please send us the 'config.guess' and 'amd -v' outputs
	  so we can complete this list.

- Bugs fixed:
	* many race conditions found and fixed by Krisztian Kovacs.

Cheers,
The am-utils team.

PS. Many thanks for Christos Zoulas for a huge amount of work he put in
towards this long-awaited release.
Introduction

   TRE is a lightweight, robust, and efficient POSIX compliant regexp
   matching library with some exciting features such as approximate
   (fuzzy) matching.

   The matching algorithm used in TRE uses linear worst-case time in
   the length of the text being searched, and quadratic worst-case
   time in the length of the used regular expression. In other words,
   the time complexity of the algorithm is O(M^2N), where M is the
   length of the regular expression and N is the length of the
   text. The used space is also quadratic on the length of the regex,
   but does not depend on the searched string. This quadratic
   behaviour occurs only on pathological cases which are probably very
   rare in practice.

Features

   TRE is not just yet another regexp matcher. TRE has some features
   which are not there in most free POSIX compatible
   implementations. Most of these features are not present in non-free
   implementations either, for that matter.

Approximate matching

   Approximate pattern matching allows matches to be approximate, that
   is, allows the matches to be close to the searched pattern under
   some measure of closeness. TRE uses the edit-distance measure (also
   known as the Levenshtein distance) where characters can be
   inserted, deleted, or substituted in the searched text in order to
   get an exact match. Each insertion, deletion, or substitution adds
   the distance, or cost, of the match. TRE can report the matches
   which have a cost lower than some given threshold value. TRE can
   also be used to search for matches with the lowest cost.

   TRE includes a version of the agrep (approximate grep) command line
   tool for approximate regexp matching in the style of grep. Unlike
   other agrep implementations (like the one by Sun Wu and Udi Manber
   from University of Arizona available here) TRE agrep allows full
   regexps of any length, any number of errors, and non-uniform costs
   for insertion, deletion and substitution.

Strict standard conformance

   POSIX defines the behaviour of regexp functions precisely. TRE
   attempts to conform to these specifications as strictly as
   possible. TRE always returns the correct matches for subpatterns,
   for example. Very few other implementations do this correctly. In
   fact, the only other implementations besides TRE that I am aware of
   (free or not) that get it right are Rx by Tom Lord, Regex++ by John
   Maddock, and the AT&T ast regex by Glenn Fowler and Doug McIlroy.

   The standard TRE tries to conform to is the IEEE Std 1003.1-2001,
   or Open Group Base Specifications Issue 6, commonly referred to as
   "POSIX".  It can be found online here. The relevant parts are the
   base specifications on regular expressions (and the rationale) and
   the description of the regcomp() API.

   For an excellent survey on POSIX regexp matchers, see the testregex
   pages by Glenn Fowler of AT&T Labs Research.

Predictable matching speed

   Because of the matching algorithm used in TRE, the maximum time
   consumed by any regexec() call is always directly proportional to
   the length of the searched string. There is one exception: if back
   references are used, the matching may take time that grows
   exponentially with the length of the string. This is because
   matching back references is an NP complete problem, and almost
   certainly requires exponential time to match in the worst case.

Predictable and modest memory consumption

   A regexec() call never allocates memory from the heap. TRE
   allocates all the memory it needs during a regcomp() call, and some
   temporary working space from the stack frame for the duration of
   the regexec() call. The amount of temporary space needed is
   constant during matching and does not depend on the searched
   string. For regexps of reasonable size TRE needs less than 50K of
   dynamically allocated memory during the regcomp() call, less than
   20K for the compiled pattern buffer, and less than two kilobytes of
   temporary working space from the stack frame during a regexec()
   call. There is no time/memory tradeoff. TRE is also small in code
   size; statically linking with TRE increases the executable size
   less than 30K (gcc-3.2, x86, GNU/Linux).

Wide character and multibyte character set support

   TRE supports multibyte character sets. This makes it possible to
   use regexps seamlessly with, for example, Japanese locales. TRE
   also provides a wide character API.

Binary pattern and data support

   TRE provides APIs which allow binary zero characters both in
   regexps and searched strings. The standard API cannot be easily
   used to, for example, search for printable words from binary data
   (although it is possible with some hacking). Searching for patterns
   which contain binary zeroes embedded is not possible at all with
   the standard API.

Completely thread safe

   TRE is completely thread safe. All the exported functions are
   re-entrant, and a single compiled regexp object can be used
   simultaneously in multiple contexts; e.g. in main() and a signal
   handler, or in many threads of a multithreaded application.

Portable

   TRE is portable across multiple platforms. Here's a table of
   platforms and compilers that have been successfully used to compile
   and run TRE:

      Platform(s)                       | Compiler(s)
      ----------------------------------+------------
      AIX 4.3.2 - 5.3.0                 | GCC, C for AIX compiler version 5
      Compaq Tru64 UNIX V5.1A/B         | Compaq C V6.4-014 - V6.5-011
      Cygwin 1.3 - 1.5                  | GCC
      Digital UNIX V4.0                 | DEC C V5.9-005
      FreeBSD 4 and above               | GCC
      GNU/Linux systems on x86, x86_64, | GCC
      ppc64, s390			|
      HP-UX 10.20- 11.00                | GCC, HP C Compiler
      IRIX 6.5                          | GCC, MIPSpro Compilers 7.3.1.3m
      Max OS X				|
      NetBSD 1.5 and above              | GCC, egcs
      OpenBSD 3.3 and above             | GCC
      Solaris 2.7-10 sparc/x86          | GCC, Sun Workshop 6 compilers
      Windows 98 - XP                   | Microsoft Visual C++ 6.0

   TRE 0.7.5 should compile without changes on all of the above
   platforms.  Tell me if you are using TRE on a platform that is not
   listed above, and I'll add it to the list. Also let me know if TRE
   does not work on a listed platform.

   Depending on the platform, you may need to install libutf8 to get
   wide character and multibyte character set support.

 Free

   TRE is released under a license which is essentially the same as
   the "2 clause" BSD-style license used in NetBSD.  See the file
   LICENSE for details.

Roadmap

   There are currently two features, both related to collating
   elements, missing from 100% POSIX compliance. These are:

     * Support for collating elements (e.g. [[.<X>.]], where <X> is a
       collating element). It is not possible to support
       multi-character collating elements portably, since POSIX does
       not define a way to determine whether a character sequence is a
       multi-character collating element or not.

     * Support for equivalence classes, for example [[=<X>=]], where
       <X> is a collating element. An equivalence class matches any
       character which has the same primary collation weight as
       <X>. Again, POSIX provides no portable mechanism for
       determining the primary collation weight of a collating
       element.

   Note that other portable regexp implementations don't support
   collating elements either. The single exception is Regex++, which
   comes with its own database for collating elements for different
   locales. Support for collating elements and equivalence classes has
   not been widely requested and is not very high on the TODO list at
   the moment.

   These are other features I'm planning to implement real soon now:

     * All the missing GNU extensions enabled in GNU regex, such as
       [[:<:]] and [[:>:]]

     * A REG_SHORTEST regexec() flag for returning the shortest match
       instead of the longest match.

     * Perl-compatible syntax

            [:^class:]
               Matches anything but the characters in class. Note that
               [^[:class:]] works already, this would be just a
               convenience shorthand.

            \A
               Match only at beginning of string

            \Z
               Match only at end of string, or before newline at the end

            \z
               Match only at end of string

            \l
               Lowercase next char (think vi)

            \u
               Uppercase next char (think vi)

            \L
               Lowercase till \E (think vi)

            \U
               Uppercase till \E (think vi)

            (?=pattern)
               Zero-width positive look-ahead assertions.

            (?!pattern)
               Zero-width negative look-ahead assertions.

            (?<=pattern)
               Zero-width positive look-behind assertions.

            (?<!pattern)
               Zero-width negative look-behind assertions.

   Documentation especially for the nonstandard features of TRE, such
   as approximate matching, is a work in progress (with "progress"
   loosely defined...)

   Mailing lists

   tre-general@lists.laurikari.net
      This list is for any discussion on the TRE software, including
      reporting bugs, feature requests, requests for help, and other
      things.

   tre-announce@lists.laurikari.net
      Subscribe to this list to get announcements of new releases of
      TRE.  Alternatively, you can subscribe to the freshmeat.net
      project and get similar announcements that way, if you prefer.

Ville Laurikari    <vl@iki.fi>
This code is structured roughly as follows:

xmalloc.c:
  - Wrappers for the malloc() functions, for error generation and
    memory leak checking purposes.

tre-mem.c:
  - A simple and efficient memory allocator.

tre-stack.c:
  - Implements a simple stack data structure.

tre-ast.c:
  - Abstract syntax tree (AST) definitions.

tre-parse.c:
  - Regexp parser.  Parses a POSIX regexp (with TRE extensions) into
    an abstract syntax tree (AST).

tre-compile.c:
  - Compiles ASTs to ready-to-use regex objects.  Comprised of two parts:
    * Routine to convert an AST to a tagged AST.  A tagged AST has
      appropriate minimized or maximized tags added to keep track of
      submatches.
    * Routine to convert tagged ASTs to tagged nondeterministic state
      machines (TNFAs) without epsilon transitions (transitions on
      empty strings).

tre-match-parallel.c:
  - Parallel TNFA matcher.
    * The matcher basically takes a string and a TNFA and finds the
      leftmost longest match and submatches in one pass over the input
      string.  Only the beginning of the input string is scanned until
      a leftmost match and longest match is found.
    * The matcher cannot handle back references, but the worst case
      time consumption is O(l) where l is the length of the input
      string.  The space consumption is constant.

tre-match-backtrack.c:
  - A traditional backtracking matcher.
    * Like the parallel matcher, takes a string and a TNFA and finds
      the leftmost longest match and submatches.  Portions of the
      input string may (and usually are) scanned multiple times.
    * Can handle back references.  The worst case time consumption,
      however, is O(k^l) where k is some constant and l is the length
      of the input string.  The worst case space consumption is O(l).

tre-match-approx.c:
  - Approximate parallel TNFA matcher.
    * Finds the leftmost and longest match and submatches in one pass
      over the input string.  The match may contain errors.  Each
      missing, substituted, or extra character in the match increases
      the cost of the match.  A maximum cost for the returned match
      can be given.  The cost of the found match is returned.
    * Cannot handle back references.  The space and time consumption
      bounds are the same as for the parallel exact matcher, but
      in general this matcher is slower than the exact matcher.

regcomp.c:
  - Implementation of the regcomp() family of functions as simple
    wrappers for tre_compile().

regexec.c:
  - Implementation of the regexec() family of functions.
    * The appropriate matcher is dispatched according to the
      features used in the compiled regex object.

regerror.c:
  - Implements the regerror() function.
Kyua (pronounced Q.A.) is a testing framework for both developers and
users.  Kyua is different from most other testing frameworks in that it
puts the end user experience before anything else.  There are multiple
reasons for users to run the tests themselves, and Kyua ensures that
they can do so in the most convenient way.

This module, kyua-testers, provides scriptable interfaces to interact
with test programs of various kinds.  The interface of such testers
allows the caller to execute a single test case of a single test program
in a controlled and homogeneous manner.

For further information on the contents of this distribution file,
please refer to the following other documents:

* AUTHORS: List of authors and contributors to this project.
* COPYING: License information.
* INSTALL: Compilation and installation instructions.
* NEWS: List of major changes between formal releases.

For general project information, please visit:

    http://code.google.com/p/kyua/
PPP Support for Microsoft's CHAP-81
===================================

Frank Cusack		frank@google.com

Some text verbatim from README.MSCHAP80,
by Eric Rosenquist, rosenqui@strataware.com

INTRODUCTION

First, please read README.MSCHAP80; almost everything there applies here.
MS-CHAP was basically devised by Microsoft because rather than store
plaintext passwords, they (Microsoft) store the md4 hash of passwords.
It provides no advantage over standard CHAP, since the hash is used
as plaintext-equivalent.  (Well, the Change-Password packet is arguably
an advantage.)  It does introduce a significant weakness if the LM hash
is used.  Additionally, the format of the failure packet potentially
gives information to an attacker.  The weakness of the LM hash is partly
addressed in RFC 2433, which deprecates its use.

MS-CHAPv2 adds 2 benefits to MS-CHAP.  (1) The LM hash is no longer
used.  (2) Mutual authentication is required.  Note that the mutual
authentication in MS-CHAPv2 is different than the case where both PPP
peers require authentication from the other; the former proves that
the server has access to the client's password, the latter proves that
the server has access to a secret which the client also has -- which
may or may not be the same as the client's password (but should not be
the same, per RFC 1994).  Whether this provides any actual benefit is
outside the scope of this document.  The details of MS-CHAPv2 can be
found in the document:

    <http://www.ietf.org/rfc/rfc2759.txt>


BUILDING THE PPPD

In addition to the requirements for MS-CHAP, MS-CHAPv2 uses the SHA-1
hash algorithm.  A public domain implementation is provided with pppd.


TROUBLESHOOTING

Assuming that everything else has been configured correctly for PPP and
CHAP, the MS-CHAPv2-specific problems you're likely to encounter are mostly
related to your Windows NT account and its settings.  A Microsoft server
returns error codes in its CHAP response.  The following are extracted from
RFC 2759:

 646 ERROR_RESTRICTED_LOGON_HOURS
 647 ERROR_ACCT_DISABLED
 648 ERROR_PASSWD_EXPIRED
 649 ERROR_NO_DIALIN_PERMISSION
 691 ERROR_AUTHENTICATION_FAILURE
 709 ERROR_CHANGING_PASSWORD

You'll see these in your pppd log as a line similar to:

   Remote message: E=649 No dialin permission

Previously, pppd would log this as:

   Remote message: E=649 R=0

Now, the text message is logged (both for MS-CHAP and MS-CHAPv2).

	     Microsoft Call Back Configuration Protocol.
			by Pedro Roque Marques
			(updated by Paul Mackerras)

The CBCP is a method by which the Microsoft Windows NT Server may
implement additional security. It is possible to configure the server
in such a manner so as to require that the client systems which
connect with it are required that following a valid authentication to
leave a method by which the number may be returned call.

It is a requirement of servers to be so configured that the protocol be
exchanged.

So, this set of patches may be applied to the pppd process to enable
the cbcp client *only* portion of the specification. It is primarily
meant to permit connection with Windows NT Servers.

The ietf-working specification may be obtained from ftp.microsoft.com
in the developr/rfc directory.

The ietf task group has decided to recommend that the LCP sequence be
extended to permit the callback operation. For this reason, these
patches are not 'part' of pppd but are an adjunct to the code.

To enable CBCP support, all that is required is to uncomment the line
in Makefile.linux that sets CBCP=y and recompile pppd.

I use such script to make a callback:

pppd debug nodetach /dev/modem 115200 crtscts modem	\
callback 222222 name NAME remotename SERVER	\
connect 'chat -v "" atz OK atdt111111 CONNECT ""'
sleep 1
pppd debug /dev/modem 115200 crtscts modem	\
name NAME remotename SERVER defaultroute	\
connect 'chat -v RING ATA CONNECT "\c"'

First we invoke pppd with 'nodetach' option in order to not detach from
the controlling terminal and 'callback NUMBER' option, then wait for
1 second and invoke pppd again which waits for a callback (RING) and
then answers (ATA). Number 222222 is a callback number, i.e. server will
call us back at this number, while number 111111 is the number we are
calling to.

You have to put in /etc/ppp/chap-secrets the following two lines:

NAME SERVER PASSWORD
SERVER NAME PASSWORD

You have to use your real login name, remote server name and password.

EAP with MD5-Challenge and SRP-SHA1 support
by James Carlson, Sun Microsystems
Version 2, September 22nd, 2002


1.  What it does

    The Extensible Authentication Protocol (EAP; RFC 2284) is a
    security protocol that can be used with PPP.  It provides a means
    to plug in multiple optional authentication methods.

    This implementation includes the required default MD5-Challenge
    method, which is similar to CHAP (RFC 1994), as well as the new
    SRP-SHA1 method.  This latter method relies on an exchange that is
    not vulnerable to dictionary attacks (as is CHAP), does not
    require the server to keep a cleartext copy of the secret (as in
    CHAP), supports identity privacy, and produces a temporary shared
    key that could be used for data encryption.

    The SRP-SHA1 method is based on draft-ietf-pppext-eap-srp-03.txt,
    a work in progress.

2.  Required libraries

    Two other packages are required first.  Download and install
    OpenSSL and Thomas Wu's SRP implementation.

	http://www.openssl.org/ (or ftp://ftp.openssl.org/source/)
	http://srp.stanford.edu/

    Follow the directions in each package to install the SSL and SRP
    libraries.  Once SRP is installed, you may run tconf as root to
    create known fields, if desired.  (This step is not required.)

3.  Installing the patch

    The EAP-SRP patch described here is integrated into this version
    of pppd.  The following patch may be used with older pppd sources:

	ftp://playground.sun.com/carlsonj/eap/ppp-2.4.1-eap-1.tar.gz

    Configure, compile, and install as root.  You may want to edit
    pppd/Makefile after configuring to enable or disable optional
    features.

	% ./configure
	% make
	% su
	# make install

    If you use csh or tcsh, run "rehash" to pick up the new commands.

    If you're using Solaris, and you run into trouble with the
    pseudonym feature on the server side ("no DES here" shows in the
    log file), make sure that you have the "domestic" versions of the
    DES libraries linked.  You should see "crypt_d" in "ldd
    /usr/local/bin/pppd".  If you see "crypt_i" instead, then make
    sure that /usr/lib/libcrypt.* links to /usr/lib/libcrypt_d.*.  (If
    you have the international version of Solaris, then you won't have
    crypt_d.  You might want to find an alternative DES library.)

4.  Adding the secrets

    On the EAP SRP-SHA1 client side, access to the cleartext secret is
    required.  This can be done in two ways:

	- Enter the client name, server name, and password in the
          /etc/ppp/srp-secrets file.  This file has the same format as
          the existing chap-secrets and pap-secrets files.

	  clientname servername "secret here"

	- Use the "password" option in any of the standard
          configuration files (or the command line) to specify the
          secret.

	  password "secret here"

    On the EAP SRP-SHA1 server side, a secret verifier is required.
    This is a one-way hash of the client's name and password.  To
    generate this value, run the srp-entry program (see srp-entry(8)).
    This program prompts for the client name and the passphrase (the
    secret).  The output will be an entry, such as the following,
    suitable for use in the server's srp-secrets file.  Note that if
    this is transferred by cut-and-paste, the entry must be a single
    line of text in the file.

pppuser srpserver 0:LFDpwg4HBLi4/kWByzbZpW6pE95/iIWBSt7L.DAkHsvwQphtiq0f6reoUy/1LC1qYqjcrV97lCDmQHQd4KIACGgtkhttLdP3KMowvS0wLXLo25FPJeG2sMAUEWu/HlJPn2/gHyh9aT.ZxUs5MsoQ1E61sJkVBc.2qze1CdZiQGTK3qtWRP6DOpM1bfhKtPoVm.g.MiCcTMWzc54xJUIA0mgKtpthE3JrqCc81cXUt4DYi5yBzeeGTqrI0z2/Gj8Jp7pS4Fkq3GmnYjMxnKfQorFXNwl3m7JSaPa8Gj9/BqnorJOsnSMlIhBe6dy4CYytuTbNb4Wv/nFkmSThK782V:2cIyMp1yKslQgE *

    The "secret" field consists of three entries separated by colons.
    The first entry is the index of the modulus and generator from
    SRP's /etc/tpasswd.conf.  If the special value 0 is used, then the
    well-known modulus/generator value is used (this is recommended,
    because it is much faster).  The second value is the verifier
    value.  The third is the password "salt."  These latter two values
    are encoded in base64 notation.

    For EAP MD5-Challenge, both client and server use the existing
    /etc/ppp/chap-secrets file.

5.  Configuration options

    There are two main options relating to EAP available for the
    client.  These are:

	refuse-eap		- refuse to authenticate with EAP
	srp-use-pseudonym	- use the identity privacy if
				  offered by server

    The second option stores a pseudonym, if offered by the EAP
    SRP-SHA1 server, in the $HOME/.ppp_pseudonym file.  The pseudonym
    is typically an encrypted version of the client identity.  During
    EAP start-up, the pseudonym stored in this file is offered to the
    peer as the identity.  If this is accepted by the peer, then
    eavesdroppers will be unable to determine the identity of the
    client.  Each time the client is authenticated, the server will
    offer a new pseudoname to the client using an obscured (reversibly
    encrypted) message.  Thus, access across successive sessions
    cannot be tracked.

    There are two main options for EAP on the server:

	require-eap		- require client to use EAP
	srp-pn-secret "string"	- set server's pseudoname secret

    The second option sets the long-term secret used on the server to
    encrypt the user's identity to produce pseudonames.  The
    pseudoname is constructed by hashing this string with the current
    date (to the nearest day) with SHA1, then using this hash as the
    key for a DES encryption of the client's name.  The date is added
    to the hash for two reasons.  First, this allows the pseudonym to
    change daily.  Second, it allows the server to decode any previous
    pseudonym by trying previous dates.

    See the pppd(8) man page for additional options.

6.  Comments welcome!

    This is still an experimental implementation.  It has been tested
    and reviewed carefully for correctness, but may still be
    incomplete or have other flaws.  All comments are welcome.  Please
    address them to the author:

		james.d.carlson@sun.com

    or, for EAP itself or the SRP extensions to EAP, to the IETF PPP
    Extensions working group:

		ietf-ppp@merit.edu
		PPPoE Support
		-------------

		Michal Ostrowski
		8 August 2001

		for ppp-2.4.2
		Updated for ppp-2.4.5 by Paul Mackerras, Sep 08

1. Introduction
---------------

This document describes the support for PPP over Ethernet (PPPoE)
included with this package.  It is assumed that the reader is
familiar with Linux PPP (as it pertains to tty/modem-based
connections).  In particular, users of PPP in the Linux 2.2 series
kernels should ensure they are familiar with the changes to the PPP
implementation in the 2.4 series kernels before attempting to use
PPPoE features.

If you are not familiar with PPP, I recommend looking at other
packages which include end-user configuration tools, such as Roaring
Penguin (http://www.roaringpenguin.com/pppoe).

PPPoE is a protocol typically used by *DSL providers to manage IP
addresses and authenticate users.  Essentially, PPPoE provides for a
PPP connection to be established not over a physical serial-line or
modem, but over a logical connection between two unique MAC-addresses
on an ethernet network.  Once the PPPoE layer discovers the end-points
to be used in the link and negotiates it, frames may be sent to and
received from the PPPoE layer just as if the link was a serial line
(or that is how it's supposed to be).

With this in mind, the goal of the implementation of PPPoE support in
Linux is to allow users to simply specify that the device they intend
to use for the PPP connection is an ethernet device (e.g. "eth0") and
the rest of the system should function as usual.

2. Using PPPoE
--------------

This section is a quick guide for getting PPPoE working, to allow one
to connect to their ISP who is providing PPPoE based services.

1.  Enable "Prompt for development and/or incomplete code/drivers" and
    "PPP over Ethernet" in your kernel configuration.  Most distributions
    will include the kernel PPPoE module by default.

2.  Compile and install your kernel.

3.  Install the ppp package.

4.  Add the following line to /etc/ppp/options:

    plugin rp-pppoe.so

    The effect of this line is simply to make "eth0", "eth1",
    ....,"ethx" all valid device names for pppd (just like ttyS0,
    ttyS1).

5.  Add the necessary authentication options to your pppd
    configuration (i.e. PAP/CHAP information).  If you wish to
    maintain seperate configurations for different devices you may
    place configuration options in device-specific configuration
    files: /etc/ppp/options.devname (devname=ttyS0, ttyS1, eth0, eth1
    or any other valid device name).

6.  Invoke pppd with the appropriate device name: e.g. "pppd eth0"


Do not include any compression or flow control options in your PPPoE
configuration.  They will be ignored.

Again, here it is assumed that the reader is familiar with the general
process of configuring PPP.  The steps outlined here refer only to the
steps and configuration options which are PPPoE specific, and it is
assumed that the reader will also configure other aspects of the system
(e.g. PAP authentication parameters).

3.  Advanced Functionality
--------------------------

For more advanced functionality (such as providing PPPoE services) and
user configuration tools, look to the Roaring Penguin PPPoE software
package (http://www.roaringpenguin.com/pppoe).

4.  Credits
-----------

The PPPoE plugin included in this package is a component of the
Roaring Penguin PPPoE package, included in this package courtesy of
Roaring Penguin Software. (http://www.roaringpenguin.com).

PPP Support for Microsoft's CHAP-80
===================================

Eric Rosenquist          rosenqui@strataware.com
(updated by Paul Mackerras)
(updated by Al Longyear)
(updated by Farrell Woods)
(updated by Frank Cusack)

INTRODUCTION

Microsoft has introduced an extension to the Challenge/Handshake
Authentication Protocol (CHAP) which avoids storing cleartext
passwords on a server.  (Unfortunately, this is not as secure as it
sounds, because the encrypted password stored on a server can be used
by a bogus client to gain access to the server just as easily as if
the password were stored in cleartext.)  The details of the Microsoft
extensions can be found in the document:

    <http://www.ietf.org/rfc/rfc2433.txt>

In short, MS-CHAP is identified as <auth chap 80> since the hex value
of 80 is used to designate Microsoft's scheme.  Standard PPP CHAP uses
a value of 5.  If you enable PPP debugging with the "debug" option and
see something like the following in your logs, the remote server is
requesting MS-CHAP:

  rcvd [LCP ConfReq id=0x2 <asyncmap 0x0> <auth MS> <magic 0x46a3>]
                                           ^^^^^^^

MS-CHAP is enabled by default under Linux in pppd/Makefile.linux by
the line "CHAPMS=y".


CONFIGURATION

If you've never used PPPD with CHAP before, read the man page (type
"man pppd") and read the description in there.  Basically, you need to
edit the "chap-secrets" file typically named /etc/ppp/chap-secrets.
This should contain the following two lines for each system with which
you use CHAP (with no leading blanks):

    RemoteHost  Account     Secret
    Account     RemoteHost  Secret

Note that you need both lines and that item 1 and 2 are swapped in the
second line.  I'm not sure why you need it twice, but it works and I didn't
have time to look into it further.  The "RemoteHost" is a somewhat
arbitrary name for the remote Windows NT system you're dialing.  It doesn't
have to match the NT system's name, but it *does* have to match what you
use with the "remotename" parameter.  The "Account" is the Windows NT
account name you have been told to use when dialing, and the "Secret" is
the password for that account.  For example, if your service provider calls
their machine "DialupNT" and tells you your account and password are
"customer47" and "foobar", add the following to your chap-secrets file:

    DialupNT    customer47  foobar
    customer47  DialupNT    foobar

The only other thing you need to do for MS-CHAP (compared to normal CHAP)
is to always use the "remotename" option, either on the command line or in
your "options" file (see the pppd man page for details).  In the case of
the above example, you would need to use the following command line:

    pppd name customer47 remotename DialupNT <other options>

or add:

    name customer47
    remotename DialupNT

to your PPPD "options" file.

The "remotename" option is required for MS-CHAP since Microsoft PPP servers
don't send their system name in the CHAP challenge packet.


E=691 (AUTHENTICATION_FAILURE) ERRORS WHEN YOU HAVE THE VALID SECRET (PASSWORD)

If your RAS server is not the domain controller and is not a 'stand-alone'
server then it must make a query to the domain controller for your domain.

You need to specify the domain name with the user name when you attempt to
use this type of a configuration. The domain name is specified with the
local name in the chap-secrets file and with the option for the 'name'
parameter.

For example, the previous example would become:

    DialupNT            domain\\customer47   foobar
    domain\\customer47  DialupNT             foobar

and

    pppd name 'domain\\customer47' remotename DialupNT <other options>

or add:

    name domain\\customer47
    remotename DialupNT

when the Windows NT domain name is simply called 'domain'.


TROUBLESHOOTING

Assuming that everything else has been configured correctly for PPP and
CHAP, the MS-CHAP-specific problems you're likely to encounter are mostly
related to your Windows NT account and its settings.  A Microsoft server
returns error codes in its CHAP response.  The following are extracted from
RFC 2433:

 646 ERROR_RESTRICTED_LOGON_HOURS
 647 ERROR_ACCT_DISABLED
 648 ERROR_PASSWD_EXPIRED
 649 ERROR_NO_DIALIN_PERMISSION
 691 ERROR_AUTHENTICATION_FAILURE
 709 ERROR_CHANGING_PASSWORD

You'll see these in your pppd log as a line similar to:

   Remote message: E=649 R=0

The "E=" is the error number from the table above, and the "R=" flag
indicates whether the error is transient and the client should retry.  If
you consistently get error 691, then either you're using the wrong account
name/password, or the DES library or MD4 hashing (in md4.c) aren't working
properly.  Verify your account name and password (use a Windows NT or
Windows 95 system to dial-in if you have one available).  If that checks
out, test the DES library with the "destest" program included with the DES
library.  If DES checks out, the md4.c routines are probably failing
(system byte ordering may be a problem) or my code is screwing up.  I've
only got access to a Linux system, so you're on your own for anything else.

Another thing that might cause problems is that some RAS servers won't
respond at all to LCP config requests without seeing the word "CLIENT"
from the other end.  If you see pppd sending out LCP config requests
without getting any reply, try putting something in your chat script
to send the word CLIENT after the modem has connected.

STILL TO DO

A site using only MS-CHAP to authenticate has no need to store cleartext
passwords in the "chap-secrets" file.  A utility that spits out the ASCII
hex MD4 hash of a given password would be nice, and would allow that hash
to be used in chap-secrets in place of the password.  The code to do this
could quite easily be lifted from chap_ms.c (you have to convert the
password to Unicode before hashing it).  The chap_ms.c file would also have
to be changed to recognize a password hash (16 binary bytes == 32 ASCII hex
characters) and skip the hashing stage.  This would have no real security
value as the hash is plaintext-equivalent.
PPPoL2TP plugin
===============

The pppol2tp plugin lets pppd use the Linux kernel driver pppol2tp.ko
to pass PPP frames in L2TP tunnels. The driver was integrated into the
kernel in the 2.6.23 release. For kernels before 2.6.23, an
out-of-tree kernel module is available from the pppol2tp-kmod package
in the OpenL2TP project.

Note that pppd receives only PPP control frames over the PPPoL2TP
socket; data frames are handled entirely by the kernel.

The pppol2tp plugin adds extra arguments to pppd and uses the Linux kernel
PPP-over-L2TP driver to set up each session's data path.

Arguments are:-

pppol2tp <fd>                   - FD for PPPoL2TP socket
pppol2tp_lns_mode               - PPPoL2TP LNS behavior. Default off.
pppol2tp_send_seq               - PPPoL2TP enable sequence numbers in
                                  transmitted data packets. Default off.
pppol2tp_recv_seq               - PPPoL2TP enforce sequence numbers in
                                  received data packets. Default off.
pppol2tp_reorderto <millisecs>  - PPPoL2TP data packet reorder timeout.
                                  Default 0 (no reordering).
pppol2tp_debug_mask <mask>      - PPPoL2TP debug mask. Bitwise OR of
				  1 - verbose debug
				  2 - control
				  4 - kernel transport
				  8 - ppp packet data
				  Default: 0 (no debug).
pppol2tp_ifname <ifname>	- Name of PPP network interface visible
				  to "ifconfig" and "ip link".
				  Default: "pppN"
pppol2tp_tunnel_id <id>		- L2TP tunnel_id tunneling this PPP
				  session.
pppol2tp_session_id <id>	- L2TP session_id of this PPP session.
				  The tunnel_id/session_id pair is used
				  when sending event messages to openl2tpd.

pppd will typically be started by an L2TP daemon for each L2TP sesion,
supplying one or more of the above arguments as required. The pppd
user will usually have no visibility of these arguments.

Two hooks are exported by this plugin.

void (*pppol2tp_send_accm_hook)(int tunnel_id, int session_id,
     uint32_t send_accm, uint32_t recv_accm);
void (*pppol2tp_ip_updown_hook)(int tunnel_id, int session_id, int up);

Credits
=======

This plugin was developed by Katalix Systems as part of the OpenL2TP
project, http://openl2tp.sourceforge.net. OpenL2TP is a full-featured
L2TP client-server, suitable for use as an enterprise L2TP VPN server
or a VPN client.

Please copy problems to the OpenL2TP mailing list:
openl2tp-users@lists.sourceforge.net.

Maintained by:
	James Chapman
	jchapman@katalix.com
	Katalix Systems Ltd
	http://www.katalix.com
This is the README file for ppp-2.4, a package which implements the
Point-to-Point Protocol (PPP) to provide Internet connections over
serial lines.


Introduction.
*************

The Point-to-Point Protocol (PPP) provides a standard way to establish
a network connection over a serial link.  At present, this package
supports IP and IPV6 and the protocols layered above them, such as TCP
and UDP.  The Linux port of this package also has support for IPX.

This PPP implementation consists of two parts:

- Kernel code, which establishes a network interface and passes
packets between the serial port, the kernel networking code and the
PPP daemon (pppd).  This code is implemented using STREAMS modules on
Solaris, and as a line discipline under Linux.

- The PPP daemon (pppd), which negotiates with the peer to establish
the link and sets up the ppp network interface.  Pppd includes support
for authentication, so you can control which other systems may make a
PPP connection and what IP addresses they may use.

The platforms supported by this package are Linux and Solaris.  I have
code for NeXTStep, FreeBSD, SunOS 4.x, SVR4, Tru64 (Digital Unix), AIX
and Ultrix but no active maintainers for these platforms.  Code for
all of these except AIX is included in the ppp-2.3.11 release.

The kernel code for Linux is no longer distributed with this package,
since the relevant kernel code is in the official Linux kernel source
(and has been for many years) and is included in all reasonably modern
Linux distributions.  The Linux kernel code supports using PPP over
things other than serial ports, such as PPP over Ethernet and PPP over
ATM.


Installation.
*************

The file SETUP contains general information about setting up your
system for using PPP.  There is also a README file for each supported
system, which contains more specific details for installing PPP on
that system.  The supported systems, and the corresponding README
files, are:

	Linux				README.linux
	Solaris				README.sol2

In each case you start by running the ./configure script.  This works
out which operating system you are using and creates the appropriate
makefiles.  You then run `make' to compile the user-level code, and
(as root) `make install' to install the user-level programs pppd, chat
and pppstats.

N.B. Since 2.3.0, leaving the permitted IP addresses column of the
pap-secrets or chap-secrets file empty means that no addresses are
permitted.  You need to put a "*" in that column to allow the peer to
use any IP address.  (This only applies where the peer is
authenticating itself to you, of course.)


What's new in ppp-2.4.7.
************************

* Fixed a potential security issue in parsing option files (CVE-2014-3158).

* There is a new "stop-bits" option, which takes an argument of 1 or 2,
  indicating the number of stop bits to use for async serial ports.

* Various bug fixes.


What was new in ppp-2.4.6.
**************************

* Man page updates.

* Several bug fixes.

* Options files can now set and unset environment variables for
  scripts.

* The timeout for chat scripts can now be taken from an environment
  variable.

* There is a new option, master_detach, which allows pppd to detach
  from the controlling terminal when it is the multilink bundle master
  but its own link has terminated, even if the nodetach option has
  been given.


What was new in ppp-2.4.5.
**************************

* Under Linux, pppd can now operate in a mode where it doesn't request
  the peer's IP address, as some peers refuse to supply an IP address.
  Since Linux supports device routes as well as gateway routes, it's
  possible to have no remote IP address assigned to the ppp interface
  and still route traffic over it.

* Pppd now works better with 3G modems that do strange things such as
  sending IPCP Configure-Naks with the same values over and over again.

* The PPP over L2TP plugin is included, which works with the pppol2tp
  PPP channel code in the Linux kernel.  This allows pppd to be used
  to set up tunnels using the Layer 2 Tunneling Protocol.

* A new 'enable-session' option has been added, which enables session
  accounting via PAM or wtwp/wtmpx, as appropriate.  See the pppd man
  page for details.

* Several bugs have been fixed.


What was new in ppp-2.4.4.
**************************

* Pppd will now run /etc/ppp/ip-pre-up, if it exists, after creating
  the ppp interface and configuring its IP addresses but before
  bringing it up.  This can be used, for example, for adding firewall
  rules for the interface.

* Lots of bugs fixed, particularly in the area of demand-dialled and
  persistent connections.

* The rp-pppoe plugin now accepts any interface name (that isn't an
  existing pppd option name) without putting "nic-" on the front of
  it, not just eth*, nas*, tap* and br*.


What was new in ppp-2.4.3.
**************************

* The configure script now accepts --prefix and --sysconfdir options.
  These default to /usr/local and /etc.  If you want pppd put in
  /usr/sbin as before, use ./configure --prefix=/usr.

* Doing `make install' no longer puts example configuration files in
  /etc/ppp.  Use `make install-etcppp' if you want that.

* The code has been updated to work with version 0.8.3 of libpcap.
  Unfortunately the libpcap maintainers removed support for the
  "inbound" and "outbound" keywords on PPP links, meaning that if you
  link pppd with libpcap-0.8.3, you can't use those keywords in the
  active-filter and pass-filter expressions.  The support has been
  reinstated in the CVS version and should be in future libpcap
  releases.  If you need the in/outbound keywords, use a later release
  than 0.8.3, or get the CVS version from http://www.tcpdump.org.

* There is a new option, child-timeout, which sets the length of time
  that pppd will wait for child processes (such as the command
  specified with the pty option) to exit before exiting itself.  It
  defaults to 5 seconds.  After the timeout, pppd will send a SIGTERM
  to any remaining child processes and exit.  A value of 0 means no
  timeout.

* Various bugs have been fixed, including some CBCP packet parsing
  bugs that could lead to the peer being able to crash pppd if CBCP
  support is enabled.

* Various fixes and enhancements to the radius and rp-pppoe plugins
  have been added.

* There is a new winbind plugin, from Andrew Bartlet of the Samba
  team, which provides the ability to authenticate the peer against an
  NT domain controller using MS-CHAP or MS-CHAPV2.

* There is a new pppoatm plugin, by various authors, sent in by David
  Woodhouse.

* The multilink code has been substantially reworked.  The first pppd
  for a bundle still controls the ppp interface, but it doesn't exit
  until all the links in the bundle have terminated.  If the first
  pppd is signalled to exit, it signals all the other pppds
  controlling links in the bundle.

* The TDB code has been updated to the latest version.  This should
  eliminate the problem that some people have seen where the database
  file (/var/run/pppd.tdb) keeps on growing.  Unfortunately, however,
  the new code uses an incompatible database format.  For this reason,
  pppd now uses /var/run/pppd2.tdb as the database filename.


What was new in ppp-2.4.2.
**************************

* The CHAP code has been rewritten.  Pppd now has support for MS-CHAP
  V1 and V2 authentication, both as server and client.  The new CHAP
  code is cleaner than the old code and avoids some copyright problems
  that existed in the old code.

* MPPE (Microsoft Point-to-Point Encryption) support has been added,
  although the current implementation shouldn't be considered
  completely secure.  (There is no assurance that the current code
  won't ever transmit an unencrypted packet.)

* James Carlson's implementation of the Extensible Authentication
  Protocol (EAP) has been added.

* Support for the Encryption Control Protocol (ECP) has been added.

* Some new plug-ins have been included:
  - A plug-in for kernel-mode PPPoE (PPP over Ethernet)
  - A plug-in for supplying the PAP password over a pipe from another
    process
  - A plug-in for authenticating using a Radius server.

* Updates and bug-fixes for the Solaris port.

* The CBCP (Call Back Control Protocol) code has been updated.  There
  are new options `remotenumber' and `allow-number'.

* Extra hooks for plugins to use have been added.

* There is now a `maxoctets' option, which causes pppd to terminate
  the link once the number of bytes passed on the link exceeds a given
  value.

* There are now options to control whether pppd can use the IPCP
  IP-Address and IP-Addresses options: `ipcp-no-address' and
  `ipcp-no-addresses'.

* Fixed several bugs, including potential buffer overflows in chat.


What was new in ppp-2.4.1.
**************************

* Pppd can now print out the set of options that are in effect.  The
  new `dump' option causes pppd to print out the option values after
  option parsing is complete.  The `dryrun' option causes pppd to
  print the options and then exit.

* The option parsing code has been fixed so that options in the
  per-tty options file are parsed correctly, and don't override values
  from the command line in most cases.

* The plugin option now looks in /usr/lib/pppd/<pppd-version> (for
  example, /usr/lib/pppd/2.4.1b1) for shared objects for plugins if
  there is no slash in the plugin name.

* When loading a plugin, pppd will now check the version of pppd for
  which the plugin was compiled, and refuse to load it if it is
  different to pppd's version string.  To enable this, the plugin
  source needs to #include "pppd.h" and have a line saying:
	char pppd_version[] = VERSION;

* There is a bug in zlib, discovered by James Carlson, which can cause
  kernel memory corruption if Deflate is used with the lowest setting,
  8.  As a workaround pppd will now insist on using at least 9.

* Pppd should compile on Solaris and SunOS again.

* Pppd should now set the MTU correctly on demand-dialled interfaces.


What was new in ppp-2.4.0.
**************************

* Multilink: this package now allows you to combine multiple serial
  links into one logical link or `bundle', for increased bandwidth and
  reduced latency.  This is currently only supported under the
  2.4.x and later Linux kernels.

* All the pppd processes running on a system now write information
  into a common database.  I used the `tdb' code from samba for this.

* New hooks have been added.

For a list of the changes made during the 2.3 series releases of this
package, see the Changes-2.3 file.


Compression methods.
********************

This package supports two packet compression methods: Deflate and
BSD-Compress.  Other compression methods which are in common use
include Predictor, LZS, and MPPC.  These methods are not supported for
two reasons - they are patent-encumbered, and they cause some packets
to expand slightly, which pppd doesn't currently allow for.
BSD-Compress and Deflate (which uses the same algorithm as gzip) don't
ever expand packets.


Contacts.
*********

The comp.protocols.ppp newsgroup is a useful place to get help if you
have trouble getting your ppp connections to work.  Please do not send
me questions of the form "please help me get connected to my ISP" -
I'm sorry, but I simply do not have the time to answer all the
questions like this that I get.

If you find bugs in this package, please report them to the maintainer
for the port for the operating system you are using:

Linux			Paul Mackerras <paulus@samba.org>
Solaris			James Carlson <carlson@workingcode.com>


Copyrights:
***********

All of the code can be freely used and redistributed.  The individual
source files each have their own copyright and permission notice.
Pppd, pppstats and pppdump are under BSD-style notices.  Some of the
pppd plugins are GPL'd.  Chat is public domain.


Distribution:
*************

The primary site for releases of this software is:

	ftp://ftp.samba.org/pub/ppp/



	Support to pass the password via a pipe to the pppd
	---------------------------------------------------

	Arvin Schnell <arvin@suse.de>
	2002-02-08


1. Introduction
---------------

Normally programs like wvdial or kppp read the online password from their
config file and store them in the pap- and chap-secrets before they start the
pppd and remove them afterwards. Sure they need special privileges to do so.

The passwordfd feature offers a simpler and more secure solution. The program
that starts the pppd opens a pipe and writes the password into it. The pppd
simply reads the password from that pipe.

This methods is used for quite a while on SuSE Linux by the programs wvdial,
kppp and smpppd.


2. Example
----------

Here is a short C program that uses the passwordfd feature. It starts the pppd
to buildup a pppoe connection.


--snip--

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <signal.h>
#include <string.h>
#include <paths.h>

#ifndef _PATH_PPPD
#define _PATH_PPPD "/usr/sbin/pppd"
#endif


// Of course these values can be read from a configuration file or
// entered in a graphical dialog.
char *device = "eth0";
char *username = "1122334455661122334455660001@t-online.de";
char *password = "hello";

pid_t pid = 0;


void
sigproc (int src)
{
    fprintf (stderr, "Sending signal %d to pid %d\n", src, pid);
    kill (pid, src);
    exit (EXIT_SUCCESS);
}


void
sigchild (int src)
{
    fprintf (stderr, "Daemon died\n");
    exit (EXIT_SUCCESS);
}


int
start_pppd ()
{
    signal (SIGINT, &sigproc);
    signal (SIGTERM, &sigproc);
    signal (SIGCHLD, &sigchild);

    pid = fork ();
    if (pid < 0) {
	fprintf (stderr, "unable to fork() for pppd: %m\n");
	return 0;
    }

    if (pid == 0) {

	int i, pppd_argc = 0;
	char *pppd_argv[20];
	char buffer[32] = "";
	int pppd_passwdfd[2];

	for (i = 0; i < 20; i++)
	    pppd_argv[i] = NULL;

	pppd_argv[pppd_argc++] = "pppd";

	pppd_argv[pppd_argc++] = "call";
	pppd_argv[pppd_argc++] = "pwfd-test";

	// The device must be after the call, since the call loads the plugin.
	pppd_argv[pppd_argc++] = device;

	pppd_argv[pppd_argc++] = "user";
	pppd_argv[pppd_argc++] = username;

	// Open a pipe to pass the password to pppd.
	if (pipe (pppd_passwdfd) == -1) {
	    fprintf (stderr, "pipe failed: %m\n");
	    exit (EXIT_FAILURE);
	}

	// Of course this only works it the password is shorter
	// than the pipe buffer. Otherwise you have to fork to
	// prevent that your main program blocks.
	write (pppd_passwdfd[1], password, strlen (password));
	close (pppd_passwdfd[1]);

	// Tell the pppd to read the password from the fd.
	pppd_argv[pppd_argc++] = "passwordfd";
	snprintf (buffer, 32, "%d", pppd_passwdfd[0]);
	pppd_argv[pppd_argc++] = buffer;

	if (execv (_PATH_PPPD, (char **) pppd_argv) < 0) {
	    fprintf (stderr, "cannot execl %s: %m\n", _PATH_PPPD);
	    exit (EXIT_FAILURE);
	}
    }

    pause ();

    return 1;
}


int
main (int argc, char **argv)
{
    if (start_pppd ())
	exit (EXIT_SUCCESS);

    exit (EXIT_FAILURE);
}

---snip---


Copy this file to /etc/ppp/peers/pwfd-test. The plugins can't be loaded on the
command line (unless you are root) since the plugin option is privileged.


---snip---

#
# PPPoE plugin for kernel 2.4
#
plugin pppoe.so

#
# This plugin enables us to pipe the password to pppd, thus we don't have
# to fiddle with pap-secrets and chap-secrets. The user is also passed
# on the command line.
#
plugin passwordfd.so

noauth
usepeerdns
defaultroute
hide-password
nodetach
nopcomp
novjccomp
noccp

---snip---

PPP Support for MPPE (Microsoft Point to Point Encryption)
==========================================================

Frank Cusack		frank@google.com
Mar 19, 2002

Updated by Paul Mackerras, Sep 2008


DISCUSSION

MPPE is Microsoft's encryption scheme for PPP links.  It is pretty much
solely intended for use with PPP over Internet links -- if you have a true
point to point link you have little need for encryption.  It is generally
used with PPTP.

MPPE is negotiated within CCP (Compression Control Protocol) as option
18.  In order for MPPE to work, both peers must agree to do it.  This
complicates things enough that I chose to implement it as strictly a binary
option, off by default.  If you turn it on, all other compression options
are disabled and MPPE *must* be negotiated successfully in both directions
(CCP is unidirectional) or the link will be disconnected.  I think this is
reasonable since, if you want encryption, you want encryption.  That is,
I am not convinced that optional encryption is useful.

While PPP regards MPPE as a "compressor", it actually expands every frame
by 4 bytes, the MPPE overhead (encapsulation).

Because of the data expansion, you'll see that ppp interfaces get their
mtu reduced by 4 bytes whenever MPPE is negotiated.  This is because
when MPPE is active, it is *required* that *every* packet be encrypted.
PPPD sets the mtu = MIN(peer mru, configured mtu).  To ensure that
MPPE frames are not larger than the peer's mru, we reduce the mtu by 4
bytes so that the network layer never sends ppp a packet that's too large.

There is an option to compress the data before encrypting (MPPC), however
the algorithm is patented and requires execution of a license with Hifn.
MPPC as an RFC is a complete farce.  I have no further details on MPPC.

Some recommendations:

- Use stateless mode.  Stateful mode is disabled by default.  Unfortunately,
  stateless mode is very expensive as the peers must rekey for every packet.
- Use 128-bit encryption.
- Use MS-CHAPv2 only.

Reference documents:

    <http://www.ietf.org/rfc/rfc3078.txt> MPPE
    <http://www.ietf.org/rfc/rfc3079.txt> MPPE Key Derivation
    <http://www.ietf.org/rfc/rfc2118.txt> MPPC
    <http://www.ietf.org/rfc/rfc2637.txt> PPTP
    <http://www.ietf.org/rfc/rfc2548.txt> MS RADIUS Attributes

You might be interested in PoPToP, a Linux PPTP server.  You can find it at
<http://www.poptop.org/>

RADIUS support for MPPE is from Ralf Hofmann, <ralf.hofmann@elvido.net>.


BUILDING THE PPPD

The userland component of PPPD has no additional requirements above
those for MS-CHAP and MS-CHAPv2.

MPPE support is now included in the mainline Linux kernel releases.


CONFIGURATION

See pppd(8) for the MPPE options.  Under Linux, if your modutils is earlier
than 2.4.15, you will need to add

    alias ppp-compress-18 ppp_mppe

to /etc/modules.conf.



-------------------------
Installing the OSD System
-------------------------

 1. Build kernel version 2.4 w/ SMP mode disabled and make sure the 
    directory /usr/src/linux points to this build.
 2. Modify initiator.h and initiator.c (as per the README) to point to 
    your intended target.
 3. Run 'make osd' to build all the OSD  executables.  In particular,
    you need uosd, so.o, intel_iscsi.o and osdfs.o.
 4. Run 'make all' to build the remaining executables. 
 5. Select a machine as the target and run "uosd -f" as root on this 
    machine. This will create files and directories in /tmp.
 
----------------------
Testing the OSD System
----------------------

 6. Run utest to make sure the target is working OK.
 7. Do an "insmod ./so.o" to install the Linux SCSI upper layer driver for OSD.
 8. Do an "insmod ./intel_iscsi.o" to install the kernel mode iSCSI initiator.
 9. Do an "insmod ./osdfs.o" to install the file system.
10. Make the OSD device with "mknod /dev/so0 b 232 0".
11. Do a "mount -t osdfs /dev/so0 /mnt" to mount the filesystem.
12. Go to /mnt and run "echo Hello, World! > foo" to create an object 
    on the OSD.
13. Do a "cat foo" to read the object. you should see "Hello, world!"

From here you should be able to use /mnt as you would any filesystem.

--------------------
2.4.18 Modifications
--------------------

include/scsi/scsi.h:143
  #define TYPE_OSD 0x0e

drivers/scsi/scsi.h:92:
  #define MAX_SCSI_DEVICE_CODE 15

drivers/scsi/scsi.h:354:
  #define MAX_COMMAND_SIZE 256

drivers/scsi/scsi.c:145:
  "OSD              ",

drivers/scsi/scsi_dma.c:248:
  SDpnt->type == TYPE_DISK || SDpnt->type == TYPE_MOD || SDpnt->type == TYPE_OSD) {

drivers/scsi/scsi_scan.c:644:
  case TYPE_OSD:

----------
Hints/Tips
----------

-Field testing on some workstations resulted in compiling errors if highmem support in the kernel as enabled

---------------------------------------
Intel iSCSI v20 Reference Implementation
---------------------------------------

This is a software implementation of iSCSI v20.  Included in this distribution
are both host and target mode drivers with built in conformance and performance
tests, and sockets tests that can be used to simulate TCP traffic identical to
that generated between a real iSCSI host and target.

See PERFORMANCE for information regarding the expected performance of this 
distribution.

This code has been successfully compiled and tested on Redhat 8.0 
(kernel version 2.4.18-14) and Redhat 9.0 (kernel version 2.4.20) 
with UP and SMP configurations.  

-------------------
Starting the System
-------------------

1a) Modify the array in initiator.c to contain your target ip addresses and port
   numbers. If you specify a TargetName there will be no discovery process. For
   example, targets 0 and 2 below will first be discovered. Target 1 will not.
   ISCSI_PORT is the default port defined in iscsi.h and currently set to 3260.

   static INITIATOR_TARGET_T g_target[CONFIG_INITIATOR_NUM_TARGETS] = {
   {"192.168.10.10", ISCSI_PORT, "",                     NULL, 0},
   {"192.168.10.11", ISCSI_PORT, "iqn.com.intel.abc123", NULL, 0},
   {"192.168.10.12", ISCSI_PORT, "",                     NULL, 0}};

   The initiator currently only connects to one of the discovered targets. If 
   multiple TargetNames and TargetAddresses are returned, all but 1 are ignored.

1b) Alternately for the kernel mode driver you may specify ip addresses
   in a file in the local directory called "./intel_iscsi_targets". This 
   file will also be looked for in the /etc directory or you may specify 
   the file name as a module parameter to the insmod command using the 
   "gfilename" argument, (insmod intel_iscsi.o gfilename="./targets"). 
   The format for the contents of of the file is: 
       ip=192.168.10.10
       ip=192.168.10.11 name=iqn.com.intel.abc123 port=3260
       ip=192.168.10.12
   The name and port fields are optional.  If name is not specified, there
   will be a discovery process.  If port is not specified, the default port
   of 3260 will be used.

1c) For the user mode intiator, if the first entry of the g_target array has ip 
   address "151.0.1.1", the initiator will prompt the user to enter the number 
   of targets and their ip addresses. 

2) Modify the following constant in initiator.h accordingly:

   #define CONFIG_INITIATOR_NUM_TARGETS 3

3) Run "make" to build each of: 

   intel_iscsi.o - kernel mode iSCSI initiator 
   kramdisk.o    - kernel mode iSCSI target ramdisk
   ufsdisk       - user mode iSCSI target (disk stored as file in /tmp)
   ufsdisk_mmap  - same as ufsdisk, but uses mmap
   uramdisk      - user mode iSCSI ramdisk
   utest         - user mode iSCSI test suite
   ktest         - invokes same tests as utest, but from within device driver
   usocktest     - user mode sockets test that simulates iSCSI traffic

4) Start a user level target (uramdisk, ufsdisk, ufsdisk_mmap) on each target 
   machine:

   Usage: -t <name>           iSCSI TargetName (dflt "iqn.com.intel.abc123")
          -p <port>           Port Number (dflt 3260)
          -b <block len>      Block Length (dflt 512)
          -n <num blocks>     Number of Blocks (dflt 204800)


   Or start the kernel level target (kramdisk.o):

   Usage: insmod kramdisk.o port=<port>
                            block_len=<block length>
                            num_blocks=<number of blocks>


   With ufsdisk and ufsdisk_mmap you can directly access a device in /dev by 
   creating a symbolic link in /tmp to point to the appropriate device. For 
   example:
 
   "ln -s /dev/sdd /tmp/iqn.com.intel.abc123_3260_iscsi_disk_lun_0"

   And kramdisk.o only operates in ramdisk mode.

5) Run utest.  If you did not successfully connect to each target machine you 
   specified in initiator.c, then there was a problem.  Make sure initiator.h 
   and initiator.c were correctly edited and all your targets had been started.

6) As root, run "insmod ./intel_iscsi.o."  You should see output similar to the
   following when either viewing /var/log/messages or running dmesg:

     *********************************************
     *           PARAMETERS NEGOTIATED           *
     *                                           *
     *        InitiatorName:               Intel *
     *       InitiatorAlias:               Intel *
     *          SessionType:              normal *
     *           TargetName: iqn.com.intel.abc12 *
     *********************************************
     *********************************************
     *              LOGIN SUCCESSFUL             *
     *                                           *
     *                  CID:                   0 *
     *                 ISID:                   0 *
     *                 TSID:                   1 *
     *                CmdSN:                   0 *
     *             MaxCmdSN:                   0 *
     *            ExpStatSN:                   0 *
     *********************************************
     Vendor: Intel     Model: Intel Labs iSCSI  Rev: 2   
     Type:   Direct-Access                      ANSI SCSI revision: 02
     Detected scsi disk sdb at scsi2, channel 0, id 0, lun 0
     SCSI device sdb: 204800 512-byte hdwr sectors (105 MB)
     sdb: unknown partition table

   You can now use the device as you would any other SCSI device.  You can also
   view driver statistics by viewing the file in /proc/scsi/iscsi.  Writing to
   the file (e.g. echo reset > /proc/scsi/iscsi/1) will reset all counters.

------------------
Testing the System
------------------

Once your targets and host have been started, you can test the installation 
using either a single target, or by creating a RAID volume over multiple 
targets.

To test a single target you can either directly read and write the device by 
opening, for example, /dev/sdd. Or you can create a filesystem on the device:

	fdisk /dev/sdd
	mkfs /dev/sdd1
	mount -t ext2 /dev/sdd1 /mnt/iscsi_fs

To test a multiple target installation, you can create a RAID volume, 
virtualizing multiple targets as one SCSI device.  The Linux RAID modules 
will either need to be installed or compiled into the kernel.  The file 
/etc/raidtab must be created to reflect your targets.  For example,

	raiddev /dev/md0
	raid-level              0
	nr-raid-disks           5
	persistent-superblock   0
	chunk-size              64

	device                  /dev/sdd
	raid-disk               0
	device                  /dev/sde
	raid-disk               1
	device                  /dev/sdf
	raid-disk               2
	device                  /dev/sdg
	raid-disk               3
	device                  /dev/sdh
	raid-disk               4
      

After initialized the raid device with "mkraid /dev/md0," you can use /dev/md0 
as though it were a normal SCSI device.  For example,

	mkfs /dev/md0
	mount -t ext2 /dev/md0 /mnt/iscsi_fs

--------------------
When Things Go Wrong
--------------------

Check the kernel error messages /var/log/messages or run the dmesg command to 
see any errors reported from the host driver. The targets will report target 
errors to standard output. 

If you need more fine grained debugging, modify the Makefile to turn on the 
compilation flag CONFIG_ISCSI_DEBUG. Then run "make clean," and then "make."  
You can select which type of debugging statements get printed by modifying 
iscsiutil.h 

-------------------------------------
Interoperability with Cisco Initiator
-------------------------------------

The target is tested to be interoperable with Cisco Initiator release
3.4.1.1
Kyua (pronounced Q.A.) is a testing framework for both developers and
users.  Kyua is different from most other testing frameworks in that it
puts the end user experience before anything else.  There are multiple
reasons for users to run the tests themselves, and Kyua ensures that
they can do so in the most convenient way.

This module, kyua-atf-compat, provides tools to ease the transition from
ATF to Kyua.  In particular, this includes a tool to convert Atffile
files to Kyuafile files in an automated manner, and drop-in replacement
wrappers for both atf-run and atf-report.

For further information on the contents of this distribution file,
please refer to the following other documents:

* AUTHORS: List of authors and contributors to this project.
* COPYING: License information.
* INSTALL: Compilation and installation instructions.
* NEWS: List of major changes between formal releases.

For general project information, please visit:

    http://code.google.com/p/kyua/
Calling conventions, stack frame and zero page:

The variables that normally are placed on the stack or in registers in C
are instead allocated in the zero page and saved on a (fictive) stack
when calling functions.  Some locations have predefined functions though.
Arrays allocated as automatics are stored on the stack with a pointer
in zero page to its destination.

0-7	Unused (by us)
10	Stack pointer
11	Frame pointer
12-14	Unused
15	Prolog private word
16	Prolog address, written in crt0
17	Epilog address, written in crt0
20-27	(Auto-increment), scratch
30-37	(Auto-decrement), scratch
40-47	Used by HW stack and MMPU
50-77	Permanent, save before use.
100-377	Addresses for subroutines, written by the linker

The normal registers (AC0-AC3) are all considered scratch registers.

Register classes are assigned as:
	AC0-AC3: AREGs.
	AC2-AC3: BREGs.
	...and eventually register pairs/floats as EREGs, double in FREGs.

In byte code the left half of a word is the first byte (big-endian).
This is bit 0-7 in Nova syntax.
The stack is growing towards lower adresses (as opposed to the Eclipse stack).
Stack pointer points to the last used stack entry, which means:
PUSH: dec sp, store value
POP:  fetch val, inc sp

Note that internally is the first 24 words stored in loc 50-77!
So an offset is subtracted in adrput().

Stack layout:

	! arg1	! 1
 fp ->	! arg0	! 0
 	! old pc! -1
 	! old fp! -2
 sp ->	! saved ! -3


Stack references to zero page are converted to NAMEs, using word addresses.
References to the stack itself are using byte offsets.

Arguments are transferred on the stack.  To avoid unneccessary double instructions
they are copied to the zp use area initially. XXX? Need tests here.
Return values in ac0 and ac1.

A reference to a struct member in assembler, a = b->c; b is in ZP 50 (or on stack)
+ is zeropage-addressing
* is fp-adressing, assume fp in ac3

# offset 0
+	lda 0,@50	# load value from indirect ZP 50 into ac0
or
*	lda 2,,3	# load value from (ac3) into ac2
*	lda 0,,2	# load value from (ac2) into ac0

# offset 12
+	lda 2,50	# load value from ZP 50 into ac2
+	lda 0,12,2	# load value from (ac2+12) into ac0
or
*	lda 2,,3	# load value from (ac3) into ac2
*	lda 0,12,2	# load value from 12(ac2) into ac0

# offset 517
+	lda %2,50	# load value from ZP 50 into ac2
+	lda %0,.L42-.,%1	# load offset from .L42 PC-indexed
+	addz %0,%2,skp	# add offset to ac2 and skip
+.L42:	.word 517	# offset value
+	lda %0,,%2	# load value from (ac2) into ac0
or

The prolog/epilog; they are implemented as subroutines.
Both can be omitted if the function do not need it.


	.word 012	# total words that needs to be saved
func:
	mov 3,0		# avoid trashing return address
	jsr @prolog	# go to prolog
	...
	jmp @epilog	# jump to epilog

#ifdef prolog
	lda 0,sp
	sta 3,@sp
	lda 1,fp
	sta 1,@sp
	sta 0,fp
	lda 1,[Css]
	sub 1,0
	sta 0,sp
#endif

...

	lda 2,fp
	lda 3,-1,2
	lda 0,-2,2
	sta 0,fp
	sta 2,sp
	jmp 0,3


#
# decrement from stack, and save permanent registers.
#	push retreg
#	push fp
#	mov sp,fp
#	sub $w,sp
#	
# prolog: return in ac3, fun in ac0.
prolog:	lda 1,sp
	sta 0,@sp
	lda 0,fp
	sta 0,@sp
	sta 1,fp
	lda 0,-3,3
	sub 0,1
	sta 1,sp
	jmp 0,3

epilog:	lda 3,fp
	sta 3,sp
	lda 3,@fp
	lda 2,@fp
	sta 2,fp
	jmp 0,3

	





prolog:
	lda 2,sp	# sp points to the first argument
	sta 2,30	# store at auto-dec location
	sta 0,@30	# store fun return address
	lda 0,fp	# fetch old fp
	sta 0,@30	# store saved fp
	sta 2,fp	# save frame pointer

	lda 0,-3,3	# stack size to subtract
	neg 0,0,snr	# Any words?
	jmp 1f		# no, get away
	lda 1,$51	# fetch zp offset
	sub 0,1		# get highest word
	sta 1,31	# at auto-dec location

2:	lda 1,@31	# fetch word to copy
	sta 1,@30	# on stack
	inc 0,0,szr	# count words
	jmp 1b		# more to go

1:	lda 0,30	# finished, get stackptr
	sta 0,sp	#
	jmp 0,3		# get back!

# epilog, need save frame pointer in ac3
epilog:
	lda 0,-3,3	# get words to save
	neg 0,0,snr	# any words to save?
	jmp 1f		# No, get out

	lda 1,$51	# get zp offset
	sub 0,1		# Highest word
	sta 1,31	# auto-dec loc

	lda 2,fp	# get fp
	adczl 1,1	# -2
	add 1,2		# 2 now offset
	sta 2,30	# auto-dec loc

2:	lda 1,@30
	sta 1,@31
	inc 0,0,szr
	jmp 2b

1:	lda 2,fp	# fetch current fp
	lda 3,-1,2	# return pc
	lda 0,-2,2	# fetch old fp
	sta 0,fp	# restore fp
	sta 2,sp	# stack pointer restored
	jmp 0,3		# Done!

#if 0
Assembler syntax and functions.

The assembler syntax mimics the DG assembler but uses AT&T syntax.
Load and store to addresses is written "lda 0,foo" to load from address foo.
If foo is not in zero page then the assembler will put the lda in the
text area close to the instruction and do an indirect pc-relative load.

Arithmetic instruction: 	
	subsl#	%0,%1,snr		skip code may be omitted
	subsl#	%0,%1

Load/store/jmp/jsr/isz/dsz:
	lda %1,@disp,2		or
	mov *disp(%2),%1
		index may be omitted if 0 (ZP)
		disp can only be +-127 words.

	It's allowed to write "mov $32,%0" which will be converted
	to an indirect load by the assembler.

	Example of AT&T arguments:
		01234	- Zero-page
		L11	- Relative.  Will be converted to
			  indirect + addr if distance too long
		(%2)	- indexed
		012(%2)	- indexed with offset
		* in front of any of these will generate indirection
#endif

lbyte:  movr 2,2        # get byte ID into C bit
        lda 0,,2        # word into ac0
        mov 2,2,snc     # skip if right byte
        movs 0,0        # swap bytes
        jmp 0,3         # get back

sbyte:  sta 3,@sp

        movr 2,2        # get byte ID into C bit
        lda 1,,2        # get word
        lda 3,[377]     # get mask
        and 3,0,snc     # clear left input + skip if right byte
        movs 1,1        # swap bytes
        and 3,1         # clear old bits
        add 0,1,snc     # swap back if necessary
        movs 1,1        # swap
        sta 1,2

        lda 3,sp
        isz sp
        jmp @0,3

macdefs.h		; machine-dependent definitions
code.c			; machine-dependent code for prologs, switches (pass 1)
local.c			; machine-dependent code for prologs, switches (pass 1)
local2.c		; misc routines and tables of register names (pass 2)
order.c			; machine-dependent code-generation strategy (pass 2)
table.c			; code templates (pass 2)

On OS X, binaries are not ELF and all binaries are compiled PIC.  To use pcc
on OS X while linking against the system libraries, use the -k option.

Current issues:

- no floating point (need mickey's patches to support >64 registers)
- mod/div on longlong not supported
- the stack frame is always 200 bytes - need to calculate size and patch
  OREGs to temporaries and arguments [see discussion below]
- function arguments are always saved to the stack [need to change MI code]
- permanent registers >R13 are not saved [need to change MI code]
- structure arguments don't work
- return of structure doesn't work
- function pointers don't work for PIC
- constant structure assignment doesn't work properly for PIC
- no built-in vararg support [shouldn't be too hard to add]

The way most modern CPUs create the stack is to allocate the frame
to contain room for the temporaries, to save the permanent registers
and to store the arguments to functions invoked from within the function.
To achieve this, all the information must be known when the prologue
is generated.  Currently we only know the size of the temporaries -
we don't know the size of the argument space for each function that
gets invoked from this function.  Even if we did know this information,
we create ops to save the register arguments (R3-R10), early in pass1
and don't know the position of the stack pointer, and the size of the
argument space required to "step over".

One solution is to have two pointers to the stack.  One for the top
of the stack and the other pointing just below the temporaries but above
the argument space.  Then our function arguments and the permanent registers can
be saved fixed-relative to this register.  If we don't know the size of
argument space, we cannot "dynamically" alter the stack (like we do with mips),
since the powerpc ABI specifies that the "lowest" address
in the stack frame is the saved stack pointer (pointing to the previous
stack frame).  While this is a nice feature for tracking back through the
stack frames (which mips has always had problems with), it makes it
next-to-impossible to increase the strack frame dynamically.

I guess the best approach is to determine the size of the argument stack
and have a second frame pointer.


PDP10 C calling convention
--------------------------
Register 1-7 are argument registers.  Types of sizes up to 36 bits are
given in one register, two otherwise.  CHAR and SHORT are given as INTs.

If the argument that would end up in register 7 requires two registers,
it is saved on the stack instead and no more registers would end up
on the stack.

struct return: a hidden argument containing the address of the struct
is stored as the first argument _on_the_stack_, never in register.

struct argument: always saved on stack, and terminates the list 
of arguments that are kept in registers.

In case of debugging all arguments are saved on stack in the function.

All variadic arguments are always saved on the stack.
	      Internet Systems Consortium DHCP Distribution
			     Version 4.3.3
			   03 September 2015

			      README FILE

You should read this file carefully before trying to install or use
the ISC DHCP Distribution.

			  TABLE OF CONTENTS

	1	WHERE TO FIND DOCUMENTATION
	2	RELEASE STATUS
	3	BUILDING THE DHCP DISTRIBUTION
	 3.1	 UNPACKING IT
	 3.2	 CONFIGURING IT
	  3.2.1	  DYNAMIC DNS UPDATES
	  3.2.2   LOCALLY DEFINED OPTIONS
	 3.3	 BUILDING IT
	4	INSTALLING THE DHCP DISTRIBUTION
	5	USING THE DHCP DISTRIBUTION
	 5.1	  FIREWALL RULES
	 5.2	 LINUX
	  5.2.1	  IF_TR.H NOT FOUND
	  5.2.2	  SO_ATTACH_FILTER UNDECLARED
	  5.2.3	  PROTOCOL NOT CONFIGURED
	  5.2.4	  BROADCAST
	  5.2.6	  IP BOOTP AGENT
	  5.2.7	  MULTIPLE INTERFACES
	 5.3	 SCO
	 5.4	 HP-UX
	 5.5	 ULTRIX
	 5.6	 FreeBSD
	 5.7	 NeXTSTEP
	 5.8	 SOLARIS
	  5.8.1 Solaris 11
	  5.8.2 Solaris 11 and ATF
	  5.8.3 Other Solaris Items
	 5.9	 AIX
	 5.10	 MacOS X
         5.11    ATF
	6	SUPPORT
	 6.1	 HOW TO REPORT BUGS
	7	HISTORY

		      WHERE TO FIND DOCUMENTATION

Documentation for this software includes this README file, the
RELNOTES file, and the manual pages, which are in the server, common,
client and relay subdirectories.  The README file (this file) includes
late-breaking operational and system-specific information that you
should read even if you don't want to read the manual pages, and that
you should *certainly* read if you run into trouble.  Internet
standards relating to the DHCP protocol are listed in the References
document that is available in html, txt and xml formats in doc/
subdirectory.  You will have the best luck reading the manual pages if
you build this software and then install it, although you can read
them directly out of the distribution if you need to.

DHCP server documentation is in the dhcpd man page.  Information about
the DHCP server lease database is in the dhcpd.leases man page.
Server configuration documentation is in the dhcpd.conf man page as
well as the dhcp-options man page.   A sample DHCP server
configuration is in the file server/dhcpd.conf.example.   The source for
the dhcpd, dhcpd.leases and dhcpd.conf man pages is in the server/ sub-
directory in the distribution.   The source for the dhcp-options.5
man page is in the common/ subdirectory.

DHCP Client documentation is in the dhclient man page.  DHCP client
configuration documentation is in the dhclient.conf man page and the
dhcp-options man page.  The DHCP client configuration script is
documented in the dhclient-script man page.   The format of the DHCP
client lease database is documented in the dhclient.leases man page.
The source for all these man pages is in the client/ subdirectory in
the distribution.   In addition, the dhcp-options man page should be
referred to for information about DHCP options.

DHCP relay agent documentation is in the dhcrelay man page, the source
for which is distributed in the relay/ subdirectory.

To read installed manual pages, use the man command.  Type "man page"
where page is the name of the manual page.   This will only work if
you have installed the ISC DHCP distribution using the ``make install''
command (described later).

If you want to read manual pages that aren't installed, you can type
``nroff -man page |more'' where page is the filename of the
unformatted manual page.  The filename of an unformatted manual page
is the name of the manual page, followed by '.', followed by some
number - 5 for documentation about files, and 8 for documentation
about programs.   For example, to read the dhcp-options man page,
you would type ``nroff -man common/dhcp-options.5 |more'', assuming
your current working directory is the top level directory of the ISC
DHCP Distribution.

Please note that the pathnames of files to which our manpages refer
will not be correct for your operating system until after you iterate
'make install' (so if you're reading a manpage out of the source
directory, it may not have up-to-date information).

			    RELEASE STATUS

This is ISC DHCP 4.3.x  The major theme for this release is "ipv6 uplift",
in which we enhance the v6 code to support many of the features found
in the v4 code.  These include: support for v6, support for on_commit,
on_expiry and on_release in v6, support for accessing v6 relay options
and better log messages for v6 addresses.  Non v6 features include:
support for the standard DDNS, better OMAPI class and sub-class support
allowing for dynamic addition and removal of sub-classes, and support for
DDNS without zone statements.

In this release, the DHCPv6 server should be fully functional on Linux,
Solaris, or any BSD.  The DHCPv6 client should be similarly functional
except on Solaris.

The DHCPv4 server, relay, and client, should be fully functional
on Linux, Solaris, any BSD, HPUX, SCO, NextSTEP, and Irix.

If you are running the DHCP distribution on a machine which is a
firewall, or if there is a firewall between your DHCP server(s) and
DHCP clients, please read the section on firewalls which appears later
in this document.

If you wish to run the DHCP Distribution on Linux, please see the
Linux-specific notes later in this document.  If you wish to run on an
SCO release, please see the SCO-specific notes later in this document.
You particularly need to read these notes if you intend to support
Windows 95 clients.  If you are running HP-UX or Ultrix, please read the 
notes for those operating systems below.  If you are running NeXTSTEP, 
please see the notes on NeXTSTEP below.

If you start dhcpd and get a message, "no free bpf", that means you
need to configure the Berkeley Packet Filter into your operating
system kernel.   On NetBSD, FreeBSD and BSD/os, type ``man bpf'' for
information.   On Digital Unix, type ``man pfilt''.


		    BUILDING THE DHCP DISTRIBUTION

			     UNPACKING IT

To build the DHCP Distribution, unpack the compressed tar file using
the tar utility and the gzip command - type something like:

	gunzip dhcp-4.3.3.tar.gz
	tar xvf dhcp-4.3.3.tar

			    CONFIGURING IT

Now, cd to the dhcp-4.3.3 subdirectory that you've just created and
configure the source tree by typing:

	./configure

If the configure utility can figure out what sort of system you're
running on, it will create a custom Makefile for you for that
system; otherwise, it will complain.  If it can't figure out what
system you are using, that system is not supported - you are on
your own.

Several options may be enabled or disabled via the configure command.
You can get a list of these by typing:

	./configure --help

			 DYNAMIC DNS UPDATES

A fully-featured implementation of dynamic DNS updates is included in
this release.  It uses libraries from BIND and, to avoid issues with
different versions, includes the necessary BIND version.  The appropriate
BIND libraries will be compiled and installed in the bind subdirectory
as part of the make step.  In order to build the necessary libraries you
will need to have "gmake" available on your build system.


There is documentation for the DDNS support in the dhcpd.conf manual
page - see the beginning of this document for information on finding
manual pages.

		       LOCALLY DEFINED OPTIONS

In previous versions of the DHCP server there was a mechanism whereby
options that were not known by the server could be configured using
a name made up of the option code number and an identifier:
"option-nnn"   This is no longer supported, because it is not future-
proof.   Instead, if you want to use an option that the server doesn't
know about, you must explicitly define it using the method described
in the dhcp-options man page under the DEFINING NEW OPTIONS heading.

			     BUILDING IT

Once you've run configure, just type ``make'', and after a while
you should have a dhcp server.  If you get compile errors on one
of the supported systems mentioned earlier, please let us know.
If you get warnings, it's not likely to be a problem - the DHCP
server compiles completely warning-free on as many architectures
as we can manage, but there are a few for which this is difficult.
If you get errors on a system not mentioned above, you will need
to do some programming or debugging on your own to get the DHCP
Distribution working.

		   INSTALLING THE DHCP DISTRIBUTION

Once you have successfully gotten the DHCP Distribution to build, you
can install it by typing ``make install''.   If you already have an old
version of the DHCP Distribution installed, you may want to save it
before typing ``make install''.

		     USING THE DHCP DISTRIBUTION

			    FIREWALL RULES

If you are running the DHCP server or client on a computer that's also
acting as a firewall, you must be sure to allow DHCP packets through
the firewall.  In particular, your firewall rules _must_ allow packets
from IP address 0.0.0.0 to IP address 255.255.255.255 from UDP port 68
to UDP port 67 through.  They must also allow packets from your local
firewall's IP address and UDP port 67 through to any address your DHCP
server might serve on UDP port 68.  Finally, packets from relay agents
on port 67 to the DHCP server on port 67, and vice versa, must be
permitted.

We have noticed that on some systems where we are using a packet
filter, if you set up a firewall that blocks UDP port 67 and 68
entirely, packets sent through the packet filter will not be blocked.
However, unicast packets will be blocked.   This can result in strange
behaviour, particularly on DHCP clients, where the initial packet
exchange is broadcast, but renewals are unicast - the client will
appear to be unable to renew until it starts broadcasting its
renewals, and then suddenly it'll work.   The fix is to fix the
firewall rules as described above.

			   PARTIAL SERVERS

If you have a server that is connected to two networks, and you only
want to provide DHCP service on one of those networks (e.g., you are
using a cable modem and have set up a NAT router), if you don't write
any subnet declaration for the network you aren't supporting, the DHCP
server will ignore input on that network interface if it can.  If it
can't, it will refuse to run - some operating systems do not have the
capability of supporting DHCP on machines with more than one
interface, and ironically this is the case even if you don't want to
provide DHCP service on one of those interfaces.

				LINUX

There are three big LINUX issues: the all-ones broadcast address,
Linux 2.1 ip_bootp_agent enabling, and operations with more than one
network interface.   There are also two potential compilation/runtime
problems for Linux 2.1/2.2: the "SO_ATTACH_FILTER undeclared" problem
and the "protocol not configured" problem.

		    LINUX: PROTOCOL NOT CONFIGURED

If you get the following message, it's because your kernel doesn't
have the linux packetfilter or raw packet socket configured:

 Make sure CONFIG_PACKET (Packet socket) and CONFIG_FILTER (Socket
 Filtering) are enabled in your kernel configuration

If this happens, you need to configure your Linux kernel to support
Socket Filtering and the Packet socket, or to select a kernel provided
by your Linux distribution that has these enabled (virtually all modern
ones do by default).

			   LINUX: BROADCAST

If you are running a recent version of Linux, this won't be a problem,
but on older versions of Linux (kernel versions prior to 2.2), there
is a potential problem with the broadcast address being sent
incorrectly.

In order for dhcpd to work correctly with picky DHCP clients (e.g.,
Windows 95), it must be able to send packets with an IP destination
address of 255.255.255.255.  Unfortunately, Linux changes an IP
destination of 255.255.255.255 into the local subnet broadcast address
(here, that's 192.5.5.223).

This isn't generally a problem on Linux 2.2 and later kernels, since
we completely bypass the Linux IP stack, but on old versions of Linux
2.1 and all versions of Linux prior to 2.1, it is a problem - pickier
DHCP clients connected to the same network as the ISC DHCP server or
ISC relay agent will not see messages from the DHCP server.   It *is*
possible to run into trouble with this on Linux 2.2 and later if you
are running a version of the DHCP server that was compiled on a Linux
2.0 system, though.

It is possible to work around this problem on some versions of Linux
by creating a host route from your network interface address to
255.255.255.255.   The command you need to use to do this on Linux
varies from version to version.   The easiest version is:

	route add -host 255.255.255.255 dev eth0

On some older Linux systems, you will get an error if you try to do
this.   On those systems, try adding the following entry to your
/etc/hosts file:

255.255.255.255	all-ones

Then, try:

	route add -host all-ones dev eth0

Another route that has worked for some users is:

	route add -net 255.255.255.0 dev eth0

If you are not using eth0 as your network interface, you should
specify the network interface you *are* using in your route command.

			LINUX: IP BOOTP AGENT

Some versions of the Linux 2.1 kernel apparently prevent dhcpd from
working unless you enable it by doing the following:

	      echo 1 >/proc/sys/net/ipv4/ip_bootp_agent


		      LINUX: MULTIPLE INTERFACES

Very old versions of the Linux kernel do not provide a networking API
that allows dhcpd to operate correctly if the system has more than one
broadcast network interface.  However, Linux 2.0 kernels with version
numbers greater than or equal to 2.0.31 add an API feature: the
SO_BINDTODEVICE socket option.  If SO_BINDTODEVICE is present, it is
possible for dhcpd to operate on Linux with more than one network
interface.  In order to take advantage of this, you must be running a
2.0.31 or greater kernel, and you must have 2.0.31 or later system
headers installed *before* you build the DHCP Distribution.

We have heard reports that you must still add routes to 255.255.255.255
in order for the all-ones broadcast to work, even on 2.0.31 kernels.
In fact, you now need to add a route for each interface.   Hopefully
the Linux kernel gurus will get this straight eventually.

Linux 2.1 and later kernels do not use SO_BINDTODEVICE or require the
broadcast address hack, but do support multiple interfaces, using the
Linux Packet Filter.

			     LINUX: OpenWrt

DHCP 4.1 has been tested on OpenWrt 7.09 and 8.09.  In keeping with
standard practice, client/scripts now includes a dhclient-script file
for OpenWrt.  However, this is not sufficient by itself to run dhcp on
OpenWrt; a full OpenWrt package for DHCP is available at
ftp://ftp.isc.org/isc/dhcp/dhcp-4.1.0-openwrt.tar.gz

		    LINUX: 802.1q VLAN INTERFACES

If you're using 802.1q vlan interfaces on Linux, it is necessary to
vconfig the subinterface(s) to rewrite the 802.1q information out of
packets received by the dhcpd daemon via LPF:

	vconfig set_flag eth1.523 1 1

Note that this may affect the performance of your system, since the
Linux kernel must rewrite packets received via this interface.  For
more information, consult the vconfig man pages.

				 SCO

ISC DHCP will now work correctly on newer versions of SCO out of the
box (tested on OpenServer 5.05b, assumed to work on UnixWare 7).

Older versions of SCO have the same problem as Linux (described earlier).
The thing is, SCO *really* doesn't want to let you add a host route to
the all-ones broadcast address.

You can try the following:

  ifconfig net0 xxx.xxx.xxx.xxx netmask 0xNNNNNNNN broadcast 255.255.255.255

If this doesn't work, you can also try the following strange hack:

  ifconfig net0 alias 10.1.1.1 netmask 8.0.0.0

Apparently this works because of an interaction between SCO's support
for network classes and the weird netmask.  The 10.* network is just a
dummy that can generally be assumed to be safe.   Don't ask why this
works.   Just try it.   If it works for you, great.

				HP-UX

HP-UX has the same problem with the all-ones broadcast address that
SCO and Linux have.   One user reported that adding the following to
/etc/rc.config.d/netconf helped (you may have to modify this to suit
your local configuration):

INTERFACE_NAME[0]=lan0
IP_ADDRESS[0]=1.1.1.1
SUBNET_MASK[0]=255.255.255.0
BROADCAST_ADDRESS[0]="255.255.255.255"
LANCONFIG_ARGS[0]="ether"
DHCP_ENABLE[0]=0

				ULTRIX

Now that we have Ultrix packet filter support, the DHCP Distribution
on Ultrix should be pretty trouble-free.  However, one thing you do
need to be aware of is that it now requires that the pfilt device be
configured into your kernel and present in /dev.  If you type ``man
packetfilter'', you will get some information on how to configure your
kernel for the packet filter (if it isn't already) and how to make an
entry for it in /dev.

			       FreeBSD

Versions of FreeBSD prior to 2.2 have a bug in BPF support in that the
ethernet driver swaps the ethertype field in the ethernet header
downstream from BPF, which corrupts the output packet.   If you are
running a version of FreeBSD prior to 2.2, and you find that dhcpd
can't communicate with its clients, you should #define BROKEN_FREEBSD_BPF 
in site.h and recompile.

Modern versions of FreeBSD include the ISC DHCP 3.0 client as part of
the base system, and the full distribution (for the DHCP server and
relay agent) is available from the Ports Collection in
/usr/ports/net/isc-dhcp3, or as a package on FreeBSD installation
CDROMs.

			      NeXTSTEP

The NeXTSTEP support uses the NeXTSTEP Berkeley Packet Filter
extension, which is not included in the base NextStep system.  You
must install this extension in order to get dhcpd or dhclient to work.

			       SOLARIS

There are two known issues seen when compiling using the Sun compiler.

The first is that older Sun compilers generate an error on some of
our uses of the flexible array option.  Newer versions only generate
a warning, which can be safely ignored.  If you run into this error
("type of struct member "buf" can not be derived from structure with
flexible array member"), upgrade your tools to  Oracle Solaris Studio 
(previously Sun Studio) 12 or something newer.

The second is the interaction between the configure script and the
makefiles for the Bind libraries.  Currently we don't pass all
environment variables between the DHCP configure and the Bind configure.

If you attempt to specify the compiler you wish to use like this:

	CC=/opt/SUNWspro/bin/cc ./configure

"make" may not build the Bind libraries with that compiler.

In order to use the same compiler for Bind and DHCP we suggest the
following commands:

	CC=/opt/SUNWspro/bin/cc ./configure
	CC=/opt/SUNWspro/bin/cc make

				Solaris 11

We have integrated a patch from Oracle to use sockets instead of
DLPI on Solaris 11.  This functionality was written for use with
Solaris Studio 12.2 and requires the system/header package.

By default this code is disabled in order to minimize disruptions
for current users.  In order to enable this code you will need to
enable both USE_SOCKETS and USE_V4_PKTINFO as part of the
configuration step.  The command line would be something like:

	  ./configure --enable-use-sockets --enable-ipv4-pktinfo

				Solaris 11 and ATF

We have reports that ATF 0.15 and 0.16 do not build on Solaris 11.  The
following changes to the ATF source code appear to fix this issue:

diff -ru atf-0.15/atf-c/tp_test.c atf-0.15-patched/atf-c/tp_test.c
--- atf-0.15/atf-c/tp_test.c 2011-12-06 06:31:11.000000000 +0100
+++ atf-0.15-patched/atf-c/tp_test.c 2012-06-19 15:54:57.000000000 +0200
@@ -28,6 +28,7 @@
*/

#include <string.h>
+#include <stdio.h>
#include <unistd.h>

#include <atf-c.h>

diff -ru atf-0.15/atf-run/requirements.cpp atf-0.15-patched/atf-run/requirements.cpp
--- atf-0.15/atf-run/requirements.cpp 2012-01-13 20:44:25.000000000 +0100
+++ atf-0.15-patched/atf-run/requirements.cpp 2012-06-19 15:41:51.000000000 +0200
@@ -29,7 +29,7 @@

extern "C" {
#include <sys/param.h>
-#include <sys/sysctl.h>
+//#include <sys/sysctl.h>
}

#include <cerrno>

				Other Solaris Items

One problem which has been observed and is not fixed in this
patchlevel has to do with using DLPI on Solaris machines.  The symptom
of this problem is that the DHCP server never receives any requests.
This has been observed with Solaris 2.6 and Solaris 7 on Intel x86
systems, although it may occur with other systems as well.  If you
encounter this symptom, and you are running the DHCP server on a
machine with a single broadcast network interface, you may wish to
edit the includes/site.h file and uncomment the #define USE_SOCKETS
line.  Then type ``make clean; make''.  As an alternative workaround,
it has been reported that running 'snoop' will cause the dhcp server
to start receiving packets.  So the practice reported to us is to run
snoop at dhcpd startup time, with arguments to cause it to receive one
packet and exit.

	snoop -c 1 udp port 67 > /dev/null &

The DHCP client on Solaris will only work with DLPI.  If you run it
and it just keeps saying it's sending DHCPREQUEST packets, but never
gets a response, you may be having DLPI trouble as described above.
If so, we have no solution to offer at this time, aside from the above
workaround which should also work here.  Also, because Solaris requires
you to "plumb" an interface before it can be detected by the DHCP client,
you must either specify the name(s) of the interface(s) you want to
configure on the command line, or must plumb the interfaces prior to
invoking the DHCP client.  This can be done with ``ifconfig iface plumb'',
where iface is the name of the interface (e.g., ``ifconfig hme0 plumb'').

It should be noted that Solaris versions from 2.6 onward include a
DHCP client that you can run with ``/sbin/ifconfig iface dhcp start''
rather than using the ISC DHCP client, including DHCPv6.  Consequently,
we don't believe there is a need for the client to run on Solaris, and
have not engineered the needed DHCPv6 modifications for the dhclient-script.
If you feel this is in error, or have a need, please contact us.

				AIX

The AIX support uses the BSD socket API, which cannot differentiate on
which network interface a broadcast packet was received; thus the DHCP
server and relay will work only on a single interface.  (They do work
on multi-interface machines if configured to listen on only one of the
interfaces.)

We have reports of Windows XP clients having difficulty retrieving
addresses from a server running on an AIX machine.  This issue
was traced to the client requiring messages be sent to the all ones
broadcast address (255.255.255.255) while the AIX server was sending 
to 192.168.0.255.

You may be able to solve this by including a relay between the client
and server with the relay configured to use a broadcast of all-ones.

A second option that worked for AIX 5.1 but doesn't seem to work for
AIX 5.3 was to:
	create a host file entry for all-ones (255.255.255.255)
and then add a route:
	route add -host all-ones -interface <local-ip-address>

The ISC DHCP distribution does not include a dhclient-script for AIX--
AIX comes with a DHCP client.  Contribution of a working dhclient-script
for AIX would be welcome.


			       MacOS X

The MacOS X system uses a TCP/IP stack derived from FreeBSD with a
user-friendly interface named the System Configuration Framework.
As it includes a builtin DHCPv4 client (you are better just using that),
this text is only about the DHCPv6 client (``dhclient -6 ...'').  The DNS
configuration (domain search list and name servers' addresses) is managed
by a System Configuration agent, not by /etc/resolv.conf (which is a link
to /var/run/resolv.conf, which itself only reflects the internal state;
the System Configuration framework's Dynamic Store).

This means that modifying resolv.conf directly doesn't have the
intended effect, instead the macos script sample creates its own
resolv.conf.dhclient6 in /var/run, and inserts the contents of this
file into the Dynamic Store.

When updating the address configuration the System Configuration
framework expects the prefix and a default router along with the
configured address. As this extra information is not available via
the DHCPv6 protocol the System Configuration framework isn't usable
for address configuration, instead ifconfig is used directly.

Note the Dynamic Store (from which /var/run/resolv.conf is built) is
recomputed from scratch when the current location/set is changed.
Running the dhclient-script reinstalls the resolv.conf.dhclient6
configuration.


			       ATF

Please see the file DHCP/doc/devel/atf.dox for a description of building
and using these tools.  

The optional unit tests use ATF (Automated Testing Framework) including
the atf-run and atf-report tools. ATF deprecated these tools in
version 0.19 and removed these tools from its sources in version 0.20,
requiring you to get an older version, use Kyua with an ATF compatibility
package or use the version included in the Bind sources.

			       SUPPORT

The Internet Systems Consortium DHCP server is developed and distributed
by ISC in the public trust, thanks to the generous donations of its
sponsors.  ISC now also offers commercial quality support contracts for
ISC DHCP, more information about ISC Support Contracts can be found at
the following URL:

	https://www.isc.org/services/support/

Please understand that we may not respond to support inquiries unless
you have a support contract.  ISC will continue its practice of always
responding to critical items that effect the entire community, and
responding to all other requests for support upon ISC's mailing lists
on a best-effort basis.

However, ISC DHCP has attracted a fairly sizable following on the
Internet, which means that there are a lot of knowledgeable users who
may be able to help you if you get stuck.  These people generally
read the dhcp-users@isc.org mailing list.  Be sure to provide as much
detail in your query as possible.

If you are going to use ISC DHCP, you should probably subscribe to
the dhcp-users or dhcp-announce mailing lists.

WHERE TO SEND FEATURE REQUESTS: We like to hear your feedback.  We may
not respond to it all the time, but we do read it.  If ISC DHCP doesn't
work well for you, or you have an idea that would improve it for your
use, please send your suggestion to dhcp-suggest@isc.org.  This is also
an excellent place to send patches that add new features.

WHERE TO REPORT BUGS: If you want the act of sending in a bug report
to result in you getting help in the form of a fixed piece of
software, you are asking for help.  Your bug report is helpful to us,
but fundamentally you are making a support request, so please use the
addresses described in the previous paragraphs.  If you are _sure_ that
your problem is a bug, and not user error, or if your bug report
includes a patch, you can send it to our ticketing system at
dhcp-bugs@isc.org.  If you have not received a notice that the ticket
has been resolved, then we're still working on it.

PLEASE DO NOT REPORT BUGS IN OLD SOFTWARE RELEASES!  Fetch the latest
release and see if the bug is still in that version of the software,
and if it is still present, _then_ report it.  ISC release versions 
always have three numbers, for example: 1.2.3.  The 'major release' is 
1 here, the 'minor release' is 2, and the 'maintenance release' is 3.  
ISC will accept bug reports against the most recent two major.minor
releases: for example, 1.0.0 and 0.9.0, but not 0.8.* or prior.

PLEASE take a moment to determine where the ISC DHCP distribution
that you're using came from.  ISC DHCP is sometimes heavily modified
by integrators in various operating systems - it's not that we
feel that our software is perfect and incapable of having bugs, but
rather that it is very frustrating to find out after many days trying
to help someone that the sources you're looking at aren't what they're
running.  When in doubt, please retrieve the source distribution from
ISC's web page and install it.

		HOW TO REPORT BUGS OR REQUEST HELP

When you report bugs or ask for help, please provide us complete
information.  A list of information we need follows.  Please read it
carefully, and put all the information you can into your initial bug
report.  This will save us a great deal of time and more informative
bug reports are more likely to get handled more quickly overall.

      1.  The specific operating system name and version of the
	  machine on which the DHCP server or client is running.
      2.  The specific operating system name and version of the
	  machine on which the client is running, if you are having
	  trouble getting a client working with the server.
      3.  If you're running Linux, the version number we care about is
	  the kernel version and maybe the library version, not the
	  distribution version - e.g., while we don't mind knowing
	  that you're running Redhat version mumble.foo, we must know
	  what kernel version you're running, and it helps if you can
	  tell us what version of the C library you're running,
	  although if you don't know that off the top of your head it
	  may be hard for you to figure it out, so don't go crazy
	  trying.
      4.  The specific version of the DHCP distribution you're
	  running, as reported by dhcpd -t.
      5.  Please explain the problem carefully, thinking through what
	  you're saying to ensure that you don't assume we know
	  something about your situation that we don't know.
      6.  Include your dhcpd.conf and dhcpd.leases file as MIME attachments
	  if they're not over 100 kilobytes in size each.  If they are
	  this large, please make them available to us eg via a hidden
	  http:// URL or FTP site.  If you're not comfortable releasing
	  this information due to sensitive contents, you may encrypt
	  the file to our release signing key, available on our website.
      7.  Include a log of your server or client running until it
	  encounters the problem - for example, if you are having
	  trouble getting some client to get an address, restart the
	  server with the -d flag and then restart the client, and
	  send us what the server prints.   Likewise, with the client,
	  include the output of the client as it fails to get an
	  address or otherwise does the wrong thing.   Do not leave
	  out parts of the output that you think aren't interesting.
      8.  If the client or server is dumping core, please run the
	  debugger and get a stack trace, and include that in your
	  bug report.   For example, if your debugger is gdb, do the
	  following:

		gdb dhcpd dhcpd.core
		(gdb) where
		      [...]
		(gdb) quit

	  This assumes that it's the dhcp server you're debugging, and
	  that the core file is in dhcpd.core.

Please see https://www.isc.org/software/dhcp/ for details on how to subscribe
to the ISC DHCP mailing lists.

			       HISTORY

ISC DHCP was originally written by Ted Lemon under a contract with
Vixie Labs with the goal of being a complete reference implementation
of the DHCP protocol.  Funding for this project was provided by
Internet Systems Consortium. The first release of the ISC DHCP
distribution in December 1997 included just the DHCP server.
Release 2 in June 1999 added a DHCP client and a BOOTP/DHCP relay
agent. DHCP 3 was released in October 2001 and included DHCP failover
support, OMAPI, Dynamic DNS, conditional behaviour, client classing,
and more. Version 3 of the DHCP server was funded by Nominum, Inc.
The 4.0 release in December 2007 introduced DHCPv6 protocol support
for the server and client.

This product includes cryptographic software written
by Eric Young (eay@cryptsoft.com).
LDAP Support in DHCP
Original Author: Brian Masney <masneyb@gftp.org>
Current Maintainer: David Cantrell <dcantrell@redhat.com>
Last updated 07-Jul-2009

This document describes setting up the DHCP server to read it's configuration
from LDAP.  This work is based on the IETF document
draft-ietf-dhc-ldap-schema-01.txt included in the doc directory.  For the
latest version of this document, please see
http://dcantrel.fedorapeople.org/dhcp/ldap-patch/

First question on most people's mind is "Why do I want to store my
configuration in LDAP?"  If you run a small DHCP server, and the configuration
on it rarely changes, then you won't need to store your configuration in LDAP.
But, if you have several DHCP servers, and you want an easy way to manage your
configuration, this can be a solution.

The first step will be to setup your LDAP server.  I am using OpenLDAP from
www.openldap.org.  Building and installing OpenLDAP is beyond the scope of
this document.  There is plenty of documentation out there about this.  Once
you have OpenLDAP installed, you will have to edit your slapd.conf file.  I
added the following 2 lines to my configuration file:

include         /etc/ldap/schema/dhcp.schema
index           dhcpHWAddress eq
index           dhcpClassData eq

The first line tells it to include the dhcp schema file.  You will find this
file under the contrib directory in this distribution.  You will need to copy
this file to where your other schema files are (maybe /etc/openldap/schema/).
The second line sets up an index for the dhcpHWAddress parameter.  The third
parameter is for reading subclasses from LDAP every time a DHCP request comes
in.  Make sure you run the slapindex command and restart slapd to have these
changes to into effect.

Now that you have LDAP setup, you should be able to use gq
(http://biot.com/gq/) to verify that the dhcp schema file is loaded into LDAP.
Pull up gq, and click on the Schema tab.  Go under objectClasses, and you
should see at least the following object classes listed: dhcpClass, dhcpGroup,
dhcpHost, dhcpOptions, dhcpPool, dhcpServer, dhcpService, dhcpSharedNetwork,
dhcpSubClass, and dhcpSubnet.  If you do not see these, you need to check over
your LDAP configuration before you go any further.

You should now be ready to build DHCP.  If you would like to enable LDAP in
dhcpd, you will need to perform the following steps:

  * Apply the patch here to the unpacked ISC dhcp source tree.
  * Regenerate the configure script (requires GNU autoconf and automake):
        aclocal
        libtoolize --copy --force
        autoconf
        autoheader
        automake --foreign --add-missing --copy
  * Run ./configure with the '--with-ldap' argument to enable OpenLDAP.
    If you want LDAP over SSL, also use the '--with-ldapcrypto' argument.
  * Run 'make' to build ISC dhcp.

Once you have DHCP installed, you will need to setup your initial plaintext
config file. In my /etc/dhcpd.conf file, I have:

ldap-server "localhost";
ldap-port 389;
ldap-username "cn=DHCP User, dc=ntelos, dc=net";
ldap-password "blah";
ldap-base-dn "dc=ntelos, dc=net";
ldap-method dynamic;
ldap-debug-file "/var/log/dhcp-ldap-startup.log";

If SSL has been enabled at compile time, the dhcp server trys to use TLS if
possible, but continues without TLS if not.

You can modify this behaviour using following option in /etc/dhcp/dhcpd.conf:

ldap-ssl <off | ldaps | start_tls | on>
   off:       disables TLS/LDAPS.
   ldaps:     enables LDAPS -- don't forget to set ldap-port to 636.
   start_tls: enables TLS using START_TLS command
   on:        enables LDAPS if ldap-port is set to 636 or TLS in 
              other cases.

See also "man 5 ldap.conf" for description the following TLS related 
options:
   ldap-tls-reqcert, ldap-tls-ca-file, ldap-tls-ca-dir, ldap-tls-cert
   ldap-tls-key, ldap-tls-crlcheck, ldap-tls-ciphers, ldap-tls-randfile

The ldap-init-retry <num> enables an optional ldap connect retry loop with
the specified number of retries with a one second sleep between each try
during the initial startup of the dhcp server.
It allows to catch the condition, that the (remote) ldap server is not yet
started at the start time of the dhcp server.

All of these parameters should be self explanatory except for the ldap-method.
You can set this to static or dynamic.  If you set it to static, the
configuration is read once on startup, and LDAP isn't used anymore.  But, if
you set this to dynamic, the configuration is read once on startup, and the
hosts that are stored in LDAP are looked up every time a DHCP request comes
in.

When the optional statement ldap-debug-file is specified, on startup the DHCP
server will write out the configuration that it generated from LDAP.  If you
are getting errors about your LDAP configuration, this is a good place to
start looking.

The next step is to set up your LDAP tree. Here is an example config that will
give a 10.100.0.x address to machines that have a host entry in LDAP.
Otherwise, it will give a 10.200.0.x address to them.  (NOTE: replace
dc=ntelos, dc=net with your base dn). If you would like to convert your
existing dhcpd.conf file to LDIF format, there is a script
dhcpd-conf-to-ldap that will convert it for you.  Type
dhcpd-conf-to-ldap --help to see the usage information for this script.

# You must specify the server's host name in LDAP that you are going to run
# DHCP on and point it to which config tree you want to use.  Whenever DHCP
# first starts up, it will do a search for this entry to find out which
# config to use
dn: cn=brian.ntelos.net, dc=ntelos, dc=net
objectClass: top
objectClass: dhcpServer
cn: brian.ntelos.net
dhcpServiceDN: cn=DHCP Service Config, dc=ntelos, dc=net

# Here is the config tree that brian.ntelos.net points to.
dn: cn=DHCP Service Config, dc=ntelos, dc=net
cn: DHCP Service Config
objectClass: top
objectClass: dhcpService
dhcpPrimaryDN: dc=ntelos, dc=net
dhcpStatements: ddns-update-style none
dhcpStatements: default-lease-time 600
dhcpStatements: max-lease-time 7200

# Set up a shared network segment
dn: cn=WV Test, cn=DHCP Service Config, dc=ntelos, dc=net
cn: WV
objectClass: top
objectClass: dhcpSharedNetwork

# Set up a subnet declaration with a pool statement.  Also note that we have
# a dhcpOptions object with this entry
dn: cn=10.100.0.0, cn=WV Test, cn=DHCP Service Config, dc=ntelos, dc=net
cn: 10.100.0.0
objectClass: top
objectClass: dhcpSubnet
objectClass: dhcpOptions
dhcpOption: domain-name-servers 10.100.0.2
dhcpOption: routers 10.100.0.1
dhcpOption: subnet-mask 255.255.255.0
dhcpOption: broadcast-address 10.100.0.255
dhcpNetMask: 24

# Set up a pool for this subnet.  Only known hosts will get these IPs
dn: cn=Known Pool, cn=10.100.0.0, cn=WV Test, cn=DHCP Service Config, dc=ntelos, dc=net
cn: Known Pool
objectClass: top
objectClass: dhcpPool
dhcpRange: 10.100.0.3 10.100.0.254
dhcpPermitList: deny unknown-clients

# Set up another subnet declaration with a pool statement
dn: cn=10.200.0.0, cn=WV Test, cn=DHCP Service Config, dc=ntelos, dc=net
cn: 10.200.0.0
objectClass: top
objectClass: dhcpSubnet
objectClass: dhcpOptions
dhcpOption: domain-name-servers 10.200.0.2
dhcpOption: routers 10.200.0.1
dhcpOption: subnet-mask 255.255.255.0
dhcpOption: broadcast-address 10.200.0.255
dhcpNetMask: 24

# Set up a pool for this subnet. Only unknown hosts will get these IPs
dn: cn=Known Pool, cn=10.200.0.0, cn=WV Test, cn=DHCP Service Config, dc=ntelos, dc=net
cn: Known Pool
objectClass: top
objectClass: dhcpPool
dhcpRange: 10.200.0.3 10.200.0.254
dhcpPermitList: deny known clients

# Set aside a group for all of our known MAC addresses
dn: cn=Customers, cn=DHCP Service Config, dc=ntelos, dc=net
objectClass: top
objectClass: dhcpGroup
cn: Customers

# Host entry for my laptop
dn: cn=brianlaptop, cn=Customers, cn=DHCP Service Config, dc=ntelos, dc=net
objectClass: top
objectClass: dhcpHost
cn: brianlaptop
dhcpHWAddress: ethernet 00:00:00:00:00:00

You can use the command ldapadd to load all of these entries into your LDAP
server. After you load this, you should be able to start up DHCP. If you run
into problems reading the configuration, try running dhcpd with the -d flag.
If you still have problems, edit the site.conf file in the DHCP source and
add the line: COPTS= -DDEBUG_LDAP and recompile DHCP. (make sure you run make
clean and rerun configure before you rebuild).

DHCPv6 requires a separate instance of the dhcpd server from the
DHCPv4 server.  

It is convenient to use distinct LDAP login DNs for the two servers,
and setup LDAP access restrictions in the LDAP server, so that each
DHCP server only has access to its own data.

You will need to create a separate configuration file,
call it /etc/dhcpd6.conf.  For example:

ldap-server "localhost";
ldap-port 389;
ldap-username "cn=DHCPv6 User, dc=ntelos, dc=net";
ldap-password "blahblah";
ldap-base-dn "dc=ntelos, dc=net";
ldap-method dynamic;
ldap-debug-file "/var/log/dhcp-ldap-startup.log";

And use these command line arguments to dhcpd:

dhcpd eth... -6 -cf /etc/dhcpd6.conf -pf /var/run/dhcpd6.pid -lf /var/lib/dhcpd6/dhcpd.leases

For DHCPv6, the client configuration is the same, but substitute the
Client ID for the Ethernet hardware address.  Here is an example of a
host definition for a DHCPv6 client:

dn: cn=examplehost,cn=XXXX:XXXX:XXXX:XXXX::/64,cn=Network-eth1,cn=DHCPv6,dc=example,dc=com
objectClass: top
objectClass: dhcpHost
cn: examplehost
dhcpClientId: XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX
dhcpStatements: fixed-address6 XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX
option host-name "examplehost.ipv6.example.com"
option domain-name "ipv6.example.com"
In order to test the DHCPv6 server, we have a configuration file with
known values, and some Perl scripts designed to send and receive
DHCPv6 packets to check various code paths.

It is not complete test converage by any means, but it should be
fairly easy to add additional tests as needed.

The scripts themselves are not very well written. There is a lot of
copied code, poor error handling, and so on. These should be rewritten
at some point.

To use, the DHCPv6 server must be running in test mode to send back to
the originating port. (The scripts can be changed to bind to the
appropriate client port, but they don't now, and have to run as root
to do this). In server/dhcpv6.c, look for this comment:

/* For testing, we reply to the sending port, so we don't need a root */
/* client */
		to_addr.sin6_port = remote_port;
/*		to_addr.sin6_port = packet->client_port;*/

And change the code to use the client_port value.

You will need to modify one of the test configuration files to use one 
of the physical subnets that your machine uses, in the subnet6 
statement.

Then run the server as root, in debug mode:

# touch /tmp/test.leases
# dhcpd -6 -cf test-a.conf -lf /tmp/test.leases -d

You can invoke the scripts then:

$ perl 000-badmsgtype.pl

The expected results vary per script, depending on the behavior that
is being tested.


Notes about scripts:

In order to manipulate IPv6 addresses, we need the Socket6 library,
available from CPAN:

http://search.cpan.org/~umemoto/Socket6-0.19/Socket6.pm

The Perl that Sun issues for Solaris 10 is compiled with the Sun
compiler. If you have the Sun compiler, then this will work fine.
Otherwise you may need to install Perl from source.

We need to get the hardware address in order to build DUID properly.
The IO::Interface module reports hardware address, but not on Solaris
10 it seems. Rather than do this the "right way", we do it the "Perl
way", and hack it. "ifconfig" does return the Ethernet address, but
only to the root user. However, we can look for files of the name
/etc/hostname.*, get the IP address from "ifconfig", and then check
for those addresses in the ARP table.

Client DUID is supposed to be an opaque value to the server, but we go
ahead and make a "real" type 1 or type 3 DUID.



    SMB/CIFS protocol and SMB/CIFS file system implementation
		for FreeBSD, version 1.4.
    
    This is native SMB/CIFS filesystem (smbfs for short) for FreeBSD.
It is a complete, kernel side implementation of SMB requester and filesystem.

    Supportted platforms:
	FreeBSD 4.X

	FreeBSD-current		kernel module is included in the base source
				tree.

    I'm would be very grateful for any feedback, bug reports etc.

    Supported SMB servers:
	Samba
	Windows 95/98/ME/2000/NT4.0 (SPs 4, 5, 6)
	IBM LanManager
	NetApp

    An updated versions of this package can be retrieved from ftp server:

    ftp://ftp.butya.kz/pub/smbfs/smbfs.tar.gz

    Perfomance
    ==========
    
    There is some perfomance benchmarks over 10Mbit network:
    
    Win95 machine as server:
IOZONE: auto-test mode
	MB      reclen  bytes/sec written   bytes/sec read
	1       512     339791              323416
	1       1024    481067              431568
	1       2048    648394              588674
	1       4096    630130              583555
	1       8192    671088              618514

    Samba 2.0.6 as server:
IOZONE: auto-test mode
	MB      reclen  bytes/sec written   bytes/sec read
	1       512     409200              437191
	1       1024    545600              596523
	1       2048    729444              798915
	1       4096    871543              919299
	1       8192    900790              1024562

Author: Boris Popov <bp@freebsd.org>
Kyua (pronounced Q.A.) is a testing framework for both developers and
users.  Kyua is different from most other testing frameworks in that it
puts the end user experience before anything else.  There are multiple
reasons for users to run the tests themselves, and Kyua ensures that
they can do so in the most convenient way.

This module, kyua-cli, provides the command-line interface to the Kyua
runtime system.  The major purpose of this tool is to run test cases and
generate unified reports for their results.

For further information on the contents of this distribution file,
please refer to the following other documents:

* AUTHORS: List of authors and contributors to this project.
* COPYING: License information.
* INSTALL: Compilation and installation instructions.
* NEWS: List of major changes between formal releases.

For general project information, please visit:

    http://code.google.com/p/kyua/
IP Filter - What's this about ?
============================
Web site: http://coombs.anu.edu.au/~avalon/ip-filter.html
How-to: http://www.obfuscation.org/ipf/ipf-howto.txt

  The idea behind this package is allow those who use Unix workstations as
routers (a common occurance in Universities it appears) to apply packet
filtering to packets going in and out of them.  This package has been
tested on all versions of SunOS 4.1 and Solaris 2.4/2.5, running on Sparcs.
It is also quite possible for this small kernel extension to be installed
and used effectively on Sun workstations which don't route IP, just for
added security.  It can also be integrated with the multicast patches.
It has also been tested successfully on all of the modern free BSDs as
well as BSDI, and SGI's IRIX 6.2.

   The filter keeps a rule list for both inbound and outbound sides of
the IP packet queue and a check is made as early as possible, aiming to
stop the packet before it even gets as far as being checked for source
route options.  In the file "BNF", a set of rules for constructing filter
rules understood by this package is given.  The files in the directory
"rules", "example.1" ... "example.sr" show example rules you might apply.

   In practise, I've successfully isolated a workstation from all
machines except the NFS file servers on its local subnets (yeah, ok, so
this doesn't really increase security, because of NFS, but you get the
drift on how it can be applied and used).  I've also successfully
setup and maintained my own firewalls using it with TIS's Firewall Toolkit,
including using it on an mbone router.

   When using it with multicast IP, the calls to fr_check() should be
before the packet is unwrapped and after it is encapsulated.  So the
filter routines will see the packet as a UDP packet, protocol XYZ.
Whether this is better or worse than having it filter on class D addresses
is debateable, but the idea behind this package is to be able to
discriminate between packets as they are on the 'wire', before they
get routed anywhere, etc.

   It is worth noting, that it is possible, using a small MTU and
generating tiny fragmented IP packets to generate a TCP packet which
doesn't contain enough information to filter on the "flags".  Filtering
on these types of packets is possible, but under the more general case
of the packets being "short".  ICMP and UDP packets which are too small
(they don't contain a complete header) are dropped and logged, no questions
asked.  When filtering on fragmented packets, the last fragment will get
through for TCP/UDP/ICMP packets.

Bugs/Problems
-------------
If you have a problem with IP Filter on your operating system, please email
a copy of the file "BugReport" with the details of your setup as required
and email to darrenr@pobox.com.

Some general notes.
-------------------
   To add/delete a rule from memory, access to the device in /dev is needed,
allowing non-root maintenaince.  The filter list in kernel memory is built
from the kernel's heap.  Each packet coming *in* or *out* is checked against
the appropriate list, rejects dropped, others passed through.  Thus this will
work on an individual host, not just gateways.  Presently there is only one
list for all interfaces, the changes required to make it a per-interface list
require more .o replacements for the kernel.  When checking a packet, the
packet is compared to the entire list from top to bottom, the last matching
line being effective.


What does what ?
----------------
if_fil.o  (Loadable kernel module)
	- additional kernel routines to check an access list as to whether
	  or not to drop or pass a packet.  It currently defaults to pass
	  on all packets.

ipfstat
	- digs through your kernel (need to check #define VMUNIX in fils.c)
	  and /dev/kmem for the access filter list and mini stats table.
	  Obviously needs to be run priviledged if required.

ipf
	- reads the files passed as parameters as input files containing new
	  filter rules to add/delete to the kernel list.  The lines are
	  inserted in order; the first line is inserted first, and ends up
	  first on the list.  Subsequent invocations append to the list
	  unless specified otherwise.

ipftest
	- test the ruleset given by filename.  Reads in the ruleset and then
	  waits for stdin.

	  See the man pages (ipf.1, ipftest.1, ipfstat.8) for more detailed
	  information on what the above do.

mkfilters
	- suggests a set of filter rules to employ and suggests how to add
	  routes to back these up.

BNF
	- BNF rule set for the filter rules

Darren Reed
darrenr@pobox.com
http://coombs.anu.edu.au/~avalon/ip-filter.html

there was a patch for fwtk with ip_filter 3.1.5 from James B. Croall
(thanx for his work) which I put onto fwtk 2.0beta.

Now, if you decide to do transparent proxying with ip-filter you
have to put -DUSE_IP_FILTER to COPTS in Makefile.config.
With Solaris 2.x you have to correctly replace the path to your
ip_filter sources. (lib/hnam.c needs ip_nat.h)

I also patched plug-gw to be configured to accept not only one
destination with the parameter "-all-destinations" in netperm-table.
Perhaps this is a security hole...

The patched fwtk worked fine for me with linux (kernel 2.0.28 and ipfadm 2.1)
and Solaris 2.5 (ip_filter 3.1.5).

If you try to enhance the transparent proxy features for other
architectures, see lib/hnam.c (getdsthost).

Michael Kutzner, Michael.Kutzner@paderlinx.de

There are two patch files in this directory, each allowing for the Firewall
Toolkit to be used in a transparent proxy configuration.

ftp-gw.diff	- A patch written by myself for use only with IP Filter and
		  ftp-gw from the Firewall Toolkit.  You need to copy ip_nat.h,
		  ip_fil.h and ip_compat.h to the ftp-gw directory to compile
		  once this patch has been applied.

fwtkp		- A set of patches written by James B. Croall (jcroall@foo.org)
		  for use with both IP Filter and ipfwadm (for Linux) and more
		  of the various FWTK gateway plugins, including:
		  ftp-gw http-gw plug-gw rlogin-gw tn-gw

Both patches when applied to the Firewall toolkit require the same
configuration for IP Filter.

Darren

IP Scan Detetor.
----------------

This program is designed to be a passive listener for TCP packets sent to
the host.  It does not exercise the promiscous mode of interfaces.  For
routing Unix boxes (and firewalls which route/proxy) this is sufficient to
detect all packets going to/through them.

Upon compiling, a predefined set of "sensitive" ports are configured into
the program.  Any TCP packets which are seen sent to these ports are counted
and the IP# of the sending host recorded, along with the time of the first
packet to that port for that IP#.

After a given number of "hits", it will write the current table of packets
out to disk.  This number defaults to 10,000.

To analyze the information written to disk, a sample program called "ipsdr"
is used (should but doesn't implement a tree algorithm for storing data)
which  reads all log files it recognises and totals up the number of ports
each host hit.  By default, all ports have the same weighting (1).  Another
group of passes is then made over this table using a netmask of 0xfffffffe,
grouping all results which fall under the same resulting IP#.  This netmask
is then shrunk back to 0, with a output for each level given.  This is aimed
at detecting port scans done from different hosts on the same subnet (although
I've not seen this done, if one was trying to do it obscurely...)

Lastly, being passive means that no action is taken to stop port scans being
done or discourage them.

Darren
darrenr@pobox.com
                             TOP
                       Version 3.8beta1

                       William LeFebvre
		     and a cast of dozens


If you do not want to read this entire file, then at least read
the section at the end entitled "KNOWN PROBLEMS".

If you are having any problems getting top to work, please read the
file "FAQ" *before* contacting me.  Thank you.

"top" is a program that will give continual reports about the state of
the system, including a list of the top cpu using processes.  Version 3
of "top" has three primary design goals: provide an accurate snapshot of
the system and process state, not be one of the top processes itself, be
as portable as possible.

Version 3 has many bug fixes from version 2.5, and it has also been
reorganized in a major way to make it easy to port to other platforms.
All system dependent code is now contained in one file.

Starting with version 3.6, top includes a "configure" script generated
by Gnu's autoconf.  This script MUST be run before attempting to
compile top.  It will explore the system and generate approriate
contents for Makefile, config.h, and top.1.

On some systems, top requires read access to the memory files
"/dev/kmem" and "/dev/mem" as well as the system's kernel image.  Most
installations have these files protected from general access.  These
sites would have to install this program in the same way that programs
such as "ps" are installed.  On most systems with a /proc file system,
top will try to read everything it can from /proc, but may need extra
permissions to do so.  The configure script will determine the
permissions needed by the top binary, and a "make install" as root
will get the binary installed correctly.  Sometimes this requires that
the binary be installed with set-group-id privileges and, in rare
cases, set-user-id to root.

CAVEAT: version 3 of top has internal commands that kill and renice
processes.  Although I have taken steps to insure that top makes
appropriate checks with these commands, I cannot guarantee that these
internal commands are totally secure.  IF YOU INSTALL top SET-USER-ID
TO ROOT, YOU DO SO AT YOUR OWN RISK!  I realize that some operating
systems will require top to run setuid root, and I will do everything
I can to make sure that top is a secure setuid program.

System support now takes the form of "modules".  Adding support for a
different architecture requires only adding a module.  These modules
are contained in the subdirectory "machine".  The "configure" script
automatically determines which module is approproate.  However, it may
not be able to determine what the correct module is.  This can happen
either because it doesn't know about the system or there is no module
to support the system.  In the former case, if you know which module
to use, you can force "configure" to choose a particular module with
the option "--with-module".  For example, if you want to force the use
of the svr4 module (which appears as "machine/m_svr4.c") then use
"configure --with-module=svr4" to generate the correct Makefile.  See
the file "Porting" for a description of how to write your own module.

To compile and install "top", read the file "INSTALL" and follow the
directions and advice contained therein.

If you make any kind of change to "top" that you feel would be
beneficial to others who use this program, or if you find and fix a bug,
please send me the change.

Be sure to read the FAQ enclosed with the distrubution.  It contains
answers to the most commonly asked questions about the configuration,
installation, and operation of top.

COLOR

Version 3.6 incorporated the idea of using ANSI color sequences to
enhance information on the screen.  By default, no color is used.  But
you can configure the use of color through the environment variable
TOPCOLORS (or, for compatibility, TOPCOLOURS).  The interface is
identical to the one first implemented by chris@spang.uk.eu.org, but
the implementation is entirely different.  The option -C can be used
to diable the feature entirely.

Any information at the top of the screen can be enhanced with color.
However, due to implementation difficulties, the per-process area
cannot be color-enhanced.  A complete description of color support can
be found in the man page.  References for ANSI color codes can be
found all over the Internet, but if you want a handy reference, look
in color.h.


AVAILABILITY

Note that top is now a sourceforge project!  Its project name is
"unixtop" and you can access its project page here:

http://sourceforge.net/projects/unixtop

On the project page you can find more information and access the
official bug and feature request trackers.  If you find a bug,
want to request a feature, or need help, please submit a request
to the appropriate tracker on sourceforge.  Thank you.

Subversion access is also provided by Sourceforge.  If Subversion is
installed on your system you can check out the project with the
following command:

	svn co https://svn.sourceforge.net/svnroot/unixtop unixtop

There is also a web site dedicated to the project, and it is here:

http://www.unixtop.org

The latest version of "top" is available as a download through
sourceforge.  Start here to access the downloadable files:

http://sourceforge.net/project/showfiles.php?group_id=72892


KNOWN PROBLEMS:

Gnu CC

Compiling via Gnu CC continued to be the source of most of the
questions I receive.  By far the most common mistake made by those
attempting to compile top with Gnu CC is out of date include files.
When the operating system is upgraded, the include files that are part
of the gcc package MUST also be updated.  Gcc maintains its own
include files.  Even a minor OS upgrade can involve changes to some of
the kernel's internal data structures, which are defined in include
files in "sys".  Top is very sensitive to these changes.  If you are
compiling with gcc and experience any sort of strange problems, please
make sure the include files you are using are up to date BEFORE
sending me a bug report.  Look in the gcc source distribution for the
shell script "fixincludes".

MacOS X

Since I don't have full time root access to a MacOS X system I cannot
provide effective support for the platform.  MacOS X uses Mach, and it
is very difficult to extract accurate system and process information
from the system.  It takes a lot of trial and error, along with root
access.  I have included the most up-to-date version of the macosx module
in the distribution, but I do not claim that it works.  If you want to
try to use it, you can configure with "./configure --with-module=macosx".

HP/UX 10.10

In their infinite wisdom, the folks at HP have decided that mere mortals
such as you and I don't need to know what the kernel's proc structure looks
like.  To that end, they have removed all useful content from the include
file <sys/proc.h> in version 10.10.  As a result, top will not compile
under 10.10.  What HP is trying to accomplish with this move is to force
iconoclasts such as myself to use "pstat" for collecting all process
information.  I have no immediate solution for this problem, but hope to
obtain a sufficiently complete definition of "struct proc" at some point in
the near future.  Stay tuned.


GRATITUDE

My perpetual thanks to all the people who have helped me support top
on so many platforms.  Without these people, top would not be what it
is.  Here is a partial list of contributors and other individuals.

	Robert Boucher, Marc Cohen, David Cutter, Casper Dik,
	Charles Hedrick, Andrew Herbert, Jeff Janvrin, Torsten Kasch,
	Petri Kutvonen, William L. Jones, Tim Pugh, Steve Scherf,
	Phillip Wu

(My apologies if I missed anyone.)


LICENSE

Top is distributed free of charge under the same terms as the BSD
license.  For an official statement, please refer to the file "LICENSE"
which should be included with the source distribution.


AUTHOR

If you wish to contact me, please send a message to the sourceforge
username "wnl".

		William LeFebvre

		U.S. Mail address:
		    William LeFebvre
		    11585 Jones Bridge Road
		    Suite 420 PMB 139
		    Alpharetta, GA  30202
#/* Copyright 1988,1990,1993 by Paul Vixie
# * All rights reserved
# */

##
## Copyright (c) 2004 by Internet Systems Consortium, Inc. ("ISC")
## Copyright (c) 1997,2000 by Internet Software Consortium, Inc.
##
## Permission to use, copy, modify, and distribute this software for any
## purpose with or without fee is hereby granted, provided that the above
## copyright notice and this permission notice appear in all copies.
##
## THE SOFTWARE IS PROVIDED "AS IS" AND ISC DISCLAIMS ALL WARRANTIES
## WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
## MERCHANTABILITY AND FITNESS.  IN NO EVENT SHALL ISC BE LIABLE FOR
## ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
## WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
## ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
## OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
##

ISC Cron - January 2004
[V4.0 was November, 2000]
[V4.0b1 was September 7, 1997]
[V3.1 was some time after 1993]
[V3.0 was December 27, 1993]
[V2.2 was some time in 1992]
[V2.1 was May 29, 1991]
[V2.0 was July 5, 1990]
[V2.0-beta was December 9, 1988]
[V1.0 was May 6, 1987]
ftp://ftp.isc.org/isc/cron/

This is a version of 'cron' that is known to run on most systems.  It
is functionally based on the SysV cron, which means that each user can have
their own crontab file (all crontab files are stored in a read-protected
directory, usually /var/cron/tabs).  No direct support is provided for
'at'; you can continue to run 'atrun' from the crontab as you have been
doing.  If you don't have atrun (i.e., System V) you are in trouble.

A messages is logged each time a command is executed; also, the files
"allow" and "deny" in /var/cron can be used to control access to the
"crontab" command (which installs crontabs).  It hasn't been tested on
SysV, although some effort has gone into making the port an easy one.

To use this: Sorry, folks, there is no cutesy 'Configure' script.  You'll
have to go edit a couple of files... So, here's the checklist:

	Read all the FEATURES, INSTALL, and CONVERSION files
	Edit config.h
	Edit Makefile
		(both of these files have instructions inside; note that
		 some things in config.h are definable in Makefile and are
		 therefore surrounded by #ifndef...#endif)
	'make'
	'su' and 'make install'
		(you may have to install the man pages by hand)
	kill your existing cron process
		(actually you can run your existing cron if you want, but why?)
	build new crontabs using /usr/lib/{crontab,crontab.local}
		(either put them all in "root"'s crontab, or divide it up
		 and rip out all the 'su' commands, collapse the lengthy
		 lists into ranges with steps -- basically, this step is
		 as much work as you want to make it)
	start up the new cron
		(must be done as root)
	watch it. test it with 'crontab -r' and watch the daemon track your
		changes.
	if you like it, change your /etc/{rc,rc.local} to use it instead of
		the old one.

Id: README,v 1.6 2004/01/23 19:03:32 vixie Exp
Lutok is a lightweight C++ API library for Lua.

Lutok provides thin C++ wrappers around the Lua C API to ease the
interaction between C++ and Lua.  These wrappers make intensive use of
RAII to prevent resource leakage, expose C++-friendly data types, report
errors by means of exceptions and ensure that the Lua stack is always
left untouched in the face of errors.  The library also provides a small
subset of miscellaneous utility functions built on top of the wrappers.

Lutok focuses on providing a clean and safe C++ interface; the drawback
is that it is not suitable for performance-critical environments.  In
order to implement error-safe C++ wrappers on top of a Lua C binary
library, Lutok adds several layers or abstraction and error checking
that go against the original spirit of the Lua C API and thus degrade
performance.

For further information on the contents of this distribution file,
please refer to the following other documents:

* AUTHORS: List of authors and contributors to this project.
* COPYING: License information.
* INSTALL: Compilation and installation instructions.
* NEWS: List of major changes between formal releases.

For general project information, please visit:

    http://code.google.com/p/lutok/
-- Id: README.BTYACC,v 1.2 2014/04/22 08:18:57 Tom.Shields Exp 

The original README from btyacc is below.

The backtracking enhancements to byacc have been merged into Thomas Dickey's
byacc baseline.

The %include and %define/%ifdef enhancements described below are not currently
incorporated.

The position management functionality ("YYPOSN", "yyposn", "YYREDUCEPOSNFUNC",
"YYREDUCEPOSNFUNCARG" & "YYCALLREDUCEPOSN") is replaced by a bison-compatible
"%locations" implementation.

The memory management functionality ("YYDELETEVAL" & "YYDELETEPOSN") is
replaced by a bison-compatible "%destructor" implementation.

The detailed syntax error processing functionality ("YYERROR_DETAILED"
& "yyerror_detailed()") is subsumed by the bison-compatible "yyerror()"
implementation, as modified by the %parse-param and %locations directives.

The debugging macro "YYDBPR()" in the parser skeleton is renamed
"YYSTYPE_TOSTRING()".

-------------------------------------------------------------------------------
	     BTYACC -- backtracking yacc
	     ===========================

BTYACC was created by Chris Dodd using ideas from many
places and lots of code from the Berkeley Yacc
distribution, which is a public domain yacc clone put
together by the good folks at Berkeley.  This code is
distributed with NO WARRANTY and is public domain.
It is certain to contain bugs, which you should
report to: chrisd@collins.com.

Vadim Maslov of Siber Systems <vadik@siber.com>
considerably modified BTYACC to make it suitable
for production environment.

Several people have suggested bug fixes that
were incorporated into BtYacc.

See the README.BYACC files for more about
Berkeley Yacc and other sources of info.

http://www.siber.com/btyacc/ is the current home of BtYacc.
It is provided courtesy of Siber Systems http://www.siber.com/.


		Version 3.0 changes
		-------------------
		  by Vadim Maslov

Changes mostly occurred in btyaccpa.ske file that
contains the parsing shift/reduce/backtrack algorithm.

Version 3.0 innovations focus on:
- text position computation and propagation,
- industrial-strength error processing and recovery.


** Added mechanism for computing and propagating
text position of tokens and non-terminals.

Compilers often need to build AST trees such that every node
in a tree can relate to the parsed program source it came from.
The following applications are very likely to need this:
- debuggers that show actual source of the debugged program,
- source-to-source translators that want
  unchanged parts of the tree to generate the unchanged code.

The new YYPOSN mechanism added in this version of BtYacc
helps you in automating the text position computation
and in assigning the computed text positions to the AST.
This mechanism is successfully used in commercial
parsers and source-to-source translators.

In standard Yaccs every token and every non-terminal
has an YYSTYPE semantic value attached to it.
In this new version every token and every non-terminal
also has an YYPOSN text position attached to it.
YYPOSN is a user-defined type that can be anything and
that has a meaning of text position attached to
token or non-terminal.

In addition to semantic value stack BtYacc now maintains
text position stack. Behavior of the text position stack
is similar to the behavior of the semantic value stack.

If using text position mechanism,
you need to define the following:

YYPOSN	Preprocessor variable that contains C/C++ type of
	the text position attached to
	every token and non-terminal.

yyposn  Global variable of type YYPOSN.
        The lexer must assign text position of
	the returned token to yyposn, just like it assigns
	semantic value of the returned token to yylval.

YYREDUCEPOSNFUNC
	Preprocessor variable that points to function that
	is called after the grammar rule reduction
	to reduce text positions located on the stack.

        This function is called by BtYacc to reduce text
	positions. The function is called immediately after
	the regular rule reduction occurs.

	The function has the following prototype:
	void ReducePosn(YYPOSN  &ret,
			YYPOSN  *terms,
			YYSTYPE *term_vals,
			int      term_no,
			int      stk_pos,
			int      yychar,
			YYPOSN  &yyposn,
			UserType extra);

        The function arguments are:
        - ret
	  Reference to the text position returned by
          the rule. The function must write the computed
          text position returned by the rule to ret.
          This is analogue of the $$ semantic value.

        - term_posns
          Array of the right-hand side rule components
	  YYPOSN text positions.  These are analogues of
	  $1, $2, ..., $N in the text position world.

        - term_vals
	  Array of the right-hand side (RHS) rule components
	  YYSTYPE values. These are the $1,...,$N themselves.

        - term_no
          Number of the components in RHS of the reduced rule.
          Equal to size of arrays term_posns and term_vals.
          Also equal to N in $1,...,$N in the reduced rule.

        - stk_pos
          YYSTYPE/YYPOSN stack position before the reduction.

        - yychar
          Lookahead token that immediately follows
 	  the reduced RHS components.

        - yyposn
          YYPOSN of the token that immediately follows
	  the reduced RHS components.

        - extra
          User-defined extra argument passed to ReducePosn.

        Typically this function extracts text positions from
	the right-hand side rule components and either
	assigns them to the returned $$ structure/tree or
        if no $$ value is returned, puts them into
	the ret text position from where
        it will be picked up by the later reduced rules.

YYREDUCEPOSNFUNCARG
	Extra user-defined argument passed to
	the ReducePosn function. This argument can use
	any variables defined in btyaccpa.ske.


** Added code to btyaccpa.ske that automatically cleans up
semantic semantic values and text positions of tokens
and non-terminals that are discarded and deleted as
a result of error processing.

In the previous versions the discarded token and non-terminal
semantic values were not cleaned that caused quite severe
leaks.  The only way to fix it was to add garbage collection
to YYSTYPE class.

Now BtYacc skeleton calls delete functions for semantic
values and positions of the discarded tokens and
non-terminals.

You need to define the following functions that BtYacc
calls when it needs to delete semantic value or text position.

YYDELETEVAL
	User-defined function that is called by BtYacc
	to delete semantic value of the token or non-terminal.

	The user-defined function must have the prototype:
	void DeleteYYval(YYSTYPE v, int type);
	v    is semantic value to delete,
	type is one of the following:
	0 	discarding token
	1       discarding state
	2       cleaning up stack when aborting

YYDELETEPOSN
	User-defined function that is called by BtYacc
	to delete text position of the token or non-terminal.

	The user-defined function must have the prototype:
	void DeleteYYposn(YYPOSN p, int type);
	v    is semantic value to delete,
	type is one of the following:
	0 	discarding token
	1       discarding state
	2       cleaning up stack when aborting


** User can define "detailed" syntax error processing
function that reports an *exact* position of
the token that caused the error.

If you define preprocessor variable YYERROR_DETAILED in
your grammar then you need define the following
error processing function:

void yyerror_detailed(char    *text,
		      int      errt,
		      YYSTYPE &errt_value,
		      YYPOSN  &errt_posn);

It receives the following arguments:
text		Error message.
errt		Code of the token that caused the error.
errt_value	Value of the token that caused the error.
errt_posn	Text position of token that caused error.


** Dropped compatibility with C.

Compatibility with C became increasingly difficult
to maintain as new features were added to btyaccpa.ske.
So we dropped it. If anybody wants to make the new version
compatible with C, we would gladly accept the changes.

Meanwhile we expect that you use C++ to write grammar
actions and everything else in grammar files.
Since C is (in a sense) subset of C++, your C-based
grammar may work if you use C++ compiler to compile it.

		Version 3.0 bugs fixed
		----------------------

Matthias Meixner <meixner@mes.th-darmstadt.de> fixed a bug:
BtYacc does not correctly handle typenames, if one typename
is a prefix of another one and if this type is used after
the longer one. In this case BTYacc produces invalid code.


		Version 2.1 changes
		-------------------
		  by Vadim Maslov

** Added preprocessor statements to BtYacc that are similar
in function and behavior to C/C++ preprocessor statements.

These statements are used to:

- Introduce modularity into a grammar by breaking it
  into several *.y files and assembling different
  grammars from the *.y modules using %include and %ifdef.

- Have several versions of the same grammar
  by using %ifdef and $endif.

- To include automatically generated grammar fragment.
  For instance, we use %include to include
  automatically generated list of tokens.

Preprocessor statements are:

%define <var-name>
	Define preprocessor variable named <var-name>.

%ifdef <var-name>
	If preprocessor variable named <var-name>
	is defined by %define, then process the text from
	this %ifdef to the closing %endif.

%endif
	Closing bracket for %ifdef preprocessor statement.
	Only one nesting level of %ifdef-%endif is allowed.

%include <file-name>
	Process contents of the file named <file-name>.
	If <file-name> is a relative name, it is looked up
        in a directory in which btyacc was started.
	Only one nesting level of %include is allowed.


		Version 2.0 changes
		-------------------
		  by Vadim Maslov


** Changed 16-bit short numbers to 32-bit int numbers in
grammar tables, so that huge grammar tables (tables that
are larger than 32768 elements) resulting from huge
grammars (Cobol grammar, for instance) can work correctly.
You need to have 32-bit integer to index table bigger than
32768 elements, 16-bit integer is not enough.

The original BtYacc just generated non-working tables
larger than 32768 elements without even notifying about
the table overflow.


** Make error recovery work correctly when error happens
while processing nested conflicts. Original BtYacc could
infinitely cycle in certain situations that involved error
recovery while in nested conflict.

More detailed explanation: when we have nested conflicts
(conflict that happens while trial-processing another
conflict), it leads btyacc into NP-complete searching of
conflict tree. The ultimate goal is YYVALID operator that
selects a particular branch of that tree as a valid one.

If no YYVALID is found on the tree, then error recovery
takes over.  The problem with this is that error recovery
is started in the same state context that exists on the
last surveyed branch of the conflict tree.  Sometimes this
last branch may be of zero length and it results in
recovering to exactly the same state as existed before
entering the conflict. BtYacc cycles then.

We solved this problem by memorizing the longest path in
the conflict tree while browsing it. If we ever get into
error recovery, we restore state that existed on the
longest path.  Effectively we say: if we have an error,
let us move forward as far as we possibly could while we
were browsing the conflict tree.


** Introduce YYVALID_NESTED operation in addition to
simply YYVALID.  When we have a nested conflict (conflict
while processing in trial mode for another conflict), we
want to relate YYVALID to a particular level of conflict
being in trial.

Since we mostly anticipate only 2-level nested conflicts
YYVALID_NESTED tells the parser to satisfy only the
internal conflict.  Therefore, in 1-level conflict
situation YYVALID_NESTED acts like a regular YYVALID, but
in 2-level conflict it is a no-op and the other YYVALID
for outer conflict will be searched for.


** Improved handling of situation where /tmp directory is
missing.  Original btyacc just died quietly when /tmp
directory was missing.  We added code that states the
problem explicitly. While on UNIX /tmp directory is always
present, it may be missing on WIN32 systems, therefore
diagnosing this situation is important.


	Version 1.0 changes: BackTracking
	=================================
		by Chris Dodd

BTYACC is a modified version of yacc that supports
automatic backtracking and semantic disambiguation to
parse ambiguous grammars, as well as syntactic sugar for
inherited attributes (which tend to introduce conflicts).
Whenever a btyacc generated parser runs into a
shift-reduce or reduce-reduce error in the parse table, it
remembers the current parse point (yacc stack and input
stream state), and goes into trial parse mode.  It then
continues parsing, ignoring most rule actions.  If it runs
into an error (either through the parse table or through
an action calling YYERROR), it backtracks to the most
recent conflict point and tries a different alternative.
If it finds a successful parse (reaches the end of the
input or an action calls YYVALID), it backtracks to the
point where it first entered trial parse mode, and
continues with a full parse (executing all actions),
following the path of the successful trial.

Actions in btyacc come in two flavors -- {}-actions, which
are only executed when not in trial mode, and []-actions
which are executed regardless of mode.  There are also
inherited attributes, which look like arguments (they are
enclosed in "()") and act like []-actions.

What this buys you:

* No more lexer feedback hack.  In yacc grammars for C, a
standard hack, know as the "lexer feedback hack" is used
to find typedef names.  The lexer uses semantic
information to decide if any given identifier is a
typedef-name or not and returns a special token.  With
btyacc, you no longer need to do this; the lexer should
just always return an identifier.  The btyacc grammar then
needs a rule of the form:

typename: ID [ if (!IsTypeName(LookupId($1))) YYERROR; ]

While the hack works adequately well for parsing C, it
becomes a nightmare when you try to parse something like
C++, where treating an ID as a typedef becomes heavily
dependent on context.

* Easy disambiguation via simple ordering.  Btyacc runs
its trials via the rule "try shifting first, then try
reducing by the order that the conflicting rules appear in
the input file".  This means you can deal with semantic a
disambiguation rule like:
    [1] If it looks like a declaration it is, otherwise
    [2] If it looks like an expression it is, otherwise
    [3] it is a syntax error
	[Ellis&Stroustrup, Annotated C++ Reference Manual, p93]

To deal with this, you need only put all the rules for
declarations before the rules for expressions in the
grammar file.

* No extra cost if you do not use it.  Backtracking is
only triggered when the parse hits a shift/reduce or
reduce/reduce conflict in the table.  If you have no
conflicts in your grammar, there is no extra cost, other
than some extra code which will never be invoked.

* C++ and ANSI C compatible parsers.  The parsers produced
by btyacc can be compiled with C++ correctly.  If you
"#define" YYSTYPE to be some C++ type with constructor and
destructor, everything will work fine.  My favorite is
"#define YYSTYPE SmartPointer", where SmartPointer is a
smart pointer type that does garbage collection on the
pointed to objects.

BTYACC was originally written to make it easy to write a
C++ parser (my goal was to be able to use the grammar out
of the back of the ARM with as few modifications as
possible).  Anyone who has ever looked at Jim Roskind
public domain C++ yacc grammar, or the yacc-based grammar
used in g++ knows how difficult this is.  BTYACC is very
useful for parsing any ambiguous grammar, particularly
ones that come from trying to merge two (or more) complete
grammars.

Limitations of the backtracking: Currently, the generated
parser does NO pruning of alternate parsing paths.  To
avoid an exponential explosion of possible paths (and
parsing time), you need to manually tell the parser when
it can throw away saved paths using YYVALID.  In practice,
this turns out to be fairly easy to do.  A C++ parser (for
example) can just put a [YYVALID;] after every complete
declaration and statement rule, corresponding to pruning
the backtracking state after seeing a ';' or '}' -- there
will never be a situation in which it is useful to
backtrack past either of these.

Inherited attributes in btyacc:

Inherited attributes look a lot like function arguments to
non-terminals, which is what they end up being in a
recursive descent parser, but NOT how they are implemented
in btyacc.  Basically they are just syntactic sugar for
embedded semantic actions and $0, $-1, ... in normal yacc.
btyacc gives you two big advantages besides just the
syntax:
    1. it does type checking on the inherited attributes,
       so you do not have to specify $<type>0 and makes sure
       you give the correct number of arguments (inherited
       attributes) to every use of a non-terminal.
    2. It "collapses" identical actions from that are produced
       from inherited attributes.  This eliminates many
       potential reduce-reduce conflicts arising from
       the inherited attributes.

You use inherited attributes by declaring the types of the
attributes in the preamble with a type declaration and
declaring names of the attributes on the lhs of the yacc
rule.  You can of course have more than one rule with the
same lhs, and you can even give them different names in
each, but the type and number must be the same.

Here is a small example:
           /* lhs takes 2 inherited attributes */
%type <t1> lhs(<t1>, <t2>)
	   stuff(<t1>, <t2>)
%%
lhs($i1, $i2) : { $$ = $i1 }
	      | lhs($i1, $i2) stuff($1,$i2) { $$ = $2; }

This is roughly equivalent to the following yacc code:
lhs :
      { $$ = $<t1>-1; }
    | lhs [ $<t1>$ = $-1; ] [ $<t2>$ = $<t2>0; ] stuff
      { $$ = $4; }
    ;

See the file "test/t2.y" for a longer and more complete
example.  At the current time, the start symbol cannot
have any arguments.

Variant parsers:

Btyacc supports the -S flag to use a different parser
skeleton, changing the way that the parser is called and
used.  The skeleton "push.skel" is included to produce a
"passive" parser that you feed tokens to (rather than
having the parser call a separate yylex routine).  With
push.skel, yyparse is defined as follows:

int yyparse(int token, YYSTYPE yylval)

You should call yyparse repeatedly with successive tokens
of input.  It returns 0 if more input is needed, 1 for a
successful parse, and -1 for an unrecoverable parse error.


	Miscellaneous Features in ver. 1.0
	----------------------------------
		by Chris Dodd

     The -r option has been implemented.  The -r option tells
Yacc to put the read-only tables in y.tab.c and the code and
variables in y.code.c.  Keith Bostic asked for this option so
that :yyfix could be eliminated.

     The -l and -t options have been implemented.  The -l
option tells Yacc not to include #line directives in the code
it produces.  The -t option causes debugging code to be
included in the compiled parser.

     The code for error recovery has been changed to
implement the same algorithm as AT&T Yacc.  There will still
be differences in the way error recovery works because AT&T
Yacc uses more default reductions than Berkeley Yacc.

     The environment variable TMPDIR determines the directory
where temporary files will be created.  If TMPDIR is defined,
temporary files will be created in the directory whose
pathname is the value of TMPDIR.  By default, temporary files
are created in /tmp.

     The keywords are now case-insensitive.  For example,
%nonassoc, %NONASSOC, %NonAssoc, and %nOnAsSoC are
all equivalent.

     Commas and semicolons that are not part of C code are
treated as commentary.

     Line-end comments, as in BCPL, are permitted.  Line-end
comments begin with // and end at the next end-of-line.
Line-end comments are permitted in C code; they are converted
to C comments on output.

     The form of y.output files has been changed to look more
like those produced by AT&T Yacc.

     A new kind of declaration has been added.
The form of the declaration is

	  %ident string

where string is a sequence of characters beginning with a
double quote and ending with either a double quote or the
next end-of-line, whichever comes first.  The declaration
will cause a #ident directive to be written near the start
of the output file.

     If a parser has been compiled with debugging code, that
code can be enabled by setting an environment variable.
If the environment variable YYDEBUG is set to 0, debugging
output is suppressed.  If it is set to 1, debugging output
is written to standard output.


		Building BtYacc
		---------------
	by Chris Dodd and Vadim Maslov

We used GCC and GNU make to compile BtYacc both on UNIX and
WIN32 paltforms.  You are welcome to try different
combinations of makes and compilers.  Most likely it will
work, but it may require Makefile changes.

There is no config script.
Just type "make" and it should compile.

AWK. If you want to change file btyaccpa.ske (backtracking
parser skeleton), you will need awk to compile it into
skeleton.c file. We used GNU AWK (gawk) version 3.0.

It is known that using older versions of gawk
may create problems in compilation, because older awks
have problems with backslashes at the end of a line.

For MSDOS, there a "makefile.dos" that should do the trick.
Note: makefile.dos was not tested for a long time.

The result of compilation should be a single executable called
"btyacc" which you can install anywhere you like;
it does not require any other files in the distribution to run.


	       Legal Stuff
	       -----------
	by Chris Dodd and Vadim Maslov

In English: BtYacc is freeware. BtYacc is distributed with
no warranty whatsoever. The author and any other contributors
take no responsibility for any and all consequences of its use.

In Legalese: LIMITATION OF LIABILITY. NEITHER SIBER SYSTEMS
NOR ANY OF ITS LICENSORS NOR ANY BTYACC CONTRIBUTOR SHALL BE
LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL OR CONSEQUENTIAL
DAMAGES, OR DAMAGES FOR LOSS OF PROFITS, REVENUE, DATA OR
DATA USE, CAUSED BY BTYACC AND INCURRED BY CUSTOMER OR ANY
THIRD PARTY, WHETHER IN AN ACTION IN CONTRACT OR TORT, EVEN
IF SIBER SYSTEMS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
DAMAGES.
-- Id: README,v 1.2 2004/03/28 17:24:53 tom Exp 

The original README is below.  I've updated this version of Berkeley Yacc
to make it ANSI C compliant - Thomas Dickey

-------------------------------------------------------------------------------
    Berkeley Yacc is an LALR(1) parser generator.  Berkeley Yacc has been made
as compatible as possible with AT&T Yacc.  Berkeley Yacc can accept any input
specification that conforms to the AT&T Yacc documentation.  Specifications
that take advantage of undocumented features of AT&T Yacc will probably be
rejected.

    Berkeley Yacc is distributed with no warranty whatever.  The code is certain
to contain errors.  Neither the author nor any contributor takes responsibility
for any consequences of its use.

    Berkeley Yacc is in the public domain.  The data structures and algorithms
used in Berkeley Yacc are all either taken from documents available to the
general public or are inventions of the author.  Anyone may freely distribute
source or binary forms of Berkeley Yacc whether unchanged or modified.
Distributers may charge whatever fees they can obtain for Berkeley Yacc.
Programs generated by Berkeley Yacc may be distributed freely.

    Please report bugs to

			robert.corbett@eng.Sun.COM

Include a small example if possible.  Please include the banner string from
skeleton.c with the bug report.  Do not expect rapid responses.
-- Id: README,v 1.1 2004/03/28 19:10:48 tom Exp 

The files in this directory are input (.y) and output (.output, .tab.c, .tab.h)
examples.
$NetBSD: README,v 1.3 2017/04/23 02:01:59 christos Exp $

To update tmux to a new version:

- Build the package from pkgsrc and write down all -D flags passed to the
  compiler.  Autoconf is not generating a config.h file, so this is the
  best we can do to get the build-time settings in place.
- Use prepare-import.sh to regenerate the dist directory.
- Update usr.bin/tmux/Makefile to sync the CPPFLAGS to the list of -D flags
  gathered earlier on.
- Update the list of source files in usr.bin/tmux/Makefile with the new
  dist/*.c listing.
- cvs import the contents of the new dist directory.
- Fix merge conflicts, if any.
- Commit the changes to the reachover Makefiles.
- Update doc/3RDPARTY with the new tmux version.
- Add a note to doc/CHANGES about the new version.
Welcome to tmux!

tmux is a "terminal multiplexer", it enables a number of terminals (or windows)
to be accessed and controlled from a single terminal. tmux is intended to be a
simple, modern, BSD-licensed alternative to programs such as GNU screen.

This release runs on OpenBSD, FreeBSD, NetBSD, Linux, OS X and Solaris.

tmux depends on libevent 2.x. Download it from:

	http://libevent.org

It also depends on ncurses, available from:

	http://invisible-island.net/ncurses/

To build and install tmux from a release tarball, use:

	$ ./configure && make
	$ sudo make install

tmux can use the utempter library to update utmp(5), if it is installed - run
configure with --enable-utempter to enable this.

To get and build the latest from version control:

	$ git clone https://github.com/tmux/tmux.git
	$ cd tmux
	$ sh autogen.sh
	$ ./configure && make

(Note that this requires at least a working C compiler, make, autoconf,
automake, pkg-config as well as libevent and ncurses libraries and headers.)

For more information see http://git-scm.com. Patches should be sent by email to
the mailing list at tmux-users@googlegroups.com or submitted through GitHub at
https://github.com/tmux/tmux/issues.

For documentation on using tmux, see the tmux.1 manpage. It can be viewed from
the source tree with:

	$ nroff -mdoc tmux.1|less

A small example configuration in example_tmux.conf.

A vim(1) syntax file is available at:

	https://github.com/ericpruitt/tmux.vim
	https://raw.githubusercontent.com/ericpruitt/tmux.vim/master/vim/syntax/tmux.vim

And a bash(1) completion file at:

	https://github.com/imomaliev/tmux-bash-completion

For debugging, running tmux with -v or -vv will generate server and client log
files in the current directory.

tmux mailing lists are available. For general discussion and bug reports:

	https://groups.google.com/forum/#!forum/tmux-users

And for Git commit emails:

	https://groups.google.com/forum/#!forum/tmux-git

Subscribe by sending an email to <tmux-users+subscribe@googlegroups.com>.

Bug reports, feature suggestions and especially code contributions are most
welcome. Please send by email to:

	tmux-users@googlegroups.com

This file and the CHANGES, FAQ, SYNCING and TODO files are licensed under the
ISC license. All other files have a license and copyright notice at their start.

-- Nicholas Marriott <nicholas.marriott@gmail.com>
libcxxabi
=========

This library implements the Code Sourcery C++ ABI, as documented here:

http://www.codesourcery.com/public/cxx-abi/abi.html

It is intended to sit below an STL implementation, and provide features required by the compiler for implementation of the C++ language.

Current Status
--------------

At present, the library implements the following parts of the ABI specification:

- RTTI classes and support for the dynamic_cast<> operator.
- Exception handling.
- Thread-safe initializers.

Exception handling requires the assistance of a stack-unwinding library
implementing the low-level parts of the ABI.  Either libgcc_s or libunwind
should work for this purpose.

The library depends on various libc features, but does not depend on any C++
features not implemented purely in the compiler or in the library itself.

Supported Platforms
-------------------

This code was initially developed on FreeBSD/x86, and has also been tested on FreeBSD/x86-64.  It should work on other platforms that use the Code Sourcery ABI, for example Itanium, however this is untested.

This library also supports the ARM EH ABI.

Installation
------------

The default build system does not perform any installation.  It is expected that this will be done by at a higher level.  The exact installation steps depend on how you plan on deploying libcxxrt.

There are three files that you may consider installing:

- cxxabi.h (and unwind.h and either unwind-arm.h or unwind-itanium.h)
- libcxxrt.a
- libcxxrt.so

The first describes the contract between this library and the compiler / STL implementation (lib[std]{cxx,c++}).  Its contents should be considered semi-private, as it is probably not a good idea to encourage any code above the STL implementation to depend on it.  Doing so will introduce portability issues.  You may install this file but I recommend simply copying or linking it into your STL implementation's build directory.

In general, I recommend against installing both the .a and the .so file.  For static linking, the .a file should be linked against the static and dynamic versions of your STL implementation.  Statically linking libcxxrt into your STL implementation means that users who dynamically link against the STL implementation can have libcxxrt upgraded automatically when you ship a new version of your STL implementation.

The other option, installing the .so, is recommended for situations where you have two or more STL implementations and wish to be able to link against both (e.g. where an application links one library using libstdc++ and another using libc++).  To support this case, you should link both STL implementations against libcxxrt.so.  

Supporting all of these options in the CMake build system is not practical - the amount of effort required to select the one that you want would be more than the effort required to perform the installation from an external script or build system.
Introductory information                        Automated Testing Framework
===========================================================================


Introduction
************

The Automated Testing Framework (ATF) is a collection of libraries to
implement test programs in a variety of languages.  At the moment, ATF
offers C, C++ and POSIX shell bindings with which to implement tests.
These bindings all offer a similar set of functionality and any test
program written with them exposes a consistent user interface.

ATF-based test programs rely on a separate runtime engine to execute them.
The runtime engine is in charge of isolating the test programs from the
rest of the system to ensure that their results are deterministic and that
they cannot affect the running system.  The runtime engine is also
responsible for gathering the results of all tests and composing reports.
The current runtime of choice is Kyua.


Other documents
***************

* AUTHORS: List of authors and contributors for this project.

* COPYING: License information.

* INSTALL: Compilation and installation instructions.  These is not the
  standard document shipped with many packages, so be sure to read it for
  things that are specific to ATF's build.

* NEWS: List of major changes between formal, published releases.


===========================================================================
vim: filetype=text:textwidth=75:expandtab:shiftwidth=2:softtabstop=2
This is NSD contributions directory and it contains various additions
to NSD that are not a part of the official distribution but may be
helpful.

USE AT YOUR OWN RISK.

* nsd.spec: a rpm specfile to generate binary and source rpms. 
  Put the source tarball in  /usr/src/redhat/SOURCES. Then 
  	rpmbuild -ba nsd.spec

* nsd.init: a shell script that can start, stop, restart the NSD daemon.
  It uses signals, and can be used in rc.d init scripts (depends on platform).

* nsd.zones2nsd.conf: a python script to convert NSD 2 nsd.zones config files
  to NSD 3 nsd.conf config files. Do not forget to set nsd_zones_name and
  key_dir variables at the top of the script. 

* bind2nsd: a slightly abridged form is included; find the full source
  at http://bind2nsd.sourceforge.net.  The bind2nsd scripts translate 
  DNS information in BIND format to NSD format, and then copy that 
  translation to an NSD server. The goal is to make it simple to run
  redundant BIND and NSD servers and keep them in sync, using only the
  BIND configuration files

* nsd_munin_ : plugin for munin statistics report
  You must have given --enable-bind8-stats (default is on) to configure.
  Copy the file to /usr/share/munin/plugins (or you munin node dir).
  You may also need to create a number of symbolic links under the names
  of the graphs you want to create (documented at head of file).
bind2nsd -- translate from BIND named.conf to NSD nsd.conf
         -- then, sync up configuration files with a Secure64 server


To install these tools on Linux systems:

   # ./install.sh

Then, copy bind2nsd.conf to the directory you will run these commands
from (e.g., /etc/named or /etc/bind9) and then edit the values properly.

Passwords for logging into a Secure64 system can be stored in the
bind2nsd.conf file (as clear text only) or can be stored encrypted
by using the 'password_file' config item and the s64-mkpw utility.


--
Copyright (c) 2007, Secure64 Software Corporation.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

1.0 Introduction
1.1 ... Basic theory of operation
1.2 ... Quick build & install
2.0 Building nsd
2.1 ... Unpacking the source
2.2 ... Configuring NSD
2.3 ... Building
2.4 ... Installing
3.0 Running NSD
3.1 ... Logging
3.2 ... AXFR access
3.3 ... Using TSIG
3.4 ... Zone expiry of secondary zones
3.5 ... Diagnosing NSD log entries
3.6 ... Interfaces
4.0 Support and Feedback
4.1 ... Your Support


1.0 Introduction

This is NSD Name Server Daemon (NSD) version 4.1.14.

The NLnet Labs Name Server Daemon (NSD) is an authoritative RFC compliant 
DNS nameserver. It was first conceived to allow for more genetic 
diversity for DNS server implementations used by the root-server system 
and it has been developed for operations in environments where speed, 
reliability, stability, and security are of high importance. NSD is 
currently used on root servers such as k.root-servers.net and is also in 
use by several top-level domain registries.

NSD is a complete implementation of an authoritative DNS name server.
For further information about what NSD is and what NSD is not please
consult the REQUIREMENTS document which is a part of this distribution.

If you are a BIND user (the named daemon) consult NSD_FOR_BIND_USERS.

The source code is available for download from:

         http://www.nlnetlabs.nl/downloads


1.1 Basic Theory of Operation

NSD consists of two programs: the zone compiler 'zonec' and the name
server 'nsd' itself. The name server works with an intermediate
database prepared by the zone compiler from standard zone files.

For NSD operation this means that zones have to be compiled by zonec
before NSD can use them.

All this can be controlled via rc.d (SIGTERM, SIGHUP) or nsd-control,
and uses a simple configuration file 'nsd.conf'.


1.2 Quick build and install

Step 1: Unpack the source with gtar -xzvf nsd-4.1.14.tar.gz

Step 2: Create user nsd or any other unprivileged user of your
        choice. In case of later make sure to use
        --with-user=<username> while running configure.
	You can also set "username: <name>" in the nsd.conf file later.
	Install openssl and libevent.

Step 3: ./configure

Step 4: make all	(or simply 'make').

Step 5: make install

Step 6: Create and edit /etc/nsd/nsd.conf file possibly from
        nsd.conf.sample template that comes with the distribution.
	(installed by default at /etc/nsd/nsd.conf.sample)
	Here you need to configure the zones you want to serve.
	TSIG keys used for secure zone transfers must be included.
	Also server parameters can be set, see nsd.conf(5) man page.

	If you have a NSD 2 nsd.zones config file take a look at the
	python script contrib/nsd.zones2nsd.conf, it will convert 
	zone and TSIG key settings for you.

Step 7: Copy necessary master zone files into appropriate directories
        under /etc/nsd/primary & /etc/nsd/secondary. 

Step 8: Run nsd-control start

Step 9: Test the NSD with dig, drill or host.

Step 10: If you're happy add a rc.d script to start into your OS boot up
	 sequence. The format of the rc.d startup script depends on 
	 the platform.  Also stop it in the shutdown sequence.
	 You can use SIGTERM to stop, or nsd-control stop.

Step 11: If desired add 'nsd-control write' to your superuser crontab to
         update the zone files with the content transferred from master
	 servers periodically, such as once per day.

         Got any problems or questions with the steps above? Read the
         rest of this file.



2.0 Building NSD


2.1 Unpacking the source

Use your favorite combination of tar and gnu zip to unpack the source,
for example

$ gtar -xzvf nsd-4.1.14.tar.gz

will unpack the source into the ./nsd-4.1.14 directory...


2.2 Configuring NSD

NSD can be configured using GNU autoconf's configure script. In
addition to standard configure options, one may use the following:

  CC=compiler
 
        Specify the C compiler.  The default is gcc or cc.  The
        compiler must support ANSI C89.

  CPPFLAGS=flags

        Specify the C preprocessor flags.  Such as -I<includedir>.

  CFLAGS=flags

        Specify the C compiler flags.  These include code generation,
        optimization, warning, and debugging flags.  These flags are
        also passed to the linker.

        The default for gcc is "-g -O2".

  LD=linker

        Specify the linker (defaults to the C compiler).

  LDFLAGS=flags

        Specify linker flags.

  LIBS=libs
 
        Specify additional libraries to link with.

  --enable-root-server

        Configure NSD as a root server. Unless this option is
        specified, NSD will refuse to serve the ``.'' zone as a
        misconfiguration safeguard.

  --disable-ipv6

        Disables IPv6 support in NSD.

  --enable-checking

        Enable some internal development checks.  Useful if you want
        to modify NSD.  This option enables the standard C "assert" macro
	and compiler warnings.

	This will instruct NSD to be stricter when validating its input. 
	This could lead to a reduced service level.

  --enable-bind8-stats

        Enables BIND8-like statistics.

  --enable-ratelimit

	Enables ratelimiting, based on query name, type and source.

   --enable-draft-rrtypes

	Enables draft RRtypes.

  --with-configdir=dir

        Specified, NSD configuration directory, default /etc/nsd

  --with-nsd_conf_file=path

	Pathname to the NSD configuration file, default /etc/nsd/nsd.conf

  --with-pidfile=path

        Pathname to the NSD pidfile, default is platform specific,
        mostly /var/run/nsd.pid

  --with-dbfile=path

        Pathname to the NSD database, default is /etc/nsd/nsd.db

  --with-zonesdir=dir

        NSD default location for master zone files, default /etc/nsd/

  --with-user=username

        User name or ID to answer the queries with, default is nsd

  --with-facility=facility

        Specify the syslog facility to use.  The default is
        LOG_DAEMON.  See the syslog(3) manual page for the available
        facilities.

  --with-libevent=path

  	Specity the location of the libevent library (or libev).
	--with-libevent=no uses a builtin portable implementation (select()).

  --with-ssl=path

        Specify the location of the OpenSSL libraries.  OpenSSL 0.9.7
        or higher is required for TSIG support.

  --with-start_priority=number

	Startup priority for NSD. 

  --with-kill_priority=number

	Shutdown priority for NSD. 

  --with-tcp-timeout=number

	Set the default TCP timeout (in seconds). Default 120 seconds.

  --disable-nsec3

  	Disable NSEC3 support. With NSEC3 support enabled, very large zones,
	also non-nsec3 zones, use about 20% more memory.

  --disable-minimal-responses

  	Disable minimal responses. If disabled, responses are more likely
	to get truncated, resulting in TCP fallback.  When enabled (by default)
	NSD will leave out RRsets to make responses fit inside one datagram,
	but for shorter responses the full normal response is carried.

  --disable-largefile

	Disable large file support (64 bit file lengths). Makes off_t
	a 32bit length during compilation.


2.3 Building

Use ``make'' to create NSD and support tools.  If you get errors, try to
use ``gmake'' (gnu version of make), especially on old systems. If so,
do a `gmake realclean` first, to remove stuff that the make call messed up.


2.4 Installing

Become a superuser (if necessary) and type ``make install''

This step should install four binaries

nsd               - the daemon itself
nsd-control-setup - a shell script that creates keys for nsd-control.
nsd-control	      - program that connects over SSL to nsd and gives commands.
nsd-checkconf	  - simple C program to check nsd.conf before use.

Plus the manual pages and a sample configuration file.


3.0 Running NSD

Before running NSD you need to create a configuration file for it.
The config file contains server settings, secret keys and zone settings.

The server settings start with a line with the keyword 'server:'.
In the server settings set 'database: <file>' with the filename of the name 
database that NSD will use. Set 'chroot: <dir>' to run nsd in a chroot-jail.
Make sure the zone files, database file, xfrdfile, difffile and pidfile
can be accessed from the chroot-jail.  Set 'username: <user>' to an 
unprivileged user, for security.  

For example:
	# This is a sample configuration
	server:
		database: "/etc/nsd/nsd.db"
		pidfile: "/etc/nsd/nsd.pid"
		chroot: "/etc/nsd/"
		username: nsd

After the global server settings to need to make entries for the
zones that you wish to serve. For each zone you need to list the zone
name, the file name with the zone contents, and access control lists.

	zone:
		name:	"example.com"
		zonefile: "example.com.zone"

The zonefile needs to be filled with the correct zone information
for master zones. For secondary zones an empty file will suffice,
a zone transfer will be initiated to obtain the slave zone contents.

Access control lists are needed for zone transfer and notifications.

For a slave zone list the masters, by IP address. Below is an example
of a slave zone with two master servers. If a master only supports AXFR
transfers and not IXFR transfers (like NSD), specify the master as
"request-xfr: AXFR <ip_address> <key>". By default, all zone transfer requests 
are made over TCP. If you want the IXFR request be transmitted over UDP, use
"request-xfr: UDP <ip address> <key>".

	zone:
		name: "example.com"
		zonefile: "example.com.zone"
		allow-notify: 168.192.185.33 NOKEY
		request-xfr: 168.192.185.33 NOKEY
		allow-notify: 168.192.199.2 NOKEY
		request-xfr: 168.192.199.2 NOKEY

By default, a slave will fallback to AXFR requests if the master told us it does 
not support IXFR. You can configure the slave not to do AXFR fallback with:

		allow-axfr-fallback: "no"

For a master zone, list the slave servers, by IP address or subnet.
Below is an example of a master zone with two slave servers.

	zone:
		name: "example.com"
		zonefile: "example.com.zone"
		notify: 168.192.133.75 NOKEY
		provide-xfr: 168.192.133.75 NOKEY
		notify: 168.192.5.44 NOKEY
		provide-xfr: 168.192.5.44 NOKEY

You also can set the outgoing interface for notifies and zone transfer requests 
to satisfy access control lists at the other end:

		outgoing-interface: 168.192.5.69

By default, NSD will retry a notify up to 5 times. You can override that
value with: 

		notify-retry: 5

Zone transfers can be secured with TSIG keys, replace NOKEY with
the name of the tsig key to use. See section 3.3.

Since NSD is written to be run on the root name servers, the config file 
can to contain something like:

	zone:
		name: "."
		zonefile: "root.zone"
		provide-xfr: 0.0.0.0/0 NOKEY # allow axfr for everyone.
		provide-xfr: ::0/0 NOKEY

You should only do that if you're intending to run a root server, NSD
is not suited for running a . cache. Therefore if you choose to serve
the .  zone you have to make sure that the complete root zone is
timely and fully updated.

To prevent misconfiguration, NSD configure has the --enable-root-server
switch, that is by default disabled.

In the config file, you can use patterns.  A pattern can have the
same configuration statements that a zone can have.  And then you can
include-pattern: <name-of-pattern> in a zone (or in another pattern)
to apply those settings.  This can be used to organise the settings.

The nsd-control tool is also controlled from the nsd.conf config file.
It uses SSL encrypted transport to 127.0.0.1, and if you want to use it
you have to setup the keys and also edit the config file.  You can leave
the remote-control disabled (the secure default), or opt to turn it on:

	# generate keys
	nsd-control-setup

	# edit nsd.conf to add this
	remote-control:
		control-enable: yes

By default nsd-control is limited to localhost, as well as encrypted, but
some people may want to remotely administer their nameserver.  What you
then do is setup nsd-control to listen to the public IP address, with
control-interface: <IP> after the control-enable statement.  Furthermore,
you copy the key files /etc/nsd/nsd_server.pem /etc/nsd/nsd_control.*
to a remote host on the internet; on that host you can run nsd-control
with -c <special config file> which references same IP address
control-interface and references the copies of the key files with
server-cert-file, control-key-file and control-cert-file config lines
after the control-enable statement.  The nsd-server authenticates the
nsd-control client, and also the nsd-control client authenticates the
nsd-server.

When you are done with the configuration file, check the syntax using

	nsd-checkconf <name of configfile>

The zone files are read by the daemon, which builds 'nsd.db' with their
contents.  You can start the daemon with

	nsd
	or with "nsd-control start" (which execs nsd again).
	or with nsd -c <name of configfile>

To check if the daemon is running look with ps, top, or if you enabled
nsd-control,

	nsd-control status

To reload changed zone files after you edited them, without stopping
the daemon, use this to check if files are modified: 

	kill -HUP `cat <name of nsd pidfile>`

If you enabled nsd-control, you can reread with

	nsd-control reload

With nsd-control you can also reread the config file (new zones, ..)

	nsd-control reconfig

To restart the daemon

	/etc/rc.d/nsd restart  # or your system(d) equivalent

To shut it down (for example on the system shutdown) do

	kill -TERM <pid of nsd>
	or nsd-control stop

NSD will automatically keep track of secondary zones and update them
when needed. When primary zones are updated and reloaded notifications
are sent to slave servers.

The zone transfers are applied to nsd.db by the daemon.  To write changed
contents of the zone files for slave zones to disk in the text-based zone
file format, issue nsd-control write.

NSD will send notifications to slave zones if a master zone is updated.
NSD will check for updates at master servers periodically and transfer
the updated zone by AXFR/IXFR and reload the new zone contents. If
you wish exert manual control use nsd-control notify, transfer and
force_transfer commands.  The transfer command will check for new versions
of the secondary zones hosted by this NSD. The notify command will send
notifications to the slave servers configured in 'notify:' statements.


3.1 Logging

NSD doesn't do any logging. We believe that logging is a separate task
and has to be done independently from the core operation.

This consciously is not part of nsd itself in order to keep nsd
focused and minimize its complexity. It is better to leave logging and
tracing to separate dedicated tools. dnsstat can also easily be
configured and/or modified to suit local statistics requirements
without any danger of affecting the name server itself. We have run
dnsstat on the same machine as nsd, we would recommend using a
multiprocessor if performance is an issue. Of course it can also run
on a separate machine that has MAC layer access to the network of the
server.

The nsd-control tool can output some statistics, with nsd-control stats
and nsd-control stats_noreset.  In contrib/nsd_munin_ there is a munin
grapher plugin that uses it.  The output of nsd-control stats is easy
to read (text only) with scripts.  The output values are documented on
the nsd-control man page.

The CAIDA dnsstat tool referenced is recommended to nsd operators as a 
means of keeping statistics and check on abnormal query loads.

    http://www.caida.org/tools/utilities/dnsstat/dnsstat-3.5.1a.tar.gz

Another tool is the dnstop, that displays DNS statistics on your network.

    http://dns.measurement-factory.com/tools/dnstop/src/dnstop-20060517.tar.gz

A sample invocation of dnsstat:

/usr/local/Coral/bin/crl_dnsstat -D -Ci=60 -Cd=240 -C'filter dst 10.1.1.3'  -h -u if:fxp1

A sample output of a slightly modified version:

# dnsstat output version: 0.2 "dfk"

# begin trace interval at 1025267664.859043, duration 15.000000
# DNS messages: 74973 (4998.200000/s); DNS queries: 151983 (10132.200000/s)
# print threshold: 30 messages/sec

#src              op type  class queries    msgs      rd notes
 208.18.162.10     - -     -         533     533       0
 "                 0 MX    IN          6
 "                 0 A     IN        264
 "                 0 ANY   IN        263
 209.11.18.248     - -     -         661     661       0
 "                 0 A     IN        655
 "                 0 MX    IN          6
 210.117.65.137    - -     -         745     745       0
 "                 0 A     IN        745
 216.54.221.131    - -     -         477     477       0
 "                 0 A     IN        477
 193.97.205.80     - -     -         681     681       0
 "                 0 A     IN          3
 "                 0 ANY   IN        678
 168.30.240.11     - -     -         685     685       0
 "                 0 A     IN        405
 "                 0 MX    IN        280
 210.94.6.67       - -     -         742     742       0
 "                 0 A     IN        742
 63.66.68.237      - -     -        1375    1375       0
 "                 0 A     IN       1375
 168.30.240.12     - -     -         493     493       0
 "                 0 A     IN        493
 139.142.205.225   - -     -        5579    5579       0
 "                 0 A     IN       3006
 "                 0 MX    IN       2573
 210.117.65.2      - -     -         700     700       0
 "                 0 A     IN        700
# end trace interval 


3.2 AXFR access

The access list for AXFR should be set with provide-xfr:
in the nsd config file. This is per zone. See nsd.conf(5).
For example to grant zone 'example.com' AXFR right to localhost for
IPv4 and IPv6, use the below config options.

zone:
	name: "example.com"
	provide-xfr: 127.0.0.1 NOKEY
	provide-xfr: ::1 NOKEY

You can use dig @localhost example.com axfr to test this.


3.3 Using TSIG

NSD supports TSIG for any query to the server, for zone transfer
and for notify sending and receiving.

TSIG keys are based on shared secrets. These must be configured
in the config file. To keep the secret in a separate file use 
include: "filename" to include that file.

An example tsig key named sec1_key.

	key:
		name: "sec1_key"
		algorithm: hmac-md5
		secret: "6KM6qiKfwfEpamEq72HQdA=="

This key can then be used for any query to the NSD server. NSD
will check if the signature is valid, and if so, return a signed
answer. Unsigned queries will be given unsigned replies.

The key can be used to restrict the access control lists, for
example to only allow zone transfer with the key, by listing
the key name on the access control line.

	# provides AXFR to the subnet when TSIG is used.
	provide-xfr: 10.11.12.0/24 sec1_key
	# allow only notifications that are signed
	allow-notify: 192.168.0.0/16 sec1_key

If the TSIG key name is used in notify or request-xfr lines,
the key is used to sign the request/notification messages.


3.4 Zone expiry of secondary zones

NSD will keep track of the status of secondary zones, according to the 
timing values in the SOA record for the zone.  When the refresh time of a
zone is reached, the serial number is checked and a zone transfer is
started if the zone has changed.  Each master server is tried in turn.

Master zones cannot expire.  They are always served.  Zones are master
zones if they have no 'request-xfr:' statements in the config file.

After the expire timeout (from the SOA record at the zone apex) is reached,
the zone becomes expired. NSD will return SERVFAIL for expired zones,
and will attempt to perform a zone transfer from any of the masters.
After a zone transfer succeeds, or if the master indicates that the SOA 
serial number is still the same, the zone will be OK again.

In contrast with e.g. BIND, the inception time for a slave zone is stored
on disk (in the xfrdfile: "xfrd.state"), together with timeouts.  If a
slave zone acquisition time is recent enough, this means that NSD can start
serving a zone immediately on loading, without querying the master server.

If your slave zone has expired, and no masters can be reached, but you 
still want NSD to serve the zone.  (i.e. ''My network is in shambles, but
serve the zone dangit!'').  You can delete the file 'xfrd.state',
but leave the zonefile for the zone intact.  Make sure to stop nsd before
you delete the file, as NSD writes it on exit.  Upon loading NSD will treat
the zonefile that you as operator have provided as recent and will serve
the zone.  Even though NSD will start to serve the zone immediately,
the zone will expire after the timeout is reached again.  NSD will also
attempt to confirm that you have provided the correct data by polling 
the masters.  So when the master servers come back up, it will transfer
the updated zone within <retry timeout from SOA> seconds.

In general it is possible to provide zone files for both master and
slave zones manually (say from email or rsync). Reload with SIGHUP
or nsd-control reload to read the new zonefile contents into the name
database.  When this is done the new zone will be served. For master
zones, NSD will issue notifications to all configured 'notify:' targets.
For slave zones the above happens; NSD attempts to validate the zone
from the master (checking its SOA serial number).


3.5 Diagnosing NSD log entries

NSD will print log messages to the system log (or 'logfile:' configuration
entry). Some of these messages are discussed below. These messages can
get extra support if errors happen.

- "Reload process <pid> failed with status <s>, continuing with old database"

This log message indicates the reload process of NSD has failed for
some reason.  The reason can be anything from a missing database file
to internal errors.  If this happens often, please let us know, this
error message can be caught in the code, and appropriate action could
be taken.  We are as of yet not sure what action is appropriate, if any.

- "snipping off trailing partial part of <ixfr.db>"

Please let us know if, and how often, this happens.

What happens is the file ixfr.db contains only part of expected data.
The corruption is removed by snipping off the trailing part.

- "memory recyclebin holds <num> bytes"

This is printed for every reload. NSD allocates and deallocates memory
to service IXFR updates. The recyclebin holds deallocated memory ready
for future use. If the number grows too large, a restart resets it.

- "xfrd: max number of tcp connections (32) reached."

This line is printed when more than 32 zones need a zone transfer at the
same time.  The value is a compile constant (xfrd-tcp.h), but if this
happens often for you, we could make this a config option.  NSD will reuse
existing TCP connections to the same master (determined by IP address)
to transfer up to 64k zones from that master.  Thus this error should
only happen with more than 32 masters or more than 64*32=2M zones that
need to be updated at the same time.

If this happens, more zones have to wait until a zone transfer completes
(or is aborted) before they can have a zone transfer too. This waiting
list has no size limit.

- "error: <zone> NSEC3PARAM entry <num> has unknown hash algo <number>"

This error means that the zone has NSEC3 chain(s) with hash algorithms
that are not supported by this version of NSD, and thus cannot be served
by NSD.  If there are also no NSECs or NSEC3 chain(s) with known hash
algorithms, NSD will not be able to serve DNSSEC authenticated denials
for the zone.


3.6 Interfaces

NSD will by default bind itself to the system default interface and
service ip4 and if available also ip6. It is possible to service only ip4
or ip6 using the -4, -6 commandline options, or the ip4-only and ip6-only
config file options.

The commandline option -a and config file option ip-address can be given
to bind to specific interfaces.  Multiple interfaces can be specified.
This is useful for two reasons:
	o The specific interface bound will result in the OS bypassing
	  routing tables for the interface selection.  This results in
	  a small performance gain.  It is not the performance gain that
	  is the problem, sometimes the routing tables can give the
	  wrong answer, see the next point.
	o The answer will be routed via the interface the query came from.
	  This makes sure that the return address on the DNS replies is the
	  same as the query was sent to.  Many resolvers require the source
	  address of the replies to be correct.  The ip-address: option is
	  easier than configuring the OS routing table to return the DNS
	  replies via the correct interface.
The above means that even for systems with multiple interfaces where you
intend to provide DNS service to all interfaces, it is prudent to specify
all the interfaces as ip-address config file options.

With the config file option ip-transparent you can allow NSD to bind to
non local addresses.

4.0 Support and Feedback

NLnet Labs is committed to support NSD and its other software products on 
a best effort basis, free of charge. This form of community support is 
offered through a mailing lists and the 'bugzilla' web interface. 

	http://www.nlnetlabs.nl/bugs/

If for any reason NLnet Labs would stop community support of NSD such 
would be announced on our web pages at least two years in advance.

The community mailing list nsd-users@nlnetlabs.nl can be used to discuss 
issues with other users of NSD. Subscribe here

	http://open.nlnetlabs.nl/mailman/listinfo/nsd-users	

NLnet Labs recognizes that in some corporate environments this commitment to
community support is not sufficient and that support needs to be codified. 
We therefore offer paid support contracts that come in 3 varieties. 

More information about these support varieties can be found at 
	<url on support varieties on www.nlnetlabs.nl>

Alternatively you can contact mailto:nsd-support@nlnetlabs.nl .

Support goes two ways.  By acquiring one of the support contracts you
also support NLnet Labs to continue to participate in the development
of the Internet architecture. We do this through our participation in
the (IETF) standards process and by developing and maintaining
reference implementations of standards and tools to support operation
and deployment of new and existing Internet technology. 

We are interested in our users and in the environment you use NSD. Please 
drop us a mail when you use NSD. Indicate in what kind of operation you 
deploy NSD and let us know what your positive and negative experiences are.
http://www.nlnetlabs.nl/nsd  and  mailto:nsd-info@nlnetlabs.nl


4.1 Your Support

NLnet Labs offers all of its software products as open source, most are
published under a BSD license. You can download them, not only from the
NLnet Labs website but also through the various OS distributions for
which NSD, ldns, and Unbound are packaged. We therefore have little idea
who uses our software in production environments and have no direct ties
with 'our customers'.

Therefore, we ask you to contact us at users@NLnetLabs.nl and tell us
whether you use one of our products in your production environment,
what that environment looks like, and maybe even share some praise.
We would like to refer to the fact that your organization is using our
products. We will only do that if you explicitly allow us. In all other
cases we will keep the information you share with us to ourselves.

In addition to the moral support you can also support us
financially. NLnet Labs is a recognized not-for-profit charity foundation
that is chartered to develop open-source software and open-standards
for the Internet. If you use our software to satisfaction please express
that by giving us a donation. For small donations PayPal can be used. For
larger and regular donations please contact us at users@NLnetLabs.nl. Also
see http://www.nlnetlabs.nl/labs/contributors/.


Id: README 4690 2016-08-22 10:38:14Z wouter 
If you build NSD directly from the Subversion repository, you will need the
(gnu) autotools to generate the configure script.

On most systems, this can simply be done by using autoreconf:

autoreconf

This will call autoconf, autoheader, aclocal etc. After this you can build
normally with configure and make, see the general README for further
information on this.

Some systems do not have a symlink to separate versions, so you will have to
use the specific version name. It should work with at least 2.53 and 2.59.

The actual executable name may differ on various systems, most of the times
it is either called autoreconf-2.59 or autoreconf259


Some systems also do not have a standard aclocal link, in which case you
will have to tell autoreconf what aclocal executable to use. This can be
done by setting the ACLOCAL environment variable. It should work with
aclocal 1.4, 1.5 and 1.9.

Examples of complete commands:

ACLOCAL=aclocal19 autoreconf259

ACLOCAL=aclocal-1.9 autoreconf-2.59

Compiling with the Intel C Compiler (ICC) version 7.0.

Configure NSD to use ICC.

$ CC=icc ./configure [configure-options]

Then everything should compile (just ignore the warnings), except on
RedHat 8.0.  This is due to a bug in the GNU C library used.  This bug
may apply to other Linux distributions or systems using the GNU C
library.

To compile NSD with the ICC compiler on RedHat 8.0 you need to patch
the file /usr/include/bits/byteswap.h.  The following patch should
work:

--- byteswap.h.orig	2003-02-26 13:59:41.000000000 +0100
+++ byteswap.h	2003-02-26 13:59:57.000000000 +0100
@@ -81,7 +81,7 @@
 	 __v; }))
 # endif
 #else
-# define __bswap_16(x) \
+# define __bswap_32(x) \
      (__extension__							      \
       ({ register unsigned int __x = (x); __bswap_constant_32 (__x); }))
 #endif
This is a list of the #define REFCLK_* stuff.

If you want to add a new refclock let us know and we'll assign you a number.

Should this list also include the name of the party responsible for the
refclock?

LOCALCLOCK	1	/* external (e.g., lockclock) */
GPS_TRAK	2	/* TRAK 8810 GPS Receiver */
WWV_PST		3	/* PST/Traconex 1020 WWV/H */
SPECTRACOM	4	/* Spectracom (generic) Receivers */
TRUETIME	5	/* TrueTime (generic) Receivers */
IRIG_AUDIO	6	/* IRIG-B/W audio decoder */
CHU_AUDIO	7	/* CHU audio demodulator/decoder */
PARSE		8	/* generic driver (usually DCF77,GPS,MSF) */
GPS_MX4200	9	/* Magnavox MX4200 GPS */
GPS_AS2201	10	/* Austron 2201A GPS */
GPS_ARBITER	11	/* Arbiter 1088A/B/ GPS */
IRIG_TPRO	12	/* KSI/Odetics TPRO-S IRIG */
ATOM_LEITCH	13	/* Leitch CSD 5300 Master Clock */
MSF_EES		14	/* EES M201 MSF Receiver */
GPSTM_TRUE	15	/* OLD TrueTime GPS/TM-TMD Receiver */
IRIG_BANCOMM	16	/* Bancomm GPS/IRIG Interface */
GPS_DATUM	17	/* Datum Programmable Time System */
NIST_ACTS	18	/* NIST Auto Computer Time Service */
WWV_HEATH	19	/* Heath GC1000 WWV/WWVH Receiver */
GPS_NMEA	20	/* NMEA based GPS clock */
GPS_VME		21	/* TrueTime GPS-VME Interface */
ATOM_PPS	22	/* 1-PPS Clock Discipline */
PTB_ACTS	NIST_ACTS
USNO		NIST_ACTS
GPS_HP		26	/* HP 58503A Time/Frequency Receiver */
ARCRON_MSF	27	/* ARCRON MSF radio clock. */
SHM		28	/* clock attached thru shared memory */
PALISADE	29	/* Trimble Navigation Palisade GPS */
ONCORE		30	/* Motorola UT Oncore GPS */
GPS_JUPITER	31	/* Rockwell Jupiter GPS receiver */
CHRONOLOG	32	/* Chrono-log K WWVB receiver */
DUMBCLOCK	33	/* Dumb localtime clock */
ULINK		34	/* Ultralink M320 WWVB receiver */
PCF		35	/* Conrad parallel port radio clock */
WWV_AUDIO	36	/* WWV/H audio demodulator/decoder */
FG		37	/* Forum Graphic GPS */
HOPF_SERIAL	38	/* hopf DCF77/GPS serial line receiver */
HOPF_PCI	39	/* hopf DCF77/GPS PCI receiver */
JJY		40	/* JJY receiver */
TT560		41	/* TrueTime 560 IRIG-B decoder */
ZYFER		42	/* Zyfer GPStarplus receiver */
RIPENCC		43	/* RIPE NCC Trimble driver */
???????		44	Claas Hilbrecht (20020711)

NTP uses A.B.C - style release numbers.

At the moment:

 A is 4, for ntp V4.
 B is the major release number.
 C is the minor release number.  Even numbers are 'stable' releases and
 odd numbers are "development" releases.

Following the release number may be the letter 'p' followed by a number.
This indicates a point (or patch) release.

Release candidates have -RC in the release number.

Here are some recent versions numbers as an example:

 4.2.2		A production release (from the ntp-stable repository)
 4.2.2p2	A production release (from the ntp-stable repository)
 4.2.3p12	A development release
 4.2.3p15-rc1	A release candidate for 4.2.4

Note that after the ntp-dev repo produces a production release it will
be copied into the ntp-stable and the cycle will repeat.

Feel free to suggest improvements...

Notes to hackers.

See README.patches for information about submitting patches.

---

Dave likes this code indented formatted in a consistent way.
The file "dot.emacs" has the emacs C-mode indentation style that Dave likes.

---

We'd like to see *all* system function declarations live in include/l_stdlib.h
and NEVER appear in the .c files.

---
In order to use the BitKeeper repository version of NTP you should visit

 http://support.ntp.org/Main/SoftwareDevelopment

for important information.

If you want to submit patches, please see the README.hackers file.
Leap Second Smearing with NTP
-----------------------------

By Martin Burnicki
with some edits by Harlan Stenn

The NTP software protocol and its reference implementation, ntpd, were
originally designed to distribute UTC time over a network as accurately as
possible.

Unfortunately, leap seconds are scheduled to be inserted into or deleted
from the UTC time scale in irregular intervals to keep the UTC time scale
synchronized with the Earth rotation.  Deletions haven't happened, yet, but
insertions have happened over 30 times.

The problem is that POSIX requires 86400 seconds in a day, and there is no
prescribed way to handle leap seconds in POSIX.

Whenever a leap second is to be handled ntpd either:

- passes the leap second announcement down to the OS kernel (if the OS
supports this) and the kernel handles the leap second automatically, or

- applies the leap second correction itself.

NTP servers also pass a leap second warning flag down to their clients via
the normal NTP packet exchange, so clients also become aware of an
approaching leap second, and can handle the leap second appropriately.


The Problem on Unix-like Systems
--------------------------------
If a leap second is to be inserted then in most Unix-like systems the OS
kernel just steps the time back by 1 second at the beginning of the leap
second, so the last second of the UTC day is repeated and thus duplicate
timestamps can occur.

Unfortunately there are lots of applications which get confused it the
system time is stepped back, e.g. due to a leap second insertion.  Thus,
many users have been looking for ways to avoid this, and tried to introduce
workarounds which may work properly, or not.

So even though these Unix kernels normally can handle leap seconds, the way
they do this is not optimal for applications.

One good way to handle the leap second is to use ntp_gettime() instead of
the usual calls, because ntp_gettime() includes a "clock state" variable
that will actually tell you if the time you are receiving is OK or not, and
if it is OK, if the current second is an in-progress leap second.  But even
though this mechanism has been available for about 20 years' time, almost
nobody uses it.


NTP Client for Windows Contains a Workaround
--------------------------------------------
The Windows system time knows nothing about leap seconds, so for many years
the Windows port of ntpd provides a workaround where the system time is
slewed by the client to compensate the leap second.

Thus it is not required to use a smearing NTP server for Windows clients,
but of course the smearing server approach also works.


The Leap Smear Approach
-----------------------
Due to the reasons mentioned above some support for leap smearing has
recently been implemented in ntpd.  This means that to insert a leap second
an NTP server adds a certain increasing "smear" offset to the real UTC time
sent to its clients, so that after some predefined interval the leap second
offset is compensated.  The smear interval should be long enough,
e.g. several hours, so that NTP clients can easily follow the clock drift
caused by the smeared time.

During the period while the leap smear is being performed, ntpd will include
a specially-formatted 'refid' in time packets that contain "smeared" time.
This refid is of the form 254.x.y.z, where x.y.z are 24 encoded bits of the
smear value.

With this approach the time an NTP server sends to its clients still matches
UTC before the leap second, up to the beginning of the smear interval, and
again corresponds to UTC after the insertion of the leap second has
finished, at the end of the smear interval.  By examining the first byte of
the refid, one can also determine if the server is offering smeared time or
not.

Of course, clients which receive the "smeared" time from an NTP server don't
have to (and even must not) care about the leap second anymore.  Smearing is
just transparent to the clients, and the clients don't even notice there's a
leap second.


Pros and Cons of the Smearing Approach
--------------------------------------
The disadvantages of this approach are:

- During the smear interval the time provided by smearing NTP servers
differs significantly from UTC, and thus from the time provided by normal,
non-smearing NTP servers.  The difference can be up to 1 second, depending
on the smear algorithm.

- Since smeared time differs from true UTC, and many applications require
correct legal time (UTC), there may be legal consequences to using smeared
time.  Make sure you check to see if this requirement affects you.

However, for applications where it's only important that all computers have
the same time and a temporary offset of up to 1 s to UTC is acceptable, a
better approach may be to slew the time in a well defined way, over a
certain interval, which is what we call smearing the leap second.


The Motivation to Implement Leap Smearing
-----------------------------------------
Here is some historical background for ntpd, related to smearing/slewing
time.

Up to ntpd 4.2.4, if kernel support for leap seconds was either not
available or was not enabled, ntpd didn't care about the leap second at all.
So if ntpd was run with -x and thus kernel support wasn't used, ntpd saw a
sudden 1 s offset after the leap second and normally would have stepped the
time by -1 s a few minutes later.  However, 'ntpd -x' does not step the time
but "slews" the 1-second correction, which takes 33 minutes and 20 seconds
to complete.  This could be considered a bug, but certainly this was only an
accidental behavior.

However, as we learned in the discussion in http://bugs.ntp.org/2745, this
behavior was very much appreciated since indeed the time was never stepped
back, and even though the start of the slewing was somewhat undefined and
depended on the poll interval.  The system time was off by 1 second for
several minutes before slewing even started.

In ntpd 4.2.6 some code was added which let ntpd step the time at UTC
midnight to insert a leap second, if kernel support was not used.
Unfortunately this also happened if ntpd was started with -x, so the folks
who expected that the time was never stepped when ntpd was run with -x found
this wasn't true anymore, and again from the discussion in NTP bug 2745 we
learn that there were even some folks who patched ntpd to get the 4.2.4
behavior back.

In 4.2.8 the leap second code was rewritten and some enhancements were
introduced, but the resulting code still showed the behavior of 4.2.6,
i.e. ntpd with -x would still step the time.  This has only recently been
fixed in the current ntpd stable code, but this fix is only available with a
certain patch level of ntpd 4.2.8.

So a possible solution for users who were looking for a way to come over the
leap second without the time being stepped could have been to check the
version of ntpd installed on each of their systems.  If it's still 4.2.4 be
sure to start the client ntpd with -x.  If it's 4.2.6 or 4.2.8 it won't work
anyway except if you had a patched ntpd version instead of the original
version.  So you'd need to upgrade to the current -stable code to be able to
run ntpd with -x and get the desired result, so you'd still have the
requirement to check/update/configure every single machine in your network
that runs ntpd.

Google's leap smear approach is a very efficient solution for this, for
sites that do not require correct timestamps for legal purposes.  You just
have to take care that your NTP servers support leap smearing and configure
those few servers accordingly.  If the smear interval is long enough so that
NTP clients can follow the smeared time it doesn't matter at all which
version of ntpd is installed on a client machine, it just works, and it even
works around kernel bugs due to the leap second.

Since all clients follow the same smeared time the time difference between
the clients during the smear interval is as small as possible, compared to
the -x approach.  The current leap second code in ntpd determines the point
in system time when the leap second is to be inserted, and given a
particular smear interval it's easy to determine the start point of the
smearing, and the smearing is finished when the leap second ends, i.e. the
next UTC day begins.

The maximum error doesn't exceed what you'd get with the old smearing caused
by -x in ntpd 4.2.4, so if users could accept the old behavior they would
even accept the smearing at the server side.

In order to affect the local timekeeping as little as possible the leap
smear support currently implemented in ntpd does not affect the internal
system time at all.  Only the timestamps and refid in outgoing reply packets
*to clients* are modified by the smear offset, so this makes sure the basic
functionality of ntpd is not accidentally broken.  Also peer packets
exchanged with other NTP servers are based on the real UTC system time and
the normal refid, as usual.

The leap smear implementation is optionally available in ntp-4.2.8p3 and
later, and the changes can be tracked via http://bugs.ntp.org/2855.


Using NTP's Leap Second Smearing
--------------------------------
- Leap Second Smearing MUST NOT be used for public servers, e.g. servers
provided by metrology institutes, or servers participating in the NTP pool
project.  There would be a high risk that NTP clients get the time from a
mixture of smearing and non-smearing NTP servers which could result in
undefined client behavior.  Instead, leap second smearing should only be
configured on time servers providing dedicated clients with time, if all
those clients can accept smeared time.

- Leap Second Smearing is NOT configured by default.  The only way to get
this behavior is to invoke the ./configure script from the NTP source code
package with the --enable-leap-smear parameter before the executables are
built.

- Even if ntpd has been compiled to enable leap smearing support, leap
smearing is only done if explicitly configured.

- The leap smear interval should be at least several hours' long, and up to
1 day (86400s).  If the interval is too short then the applied smear offset
is applied too quickly for clients to follow.  86400s (1 day) is a good
choice.

- If several NTP servers are set up for leap smearing then the *same* smear
interval should be configured on each server.

- Smearing NTP servers DO NOT send a leap second warning flag to client time
requests.  Since the leap second is applied gradually the clients don't even
notice there's a leap second being inserted, and thus there will be no log
message or similar related to the leap second be visible on the clients.

- Since clients don't (and must not) become aware of the leap second at all,
clients getting the time from a smearing NTP server MUST NOT be configured
to use a leap second file.  If they had a leap second file they would apply
the leap second twice: the smeared one from the server, plus another one
inserted by themselves due to the leap second file.  As a result, the
additional correction would soon be detected and corrected/adjusted.

- Clients MUST NOT be configured to poll both smearing and non-smearing NTP
servers at the same time.  During the smear interval they would get
different times from different servers and wouldn't know which server(s) to
accept.


Setting Up A Smearing NTP Server
--------------------------------
If an NTP server should perform leap smearing then the leap smear interval
(in seconds) needs to be specified in the NTP configuration file ntp.conf,
e.g.:

 leapsmearinterval 86400

Please keep in mind the leap smear interval should be between several and 24
hours' long.  With shorter values clients may not be able to follow the
drift caused by the smeared time, and with longer values the discrepancy
between system time and UTC will cause more problems when reconciling
timestamp differences.

When ntpd starts and a smear interval has been specified then a log message
is generated, e.g.:

 ntpd[31120]: config: leap smear interval 86400 s

While ntpd is running with a leap smear interval specified the command:

 ntpq -c rv

reports the smear status, e.g.:

# ntpq -c rv
associd=0 status=4419 leap_add_sec, sync_uhf_radio, 1 event, leap_armed,
version="ntpd 4.2.8p3-RC1@1.3349-o Mon Jun 22 14:24:09 UTC 2015 (26)",
processor="i586", system="Linux/3.7.1", leap=01, stratum=1,
precision=-18, rootdelay=0.000, rootdisp=1.075, refid=MRS,
reftime=d93dab96.09666671 Tue, Jun 30 2015 23:58:14.036,
clock=d93dab9b.3386a8d5 Tue, Jun 30 2015 23:58:19.201, peer=2335,
tc=3, mintc=3, offset=-0.097015, frequency=44.627, sys_jitter=0.003815,
clk_jitter=0.451, clk_wander=0.035, tai=35, leapsec=201507010000,
expire=201512280000, leapsmearinterval=86400, leapsmearoffset=-932.087

In the example above 'leapsmearinterval' reports the configured leap smear
interval all the time, while the 'leapsmearoffset' value is 0 outside the
interval and increases from 0 to -1000 ms over the interval.  So this can be
used to monitor if and how the time sent to clients is smeared.  With a
leapsmearoffset of -.932087, the refid reported in smeared packets would be
254.196.88.176.

Submit patches, bug reports, and enhancement requests via

			http://bugs.ntp.org

		  The ntp Distribution Base Directory

This directory and its subdirectories contain the Network Time Protocol
Version 4 (NTP) distribution for Unix and Windows/NT systems.  This release
may still work on VxWorks, too.

The contents of the base directory are given in this file. The contents of
subdirectories are given in the README files in each subdirectory.

A complete explanation of the configure, compile and install process, as
well as setting up an NTP subnet, is in the HTML pages in the ./html/
directory. For more information on NTP and how to get a working setup,
read WHERE-TO-START.

For Windows/NT, visit html/build/hints/winnt.html .

The base directory ./ contains the autoconfiguration files, source
directories and related stuff:

COPYRIGHT	Excerpt from the HTML file ./html/copyright.html. This file
		specifies copyright conditions, together with a list of
		major authors and electric addresses.

INSTALL		Generic installation instructions for autoconf-based programs.
		Unless you really know what you are doing, you should read the
		directions in the HTML pages, starting with ./html/index.html.

NEWS		What's new in this release.

README		This file.

README.bk	Instructions for folks who use the BitKeeper-repository
		version of NTP.

README.hackers	Notes to folks who want to hack on the code.

TODO            List of items the NTP developers are working on.

WHERE-TO-START	Hints on what to read in order to get a working
		configuration.

Makefile.am	Automake file configuration file. Edit only if you have the
		GNU automake and autoconf utilities installed.

Makefile.in	Autoconf make file template for Unix.

adjtimed        Directory containing the sources for the adjtime daemon
		for HP/UX systems prior to HP-UX 10.0.

authstuff       Directory containing sources for miscellaneous programs
		to test, calibrate and certify the cryptographic
		mechanisms for DES and MD5 based authentication. These
		programs do not include the cryptographic routines
		themselves, so are free of U.S. export restrictions.

build		A script to build the distribution in A.`config.guess`
		subdirectory (more or less).

clockstuff	Directory containing sources for miscellaneous programs
		to test certain auxiliary programs used with some kernel
		configurations, together with a program to calculate
		propagation delays for use with radio clocks and
		national time dissemination services such as WWV/WWVH,
		WWVB and CHU.

conf            Directory containing a motley collection of
		configuration files for various systems. For example only.

config.guess	Script used to identify the machine architecture and
		operating system.

config.h.in	Configuration file generated automatically from
		configure.in. Do not edit.

configure	Script used to configure the distribution. See the HTML pages
		(./html/index.html) for a complete description of the options
		available.

configure.in	Master configuration template. Edit only if you have the
		GNU automake and autoconf utilities installed.

dot.emacs	C-mode indentation rules for code "Just the way Dave likes it".

flock_build	(UDel only) Build the distribution on a number of
		different platforms.

html            Directory containing a complete set of documentation on
		building and configuring a NTP server or client. The
		documentation is in the form of HTML files suitable for
		browsing and contains links to additional documentation
		at various web sites. If a browser is unavailable, an
		ordinary text editor can be used.

include		Directory containing include header files used by most
		programs in the distribution.

install-sh	Script to install a program, script or data file.

kernel		Directory containing sources for kernel programs such as
		line disciplines and STREAMS modules used with the CHU
		decoder and precision PPS signals.

libntp		Directory containing library source code used by most
		programs in the distribution.

ntpdate		Directory containing sources for a program to set the
		local machine time from one or more remote machines
		running NTP.  Operates like rdate, but much more accurate.

ntpq            Directory containing sources for a utility program to
		query local and remote NTP peers for state variables and
		related timekeeping information. This program conforms
		to Appendix A of the NTP Version 3 Specification RFC 1305.

ntptrace        Directory containing sources for a utility program that
		can be used to reveal the chain of NTP peers from a
		designated peer to the primary server at the root of the
		timekeeping subnet.

parse		Directory containing files belonging to the generic
		parse reference clock driver. For reasonably simple
		clocks it is possible to get away with about 3-4Kb of
		code. additionally the SunOS 4.x/Solaris 5.3 streams
		module for parse squats here.

patches		Directory containing patches already applied to this
		distribution. These are included for record and to help
		in possible porting problems.

scripts		Directory containing scripts to build the configuration
		files in this directory and then the makefiles used in
		various dependent directories. the subdirectories
		monitoring and support hold various perl and shell
		scripts for visualizing synchronization and daemon startup.

stamp.h.in	Configuration file generated automatically from configure.in.
		Do not edit.

util            Directory containing sources for various utility and
		testing programs.

David L. Mills (mills@udel.edu)
21 June 1998
See README.hackers for notes on coding styles.

The master copy of this information can be found at:

 http://support.ntp.org/Dev/MaintainerIssues#How_to_work_on_a_bug_using_BitKe

If you are going to patch both ntp-stable and ntp-dev
please do it this way:

 > cd ntp-stable
 > (make and test your changes to ntp-stable first)
 > (commit your changes to ntp-stable)
 > cd ../ntp-dev
 > bk pull ../ntp-stable	(get your changes from ntp-stable)
 > (resolve any problems and test your changes)
 > (commit your changes to ntp-dev)

With the current release of bitkeeper it is *much* easier to move changes
from ntp-stable to ntp-dev than it is to move changes from ntp-dev to
ntp-stable.

If you make your changes in the above order and then submit them,
it will be trivial to apply your patches.

Otherwise, it will be much more difficult to apply your patches.

You are pretty much done now if your repos are on pogo.udel.edu.

If these patches are for a bugzilla issue, mark the issue as Resolved/READY
with a comment of "Please pick up the patches in pogo:/wherever"

---

Please read (and follow) the previous section if you want to submit
patches for both ntp-stable and ntp-dev.

If you cannot easily get your patches to pogo, you may submit patches
via the 'bk send' command:

 > cd REPO
 > bk citool	(or bk ci ... ;  bk commit ... )
 > bk pull	# make sure your repo is up-to-date
 > bk send -d -ubk://www.ntp.org/home/bk/REPO - > file-containing-the-patch
 > bk receive -vv -a < file-containing-the-patch
		# Sanity check.

 # Open a bugzilla item at <http://bugzilla.ntp.org>

 # After the bug is opened, visit the bug and attach file-containing-the-patch
See README.hackers for notes on coding styles.

The NTP project's github repository is at https://github.com/ntp-project/ntp.

There are two branches, master and stable.

The stable branch is the current supported production code branch, the
ntp-stable code (even 2nd number).

The master branch is for new development, also known as ntp-dev (which
has an odd 2nd number).

If you have some work you'd like to add, then if there is any interest
in seeing that work in the current production release then base your work
on the stable branch, and pull your work into a master copy to allow for
publishing your changes in the ntp-dev or master branch.

If there is no expectation that your work will be included in the
current stable release (the ntp-stable code) then it's better to do your
work on a copy of the master branch.

Make sure that any changes you make to stable pull cleanly into master.

It's possible that after pulling your changes from stable to master that
some additional cleanup will be required in master.  Please do this.

If you follow this method, then if you submit a pull request for either
master or for master+stable, it will be easy for us to evaluate and
incorporate your work.

Please also note that your submissions will be able to be evaluated and
handled sooner if the repo that contains your pull requests also includes
test cases.

The general workflow is as follows:

1) If you haven't, create a fork of ntp-project/ntp with your github account.
   i) Log on to github.com with your github account.
       - If you don't have one, create one first. (read: https://help.github.com/articles/signing-up-for-a-new-github-account)
       - Make sure you also have a SSH key associated with your github account.
         (read: https://help.github.com/articles/generating-ssh-keys/)
   ii) Go to https://github.com/ntp-project/ntp
   iii) On the top right corner, right below the header bar, there is
        a button labeled "Fork".  Click on it.  This will fork the current
	ntp master to your own account. Once done, it will go to your account's
	version of the ntp repository. (Your fork of ntp source)
   iv) Clone a local version of your fork. 
        - git clone git@github.com:<your_username>/ntp

2) Look through the bugs listed in the bug tracker: http://bugs.ntp.org/

3) Once you've found a bug to work on:

   i) Create a branch off your own master branch of your local fork.
      (the <branchname> can be any valid short string that will tell you
       what you're working on)
      - git checkout -b <branchname>
        
   ii) Start working on the bug.
   iii) When you create changes in the source, it would help you to 
        keep track of your changes by committing to your local repo.
	(This way, every small change is tracked and when you've
	 made a mistake, you can always go back.)
	 - git commit -a -m "description of change"
   iv) Once you are satisfied, you can push to your github account's
       repository.
         - git push origin <branchname>
    v) (go to step iii).

4) Once you feel you've fixed the bug (and tested it), you need to 
   create a pull request on your branch on github.  (Read up on
   pull requests @ https://help.github.com/articles/using-pull-requests)

    i) Create your pullrequest by following the instructions @
       https://help.github.com/articles/creating-a-pull-request/

5) Your pull request will be reviewed by committers and when it
   passes review, it will be merged by the reviewer/allowed committer.

6) You have fixed a bug.  Goto step #2.

If these patches are for a bugzilla issue, mark the issue as Resolved/READY
with a comment of "Please pick up the patches from XXX" where XXX is
something like:

 hostname:~user/path	if it's a machine the reviewers have access to, or
 github-pull-request-URI

---

README file for directory ./libntp of the NTP Version 4 distribution

This directory contains the sources for the NTP library used by most
programs in this distribution. See the README and RELNOTES files in the
parent directory for directions on how to make this library.
README file for directory ./scripts of the NTP Version 4 distribution

This directory contains shell and perl script files for the configuration,
monitoring and support of NTP installations. See the README and RELNOTES
files in the parent directory for directions on how to use these files.

calc_tickadj	Calculates "optimal" value for tick given ntp.drift file

monitoring      directory containing perl scripts useful for monitoring
                operations

rc		start/stop scripts for NTP

ntp-wait	Blocks until ntpd is in state 4 (synchronized).
		Useful at boot time, to delay the boot sequence
		until after "ntpd -g" has set the time.

ntpsweep	prints per host given in <file> the NTP stratum level, the
		clock offset in seconds, the daemon version, the operating
		system and the processor.

ntptrace        Trace ntp peers of a server up to stratum 1.

stats		directory containing awk and shell scripts useful for
		maintaining statistics summaries of clockstats, loopstats
		and peerstats files

summary         Generate summary files out of stat files produced by NTP
                daemon.

plot_summary    Plot summaries generated by summary script.
This directory contains some example rc scripts for ntpd.

In general, ntpd should be started as soon as possible in the boot process. If
any services require stable system clock, the ntpwait script should be run
before them as late as possible.

The rc.d contains scripts for systems using rc.d init system (originated in
NetBSD). If a service requires stable system time, indicate it with TIMESYNC
dependency and set ntpwait_enable to YES.

For SysV init systems, you'll have to create links as /etc/rc2.d/S20ntpd and
/etc/rc2.d/S80ntpwait yourself. (The numbers are just examples, try to give
ntpd as much time as possible to synchronize before running ntpwait).
This directory contains support for monitoring the local clock of xntp daemons.

WARNING: The scripts and routines contained in this directory are beta
	 release!  Do not depend on their correct operation. They are,
	 however, in regular use at University of Erlangen-Nuernberg.
	 No severe problems are known for this code.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
PLEASE THINK TWICE BEFORE STARTING MONITORING REMOTE XNTP DEAMONS !!!!
MONITORING MAY INCREASE THE LOAD OF THE DEAMON MONITORED AND MAY
INCREASE THE NETWORK LOAD SIGNIFICANTLY
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


Files are:

README:
	This file

ntptrap:
	perl script to log ntp mode 6 trap messages.

	It sends a set_trap request to each server given and dumps the
	trap messages received. It handles refresh of set_trap.
	Currently it handles only NTP V2, however the NTP V3 servers
	also accept v2 requests. It will not interpret v3 system and
	peer stati correctly.

	usage:
	  ntptrap [-n] [-p <port>] [-l <debug-output>] servers...
	
	-n:		do not send set_trap requests

	port:		port to listen for responses
			useful if you have a configured trap

	debug-output:	file to write trace output to (for debugging)

	This script convinced me that ntp trap messages are only of
	little use.

ntploopstat:
	perl script to gather loop info statistics from xntpd via mode 7
	LOOP_INFO requests.

	This script collects data to allow monitoring of remote xntp servers
	where it is not possible to directly access the loopstats file
	produced by xntpd itself. Of course, it can be used to sample
	a local server if it is not configured to produce a loopstats file.

	Please note, this program poses a high load on the server as
	a communication takes place every delay seconds ! USE WITH CARE !

	usage:
	  ntploopstat [-d<delay>] [-t<timeout>] [-l <logfile>] [-v] [ntpserver]
	
	delay:		number of seconds to wait between samples
			default: 60 seconds
	timeout:	number of seconds to wait for reply
			default 12 seconds
	logfile:	file to log samples to
			default: loopstats:<ntpserver>:
			(note the trailing colon)
			This name actually is a prefix.
			The file name is dynamically derived by appending
			the name of the month the sample belongs to.
			Thus all samples of a month end up in the same file.

	the format of the files generated is identical to the format used by
	xntpd with the loopstats file:
		MJD <seconds since midnight UTC> offset frequency compliance
	
	if a timeout occurs the next sample is tried after delay/2 seconds

	The script will terminate after MAX_FAIL (currently 60)
	consecutive errors.

	Errors are counted for:
		- error on send call
		- error on select call
		- error on recv call
		- short packet received
		- bad packet 
		- error on open for logfile

ntploopwatch:
	perl script to display loop filter statistics collected by ntploopstat
	or dumped directly by xntpd.

	Gnuplot is used to produce a graphical representation of the sample
	values, that have been preprocessed and analysed by this script.

	It can either be called to produce a printout of specific data set or
	used to continously monitor the values. Monitoring is achieved by 
	periodically reprocessing the logfiles, which are updated regularly
	either by a running ntploopstat process or by the running xntpd.

	usage:
	  to watch statistics permanently:
	     ntploopwatch [-v[<level>]] [-c <config-file>] [-d <working-dir>]

	  to get a single print out specify also
			  -P<printer> [-s<samples>]
				      [-S <start-time>] [-E <end-time>]
				      [-O <MaxOffs>] [-o <MinOffs>]
	
	level:		level of verbosity for debugging
	config-file:	file to read configurable settings from
			On each iteration it is checked and reread
			if it has been changed
			default: loopwatch.config
	working-dir:	specify working directory for process, affects
			interpretation of relative file names
	
	All other flags are only useful with printing plots, as otherwise
	command line values would be replaced by settings from the config file.

	printer:	specify printer to print plot
			BSD print systems semantics apply; if printer
			is omitted the name "ps" is used; plots are
			prepared using PostScript, thus the printer
			should best accept postscript input

	For the following see also the comments in loopwatch.config.SAMPLE

	samples:	use last # samples from input data
	start-time:	ignore input samples before this date
	end-time:	ignore input samples after this date
			if both start-time and end-time are specified
			a given samples value is ignored
	MaxOffs:
	MinOffs:	restrict value range 

loopwatch.config.SAMPLE:
	sample config file for ntploopwatch
	each configurable option is explained there

lr.pl:
	linear regression package used by ntploopwatch to compute
	linear approximations for frequency and offset values
	within display range

timelocal.pl:

	used during conversion of ISO_DATE_TIME values specified in
	loopwatch config files to unix epoch values (seconds since
	1970-01-01_00:00_00 UTC)

	A version of this file is distributed with perl-4.x, however,
	it has a bug related to dates crossing 1970, causing endless loops..
	The version contained here has been fixed.

ntp.pl:
	perl support for ntp v2 mode 6 message handling
	WARNING: This code is beta level - it triggers a memory leak;
		 as for now it is not quite clear, wether this is caused by a
		 bug in perl or by bad usage of perl within this script.

Statistics file formats (README.stats)

The xntp3 daemon can produce a variety of statistics files which are
useful for maintenance, evaluation and retrospective calibration
purposes. See the xntpd.8 man page for instructions on how to configure
this feature. Since these files can become rather large and cumbersome,
they are ordinarily reduced to summary form by running the summary.sh
shell script once per day, week or month, as appropriate. There are
three file collections presently defined: peerstats, loopstats and
clockstats, each of which is described in this note.

peerstats

The following data are collected in the peerstats files. The files are
reduced to summary data using the peer.sh shell script. See the peer.awk
script for further information. A line in the file is produced upon
reception of each valid update from a configured peer.

  49236 30.756 140.173.96.1 9474 0.000603 0.37532

  49236             modified Julian day number
  30.756            time of day (s) past midnight UTC
  140.173.96.1      peer identifier (IP address or receiver identifier)
  9474              peer status word (hex) (see NTP specification)
  0.000603          offset (s)
  0.08929           delay (s)
  0.37532           dispersion (s)

loopstats

The following data are collected in the loopstats files. The files are
reduced to summary data using the loop.sh shell script. See the loop.awk
script for further information. A line in the file is produced at each
valid update of the local clock.

  49236 11.897 -0.000004 -35.9384 0

  49236             modified Julian day number
  11.897            time of day (s) past midnight UTC
  -0.000004         time offset (s)
  -35.9384          frequency offset (ppm)
  0                 phase-lock loop time constant

clockstats

The following data are collected in the clockstats files. The files are
reduced to summary data using the clock.sh shell script, which also
updates the ensemble, etf, itf and tdata data files as well. See the
clock.awk, ensemble.awk, etf.awk, itf.awk and tdta.awk scripts for
further information. A line in the file is produced at each valid update
received from a configured radio clock. Data are at present recorded for
several radios. The first part of each data line is similar for all
radios, e.g.:

  49234 60517.826 127.127.4.1   93 247 16:48:21.814

  49234             modified Julian day number
  60517.826         time of day (s) past midnight UTC
  127.127.4.1       receiver identifier (Spectracom 8170/Netclock-2)
  93 247 16:48:21.814  timecode (format varies)

In the case of the Austron GPS receiver, a good deal of additional
information is extracted from the radio, as described below. The formats
shown consist of one line with all the fields shown in order. The
timecode formats specific to each radio follow. See the file
README.timecodes for detailed information on the timecode formats used
by these radios.

Spectracom 8170/Netclock-2 WWVB receiver

  49234 60517.826 127.127.4.1 ?A93 247 16:48:21.814

  The '?' and 'A' characters are present only when the receiver is
  unsynchronized; otherwise, they are replaced by space ' ' characters.

IRIG audio decoder

  49234 60517.826 127.127.6.0 247 16:48:21?

  The '?' character is present only when the receiver is unsynchronized.

Austron 2200A/2201A GPS receiver

  49234 60580.843 127.127.10.1 93:247:16:49:24.814?

  The '?' character is present only when the receiver is unsynchronized.

Depending on the installed options, the Austron 2200A/2201A recognizes a
number of special commands that report various data items. See the
refclock_as2201.c source module for a list of the commands used. These
data are collected only if the following line is included in the
configuration file ntp.conf:

  fudge 127.127.10.1 flag4 1    # enable extended statistics collection

The format of each data line returned is summarized in the following
list.

External time/frequency data (requires input buffer option IN)

These data determine the deviations of external time/frequency inputs
relative to receiver oscillator time. The following data are typical
using an external cesium oscillator PPS and 5-MHz outputs.

  49234 60580.843 127.127.10.1 93:247:16:49:24.814 ETF

  -85.9             time interval (ns)
  -89.0             average time interval (ns)
  4.0               time interval sigma (ns)
  +1.510E-11        time interval rate
  -4.500E-11        deltaf/f
  +1.592E-11        average deltaf/f
  5.297E-13         sigma deltaf/f
  500               number of samples

Model and option identifiers

These data show the receiver model number and option configuration.

  49234 60708.848 127.127.10.1 93:247:16:51:32.817 ID;OPT;VER

  GPS 2201A         model ident (must be "GPS 2200A" or "GPS 2201A")
  TTY1              rs232 option present (required)
  TC1               IRIG option present (optional)
  LORAN             LORAN assist option present (optional)
  IN                input buffer option present (optional)
  OUT1              output buffer option present (required)
  B.00              data processor software version ("B.00" or later)
  B.00              signal processor software version ("B.00" or later)
  28-Apr-93         software version date ("28-Apr-93" or later)

Internal time/frequency data

These data determine the deviations of the receiver oscillator with
respect to satellite time.

  49234 60564.846 127.127.10.1 93:247:16:49:08.816 ITF

  COCO              current mode (must be "COCO")
  0                 code coast mode (must be zero)
  +6.6152E-08       code sigma (s)
  -3.5053E-08       code delta t (s)
  -4.0361E-11       deltat/t
  -6.4746E-11       oscillator ageing rate
  500.00            loop time constant
  4.984072          electrical tuning (V)

GPS/LORAN ensemble data (requires LORAN assist option LORAN)

These data determine the deviations and weights to calculate ensemble
time from GPS and LORAN data.

  49234 60596.852 127.127.10.1 93:247:16:49:40.812 LORAN ENSEMBLE

  +9.06E-08         GPS t (s)
  +3.53E-08         GPS sigma (s)
  .532              GPS weight
  +3.71E-08         LORAN t (s)
  +3.76E-08         LORAN sigma (s)
  .468              LORAN weight
  +6.56E-08         ensemble t
  +6.94E-08         ensemble sigma (s)

LORAN stationkeeping data (requires LORAN assist option LORAN)

These data determine which stations of the LORAN chain are being
tracked, together with individual signal/noise ratios, deviations and
weights.

  49234 60532.850 127.127.10.1 93:247:16:48:36.820 LORAN TDATA

  M                 station identifier; data follows
  OK                status (must be "OK" for tracking)
  0                 cw flag
  0                 sw flag
  1162.17           time of arrival
  -4.6              snr (-30.0 if not "OK" status)
  1.67E-07          2-sample phase-time deviation
  .507              weight (included only if "OK" status)
  W AQ 0 0 3387.80 -31.0  station identifier and data
  X OK 0 0 1740.27 -11.2 2.20E-07 .294  station identifier and data
  Y OK 0 0 2180.71 -4.6 2.68E-07 .198  station identifier and data
  Z CV 0 0 3392.94 -30.0  station identifier and data

Oscillator status and environment

These data determine the receiver oscillator type, mode, status and
environment. Nominal operating conditions are shown below.

  49234 60628.847 127.127.10.1 93:247:16:50:12.817 OSC;ET;TEMP

  1121 Software     Control  oscillator model and mode (must be
                    "Software Control")
  Locked            status (must be "Locked")
  4.979905          electrical tuning (V)
  44.81             oscillator cavity temperature

Receiver position, status and offsets

These data determine the receiver position and elevation, together with
programmable delay corrections for the antenna cable and receiver.

  49234 60788.847 127.127.10.1 93:247:16:52:52.817 POS;PPS;PPSOFF

  +39:40:48.425     receiver latitude (N)
  -075:45:02.392    receiver longitude (E)
  +74.09            receiver elevation (m)
  Stored            position status (must be "Stored")
  UTC               PPS/PPM alignment (must be "UTC")
  0                 receiver delay (ns) (should be zero for calibrated
                    receiver)
  200               cable delay (ns)
  0                 user time bias (ns) (must be zero)

Satellite tracking status

These data determine how many satellites are being tracked. At the
present state of constellation development, there should be at least
three visible satellites in view. Much of the time the maximum of
seven are being tracked; rarely this number drops to two.

  49234 60612.850 127.127.10.1 93:247:16:49:56.820 TRSTAT

  24 T              satellite prn and status (T = track, A = acquire)
  16 A 13 T 20 T 18 T 07 T 12 T  list continued

UTC leap-second information

These data determine when the next leap second is to occur. The exact
method to use is obscure.

  49234 60548.847 127.127.10.1 93:247:16:48:52.818 UTC

  -1.2107E-08       A0 term (s)
  -1.2790E-13       A1 term (s)
  +9.0000E+00       current leap seconds (s)
  +2.0480E+05       time for leap seconds (s)
  +2.0100E+02       week number for delta leap (weeks)
  +1.9100E+02       week number for future leap (weeks)
  +4.0000E+00       day number for future leap (days)
  +9.0000E+00       future leap seconds (s)

David L. Mills
University of Delaware
mills@udel.edu
23 October 1993
Radio Timecode Formats (README.timecodes)

Following are examples of the serial timecode formats used by various
timecode receivers as given in the instruction manuals. These examples
are intended only for illustration and not as the basis of system
design. The following symbols are used to identify the timecode
character that begins a subfield. The values given after this symbol
represent the character offset from the beginning of the timecode string
as edited to remove control characters.

C         on-time character (start bit)
Y         year of century
T         time of day
D         day of year or month/day
A         alarm indicator (format specific)
Q         quality indicator (format specific)
<LF>      ASCII line feed (hex 0a)
<CR>      ASCII carriage return (hex 0d)
<SP>      ASCII space (hex 20)

In order to promote uniform behavior in the various implementations, it
is useful to have a common interpretation of alarm conditions and signal
quality. When the alarm indicator it on, the receiver is not operating
correctly or has never synchronized to the broadcast signal. When the
alarm indicator is off and the quality indicator is on, the receiver has
synchronized to the broadcast signal, then lost the signal and is
coasting on its internal oscillator.

In the following uppercase letters, punctuation marks and spaces <SP>
stand for themselves; lowercase letters stand for fields as described.
Special characters other than <LF>, <CR> and <SP> are preceded by ^.

Spectracom 8170 and Netclock/2 WWV Synchonized Clock (format 0)

"<CR><LF>i  ddd hh:mm:ss  TZ=zz<CR><LF>"
 C       A  D   T

     poll: ?; offsets: Y = none, D = 3, T = 7, A = 0, Q = none
     i = synchronization flag (<SP> = in synch, ? = out synch)
     ddd = day of year
     hh:mm:ss = hours, minutes, seconds
     zz = timezone offset (hours from UTC)

     Note: alarm condition is indicated by other than <SP> at A, which
     occurs during initial synchronization and when received signal has
     been lost for about ten hours

     example: "   216 15:36:43  TZ=0"
               A  D   T

Netclock/2 WWV Synchonized Clock (format 2)

"<CR><LF>iqyy ddd hh:mm:ss.fff ld"
 C       AQY  D   T

     poll: ?; offsets: Y = 2, D = 5, T = 9, A = 0, Q = 1
     i = synchronization flag (<SP> = in synch, ? = out synch)
     q = quality indicator (<SP> < 1ms, A < 10 ms, B < 100 ms, C < 500
     ms, D > 500 ms)
     yy = year (as broadcast)
     ddd = day of year
     hh:mm:ss.fff = hours, minutes, seconds, milliseconds of day
     l = leap-second warning (L indicates leap at end of month)
     d = standard/daylight time indicator (<SP> standard, D daylight)

     Note: alarm condition is indicated by other than <SP> at A, which
     occurs during initial synchronization and when received signal has
     been lost for about ten hours; unlock condition is indicated by
     other than <SP> at Q, with time since last lock indicated by the
     letter code A < 13 min, B < 1.5 hr, C < 7 hr, D > 7 hr.

     example: "  92 216 15:36:43.640  D"
               AQ   D   T

TrueTime 468-DC Satellite Synchronized Clock (and other TrueTime
receivers)

"<CR><LF><^A>ddd:hh:mm:ssq<CR>"
              D   T       QC

     poll: none; offsets: Y = none, D = 0, T = 4, A = 12, Q = 12
     hh:mm:ss = hours, minutes, seconds
     q = quality/alarm indicator (<SP> = locked, ? = alarm)

     Note: alarm condition is indicated by ? at A, which occurs during
     initial synchronization and when received signal is lost for an
     extended period; unlock condition is indicated by other than <SP>
     at Q

     example: "216:15:36:43 "
               D   T       Q

Heath GC-1000 Most Accurate Clock (WWV/H)

"<CR>hh:mm:ss.f     dd/mm/yy<CR>"
 C   T        A     D

     poll: none; offsets: Y = none, D = 15, T = 0, A = 9, Q = none
     hh:mm:ss = hours, minutes, seconds
     f = deciseconds (? when out of spec)
     dd/mm = day, month
     yy = year of century (from DIPswitches)

     Note: 0?:??:??.? is displayed before synch is first established and
     hh:mm:ss.? once synch is established and then lost again for about
     a day.

     example: "15:36:43.6     04/08/91"
               T        A     D     Y

PST/Traconex 1020 Time Source (WWV/H) (firmware revision V4.01)

"frdzycchhSSFTttttuuxx<CR>" "ahh:mm:ss.fffs<CR>" "yy/dd/mm/ddd<CR>"
          A   Q               T                   Y  D

     poll: "QMQDQT"; offsets: Y = 0, D = 3 T = 1,, A = 11, Q = 13
     f = frequency enable (O = all frequencies enabled)
     r = baud rate (3 = 1200, 6 = 9600)
     d = features indicator (@ = month/day display enabled)
     z = time zone (0 = UTC)
     y = year (5 = 1991)
     cc = WWV propagation delay (52 = 22 ms)
     hh = WWVH propagation delay (81 = 33 ms)
     SS = status (80 or 82 = operating correctly)
     F = current receive frequency (1-5 = 2.5, 5, 10, 15, 20 MHz)
     T = transmitter (C = WWV, H = WWVH)
     tttt = time since last update (minutes)
     uu = flush character (03 = ^C)
     xx = 94 (unknown) (firmware revision X4.01.999 only)

     a = AM/PM indicator (A = AM, P = PM, <SP> - 24-hour format)
     hh:mm:ss.fff = hours, minutes, seconds, milliseconds of day
     s = daylight-saving indicator (<SP> standard, D daylight)

     yy = year of century (from DIPswitches)
     dd/mm/ddd = day of month, month of year, day of year

     Note: The alarm condition is indicated by other than ? at A, which
     occurs during initial synchronization and when received signal is
     lost for an extended period. A receiver unlock condition is
     indicated by other than "0000" in the tttt subfield at Q.

     example: "O3@055281824C00000394 91/08/04/216  15:36:43.640"
                             T       Y        D    T

David L. Mills
University of Delaware
mills@udel.edu
23 October 1993
Statistics processing scripts (README)

This directory contains a number of scripts for use with the filegen
facility. Those files ending in .awk are for the Unix awk utility, while
those ending in .sh are for the csh utility. Normally, the summary.sh
script is called from a cron job once per day. This script processes the
daily loopstats, peerstats and clockstats files produced by the daemon,
updates the loop_summary, peer_summary and clock_summary archive files,
and deletes the daily files.

In the case of the Austron 2201A GPS receiver, the clockstats file
contains a wealth of additional monitoring data. These data are summarized
and writted to the clock_summary file, then a series of special files are
constructed for later processing by the S utility.

The summary.sh script invokes a number of awk scripts to actually produce
the data. This may result in multiple scans of the same input file.
The input file is deleted after processing. In fact, the shell scripts will
process all input files found of the correct type in chronological order,
deleting each one as it is scanned, except the current day file.

The summary.sh script can produce input files for the S utility, if it
is found on the search path. This utility makes PostScript graphs of the
loopstats data for each day, as well as various statistics produced by
the Austorn 220aA GPS receiver. The S utility is automatically run
as a background job. Its control files have the .S extension.

The psummary.awk script can be used to scan the peer_summary file and
construct an historical reprise of the daily summaries. 

The file formats are documented in the README.stats file and in the
scripts themselves. Further detail on the radio clock ASCII timecode
formats and related data are in the README.timecode file.

David L. Mills
University of Delaware
mills@udel.edu
1 November 1993
Revised 12 April 1994 
------------------------------------------------------------------------------
The adjtimed daemon emulates the BSD adjtime(2) system call.  The
adjtime() routine communicates with this daemon via SYSV messages.

The emulation uses an undocumented kernel variable (as of 6.0/2.0
and later releases) and as such it cannot be guaranteed to work in
future HP-UX releases.  Perhaps HP-UX will have a real adjtime(2)
system call in the future.

Author: Tai Jin (tai@sde.hp.com)
------------------------------------------------------------------------------

IMPORTANT NOTE: This stuff must be compiled with no optimization !!

NOTE: This code is known to work as of 8.0 on s300's, s700's and s800's.
      PLEASE do not modify it unless you have access to kernel sources
      and fully understand the implications of any changes you are making.
      One person already has trashed adjtimed by making it do "the right 
      thing".  This is not an exact replacement for BSD adjtime(2), don't
      try to make it into one.

	 -- Ken
If you look at the files here you should be able to figure out what is
being done.

../scripts/genLocInfo handles the parsing of these files.

If no "more specific" file has been found and cvo.sh says the OS is
redhat* or fedora*, we will look for loc/redhat .

If no "mode specific" file has been found and the "uname" command returns
"Linux", we will look for loc/linux .

Note that automake has interesting behavior - we build the man pages using
the man_MANS target, but they are actually installed using the empty
manX_MANS= target, which "does the right thing".  This means that we
need to have empty manX_MANS= targets for each possible X, based on
the data in the loc/ files.  For executables, this generally means
sections 1 and 8.
        THIS TARBALL IS NOT A FULL DISTRIBUTION.

The contents of this tarball is designed to be incorporated into
software packages that utilize the AutoOpts option automation package
and are intended to be installed on systems that may not have libopts
installed.  It is redistributable under the terms of either the LGPL
(see COPYING.lgpl) or under the terms of the advertising clause free BSD
license (see COPYING.mbsd).

Usage Instructions for autoconf/automake/libtoolized projects:

1. Install the unrolled tarball into your package source tree,
   copying ``libopts.m4'' to your autoconf macro directory.

   In your bootstrap (pre-configure) script, you can do this:

      rm -rf libopts libopts-*
      gunzip -c `autoopts-config libsrc` | tar -xvf -
      mv -f libopts-*.*.* libopts
      cp -fp libopts/m4/*.m4 m4/.

   I tend to put my configure auxiliary files in "m4".
   Whatever directory you choose, if it is not ".", then
   be sure to tell autoconf about it with:

      AC_CONFIG_AUX_DIR(m4)

   This is one macro where you *MUST* remember to *NOT* quote
   the argument.  If you do, automake will get lost.

2. Add an invocation of either LIBOPTS_CHECK or LIBOPTS_CHECK_NOBUILD
   to your configure.ac file.  See LIBOPTS_CHECK: below for details.

3. Add the following to your top level ``Makefile.am'' file:

      if NEED_LIBOPTS
         SUBDIRS += $(LIBOPTS_DIR)
      endif

   where ``<...>'' can be whatever other files or directories you may
   need.  The SUBDIRS must be properly ordered.  *PLEASE NOTE* it is
   crucial that the SUBDIRS be set under the control of an automake
   conditional.  To work correctly, automake has to know the range of
   possible values of SUBDIRS.  It's a magical name with magical
   properties.  ``NEED_LIBOPTS'' will be correctly set by the
   ``LIBOPTS_CHECK'' macro, above.

4. Add ``$(LIBOPTS_CFLAGS)'' to relevant compiler flags and
   ``$(LIBOPTS_LDADD)'' to relevant link options whereever
   you need them in your build tree.

5. Make sure your object files explicitly depend upon the
   generated options header file.  e.g.:

     $(prog_OBJECTS) : prog-opts.h
     prog-opts.h : prog-opts.c
     prog-opts.c : prog-opts.def
         autogen prog-opts.def

6. *OPTIONAL* --
   If you are creating man pages and texi documentation from
   the program options, you will need these rules somewhere, too:

     man_MANS = prog.1
     prog.1 : prog-opts.def
         autogen -Tagman-cmd.tpl -bprog prog-opts.def

     invoke-prog.texi : prog-opts.def
         autogen -Tagtexi-cmd.tpl prog-opts.def

If your package does not utilize the auto* tools, then you
will need to hand craft the rules for building the library.

LIBOPTS_CHECK:

The arguments to both macro are a relative path to the directory with
the libopts source code.  It is optional and defaults to "libopts".
These macros work as follows:

1.  LIBOPTS_CHECK([libopts/rel/path/optional])

    Adds two command-line options to the generated configure script,
    --enable-local-libopts and --disable-libopts-install.  AC_SUBST's
    LIBOPTS_CFLAGS, LIBOPTS_LDADD, and LIBOPTS_DIR for use in
    Makefile.am files.  Adds Automake conditional NEED_LIBOPTS which
    will be true when the local copy of libopts should be built.  Uses
    AC_CONFIG_FILES([$libopts-dir/Makefile]) to cause the local libopts
    into the package build.  If the optional relative path to libopts is
    not provided, it defaults to simply "libopts".

2.  LIBOPTS_CHECK_NOBUILD([libopts/rel/path/optional])

    This variant of LIBOPTS_CHECK is useful when multiple configure.ac
    files in a package make use of a single libopts tearoff.  In that
    case, only one of the configure.ac files should build libopts and
    others should simply use it.  Consider this package arrangment:

    all-tools/
      configure.ac
      common-tools/
        configure.ac
        libopts/

    The parent package all-tools contains a subpackage common-tools
    which can be torn off and used independently.  Programs configured
    by both configure.ac files link against the common-tools/libopts
    tearoff, when not using the system's libopts.  The top-level
    configure.ac uses LIBOPTS_CHECK_NOBUILD([common-tools/libopts]),
    while common-tools/configure.ac uses LIBOPTS_CHECK.  The difference
    is LIBOPTS_CHECK_NOBUILD will never build the libopts tearoff,
    leaving that to the subpackage configure.ac's LIBOPTS_CHECK.
    Specifically, LIBOPTS_CHECK_NOBUILD always results in the
    NEED_LIBOPTS Automake conditional being false, and does not invoke
    AC_CONFIG_FILES(path-to-libopts/Makefile).

LICENSING:

This material is Copyright (C) 1992-2015 by Bruce Korb.  You are
licensed to use this under the terms of either the GNU Lesser General
Public License (see: COPYING.lgpl), or, at your option, the modified
Berkeley Software Distribution License (see: COPYING.mbsd).  Both of
these files should be included with this tarball.
If you look at the files here you should be able to figure out what is
being done.

../scripts/genHardFlags handles the parsing of these files.

If no "more specific" file has been found and cvo.sh says the OS is
redhat* or fedora*, we will look for loc/redhat .

If no "mode specific" file has been found and the "uname" command returns
"Linux", we will look for loc/linux .

README file for directory ./conf of the NTP Version 4 distribution

This directory contains example run-time configuration files for the
NTP Version 4 daemon ntpd. These files illustrate some of the more
obtuse configurations you may run into. They are not likely to do
anything good if run on machines other than their native spot, so don't
just blindly copy something and put it up. Additional information can
be found in the ./doc directory of the base directory.

Included also are example public key and symmetric key files produced
by the ntp-genkeys program with names prefixed by ntpkey. These are
ordinarily kept in /usr/local/etc and used by the Autokey scheme. See
the authopt.htm pnd genkeys.htm ages for further information.
README file for directory ./include of the NTP Version 4 distribution

This directory contains the include files used by most programs in this
distribution.
README file for directory ./ntpq of the NTP Version 4 distribution

This directory contains the sources for the ntpq utility program. See
the README and RELNOTES files in the parent directory for directions on
how to make and install this program. The current version number of this
program is in the version.c file.

JSMN
====

jsmn (pronounced like 'jasmine') is a minimalistic JSON parser in C.  It can be
easily integrated into resource-limited or embedded projects.

You can find more information about JSON format at [json.org][1]

Library sources are available at [bitbucket.org/zserge/jsmn][2]

The web page with some information about jsmn can be found at
[http://zserge.com/jsmn.html][3]

Philosophy
----------

Most JSON parsers offer you a bunch of functions to load JSON data, parse it
and extract any value by its name. jsmn proves that checking the correctness of
every JSON packet or allocating temporary objects to store parsed JSON fields
often is an overkill. 

JSON format itself is extremely simple, so why should we complicate it?

jsmn is designed to be	**robust** (it should work fine even with erroneous
data), **fast** (it should parse data on the fly), **portable** (no superfluous
dependencies or non-standard C extensions). An of course, **simplicity** is a
key feature - simple code style, simple algorithm, simple integration into
other projects.

Features
--------

* compatible with C89
* no dependencies (even libc!)
* highly portable (tested on x86/amd64, ARM, AVR)
* about 200 lines of code
* extremely small code footprint
* API contains only 2 functions
* no dynamic memory allocation
* incremental single-pass parsing
* library code is covered with unit-tests

Design
------

The rudimentary jsmn object is a **token**. Let's consider a JSON string:

	'{ "name" : "Jack", "age" : 27 }'

It holds the following tokens:

* Object: `{ "name" : "Jack", "age" : 27}` (the whole object)
* Strings: `"name"`, `"Jack"`, `"age"` (keys and some values)
* Number: `27`

In jsmn, tokens do not hold any data, but point to token boundaries in JSON
string instead. In the example above jsmn will create tokens like: Object
[0..31], String [3..7], String [12..16], String [20..23], Number [27..29].

Every jsmn token has a type, which indicates the type of corresponding JSON
token. jsmn supports the following token types:

* Object - a container of key-value pairs, e.g.:
	`{ "foo":"bar", "x":0.3 }`
* Array - a sequence of values, e.g.:
	`[ 1, 2, 3 ]`
* String - a quoted sequence of chars, e.g.: `"foo"`
* Primitive - a number, a boolean (`true`, `false`) or `null`

Besides start/end positions, jsmn tokens for complex types (like arrays
or objects) also contain a number of child items, so you can easily follow
object hierarchy.

This approach provides enough information for parsing any JSON data and makes
it possible to use zero-copy techniques.

Install
-------

To clone the repository you should have mercurial installed. Just run:

	$ hg clone http://bitbucket.org/zserge/jsmn jsmn

Repository layout is simple: jsmn.c and jsmn.h are library files, tests are in
the jsmn\_test.c, you will also find README, LICENSE and Makefile files inside.

To build the library, run `make`. It is also recommended to run `make test`.
Let me know, if some tests fail.

If build was successful, you should get a `libjsmn.a` library.
The header file you should include is called `"jsmn.h"`.

API
---

Token types are described by `jsmntype_t`:

	typedef enum {
		JSMN_PRIMITIVE = 0,
		JSMN_OBJECT = 1,
		JSMN_ARRAY = 2,
		JSMN_STRING = 3
	} jsmntype_t;

**Note:** Unlike JSON data types, primitive tokens are not divided into
numbers, booleans and null, because one can easily tell the type using the
first character:

* <code>'t', 'f'</code> - boolean 
* <code>'n'</code> - null
* <code>'-', '0'..'9'</code> - number

Token is an object of `jsmntok_t` type:

	typedef struct {
		jsmntype_t type; // Token type
		int start;       // Token start position
		int end;         // Token end position
		int size;        // Number of child (nested) tokens
	} jsmntok_t;

**Note:** string tokens point to the first character after
the opening quote and the previous symbol before final quote. This was made 
to simplify string extraction from JSON data.

All job is done by `jsmn_parser` object. You can initialize a new parser using:

	jsmn_parser parser;
	jsmntok_t tokens[10];

	jsmn_init(&parser);

	// js - pointer to JSON string
	// tokens - an array of tokens available
	// 10 - number of tokens available
	jsmn_parse(&parser, js, tokens, 10);

This will create a parser, and then it tries to parse up to 10 JSON tokens from
the `js` string.

A non-negative reutrn value of `jsmn_parse` is the number of tokens actually
used by the parser.
Passing NULL instead of the tokens array would not store parsing results, but
instead the function will return the value of tokens needed to parse the given
string. This can be useful if you don't know yet how many tokens to allocate.

If something goes wrong, you will get an error. Error will be one of these:

* `JSMN_ERROR_INVAL` - bad token, JSON string is corrupted
* `JSMN_ERROR_NOMEM` - not enough tokens, JSON string is too large
* `JSMN_ERROR_PART` - JSON string is too short, expecting more JSON data

If you get `JSON_ERROR_NOMEM`, you can re-allocate more tokens and call
`jsmn_parse` once more.  If you read json data from the stream, you can
periodically call `jsmn_parse` and check if return value is `JSON_ERROR_PART`.
You will get this error until you reach the end of JSON data.

Other info
----------

This software is distributed under [MIT license](http://www.opensource.org/licenses/mit-license.php),
 so feel free to integrate it in your commercial products.

[1]: http://www.json.org/
[2]: https://bitbucket.org/zserge/jsmn/wiki/Home
[3]: http://zserge.com/jsmn.html
README file for directory ./xntpdc of the NTP Version 4 distribution

This directory contains the sources for the xntpdc utility program. See
the README and RELNOTES files in the parent directory for directions on
how to make and install this program. The current version number of this
program is in the version.c file.
NTP SNMP subagent for Net-SNMP

Installation Guides:

- install net-snmp from source (configure, make;, make install)
- edit the snmpd configuration file (/usr/local/share/snmp/snmpd.conf):
  add the lines
   master agentx
   agentXSocket   tcp:localhost:705
  and check which read-only community is configured (should be "rocommunity  public")  
- start snmpd (sudo /usr/local/sbin/snmpd) and check that it is running correctly by running the command
  snmpwalk -v2c -c public localhost
  (which should output a lot of data values for the supported built-in MIBs of net-snmp)
- build the libntpq and the libntp library
- build the ntpsnmpd application (make) and run it (./ntpsnmpd)
- now you can run 
   snmpwalk -v2c -c public localhost enterprises.5597.99
  which should give you a list of all currently supported NTP MIB objects and their current values
  
Please note that currently I use a private MIB OID (enterprises.5597 is the Meinberg top level OEM OID and 99 is my temporary working space for this project). 
The final OID has to be registered with IANA and this is done by the RFC Editor when the NTPv4 MIB RFC is standardized. 
I will try to do this earlier in order to be able to have a working solution at the end of this project.

In its current state the daemon supports these objects:

ntpEntSoftwareName
ntpEntSoftwareVersion
ntpEntSoftwareVersionVal
ntpEntSoftwareVendor
ntpEntSystemType
ntpEntTimeResolution
ntpEntTimeResolutionVal
ntpEntTimePrecision
ntpEntTimePrecisionVal
ntpEntTimeDistance

They all use the libntpq library to access information from the ntpd instance with mode 6 packets.

Next step is to implement the status section of the MIB (section 2). 

README file for directory ./clockstuff of the NTP Version 4 distribution

This directory contains the sources for utility programs designed to
support radio clocks.  chutest.c is desgined to test the depredated
chu_clk line discipline or STREAMS module and can also test a CHU
modem in raw mode.

You can make things in here by typing one or more of:

	make propdelay (or `make')
	make chutest

Propdelay computes high frequency propagation delays, given the
longitude and latitude of the transmitter and receiver.  Use
this for WWV/H and CHU.  Don't use it for WWVB (the computation
is easier for that).

Chutest can be used to input and process data from a CHU modem
attached to a serial port.  It will use the CHU line discipline
(if installed), or raw mode otherwise.  This was used to test
out the initial reduction algorithms, and may not be up to date.
README file for directory ./ntpdate of the NTP Version 4 distribution

This directory contains the sources for the ntpdate utility program. See
the README and RELNOTES files in the parent directory for directions on
how to make and install this program. The current version number of this
program is in the version.c file.

README file for directory ./util of the NTP Version 4 distribution

This directory contains the sources for the various utility programs.
See the README and RELNOTES files in the parent directory for directions
on how to make and install these programs.

The ntptime.c program checks the kernel configuration for the NTP user
interface syscalls ntp_gettime() and ntp_adjtime().  If present, the
current timekeeping data are displayed.  If not, a dissapointment is
displayed.  See the kernel page file in the HTML documentation in
distribution for further details. ntptime will be built be if configure
believes your system can use it.

The jitter.c program can be used to determine the timing jitter due to
the operating system in a gettimeofday() call.  For most systems the
dominant contribution to the jitter budget is the period of the hardware
interrupt, usually in the range 10 us-1 ms.  For those systems with
microsecond counters, such as recent Sun and certain HP and DEC systems,
the jitter is dominated only by the operating system.

The timetrim.c program can be used with SGI machines to implement a
scheme to discipline the hardware clock frequency.  See the source code
for further information.

The byteorder.c and longsize.c programs are used during the configuration
process to determine the byte order (little or big endian) and longword
size (32 or 64 bits).  See the configure scripts for further details.

The testrs6000.c program is used for testing purposes with the IBM
RS/6000 AIX machines. Bill Jones <jones@chpc.utexas.edu> reports:
"I could not get a tickadj of less than 40 us to work on a RS6000.
If you set it less than 40 us do so at your own risk!"

The tickadj.c program can be used to read and set various kernel
parameters affecting NTP operations. See the tickadj page in the HTML
documentation for further details.  tickadj will be built if configure
believes your system can use it.

tg.c and tg2.c are tone generators.  They make audio signals
that emulate WWV or IRIG (-B and -E).  tg runs on Solaris.
tg2 is a clone that runs on Linux, FreeBSD, and NetBSD.
Read the source for the fine print.  tg2 has a help option
available via -h.

README file for directory ./kernel/sys of the NTP Version 3 distribution

This directory contains system header files used by the clock discipline
and STREAMS modules in the .. (./kernel) directory.

If the precision-time kernel (KERNEL_PLL define) is configured, the
installation process requires the header file /usr/include/sys/timex.h
for the particular architecture to be in place. The file timex.h included
in this distribution is for Suns; the files for other systems can be
found in the kernel distributions available from the manufacturer's
representatives.
This directory contains some DCF77 related programs.
They have not yet fully been ported to architectures other than Sun with
SunOS 4.x.  So if you want to try them you are on your own - a little
porting may be necessary.

parsetest:	simple parse streams module test
		Works only under SunOS with parse Streams Module loaded
		and Meinberg-Clocks

testdcf:	simple DCF77 raw impulse test program via 50Baud RS232

dcfd:		simple DCF77 raw impulse receiver with NTP loopfilter
		mechanics for synchronisation (allows DCF77 synchronisation
		without network code in a nutshell)

Frank Kardel
PARSE reference clock driver:

This directory contains the files making up the parser for
the parse refclock driver. For reasonably sane clocks this refclock
drivers allows a refclock implementation by just providing a
conversion routine and the appropriate NTP parameters. Refclock
support can run as low a 3k code with the parse refclock driver.

The modules in here are designed to live in two worlds. In userlevel
as part of the xntp daemon and in kernel land as part of a STREAMS module
or, if someone gets to it, as part of a line discipline. Currently only
SunOS4.x/SunOS5.x STREAMS are supported (volunteers for other vendors like HP?).
This structure means, that refclock_parse can work with or without kernel
support. Kernelsupport increases accuracy tremendingly. The current restriction
of the parse driver is that it only supports SYSV type ttys and that kernel
support is only available for Suns right now.

Three kernel modules are part of this directory. These work only on
SunOS (SunOS4 and SunOS5).

	SunOS4 (aka Solaris 1.x):
		parsestreams.loadable_module.o	- standard parse module for SunOS 4

		Both modules can be loaded via modload <modulename>.

	SunOS5 (aka Solaris 2.x):
		parse		- auto loadable streams module

		To install just drop "parse" into /kernel/strmod and
		start the daemon (SunOS5 will do the rest).

The structure of the parse reference clock driver is as follows:

	ntpd	- contains NTP implementation and calls a reference clock
		  127.127.8.x which is implemented by
	 	  refclock_parse.c
		  - which contains several refclock decriptions. These are
		    selected by the x part of the refclock address.
		    The lower two bits specify the device to use. Thus the
		    value (x % 4) determines the device to open
		    (/dev/refclock-0 - /dev/refclock-3).

		    The kind of clock is selected by the mode parameter. This parameter
		    selects the clock type which deterimines how I/O is done,
		    the tty parameters and the NTP parameters.

		    refclock_parse operates on an abstract reference clock
		    that delivers time stamps and stati. Offsets and sychron-
		    isation information is derived from this data and passed
		    on to refclock_receive of xntp which uses that data for
		    syncronisation.

		    The abstract reference clock is generated by the parse*
		    routines. They parse the incoming data stream from the
		    clock and convert it to the appropriate time stamps.
		    The data is also mapped int the abstract clock states
		    	POWERUP - clock has no valid phase and time code
				  information

			NOSYNC	- Time code is not confirmed, phase is probably
				  ok.
			SYNC	- Time code and phase are correct.

		    A clock is trusted for a certain time (type parameter) when
		    it leaves the SYNC state. This is derived from the
		    observation that quite a few clocks can still generate good
		    time code information when losing contact to their
		    synchronisation source. When the clock does not reagain
		    synchronisation in that trust period it will be deemed
		    unsynchronised until it regains synchronisation. The same
		    will happen if xntp sees the clock unsynchronised at
		    startup.

		    The upper bit of x specifies that all samples delivered
		    from the clock should be used to discipline the NTP
		    loopfilter. For clock with accurate once a second time
		    information this means big improvements for time keeping.
		    A prerequisite for passing on the time stamps to
		    the loopfilter is, that the clock is in synchronised state.

	   parse.c  These are the general routines to parse the incoming data
		    stream. Usually these routines should not require
		    modification.

	   clk_*.c  These files hole the conversion code for the time stamps
		    and the description how the time code can be parsed and
		    where the time stamps are to be taken.
		    If you want to add a new clock type this is the file
		    you need to write in addition to mention it in
		    parse_conf.c and setting up the NTP and TTY parameters
		    in refclock_parse.c.

Further information can be found in parse/README.parse and the various source
files.

Frank Kardel
  ----------------------------------------------------------------
  This file is part of bzip2/libbzip2, a program and library for
  lossless, block-sorting data compression.

  bzip2/libbzip2 version 1.0.6 of 6 September 2010
  Copyright (C) 1996-2010 Julian Seward <jseward@bzip.org>

  Please read the WARNING, DISCLAIMER and PATENTS sections in the 
  README file.

  This program is released under the terms of the license contained
  in the file LICENSE.
  ----------------------------------------------------------------

The script xmlproc.sh takes an xml file as input,
and processes it to create .pdf, .html or .ps output.
It uses format.pl, a perl script to format <pre> blocks nicely,
 and add CDATA tags so writers do not have to use eg. &lt; 

The file "entities.xml" must be edited to reflect current
version, year, etc.


Usage:

  ./xmlproc.sh -v manual.xml
  Validates an xml file to ensure no dtd-compliance errors

  ./xmlproc.sh -html manual.xml
  Output: manual.html

  ./xmlproc.sh -pdf manual.xml
  Output: manual.pdf

  ./xmlproc.sh -ps manual.xml
  Output: manual.ps


Notum bene: 
- pdfxmltex barfs if given a filename with an underscore in it

- xmltex won't work yet - there's a bug in passivetex
    which we are all waiting for Sebastian to fix.
  So we are going the xml -> pdf -> ps route for the time being,
    using pdfxmltex.

This is the README for bzip2/libzip2.
This version is fully compatible with the previous public releases.

------------------------------------------------------------------
This file is part of bzip2/libbzip2, a program and library for
lossless, block-sorting data compression.

bzip2/libbzip2 version 1.0.6 of 6 September 2010
Copyright (C) 1996-2010 Julian Seward <jseward@bzip.org>

Please read the WARNING, DISCLAIMER and PATENTS sections in this file.

This program is released under the terms of the license contained
in the file LICENSE.
------------------------------------------------------------------

Complete documentation is available in Postscript form (manual.ps),
PDF (manual.pdf) or html (manual.html).  A plain-text version of the
manual page is available as bzip2.txt.


HOW TO BUILD -- UNIX

Type 'make'.  This builds the library libbz2.a and then the programs
bzip2 and bzip2recover.  Six self-tests are run.  If the self-tests
complete ok, carry on to installation:

To install in /usr/local/bin, /usr/local/lib, /usr/local/man and
/usr/local/include, type

   make install

To install somewhere else, eg, /xxx/yyy/{bin,lib,man,include}, type

   make install PREFIX=/xxx/yyy

If you are (justifiably) paranoid and want to see what 'make install'
is going to do, you can first do

   make -n install                      or
   make -n install PREFIX=/xxx/yyy      respectively.

The -n instructs make to show the commands it would execute, but not
actually execute them.


HOW TO BUILD -- UNIX, shared library libbz2.so.

Do 'make -f Makefile-libbz2_so'.  This Makefile seems to work for
Linux-ELF (RedHat 7.2 on an x86 box), with gcc.  I make no claims
that it works for any other platform, though I suspect it probably
will work for most platforms employing both ELF and gcc.

bzip2-shared, a client of the shared library, is also built, but not
self-tested.  So I suggest you also build using the normal Makefile,
since that conducts a self-test.  A second reason to prefer the
version statically linked to the library is that, on x86 platforms,
building shared objects makes a valuable register (%ebx) unavailable
to gcc, resulting in a slowdown of 10%-20%, at least for bzip2.

Important note for people upgrading .so's from 0.9.0/0.9.5 to version
1.0.X.  All the functions in the library have been renamed, from (eg)
bzCompress to BZ2_bzCompress, to avoid namespace pollution.
Unfortunately this means that the libbz2.so created by
Makefile-libbz2_so will not work with any program which used an older
version of the library.  I do encourage library clients to make the
effort to upgrade to use version 1.0, since it is both faster and more
robust than previous versions.


HOW TO BUILD -- Windows 95, NT, DOS, Mac, etc.

It's difficult for me to support compilation on all these platforms.
My approach is to collect binaries for these platforms, and put them
on the master web site (http://www.bzip.org).  Look there.  However
(FWIW), bzip2-1.0.X is very standard ANSI C and should compile
unmodified with MS Visual C.  If you have difficulties building, you
might want to read README.COMPILATION.PROBLEMS.

At least using MS Visual C++ 6, you can build from the unmodified
sources by issuing, in a command shell: 

   nmake -f makefile.msc

(you may need to first run the MSVC-provided script VCVARS32.BAT
 so as to set up paths to the MSVC tools correctly).


VALIDATION

Correct operation, in the sense that a compressed file can always be
decompressed to reproduce the original, is obviously of paramount
importance.  To validate bzip2, I used a modified version of Mark
Nelson's churn program.  Churn is an automated test driver which
recursively traverses a directory structure, using bzip2 to compress
and then decompress each file it encounters, and checking that the
decompressed data is the same as the original.



Please read and be aware of the following:

WARNING:

   This program and library (attempts to) compress data by 
   performing several non-trivial transformations on it.  
   Unless you are 100% familiar with *all* the algorithms 
   contained herein, and with the consequences of modifying them, 
   you should NOT meddle with the compression or decompression 
   machinery.  Incorrect changes can and very likely *will* 
   lead to disastrous loss of data.


DISCLAIMER:

   I TAKE NO RESPONSIBILITY FOR ANY LOSS OF DATA ARISING FROM THE
   USE OF THIS PROGRAM/LIBRARY, HOWSOEVER CAUSED.

   Every compression of a file implies an assumption that the
   compressed file can be decompressed to reproduce the original.
   Great efforts in design, coding and testing have been made to
   ensure that this program works correctly.  However, the complexity
   of the algorithms, and, in particular, the presence of various
   special cases in the code which occur with very low but non-zero
   probability make it impossible to rule out the possibility of bugs
   remaining in the program.  DO NOT COMPRESS ANY DATA WITH THIS
   PROGRAM UNLESS YOU ARE PREPARED TO ACCEPT THE POSSIBILITY, HOWEVER
   SMALL, THAT THE DATA WILL NOT BE RECOVERABLE.

   That is not to say this program is inherently unreliable.  
   Indeed, I very much hope the opposite is true.  bzip2/libbzip2 
   has been carefully constructed and extensively tested.


PATENTS:

   To the best of my knowledge, bzip2/libbzip2 does not use any 
   patented algorithms.  However, I do not have the resources 
   to carry out a patent search.  Therefore I cannot give any 
   guarantee of the above statement.



WHAT'S NEW IN 0.9.0 (as compared to 0.1pl2) ?

   * Approx 10% faster compression, 30% faster decompression
   * -t (test mode) is a lot quicker
   * Can decompress concatenated compressed files
   * Programming interface, so programs can directly read/write .bz2 files
   * Less restrictive (BSD-style) licensing
   * Flag handling more compatible with GNU gzip
   * Much more documentation, i.e., a proper user manual
   * Hopefully, improved portability (at least of the library)

WHAT'S NEW IN 0.9.5 ?

   * Compression speed is much less sensitive to the input
     data than in previous versions.  Specifically, the very
     slow performance caused by repetitive data is fixed.
   * Many small improvements in file and flag handling.
   * A Y2K statement.

WHAT'S NEW IN 1.0.0 ?

   See the CHANGES file.

WHAT'S NEW IN 1.0.2 ?

   See the CHANGES file.

WHAT'S NEW IN 1.0.3 ?

   See the CHANGES file.

WHAT'S NEW IN 1.0.4 ?

   See the CHANGES file.

WHAT'S NEW IN 1.0.5 ?

   See the CHANGES file.

WHAT'S NEW IN 1.0.6 ?

   See the CHANGES file.


I hope you find bzip2 useful.  Feel free to contact me at
   jseward@bzip.org
if you have any suggestions or queries.  Many people mailed me with
comments, suggestions and patches after the releases of bzip-0.15,
bzip-0.21, and bzip2 versions 0.1pl2, 0.9.0, 0.9.5, 1.0.0, 1.0.1,
1.0.2 and 1.0.3, and the changes in bzip2 are largely a result of this
feedback.  I thank you for your comments.

bzip2's "home" is http://www.bzip.org/

Julian Seward
jseward@bzip.org
Cambridge, UK.

18     July 1996 (version 0.15)
25   August 1996 (version 0.21)
 7   August 1997 (bzip2, version 0.1)
29   August 1997 (bzip2, version 0.1pl2)
23   August 1998 (bzip2, version 0.9.0)
 8     June 1999 (bzip2, version 0.9.5)
 4     Sept 1999 (bzip2, version 0.9.5d)
 5      May 2000 (bzip2, version 1.0pre8)
30 December 2001 (bzip2, version 1.0.2pre1)
15 February 2005 (bzip2, version 1.0.3)
20 December 2006 (bzip2, version 1.0.4)
10 December 2007 (bzip2, version 1.0.5)
 6     Sept 2010 (bzip2, version 1.0.6)
------------------------------------------------------------------
This file is part of bzip2/libbzip2, a program and library for
lossless, block-sorting data compression.

bzip2/libbzip2 version 1.0.6 of 6 September 2010
Copyright (C) 1996-2010 Julian Seward <jseward@bzip.org>

Please read the WARNING, DISCLAIMER and PATENTS sections in the 
README file.

This program is released under the terms of the license contained
in the file LICENSE.
------------------------------------------------------------------

bzip2-1.0.6 should compile without problems on the vast majority of
platforms.  Using the supplied Makefile, I've built and tested it
myself for x86-linux and amd64-linux.  With makefile.msc, Visual C++
6.0 and nmake, you can build a native Win32 version too.  Large file
support seems to work correctly on at least on amd64-linux.

When I say "large file" I mean a file of size 2,147,483,648 (2^31)
bytes or above.  Many older OSs can't handle files above this size,
but many newer ones can.  Large files are pretty huge -- most files
you'll encounter are not Large Files.

Early versions of bzip2 (0.1, 0.9.0, 0.9.5) compiled on a wide variety
of platforms without difficulty, and I hope this version will continue
in that tradition.  However, in order to support large files, I've had
to include the define -D_FILE_OFFSET_BITS=64 in the Makefile.  This
can cause problems.

The technique of adding -D_FILE_OFFSET_BITS=64 to get large file
support is, as far as I know, the Recommended Way to get correct large
file support.  For more details, see the Large File Support
Specification, published by the Large File Summit, at

   http://ftp.sas.com/standards/large.file

As a general comment, if you get compilation errors which you think
are related to large file support, try removing the above define from
the Makefile, ie, delete the line

   BIGFILES=-D_FILE_OFFSET_BITS=64 

from the Makefile, and do 'make clean ; make'.  This will give you a
version of bzip2 without large file support, which, for most
applications, is probably not a problem.  

Alternatively, try some of the platform-specific hints listed below.

You can use the spewG.c program to generate huge files to test bzip2's
large file support, if you are feeling paranoid.  Be aware though that
any compilation problems which affect bzip2 will also affect spewG.c,
alas.

AIX: I have reports that for large file support, you need to specify
-D_LARGE_FILES rather than -D_FILE_OFFSET_BITS=64.  I have not tested
this myself.
Introduction

	ISC's libbind provides the standard resolver library,
	along with header files and documentation, for communicating
	with domain name servers, retrieving network host entries
	from /etc/hosts or via DNS, converting CIDR network addresses,
	perform Hesiod information lookups, retrieve network entries
	from /etc/networks, implement TSIG transaction/request
	security of DNS messages, perform name-to-address and
	address-to-name translations, utilize /etc/resolv.conf
	for resolver configuration.

	It contains many of the same historical functions and headers
	included with many Unix operating systems.

	Originally written for BIND 8, it was included in BIND 9 as
	optionally-compiled code through release 9.5.  It has been
	removed from subsequent releases of BIND 9 and is now
	provided as a separate package.

Building

	The libbind library requires a system with an ANSI C compiler
	and basic POSIX support.

	To build, just

		./configure
		make

	Several environment variables that can be set before running
	configure will affect compilation:

	    CC
		The C compiler to use.	configure tries to figure
		out the right one for supported systems.

	    CFLAGS
		C compiler flags.  Defaults to include -g and/or -O2
		as supported by the compiler.  

	    STD_CINCLUDES
		System header file directories.	 Can be used to specify
		where add-on thread or IPv6 support is, for example.
		Defaults to empty string.

	    STD_CDEFINES
		Any additional preprocessor symbols you want defined.
		Defaults to empty string.

		Possible settings:
		Change the default syslog facility of named/lwresd.
		  -DISC_FACILITY=LOG_LOCAL0	
		Enable DNSSEC signature chasing support in dig.
		  -DDIG_SIGCHASE=1 (sets -DDIG_SIGCHASE_TD=1 and
				    -DDIG_SIGCHASE_BU=1)
		Disable dropping queries from particular well known ports.
		  -DNS_CLIENT_DROPPORT=0
		Sibling glue checking in named-checkzone is enabled by default.
		To disable the default check set.  -DCHECK_SIBLING=0
		named-checkzone checks out-of-zone addresses by default.
		To disable this default set.  -DCHECK_LOCAL=0
		Enable workaround for Solaris kernel bug about /dev/poll
		  -DISC_SOCKET_USE_POLLWATCH=1
		  The watch timeout is also configurable, e.g.,
		  -DISC_SOCKET_POLLWATCH_TIMEOUT=20

	    LDFLAGS
		Linker flags. Defaults to empty string.

	The following need to be set when cross compiling.

	    BUILD_CC
		The native C compiler.
	    BUILD_CFLAGS (optional)
	    BUILD_CPPFLAGS (optional)
		Possible Settings:
		-DNEED_OPTARG=1		(optarg is not declared in <unistd.h>)
	    BUILD_LDFLAGS (optional)
	    BUILD_LIBS (optional)

	"make install" will install the library.  By default, installation
	is into /usr/local, but this can be changed with the "--prefix"
	option when running "configure".

	To see additional configure options, run "configure --help".

	If you need to re-run configure please run "make distclean" first.
	This will ensure that all the option changes take.

Notes on Usage

        - Installing both libbind and BIND 9 on the same system
          will produce two incompatible header files with similar
          names: $PREFIX/include/isc/list.h (from BIND 9) and
          $PREFIX/include/bind/isc/list.h (from libbind).  When
          compiling code against libbind, be sure to set -I flags
          appropriately.

Documentation

	Man pages for libbind routines, in *roff and plaintext format,
	are included with the release.

Bug Reports and Mailing Lists

	Bugs reports should be sent to

		libbind-bugs@isc.org

	Discussions of libbind can be send to the BIND Users mailing
	list.  To subscribe, send mail to:

		bind-users-subscribe@isc.org

	Archives of that list can be found at:

		https://lists.isc.org/pipermail/bind-users/

	If you're planning on making changes to the libbind source
	code, you might want to join the BIND Workers mailing list.
	To subscribe, send mail to:

		bind-workers-subscribe@isc.org

# tcpdump

[![Build
Status](https://travis-ci.org/the-tcpdump-group/tcpdump.png)](https://travis-ci.org/the-tcpdump-group/tcpdump)

To report a security issue please send an e-mail to security@tcpdump.org.

To report bugs and other problems, contribute patches, request a
feature, provide generic feedback etc please see the file
CONTRIBUTING in the tcpdump source tree root.

TCPDUMP 4.x.y
Now maintained by "The Tcpdump Group"
See 		www.tcpdump.org

Anonymous Git is available via:

	git clone git://bpf.tcpdump.org/tcpdump

formerly from 	Lawrence Berkeley National Laboratory
		Network Research Group <tcpdump@ee.lbl.gov>  
		ftp://ftp.ee.lbl.gov/old/tcpdump.tar.Z (3.4)

This directory contains source code for tcpdump, a tool for network
monitoring and data acquisition.  This software was originally
developed by the Network Research Group at the Lawrence Berkeley
National Laboratory.  The original distribution is available via
anonymous ftp to `ftp.ee.lbl.gov`, in `tcpdump.tar.Z`.  More recent
development is performed at tcpdump.org, http://www.tcpdump.org/

Tcpdump uses libpcap, a system-independent interface for user-level
packet capture.  Before building tcpdump, you must first retrieve and
build libpcap, also originally from LBL and now being maintained by
tcpdump.org; see http://www.tcpdump.org/ .

Once libpcap is built (either install it or make sure it's in
`../libpcap`), you can build tcpdump using the procedure in the `INSTALL.txt`
file.

The program is loosely based on SMI's "etherfind" although none of the
etherfind code remains.  It was originally written by Van Jacobson as
part of an ongoing research project to investigate and improve tcp and
internet gateway performance.  The parts of the program originally
taken from Sun's etherfind were later re-written by Steven McCanne of
LBL.  To insure that there would be no vestige of proprietary code in
tcpdump, Steve wrote these pieces from the specification given by the
manual entry, with no access to the source of tcpdump or etherfind.

Over the past few years, tcpdump has been steadily improved by the
excellent contributions from the Internet community (just browse
through the `CHANGES` file).  We are grateful for all the input.

Richard Stevens gives an excellent treatment of the Internet protocols
in his book *"TCP/IP Illustrated, Volume 1"*. If you want to learn more
about tcpdump and how to interpret its output, pick up this book.

Some tools for viewing and analyzing tcpdump trace files are available
from the Internet Traffic Archive:

* http://www.sigcomm.org/ITA/

Another tool that tcpdump users might find useful is tcpslice:

* https://github.com/the-tcpdump-group/tcpslice

It is a program that can be used to extract portions of tcpdump binary
trace files. See the above distribution for further details and
documentation.

Current versions can be found at www.tcpdump.org.

 - The TCPdump team

original text by: Steve McCanne, Craig Leres, Van Jacobson

-------------------------------------
```
This directory also contains some short awk programs intended as
examples of ways to reduce tcpdump data when you're tracking
particular network problems:

send-ack.awk
	Simplifies the tcpdump trace for an ftp (or other unidirectional
	tcp transfer).  Since we assume that one host only sends and
	the other only acks, all address information is left off and
	we just note if the packet is a "send" or an "ack".

	There is one output line per line of the original trace.
	Field 1 is the packet time in decimal seconds, relative
	to the start of the conversation.  Field 2 is delta-time
	from last packet.  Field 3 is packet type/direction.
	"Send" means data going from sender to receiver, "ack"
	means an ack going from the receiver to the sender.  A
	preceding "*" indicates that the data is a retransmission.
	A preceding "-" indicates a hole in the sequence space
	(i.e., missing packet(s)), a "#" means an odd-size (not max
	seg size) packet.  Field 4 has the packet flags
	(same format as raw trace).  Field 5 is the sequence
	number (start seq. num for sender, next expected seq number
	for acks).  The number in parens following an ack is
	the delta-time from the first send of the packet to the
	ack.  A number in parens following a send is the
	delta-time from the first send of the packet to the
	current send (on duplicate packets only).  Duplicate
	sends or acks have a number in square brackets showing
	the number of duplicates so far.

	Here is a short sample from near the start of an ftp:
		3.00    0.20   send . 512
		3.20    0.20    ack . 1024  (0.20)
		3.20    0.00   send P 1024
		3.40    0.20    ack . 1536  (0.20)
		3.80    0.40 * send . 0  (3.80) [2]
		3.82    0.02 *  ack . 1536  (0.62) [2]
	Three seconds into the conversation, bytes 512 through 1023
	were sent.  200ms later they were acked.  Shortly thereafter
	bytes 1024-1535 were sent and again acked after 200ms.
	Then, for no apparent reason, 0-511 is retransmitted, 3.8
	seconds after its initial send (the round trip time for this
	ftp was 1sec, +-500ms).  Since the receiver is expecting
	1536, 1536 is re-acked when 0 arrives.

packetdat.awk
	Computes chunk summary data for an ftp (or similar
	unidirectional tcp transfer). [A "chunk" refers to
	a chunk of the sequence space -- essentially the packet
	sequence number divided by the max segment size.]

	A summary line is printed showing the number of chunks,
	the number of packets it took to send that many chunks
	(if there are no lost or duplicated packets, the number
	of packets should equal the number of chunks) and the
	number of acks.

	Following the summary line is one line of information
	per chunk.  The line contains eight fields:
	   1 - the chunk number
	   2 - the start sequence number for this chunk
	   3 - time of first send
	   4 - time of last send
	   5 - time of first ack
	   6 - time of last ack
	   7 - number of times chunk was sent
	   8 - number of times chunk was acked
	(all times are in decimal seconds, relative to the start
	of the conversation.)

	As an example, here is the first part of the output for
	an ftp trace:

	# 134 chunks.  536 packets sent.  508 acks.
	1       1       0.00    5.80    0.20    0.20    4       1
	2       513     0.28    6.20    0.40    0.40    4       1
	3       1025    1.16    6.32    1.20    1.20    4       1
	4       1561    1.86    15.00   2.00    2.00    6       1
	5       2049    2.16    15.44   2.20    2.20    5       1
	6       2585    2.64    16.44   2.80    2.80    5       1
	7       3073    3.00    16.66   3.20    3.20    4       1
	8       3609    3.20    17.24   3.40    5.82    4       11
	9       4097    6.02    6.58    6.20    6.80    2       5

	This says that 134 chunks were transferred (about 70K
	since the average packet size was 512 bytes).  It took
	536 packets to transfer the data (i.e., on the average
	each chunk was transmitted four times).  Looking at,
	say, chunk 4, we see it represents the 512 bytes of
	sequence space from 1561 to 2048.  It was first sent
	1.86 seconds into the conversation.  It was last
	sent 15 seconds into the conversation and was sent
	a total of 6 times (i.e., it was retransmitted every
	2 seconds on the average).  It was acked once, 140ms
	after it first arrived.

stime.awk
atime.awk
	Output one line per send or ack, respectively, in the form
		<time> <seq. number>
	where <time> is the time in seconds since the start of the
	transfer and <seq. number> is the sequence number being sent
	or acked.  I typically plot this data looking for suspicious
	patterns.


The problem I was looking at was the bulk-data-transfer
throughput of medium delay network paths (1-6 sec.  round trip
time) under typical DARPA Internet conditions.  The trace of the
ftp transfer of a large file was used as the raw data source.
The method was:

  - On a local host (but not the Sun running tcpdump), connect to
    the remote ftp.

  - On the monitor Sun, start the trace going.  E.g.,
      tcpdump host local-host and remote-host and port ftp-data >tracefile

  - On local, do either a get or put of a large file (~500KB),
    preferably to the null device (to minimize effects like
    closing the receive window while waiting for a disk write).

  - When transfer is finished, stop tcpdump.  Use awk to make up
    two files of summary data (maxsize is the maximum packet size,
    tracedata is the file of tcpdump tracedata):
      awk -f send-ack.awk packetsize=avgsize tracedata >sa
      awk -f packetdat.awk packetsize=avgsize tracedata >pd

  - While the summary data files are printing, take a look at
    how the transfer behaved:
      awk -f stime.awk tracedata | xgraph
    (90% of what you learn seems to happen in this step).

  - Do all of the above steps several times, both directions,
    at different times of day, with different protocol
    implementations on the other end.

  - Using one of the Unix data analysis packages (in my case,
    S and Gary Perlman's Unix|Stat), spend a few months staring
    at the data.

  - Change something in the local protocol implementation and
    redo the steps above.

  - Once a week, tell your funding agent that you're discovering
    wonderful things and you'll write up that research report
    "real soon now".
```
## README for file(1) Command ##

    @(#) $File: README,v 1.50 2016/04/16 22:40:54 christos Exp $

Mailing List: file@mx.gw.com  
Mailing List archives: http://mx.gw.com/pipermail/file/  
Bug tracker: http://bugs.gw.com/  
E-mail: christos@astron.com

[![Build Status](https://travis-ci.org/file/file.png?branch=master)](https://travis-ci.org/file/file)

Phone: Do not even think of telephoning me about this program. Send cash first!

This is Release 5.x of Ian Darwin's (copyright but distributable)
file(1) command, an implementation of the Unix File(1) command.
It knows the 'magic number' of several thousands of file types.
This version is the standard "file" command for Linux,
*BSD, and other systems. (See "patchlevel.h" for the exact release number).

You can download the latest version of the original sources for file from:

	ftp://ftp.astron.com/pub/file/

A public read-only git repository of the same sources is available at:

	https://github.com/file/file

The major changes for 5.x are CDF file parsing, indirect magic, name/use
(recursion) and overhaul in mime and ascii encoding handling.

The major feature of 4.x is the refactoring of the code into a library,
and the re-write of the file command in terms of that library. The library
itself, libmagic can be used by 3rd party programs that wish to identify
file types without having to fork() and exec() file. The prime contributor
for 4.0 was Mans Rullgard.

UNIX is a trademark of UNIX System Laboratories.

The prime contributor to Release 3.8 was Guy Harris, who put in megachanges
including byte-order independence.

The prime contributor to Release 3.0 was Christos Zoulas, who put
in hundreds of lines of source code changes, including his own
ANSIfication of the code (I liked my own ANSIfication better, but
his (__P()) is the "Berkeley standard" way of doing it, and I wanted UCB
to include the code...), his HP-like "indirection" (a feature of
the HP file command, I think), and his mods that finally got the
uncompress (-z) mode finished and working.

This release has compiled in numerous environments; see PORTING
for a list and problems.

This fine freeware file(1) follows the USG (System V) model of the file
command, rather than the Research (V7) version or the V7-derived 4.[23]
Berkeley one. That is, the file /etc/magic contains much of the ritual
information that is the source of this program's power. My version
knows a little more magic (including tar archives) than System V; the
/etc/magic parsing seems to be compatible with the (poorly documented)
System V /etc/magic format (with one exception; see the man page).

In addition, the /etc/magic file is built from a subdirectory
for easier(?) maintenance.  I will act as a clearinghouse for
magic numbers assigned to all sorts of data files that
are in reasonable circulation. Send your magic numbers,
in magic(5) format please, to the maintainer, Christos Zoulas.

COPYING - read this first.  
README - read this second (you are currently reading this file).  
INSTALL - read on how to install
src/localtime_r.c
src/magic.c
src/magic.h
src/mygetopt.h
src/newtest2.c
src/newtest3.c
src/pread.c
src/print.c
src/readcdf.c
src/readelf.c
src/readelf.h
src/regex.c
src/regex2.c
src/softmagic.c
src/strcasestr.c
src/strlcat.c
src/strlcpy.c
src/strndup.c
src/tar.h
src/teststrchr.c
src/vasprintf.c
src/x.c
src/apprentice.c - parses /etc/magic to learn magic  
src/apptype.c - used for OS/2 specific application type magic  
src/ascmagic.c - third & last set of tests, based on hardwired assumptions.  
src/asctime_r.c - replacement for OS's that don't have it.  
src/asprintf.c - replacement for OS's that don't have it.  
src/asctime_r.c - replacement for OS's that don't have it.  
src/asprintf.c - replacement for OS's that don't have it.  
src/cdf.[ch] - parser for Microsoft Compound Document Files  
src/cdf_time.c - time converter for CDF.  
src/compress.c - handles decompressing files to look inside.  
src/ctime_r.c - replacement for OS's that don't have it.  
src/der.[ch] - parser for Distinguished Encoding Rules
src/dprintf.c - replacement for OS's that don't have it.
src/elfclass.h - common code for elf 32/64.
src/encoding.c - handles unicode encodings  
src/file.c - the main program  
src/file.h - header file  
src/file_opts.h - list of options
src/fmtcheck.c - replacement for OS's that don't have it.  
src/fsmagic.c - first set of tests the program runs, based on filesystem info  
src/funcs.c - utilility functions  
src/getline.c - replacement for OS's that don't have it.  
src/getopt_long.c - replacement for OS's that don't have it.  
src/gmtime_r.c - replacement for OS's that don't have it.  
src/is_tar.c, tar.h - knows about Tape ARchive format (courtesy John Gilmore).  
src/localtime_r.c - replacement for OS's that don't have it.  
src/magic.h.in - source file for magic.h
src/mygetopt.h - replacement for OS's that don't have it.  
src/magic.c - the libmagic api  
src/names.h - header file for ascmagic.c  
src/pread.c - replacement for OS's that don't have it.  
src/print.c - print results, errors, warnings.  
src/readcdf.c - CDF wrapper.  
src/readelf.[ch] - Stand-alone elf parsing code.  
src/softmagic.c - 2nd set of tests, based on /etc/magic  
src/mygetopt.h - replacement for OS's that don't have it.  
src/strcasestr.c - replacement for OS's that don't have it.  
src/strlcat.c - replacement for OS's that don't have it.  
src/strlcpy.c - replacement for OS's that don't have it.  
src/tar.h - tar file definitions
src/vasprintf.c - for systems that don't have it.  
doc/file.man - man page for the command  
doc/magic.man - man page for the magic file, courtesy Guy Harris.
	Install as magic.4 on USG and magic.5 on V7 or Berkeley; cf Makefile.

Magdir - directory of /etc/magic pieces
------------------------------------------------------------------------------

If you submit a new magic entry please make sure you read the following
guidelines:

- Initial match is preferably at least 32 bits long, and is a _unique_ match
- If this is not feasible, use additional check
- Match of <= 16 bits are not accepted
- Delay printing string as much as possible, don't print output too early
- Avoid printf arbitrary byte as string, which can be a source of
  crash and buffer overflow

- Provide complete information with entry:
  * One line short summary
  * Optional long description
  * File extension, if applicable
  * Full name and contact method (for discussion when entry has problem)
  * Further reference, such as documentation of format

------------------------------------------------------------------------------

Parts of this software were developed at SoftQuad Inc., developers
of SGML/HTML/XML publishing software, in Toronto, Canada.
SoftQuad was swallowed up by Corel in 2002 and does not exist any longer.
file tests
==========

This directory contains tests for file. It is highly encouraged to add
one each time a bug is found, and each time new magic is added. Each
test consists of two files:

  TEST.testfile
  TEST.result

where TEST is the base name of the test, TEST.testfile is the input,
and TEST.result is the desired output from file.

To add a new test just add the test files to the directory.
/****************************************************************
Copyright (C) Lucent Technologies 1997
All Rights Reserved

Permission to use, copy, modify, and distribute this software and
its documentation for any purpose and without fee is hereby
granted, provided that the above copyright notice appear in all
copies and that both that the copyright notice and this
permission notice and warranty disclaimer appear in supporting
documentation, and that the name Lucent Technologies or any of
its entities not be used in advertising or publicity pertaining
to distribution of the software without specific, written prior
permission.

LUCENT DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS.
IN NO EVENT SHALL LUCENT OR ANY OF ITS ENTITIES BE LIABLE FOR ANY
SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
THIS SOFTWARE.
****************************************************************/

This is the version of awk described in "The AWK Programming Language",
by Al Aho, Brian Kernighan, and Peter Weinberger
(Addison-Wesley, 1988, ISBN 0-201-07981-X).

Changes, mostly bug fixes and occasional enhancements, are listed
in FIXES.  If you distribute this code further, please please please
distribute FIXES with it.  If you find errors, please report them
to bwk@cs.princeton.edu.  Thanks.

The program itself is created by
	make
which should produce a sequence of messages roughly like this:

	yacc -d awkgram.y

conflicts: 43 shift/reduce, 85 reduce/reduce
	mv y.tab.c ytab.c
	mv y.tab.h ytab.h
	cc -c ytab.c
	cc -c b.c
	cc -c main.c
	cc -c parse.c
	cc maketab.c -o maketab
	./maketab >proctab.c
	cc -c proctab.c
	cc -c tran.c
	cc -c lib.c
	cc -c run.c
	cc -c lex.c
	cc ytab.o b.o main.o parse.o proctab.o tran.o lib.o run.o lex.o -lm

This produces an executable a.out; you will eventually want to
move this to some place like /usr/bin/awk.

If your system does not have yacc or bison (the GNU
equivalent), you must compile the pieces manually.  We have
included yacc output in ytab.c and ytab.h, and backup copies in
case you overwrite them.  We have also included a copy of
proctab.c so you do not need to run maketab.

NOTE: This version uses ANSI C, as you should also.  We have
compiled this without any changes using gcc -Wall and/or local C
compilers on a variety of systems, but new systems or compilers
may raise some new complaint; reports of difficulties are
welcome.

This also compiles with Visual C++ on all flavors of Windows,
*if* you provide versions of popen and pclose.  The file
missing95.c contains versions that can be used to get started
with, though the underlying support has mysterious properties,
the symptom of which can be truncated pipe output.  Beware.  The
file makefile.win gives hints on how to proceed; if you run
vcvars32.bat, it will set up necessary paths and parameters so
you can subsequently run nmake -f makefile.win.  Beware also that
when running on Windows under command.com, various quoting
conventions are different from Unix systems: single quotes won't
work around arguments, and various characters like % are
interpreted within double quotes.

This compiles without change on Macintosh OS X using gcc and
the standard developer tools.

This is also said to compile on Macintosh OS 9 systems, using the
file "buildmac" provided by Dan Allen (danallen@microsoft.com),
to whom many thanks.

The version of malloc that comes with some systems is sometimes
astonishly slow.  If awk seems slow, you might try fixing that.
More generally, turning on optimization can significantly improve
awk's speed, perhaps by 1/3 for highest levels.
#
# $NetBSD: README.NetBSD,v 1.1 2009/12/02 15:07:09 martti Exp $
#
# Author: Martti Kuparinen <martti@NetBSD.org>
#
# This file contains copy-pastable commands to import a new PF release
# into the NetBSD repository. Before importing the code into the official
# NetBSD repository, you'll import the code into your own local test
# repository, resolve the conflicts and make sure everything works as expected.
#
# Steps in this document:
#  1) Get the PF dist files and a copy of the CVS repository
#  2) Create a local test repository for the test imports
#  3) Import the new PF release into the test repository
#  4) Resolve conflicts and make other adjustments
#  5) Save the required modifications
#  6) Re-create a local test repository for the final test import
#  7) Import the new PF release into the test repository
#  8) Apply the fixes
#  9) Compile everything and make sure the new version really works
# 10) Update your /usr/src (cd /usr/src && cvs update -dPA)
# 11) Import the new PF release into the NetBSD repository
# 12) Apply the fixes
# 13) Update your /usr/src one more time (cd /usr/src && cvs update -dPA)
# 14) Compile and install everything
# 15) Update src/doc/3RDPARTY and src/doc/CHANGES
# 16) Send an announcement to current-users
#

###############################################################################
###############################################################################
# COMMON SETTINGS FOR EVERYTHING BELOW
###############################################################################
###############################################################################

export NETBSDUSERNAME="fixthis"
export VERSION="4.6"
export VERTAG="`echo ${VERSION} | sed 's+\.+_+g'`"
export VEROLD="v4_2"
export VERNEW="v4_6"
export ORIG="${HOME}/netbsd/orig"
export WORK="${HOME}/netbsd/work"
export FIXES="${HOME}/netbsd/fixes/${VERSION}"
export RSYNC_RSH="ssh -4"
export RSYNC="rsync -avzr --delete"

###############################################################################
###############################################################################
# Fetch the official PF sources
###############################################################################
###############################################################################

mkdir -p ${ORIG}/openbsd-${VERSION}
cd ${ORIG}/openbsd-${VERSION}
if [ ! -f src.tar.gz ]; then
  ftp ftp://ftp.df.lth.se/pub/OpenBSD/${VERSION}/src.tar.gz
fi
if [ ! -f sys.tar.gz ]; then
  ftp ftp://ftp.df.lth.se/pub/OpenBSD/${VERSION}/sys.tar.gz
fi

###############################################################################
###############################################################################
# GET COPY OF THE CVS REPOSITORY FOR LOCAL TEST IMPORTS
###############################################################################
###############################################################################

RSYNC_RSH="ssh -4"
RSYNC="rsync -avzr --delete -e ssh"
S="${NETBSDUSERNAME}@cvs.netbsd.org::cvsroot"
D="${ORIG}/cvsroot"

if [ "${NETBSDUSERNAME}" = "fixthis" ]; then
  echo "You MUST use your real NetBSD user name..."
  sleep 86400
fi
${RSYNC} ${S}/src/ ${D}/src
${RSYNC} --exclude 'commitlog*' --exclude 'history*' ${S}/CVSROOT/ ${D}/CVSROOT

###############################################################################
###############################################################################
# TEST IMPORT 1
###############################################################################
###############################################################################

# Get a new repository
mkdir -p ${WORK}
${RSYNC} ${ORIG}/cvsroot ${WORK}
rm -f ${WORK}/cvsroot/CVSROOT/commitinfo*
rm -f ${WORK}/cvsroot/CVSROOT/loginfo*
touch ${WORK}/cvsroot/CVSROOT/commitinfo
touch ${WORK}/cvsroot/CVSROOT/loginfo
chmod 644 ${WORK}/cvsroot/CVSROOT/config
sed 's+/cvslock+/tmp/cvslock+' < ${WORK}/cvsroot/CVSROOT/config \
  > ${WORK}/cvsroot/CVSROOT/config.1
grep -v -e tag= -e AdminGroup= -e AdminOptions= \
  < ${WORK}/cvsroot/CVSROOT/config.1 > ${WORK}/cvsroot/CVSROOT/config
chmod 444 ${WORK}/cvsroot/CVSROOT/config
rm -f ${WORK}/cvsroot/CVSROOT/config.1
mkdir -p /tmp/cvslock
export CVSROOT="${WORK}/cvsroot"

# Checkout a working copy 
cd ${WORK}
rm -rf src
cvs co -P src

or

cd ${WORK}/src
rm -rf dist/pf sys/dist/pf
cvs update -dPA

or

cd ${WORK}/src
rm -rf dist/pf sys/dist/pf
cvs update -dPA dist/pf sys/dist/pf

# cvs tag
cd ${WORK}/src
cvs tag BEFORE-PF-${VERTAG} dist/pf sys/dist/pf

# Prepare the files for import
rm -rf /tmp/openbsd-${VERSION}
mkdir -p /tmp/openbsd-${VERSION}
cd /tmp/openbsd-${VERSION}
tar -xzf ${ORIG}/openbsd-${VERSION}/src.tar.gz
tar -xzf ${ORIG}/openbsd-${VERSION}/sys.tar.gz
rm -rf /tmp/netbsd-${VERSION}
${WORK}/src/dist/pf/pf2netbsd /tmp/openbsd-${VERSION} /tmp/netbsd-${VERSION}
cd /tmp/netbsd-${VERSION}

# Import the new version
cd /tmp/netbsd-${VERSION}
cvs import -I ! -I CVS -m "Import PF from OpenBSD ${VERSION}" \
  src OPENBSD ${VERNEW}

# src/dist/pf
cd ${WORK}/src/dist/pf
cvs update -kk -j${VEROLD} -j${VERNEW} -d
A=`cvs update | grep ^C | awk '{ print $2 }'`
cvs update | grep ^C

##vim $A
##for i in $A; do mkdir -p ${FIXES}/1/src/dist/pf/`dirname $i`; done
##for i in $A; do cp $i ${FIXES}/1/src/dist/pf/$i; done
cvs diff -u -kk -rOPENBSD $A | less
cvs diff -u -kk -rOPENBSD | less
cvs update -dPA

# src/sys/dist/pf
cd ${WORK}/src/sys/dist/pf
cvs update -kk -j${VEROLD} -j${VERNEW} -d
B=`cvs update | grep ^C | awk '{ print $2 }'`
cvs update | grep ^C

##vim $B
##for i in $B; do mkdir -p ${FIXES}/1/src/sys/dist/pf/`dirname $i`; done
##for i in $B; do cp $i ${FIXES}/1/src/sys/dist/pf/$i; done
cvs diff -u -kk -rOPENBSD $B | less
cvs diff -u -kk -rOPENBSD | less

# Commit changes
cd ${WORK}/src
cvs ci -m "Upgraded PF to ${VERSION}" dist/pf sys/dist/pf

# See the changes
cd ${WORK}/src
cvs diff -u -kk -rOPENBSD dist/pf sys/dist/pf | less

###############################################################################
###############################################################################
# TEST IMPORT 2
###############################################################################
###############################################################################

# Get a new repository
mkdir -p ${WORK}
${RSYNC} ${ORIG}/cvsroot ${WORK}
rm -f ${WORK}/cvsroot/CVSROOT/commitinfo*
rm -f ${WORK}/cvsroot/CVSROOT/loginfo*
touch ${WORK}/cvsroot/CVSROOT/commitinfo
touch ${WORK}/cvsroot/CVSROOT/loginfo
chmod 644 ${WORK}/cvsroot/CVSROOT/config
sed 's+/cvslock+/tmp/cvslock+' < ${WORK}/cvsroot/CVSROOT/config \
  > ${WORK}/cvsroot/CVSROOT/config.1
grep -v -e tag= -e AdminGroup= -e AdminOptions= \
  < ${WORK}/cvsroot/CVSROOT/config.1 > ${WORK}/cvsroot/CVSROOT/config
chmod 444 ${WORK}/cvsroot/CVSROOT/config
rm -f ${WORK}/cvsroot/CVSROOT/config.1
mkdir -p /tmp/cvslock
export CVSROOT="${WORK}/cvsroot"

# Checkout a working copy 
cd ${WORK}/src
rm -rf dist/pf sys/dist/pf
cvs update -dPA dist/pf sys/dist/pf

# cvs tag
cd ${WORK}/src
cvs tag BEFORE-PF-${VERTAG} dist/pf sys/dist/pf

# Prepare the files for import
rm -rf /tmp/openbsd-${VERSION}
mkdir -p /tmp/openbsd-${VERSION}
cd /tmp/openbsd-${VERSION}
tar -xzf ${ORIG}/openbsd-${VERSION}/src.tar.gz
tar -xzf ${ORIG}/openbsd-${VERSION}/sys.tar.gz
rm -rf /tmp/netbsd-${VERSION}
${WORK}/src/dist/pf/pf2netbsd /tmp/openbsd-${VERSION} /tmp/netbsd-${VERSION}
cd /tmp/netbsd-${VERSION}

# Import the new version
cd /tmp/netbsd-${VERSION}
cvs import -I ! -I CVS -m "Import PF from OpenBSD ${VERSION}" \
  src OPENBSD ${VERNEW}

# Merge and fix conflicts
cd ${WORK}/src/dist/pf
cvs update -kk -j${VEROLD} -j${VERNEW} -d
cvs update | grep ^C

cd ${WORK}/src/sys/dist/pf
cvs update -kk -j${VEROLD} -j${VERNEW} -d
cvs update | grep ^C

cd ${FIXES}/1 && tar cf - * | tar xvf - -C ${WORK}
cd ${WORK}/src
cvs ci -m "Upgraded PF to ${VERSION}" dist/pf sys/dist/pf

cd ${FIXES}/2 && tar cf - * | tar xvf - -C ${WORK}
cd ${WORK}/src
cvs ci -m "Sync with official PF" dist/pf sys/dist/pf

###############################################################################
###############################################################################
# FINAL IMPORT IN TO NETBSD REPOSITORY
###############################################################################
###############################################################################

# Settings
if [ "${NETBSDUSERNAME}" = "fixthis" ]; then
  echo "You MUST use your real NetBSD user name..."
  sleep 86400
fi
export WORK="/usr"
export CVSROOT="${NETBSDUSERNAME}@cvs.netbsd.org:/cvsroot"

# Update the local copy first
cd ${WORK}/src
cvs update -dPA dist/pf sys/dist/pf

# cvs tag
cvs tag BEFORE-PF-${VERTAG} dist/pf sys/dist/pf

# Prepare the files for import
rm -rf /tmp/openbsd-${VERSION}
mkdir -p /tmp/openbsd-${VERSION}
cd /tmp/openbsd-${VERSION}
tar -xzf ${ORIG}/openbsd-${VERSION}/src.tar.gz
tar -xzf ${ORIG}/openbsd-${VERSION}/sys.tar.gz
rm -rf /tmp/netbsd-${VERSION}
${WORK}/src/dist/pf/pf2netbsd /tmp/openbsd-${VERSION} /tmp/netbsd-${VERSION}
cd /tmp/netbsd-${VERSION}

# Import the new version
cd /tmp/netbsd-${VERSION}
cvs import -I ! -I CVS -m "Import PF from OpenBSD ${VERSION}" \
  src OPENBSD ${VERNEW}

# Merge and fix conflicts
cd ${WORK}/src/dist/pf
cvs update -kk -j${VEROLD} -j${VERNEW} -d
cvs update | grep ^C

cd ${WORK}/src/sys/dist/pf
cvs update -kk -j${VEROLD} -j${VERNEW} -d
cvs update | grep ^C

cd ${FIXES}/1 && tar cf - * | tar xvf - -C ${WORK}
cd ${WORK}/src
cvs ci -m "Upgraded PF to ${VERSION}" dist/pf sys/dist/pf

cd ${FIXES}/2 && tar cf - * | tar xvf - -C ${WORK}
cd ${WORK}/src
cvs ci -m "Sync with official PF" dist/pf sys/dist/pf

# fix doc/3RDPARTY and doc/CHANGES
# send mail to current-users@

HEADSUP
=======
Hi!

I have just upgraded PF to the latest version on NetBSD -current. I've
tested this by building i386 and amd64 release and installed the sets
on my test hosts.

If you detect any errors (or have improvements), please send a problem
report with the send-pr tool.
	$NetBSD: README.dirs,v 1.12 2013/01/08 13:12:26 pooka Exp $


The following is a quick rundown of the current directory structure.
First, components in the kernel namespace, i.e. compiled with -D_KERNEL

sys/rump/librump - rump kernel base and factions
  /rumpkern	- kernel core, e.g. syscall, interrupt and lock support

  /rumpdev	- device support, e.g. autoconf subsystem
  /rumpnet	- networking support and sockets layer
  /rumpvfs	- file system support

sys/rump/include
  /machine - used for architectures where the rump kernel ABI is not yet the
	     same as the kernel module ABI.  will eventually disappear
	     completely
  /rump    - kernel headers installed to userspace

sys/rump/dev - device components, e.g. audio, raidframe, usb drivers

sys/rump/fs - file system components
  /lib/lib${fs}  - kernel file system code

sys/rump/net - networking components
  /lib/libnet	  - subroutines from sys/net, e.g. route and if_ethersubr
  /lib/libnetinet - TCP/IP
  /lib/libvirtif  - a virtual interface which uses host tap(4) to shovel
		    packets.  This is used by netinet and if_ethersubr.
  /lib/libshmif   - a virtual interface which uses a memory mapped file
		    as an ethernet bus.  works completely unprivileged.
  /lib/libsockin  - implements PF_INET using host kernel sockets.  This is
		    mutually exclusive with net, netinet and virtif.



The rest are out-of-kernel components (i.e. no -D_KERNEL).

hypercall interface:
src/lib/librumpuser
  The "rumpuser" hypercall interfaces are used by a rump kernel to
  access host resources.

remote client interface:
src/lib/librumpclient
  The rumpclient library provides remote access to rump kernel servers.

system call hijacking:
src/lib/librumphijack
  The rumphijack library allows intercepting system calls and redirecting
  them to a rump kernel server instead of the host kernel.  In other
  words, it allows existing binaries to request indicated services from
  a rump kernel instead of from the host kernel.

Users:
src/lib
  /libp2k  - puffs-to-vfs adaption layer, userspace namespace
  /libukfs - user kernel file system, a library to access file system
	     images (or devices) directly in userspace without going
	     through a system call and puffs.  It provides a slightly
	     higher interface than syscalls.

src/usr.sbin/puffs
  rump_$fs - userspace file system daemons using the kernel fs code

src/share/examples/rump
  Various examples detailing use of rump kernels in different scenarios.
  These are provided source-only.
	$NetBSD: README.compileopts,v 1.14 2016/01/25 00:24:23 pooka Exp $

This file describes compile-time options for rump kernels.  Additionally,
NetBSD build options will have an effect.  See src/share/mk/bsd.README
for a desciption of NetBSD build options.

Note: after changing an option, do a clean build.

Global options:


    RUMP_DIAGNOSTIC

values:	yes|no
defval:	yes
effect:	Iff "yes", build with -DDIAGNOSTIC.


    RUMP_DEBUG

values:	<defined> / <undefined>
defval:	<undefined>
effect:	Iff defined, build with -DDEBUG.


    RUMP_LOCKDEBUG

values:	<defined> / <undefined>
defval:	<undefined>
effect:	Iff defined, build with -DLOCKDEBUG.


    RUMP_KTRACE

values:	yes|no
defval:	yes
effect:	Iff "yes", build with -DKTRACE.


    RUMP_LOCKS_UP

values: yes|no
defval:	no
effect: If "yes", build rump kernel with uniprocess-optimized locking.
	An implication of this is that RUMP_NCPU==1 is required at
	runtime.  If "no", build with multiprocessor-capable locking.


    RUMP_VIRTIF

values:	yes|no
defval:	yes
effect:	Iff "yes", build the virt(4) network interface.  Turning this
	off may be necessary on systems that lack the necessary headers,
	e.g. musl libc based Linux.


    RUMP_CURLWP

values: hypercall/__thread/register or <undefined>
defval: <undefined>
effect: Control how curlwp is obtained in a rump kernel.  This is
	a very frequently accessed thread-local variable, and optimizing
	access has a significant performance impact.  Note that all
	options are not available on hosts/machine architectures.
	<undefined> - use default implementation (currently "hypercall")
	hypercall   - use a hypercall to fetch the value
	__thread    - use the __thread feature to fetch value via TLS
	register    - use a dedicated register (implies -ffixed)


    RUMP_NBCOMPAT

values: comma-separated list of releases; e.g. "60,70";
	or "all" or "default" or "none".  Currently default == all (but
	might not be so in the future)
defval:	all
effect:	Builds NetBSD COMPAT_nn code for each of the elements in the list.
	This option is useful only when building rump kernels for
	NetBSD userspace, and an empty value may be supplied elsewhere.


================================================================================

Per-component options:

    RUMP_SYM_NORENAME

values: regexp matching symbol names
defval: <undefined>
effect: Causes matching symbols from the component to not be renamed
	into the rump kernel symbol namespace (rumpns_).  This option
	can only be used in embedded environments where there is full
	control over the platform's namespace.	Conversely, this option
	cannot be used in kernel components which are not meant to be
	tied to a specific platform.  Note: the value is processed by
	make and must be appropriately escaped.  example:
	RUMP_SYM_NORENAME=HYPERVISOR_|block$$
	will not rename "^HYPERVISOR_" or "^block$"


================================================================================


The rest of the options described in this file are not intended to be
set by users, but by the package building rump kernels.


    RUMP_KERNEL_IS_LIBC

values:	defined / not defined
effect: Iff defined, export normal system call symbols from libc.
	For example, without this option rump_sys_open() is exported.
	With this option, both open() and rump_sys_open() are exported.
	This option is meant for building systems where a rump kernel
	is the only operating system like component.


    RUMP_LDSCRIPT

values: no/GNU/sun/ctor
defval: GNU
effect: Select the linker script to be used for linking rump kernel shared
	library components.
	no	- do not use a linker script
	GNU	- use a linker script for GNU ld 2.18 and later
	sun	- use a linker script for the Solaris linker
	ctor	- do not use a linker script, make the code
		  generate __attribute__((constructor))
	$NetBSD: README,v 1.1 2016/01/26 23:12:17 pooka Exp $

This directory contains files which are kernel-internal interfaces
provided by rump kernels.  Like any other NetBSD kernel interfaces,
they may change as long as __NetBSD_Version__ is bumped at the
same time.
NCR 53C80/53C400 driver

BACKGROUND
----------
The NCR 53C80 SCSI Bus Controller (SBC) is an early single-chip solution
which formed the basis of many early SCSI host adapters for both the
i386 and m68k platforms. The NCR 53C400 is a slightly more advanced
chip which retains backward compatibility with the 53C80.

On the PC, the NCR 53C80 was most commonly used to implement simple, cheap
SCSI host adapters that were bundled with tape and CD-ROM drives. Since
these controllers were not bus-mastering (and in some cases were not even
interrupt-driven), they (like IDE adapters) required the CPU to perform
much of the actual processing. These days, these controllers are cheap
and plentiful since many are not supported by Windows 95.

Similarly, NetBSD, although it has had an MI 53C80 driver (used by the
Sun3 and Mac68k ports) for some time, has not had a i386 driver.

Until now, that is...

OVERVIEW
--------
The NCR 53C80/53C400 driver (the 'nca' device) consists of two pieces:

	1) Patches for the 53C80 MI driver to make it use bus_space()
	   functions. (This requires an optional define. By default,
	   the driver will compile in "legacy" memory-mapped mode.

	2) A machine-dependent driver (nca) containing probe and
	   attachment routines.

This driver has bene tested with the following adapters:

	NCS-250 (Chinon)	53C80, port-mapped, polled-mode
				(This is used in my primary development
				box to drive an external Zip drive.)
	Sumo SCSI-AT		53C80, port-mapped, interrupt driven
				(Note: This is an odd card in that its
				own firmware seems to have trouble detecting
				attached drives. Under NetBSD, however,
				it operates with no problems.)
	Trantor T-160		53C400, port-mapped, interrupt driven
				This card was often bundled with NEC
				CD-ROM drives. (My standalone test box
				is using this as its primary adapter.)
	DTC 3150V		53C400, memory-mapped, interrupt driven
				This a simple card designed to drive
				a CD-ROM.

CONFIGURATION
-------------
To setup the nca driver, the configuration file must contain the following:

	options	NCR5380_USE_BUS_SPACE

This line is required to add bus_space() compatibility to the MI driver.

Next you need to add one or more configuration lines for the nca devices:

	nca0	at isa? port 0x360 irq 15
	nca1	at isa? iomem 0xd8000 irq 5

The first is for a port-mapped controller at 0x360, IRQ 15. The second line
is for a memory-mapped controller (Trantor T128 or equivalent) at
0xd800-0xdff, IRQ 5.

You can also set up the driver in "polled" mode (i.e., no interrupts) by
leaving off the "irq" portion of the line:

	nca0	at isa? port 0x360
	nca1	at isa? iomem 0xd8000

Lastly, you need to add a scsibus attachment line for the nca device:

	scsibus* at nca?

The following is the probe output from my test system:

	Copyright (c) 1996, 1997, 1998
	    The NetBSD Foundation, Inc.  All rights reserved.
	Copyright (c) 1982, 1986, 1989, 1991, 1993
	    The Regents of the University of California.  All rights reserved.

	NetBSD 1.3.2 (GENERIC) #2: Sun Oct  4 17:11:43 EDT 1998
	    root@hefalump:/usr/src/sys/arch/i386/compile/GENERIC
	cpu0: Intel 486DX (486-class)
	real mem  = 7995392
	avail mem = 5349376
	using 123 buffers containing 503808 bytes of memory
	mainbus0 (root)
	isa0 at mainbus0
	com1 at isa0 port 0x2f8-0x2ff irq 3: ns8250 or ns16450, no fifo
	com2 at isa0 port 0x3e8-0x3ef irq 5: ns8250 or ns16450, no fifo
	lpt0 at isa0 port 0x378-0x37b irq 7
	nca0 at isa0 port 0x360-0x36f irq 15
	nca0: NCR 53C400 detected
	scsibus0 at nca0: 8 targets
	sd0 at scsibus0 targ 0 lun 0: <HP, C2235, 0B11> SCSI2 0/direct fixed
	sd0: 402MB, 1574 cyl, 9 head, 58 sec, 512 bytes/sect x 825012 sectors
	cd0 at scsibus0 targ 6 lun 0: <CHINON, CD-ROM CDS-535, Q20> SCSI2 5/cdrom removable
	nca1 at isa0 iomem 0xdb878-0xdb887 irq 5
	nca1: NCR 53C400 detected
	scsibus1 at nca1: 8 targets
	sd1 at scsibus1 targ 5 lun 0: <IOMEGA, ZIP 100, J.02> SCSI2 0/direct removable
	sd1: 96MB, 96 cyl, 64 head, 32 sec, 512 bytes/sect x 196608 sectors
	npx0 at isa0 port 0xf0-0xff: using exception 16
	pc0 at isa0 port 0x60-0x6f irq 1: color
	pc0: console
	fdc0 at isa0 port 0x3f0-0x3f7 irq 6 drq 2
	fd0 at fdc0 drive 0: 1.44MB, 80 cyl, 2 head, 18 sec
	biomask 8060 netmask 8460 ttymask 84e2
	boot device: sd0
	root on sd0a dumps on sd0b
	root file system type: ffs

In this output, nca0 is a Trantor T-160 and nca1 is a DTC 3150V. Both happen
to be 53C400-based controllers.

LIMITATIONS
-----------
As of this writing, the nca driver has two known limitations:

1) No DMA or pseudo-DMA support

This is unfortunate, but may be remedied in a later release. I would welcome
any help by someone more familiar with DMA, particularly in relation to
bus_space().

As it is, however, performance of the nca driver is acceptable, though some
of that may depend on one's definition of "acceptable". Remember that these
were not high speed controller under the best conditions, so much of it is
really the nature of the beast. It should be adequate for tapes, CD-ROMS,
and low-usage disk devices (e.g., Zip drives). If you want to drive a CD-R
drive, then invest in an Adaptec 154X or a PCI controller.

2) No support for the SCSI port of the Pro AudioStudio 16.

This is also unfortunate and may not be able to be remedied withing the
current framework of the bus_space() functions and the nca driver.

The problem is this: In most adapters, the eight 53C80 registers are mapped
to eight sequential locations, either ports or memory addresses. On the
PAS-16, however, the registers are mapped to two sets of ports- four
sequential ports at the base address and four sequential ports located
0x2000 higher. As I currently understand it, this is not supportable by
the current bus_space() implementation nor is it possible for the driver
to allocate a second bus_space_tag and _handle itself to accommodate the
second set of ports. Without either, it is very difficult to imagine how
a portable linkage to the MI driver could be made.

Again, I welcome suggestions.

HISTORY
-------
An nca driver first appeared in FreeBSD.

This particular one borrows a little code from it and some from the i386
'esp' and sun3 'si' drivers. It, like many things in the free unix world,
was written because it solved a problem- mine! In my case, it was a need
of a SCSI card and a lack of IRQs. The good news was that I had one
(NCS-250); the bad news was that it was not supported under NetBSD. The
rest is history.

DISCLAIMER
----------
Like most things, you should take this code with a grain of salt. I have
tried to test it sufficiently, but it is always possible that it is not
compatible with some aspect of your system. If you end up suffering
massive data loss and destruction, you have my sympathies, but I do not
and will not allow myself to be held responsible.

CREDITS
-------
My thanks to Jason Thorpe and the rest of the NetBSD team for making it
so easy to write this driver. My thanks also to the authors of the
FreeBSD nca driver for inspiration and 53C400 support.

In the end, I hope that someone else can find this driver as useful as I
have. If so, please drop me a line at jruschme@exit109.com and let me
know about it.

Share and enjoy

John Ruschmeyer (jruschme@exit109.com)
11 October 1998
$NetBSD: README,v 1.3 1998/08/15 03:02:46 mycroft Exp $

This directory contains files which are used during PCI configuration
and PCI device drivers.  Eventually, most of the device drivers and
some of the configuration support should become machine-independent
and be moved to a more general location.

The configuration support was implemented according to the `PCI Local
Bus Specification, Production Version, Revision 2.0' dated April 30,
1993.  Section numbers referred to in the code may be specific to that
edition of the specification.

Some attempt has been made to insure that the code works on rogue
machines where the BIOS doesn't do its job, but in general I can't
guarantee that.

--
- Charles M. Hannum
  NetBSD group
  August 8, 1994
README FOR BT848/BT878 DRIVER
Updated 20th March 2000

  Roger Hardiman
  roger@cs.strath.ac.uk
  roger@freebsd.org
  http://www.telepresence.strath.ac.uk/bt848
  http://www.freebsd.org/~roger

Introduction
------------

The BKTR driver is a driver for Bt848 and Bt878 based Video Capture
cards and TV Tuner Cards.
The driver was written by Amancio Hasty for FreeBSD but is now
maintained by Roger Hardiman.

The driver has been ported from FreeBSD to NetBSD, OpenBSD and BSD/OS.
There is also a Linux version on an old version.


Please read the README file for your specific Operating System for
more information.

Thanks
Roger
--
Roger Hardiman

$SourceForge: README,v 1.2 2003/03/11 23:11:16 thomasklausner Exp $
$NetBSD: README,v 1.1 1999/12/17 01:41:52 ad Exp $

This code is depreciated. Use wscons instead.
#	$NetBSD: README.seagate,v 1.5 2005/12/11 12:22:02 christos Exp $

The hardware:

The ST01/02, and Future Domain 950 are very simple SCSI controllers. They are
not busmastering, so the processor must do all transfers a la IDE. They support
blind transfer by adding wait states (up to a certain limit). Interrupt is
generated for reconnect and parity errors (maybe also for some other events).

The card consists of one command port that writes to scsi control lines, reads
status lines, and a data port that read/writes to the 8 data lines. The address
decoding gives both data and control ports large memory areas to a single
port. This is used by the code.

The ST01/02 differs from the FD950 in memory address location and SCSI id.

Probing for the card:

A card is recognized by comparing the BIOS signature with known signatures. A
new card may not be recognized if the BIOS signature has changed. Please send
new signatures to me.

Driver function:

A scsi command is sent to scsi_cmd function. The command is either placed in
the queue or an retryable message is returned. The routine may wait for
completion of the command depending on the supplied flags. A timer is started
for every command placed in the queue. The commands are added in the order they
are received. There is a possiblity to make all REQUEST SENSE commands be
queued before all other commands, but I dont think it is a good thing (Linux
do however use this).

The card is mostly controlled by the sea_main function. It is called by
scsi_cmd, the interrupt routine, and the timeout routine. The sea_main routine
runs as long there are something to do (transfer data, issue queued commands,
and handle reconnected commands).

The data transfers may be done in two different ways: Blind and polled
transfers. They differ in the way the driver does it handshaking with the
target. During a blind transfer, the driver code blindly transfers a block
of data without checking for changed phase. During polled transfers, the
phase is checked between every character transfered. The polled transfers
are always used for status information and command transfers.

Because the card does not use DMA in any way, there is no need to handle
physical addresses. There is no problem with the isa-bus address limit of
16MB, making bounce-buffers unnecessary.

The data structures:

Every card has a sea_softc structure keeping the queues of commands waiting to
be issued, and commands currently disconnected. The type of card (Seagate or
Future Domain), data and control port addresses, scsi id, busy flags for all
possible targets, and interrupt vector for the card.

Every scsi command to be issued are stored in a sea_scb structure. It contains
a flag describing status/error of the command, current data buffer position,
and number of bytes remaining to be transfered.


PROBLEMS

I have had problems getting the ST02 boot using the boot floppies. I think it
is some problem with BIOS calls not working. It is unfortunately impossible to
disconnect the ST02 floppy controller.

I have had problem to get the driver talk to a 40 MB Seagate disk. I dont have
access to it any more, so I can't do any more checks on that.

NOTE: The ST02 creates its own description of the disk attached. This is not
the same as the disk says. This translation problem may cause problems when
sharing a disk between both DOS and BSD. It is however not impossible.
$NetBSD: README,v 1.6 2013/12/02 14:05:51 tsutsui Exp $

This directory contains `rasops', a set of raster operations intended to
replace the dev/rcons/raster stuff for both wscons and rcons. It yields
significantly improved performance, supports multiple depths and color.

Issues/TODO:

- There is no generic `putchar' function for 2bpp
- Color handling for 2bpp is broken
- 64-bit types are not used on machines that are 64-bit
- We should never be doing reads/writes of less than 32-bits
- Flags in attribute values are hardcoded
- Need a manpage
- Should handle multiple fonts simulatneously
- Generate an `empty' box character when we have no match?
- Use 'int' in lieu of 'int32' where we can
- Compress some cases in rasops1.c
#	$NetBSD: README,v 1.6 2014/01/18 09:51:08 apb Exp $

This is the Intel ACPI Component Architecture, Intel's reference
implementation of the core operating system ACPI support.  The
portion in the acpica/ subdirectory is provided by the operating
system as the glue between the OS and the ACPICA.

Please, do not import an updated ACPICA snapshot from Intel unless
you absolutely know what you're doing -- The Intel directory layout
changes from release to release, and we must munge it (by hand) into
something sane that we can use. The current version of munge is:

	mv source/* .
	rmdir source
	mv components/* .
	rmdir components

Please also update ACPI_DATE in include/acapps.h.

The routines that the operating system must provide are documented
in the following document:

	Intel Corp., ACPI Component Architecture Programmer Reference

Copies of the document may be retrieved from:

	http://www.acpica.org/download/acpica-reference.pdf

Structure:

sys/external/bsd/acpica/dist	The imported source
sys/external/bsd/acpica/conf	The config glue
sys/dev/acpi			Device drivers
sys/dev/acpica			OS dependent functions that are required
$NetBSD: README,v 1.1 2017/06/16 21:45:05 jmcneill Exp $

The headers in this directory are mostly either GPLv2 or GPLv2/X11 dual
licensed. To be safe, DO NOT include dt-bindings headers from kernel sources.

These headers are only meant to be used when generating .dtb files from .dts
sources.
Compiler-RT
================================

This directory and its subdirectories contain source code for the compiler
support routines.

Compiler-RT is open source software. You may freely distribute it under the
terms of the license agreement found in LICENSE.txt.

================================

Compiler-RT
================================

This directory and its subdirectories contain source code for the compiler
support routines.

Compiler-RT is open source software. You may freely distribute it under the
terms of the license agreement found in LICENSE.txt.

================================

This is a replacement library for libgcc.  Each function is contained
in its own file.  Each function has a corresponding unit test under
test/Unit.

A rudimentary script to test each file is in the file called
test/Unit/test.

Here is the specification for this library:

http://gcc.gnu.org/onlinedocs/gccint/Libgcc.html#Libgcc

Here is a synopsis of the contents of this library:

typedef      int si_int;
typedef unsigned su_int;

typedef          long long di_int;
typedef unsigned long long du_int;

// Integral bit manipulation

di_int __ashldi3(di_int a, si_int b);      // a << b
ti_int __ashlti3(ti_int a, si_int b);      // a << b

di_int __ashrdi3(di_int a, si_int b);      // a >> b  arithmetic (sign fill)
ti_int __ashrti3(ti_int a, si_int b);      // a >> b  arithmetic (sign fill)
di_int __lshrdi3(di_int a, si_int b);      // a >> b  logical    (zero fill)
ti_int __lshrti3(ti_int a, si_int b);      // a >> b  logical    (zero fill)

si_int __clzsi2(si_int a);  // count leading zeros
si_int __clzdi2(di_int a);  // count leading zeros
si_int __clzti2(ti_int a);  // count leading zeros
si_int __ctzsi2(si_int a);  // count trailing zeros
si_int __ctzdi2(di_int a);  // count trailing zeros
si_int __ctzti2(ti_int a);  // count trailing zeros

si_int __ffsdi2(di_int a);  // find least significant 1 bit
si_int __ffsti2(ti_int a);  // find least significant 1 bit

si_int __paritysi2(si_int a);  // bit parity
si_int __paritydi2(di_int a);  // bit parity
si_int __parityti2(ti_int a);  // bit parity

si_int __popcountsi2(si_int a);  // bit population
si_int __popcountdi2(di_int a);  // bit population
si_int __popcountti2(ti_int a);  // bit population

uint32_t __bswapsi2(uint32_t a);   // a byteswapped, arm only
uint64_t __bswapdi2(uint64_t a);   // a byteswapped, arm only

// Integral arithmetic

di_int __negdi2    (di_int a);                         // -a
ti_int __negti2    (ti_int a);                         // -a
di_int __muldi3    (di_int a, di_int b);               // a * b
ti_int __multi3    (ti_int a, ti_int b);               // a * b
si_int __divsi3    (si_int a, si_int b);               // a / b   signed
di_int __divdi3    (di_int a, di_int b);               // a / b   signed
ti_int __divti3    (ti_int a, ti_int b);               // a / b   signed
su_int __udivsi3   (su_int n, su_int d);               // a / b   unsigned
du_int __udivdi3   (du_int a, du_int b);               // a / b   unsigned
tu_int __udivti3   (tu_int a, tu_int b);               // a / b   unsigned
si_int __modsi3    (si_int a, si_int b);               // a % b   signed
di_int __moddi3    (di_int a, di_int b);               // a % b   signed
ti_int __modti3    (ti_int a, ti_int b);               // a % b   signed
su_int __umodsi3   (su_int a, su_int b);               // a % b   unsigned
du_int __umoddi3   (du_int a, du_int b);               // a % b   unsigned
tu_int __umodti3   (tu_int a, tu_int b);               // a % b   unsigned
du_int __udivmoddi4(du_int a, du_int b, du_int* rem);  // a / b, *rem = a % b  unsigned
tu_int __udivmodti4(tu_int a, tu_int b, tu_int* rem);  // a / b, *rem = a % b  unsigned
su_int __udivmodsi4(su_int a, su_int b, su_int* rem);  // a / b, *rem = a % b  unsigned
si_int __divmodsi4(si_int a, si_int b, si_int* rem);   // a / b, *rem = a % b  signed



//  Integral arithmetic with trapping overflow

si_int __absvsi2(si_int a);           // abs(a)
di_int __absvdi2(di_int a);           // abs(a)
ti_int __absvti2(ti_int a);           // abs(a)

si_int __negvsi2(si_int a);           // -a
di_int __negvdi2(di_int a);           // -a
ti_int __negvti2(ti_int a);           // -a

si_int __addvsi3(si_int a, si_int b);  // a + b
di_int __addvdi3(di_int a, di_int b);  // a + b
ti_int __addvti3(ti_int a, ti_int b);  // a + b

si_int __subvsi3(si_int a, si_int b);  // a - b
di_int __subvdi3(di_int a, di_int b);  // a - b
ti_int __subvti3(ti_int a, ti_int b);  // a - b

si_int __mulvsi3(si_int a, si_int b);  // a * b
di_int __mulvdi3(di_int a, di_int b);  // a * b
ti_int __mulvti3(ti_int a, ti_int b);  // a * b


// Integral arithmetic which returns if overflow

si_int __mulosi4(si_int a, si_int b, int* overflow);  // a * b, overflow set to one if result not in signed range
di_int __mulodi4(di_int a, di_int b, int* overflow);  // a * b, overflow set to one if result not in signed range
ti_int __muloti4(ti_int a, ti_int b, int* overflow);  // a * b, overflow set to
 one if result not in signed range


//  Integral comparison: a  < b -> 0
//                       a == b -> 1
//                       a  > b -> 2

si_int __cmpdi2 (di_int a, di_int b);
si_int __cmpti2 (ti_int a, ti_int b);
si_int __ucmpdi2(du_int a, du_int b);
si_int __ucmpti2(tu_int a, tu_int b);

//  Integral / floating point conversion

di_int __fixsfdi(      float a);
di_int __fixdfdi(     double a);
di_int __fixxfdi(long double a);

ti_int __fixsfti(      float a);
ti_int __fixdfti(     double a);
ti_int __fixxfti(long double a);
uint64_t __fixtfdi(long double input);  // ppc only, doesn't match documentation

su_int __fixunssfsi(      float a);
su_int __fixunsdfsi(     double a);
su_int __fixunsxfsi(long double a);

du_int __fixunssfdi(      float a);
du_int __fixunsdfdi(     double a);
du_int __fixunsxfdi(long double a);

tu_int __fixunssfti(      float a);
tu_int __fixunsdfti(     double a);
tu_int __fixunsxfti(long double a);
uint64_t __fixunstfdi(long double input);  // ppc only

float       __floatdisf(di_int a);
double      __floatdidf(di_int a);
long double __floatdixf(di_int a);
long double __floatditf(int64_t a);        // ppc only

float       __floattisf(ti_int a);
double      __floattidf(ti_int a);
long double __floattixf(ti_int a);

float       __floatundisf(du_int a);
double      __floatundidf(du_int a);
long double __floatundixf(du_int a);
long double __floatunditf(uint64_t a);     // ppc only

float       __floatuntisf(tu_int a);
double      __floatuntidf(tu_int a);
long double __floatuntixf(tu_int a);

//  Floating point raised to integer power

float       __powisf2(      float a, si_int b);  // a ^ b
double      __powidf2(     double a, si_int b);  // a ^ b
long double __powixf2(long double a, si_int b);  // a ^ b
long double __powitf2(long double a, si_int b);  // ppc only, a ^ b

//  Complex arithmetic

//  (a + ib) * (c + id)

      float _Complex __mulsc3( float a,  float b,  float c,  float d);
     double _Complex __muldc3(double a, double b, double c, double d);
long double _Complex __mulxc3(long double a, long double b,
                              long double c, long double d);
long double _Complex __multc3(long double a, long double b,
                              long double c, long double d); // ppc only

//  (a + ib) / (c + id)

      float _Complex __divsc3( float a,  float b,  float c,  float d);
     double _Complex __divdc3(double a, double b, double c, double d);
long double _Complex __divxc3(long double a, long double b,
                              long double c, long double d);
long double _Complex __divtc3(long double a, long double b,
                              long double c, long double d);  // ppc only


//         Runtime support

// __clear_cache() is used to tell process that new instructions have been
// written to an address range.  Necessary on processors that do not have
// a unified instruction and data cache.
void __clear_cache(void* start, void* end);

// __enable_execute_stack() is used with nested functions when a trampoline
// function is written onto the stack and that page range needs to be made
// executable.
void __enable_execute_stack(void* addr);

// __gcc_personality_v0() is normally only called by the system unwinder.
// C code (as opposed to C++) normally does not need a personality function
// because there are no catch clauses or destructors to be run.  But there
// is a C language extension __attribute__((cleanup(func))) which marks local
// variables as needing the cleanup function "func" to be run when the
// variable goes out of scope.  That includes when an exception is thrown,
// so a personality handler is needed.  
_Unwind_Reason_Code __gcc_personality_v0(int version, _Unwind_Action actions,
         uint64_t exceptionClass, struct _Unwind_Exception* exceptionObject,
         _Unwind_Context_t context);

// for use with some implementations of assert() in <assert.h>
void __eprintf(const char* format, const char* assertion_expression,
				const char* line, const char* file);

// for systems with emulated thread local storage
void* __emutls_get_address(struct __emutls_control*);


//   Power PC specific functions

// There is no C interface to the saveFP/restFP functions.  They are helper
// functions called by the prolog and epilog of functions that need to save
// a number of non-volatile float point registers.  
saveFP
restFP

// PowerPC has a standard template for trampoline functions.  This function
// generates a custom trampoline function with the specific realFunc
// and localsPtr values.
void __trampoline_setup(uint32_t* trampOnStack, int trampSizeAllocated, 
                                const void* realFunc, void* localsPtr);

// adds two 128-bit double-double precision values ( x + y )
long double __gcc_qadd(long double x, long double y);  

// subtracts two 128-bit double-double precision values ( x - y )
long double __gcc_qsub(long double x, long double y); 

// multiples two 128-bit double-double precision values ( x * y )
long double __gcc_qmul(long double x, long double y);  

// divides two 128-bit double-double precision values ( x / y )
long double __gcc_qdiv(long double a, long double b);  


//    ARM specific functions

// There is no C interface to the switch* functions.  These helper functions
// are only needed by Thumb1 code for efficient switch table generation.
switch16
switch32
switch8
switchu8

// There is no C interface to the *_vfp_d8_d15_regs functions.  There are
// called in the prolog and epilog of Thumb1 functions.  When the C++ ABI use
// SJLJ for exceptions, each function with a catch clause or destuctors needs
// to save and restore all registers in it prolog and epliog.  But there is 
// no way to access vector and high float registers from thumb1 code, so the 
// compiler must add call outs to these helper functions in the prolog and 
// epilog.
restore_vfp_d8_d15_regs
save_vfp_d8_d15_regs


// Note: long ago ARM processors did not have floating point hardware support.
// Floating point was done in software and floating point parameters were 
// passed in integer registers.  When hardware support was added for floating
// point, new *vfp functions were added to do the same operations but with 
// floating point parameters in floating point registers.

// Undocumented functions

float  __addsf3vfp(float a, float b);   // Appears to return a + b
double __adddf3vfp(double a, double b); // Appears to return a + b
float  __divsf3vfp(float a, float b);   // Appears to return a / b
double __divdf3vfp(double a, double b); // Appears to return a / b
int    __eqsf2vfp(float a, float b);    // Appears to return  one
                                        //     iff a == b and neither is NaN.
int    __eqdf2vfp(double a, double b);  // Appears to return  one
                                        //     iff a == b and neither is NaN.
double __extendsfdf2vfp(float a);       // Appears to convert from
                                        //     float to double.
int    __fixdfsivfp(double a);          // Appears to convert from
                                        //     double to int.
int    __fixsfsivfp(float a);           // Appears to convert from
                                        //     float to int.
unsigned int __fixunssfsivfp(float a);  // Appears to convert from
                                        //     float to unsigned int.
unsigned int __fixunsdfsivfp(double a); // Appears to convert from
                                        //     double to unsigned int.
double __floatsidfvfp(int a);           // Appears to convert from
                                        //     int to double.
float __floatsisfvfp(int a);            // Appears to convert from
                                        //     int to float.
double __floatunssidfvfp(unsigned int a); // Appears to convert from
                                        //     unisgned int to double.
float __floatunssisfvfp(unsigned int a); // Appears to convert from
                                        //     unisgned int to float.
int __gedf2vfp(double a, double b);     // Appears to return __gedf2
                                        //     (a >= b)
int __gesf2vfp(float a, float b);       // Appears to return __gesf2
                                        //     (a >= b)
int __gtdf2vfp(double a, double b);     // Appears to return __gtdf2
                                        //     (a > b)
int __gtsf2vfp(float a, float b);       // Appears to return __gtsf2
                                        //     (a > b)
int __ledf2vfp(double a, double b);     // Appears to return __ledf2
                                        //     (a <= b)
int __lesf2vfp(float a, float b);       // Appears to return __lesf2
                                        //     (a <= b)
int __ltdf2vfp(double a, double b);     // Appears to return __ltdf2
                                        //     (a < b)
int __ltsf2vfp(float a, float b);       // Appears to return __ltsf2
                                        //     (a < b)
double __muldf3vfp(double a, double b); // Appears to return a * b
float __mulsf3vfp(float a, float b);    // Appears to return a * b
int __nedf2vfp(double a, double b);     // Appears to return __nedf2
                                        //     (a != b)
double __negdf2vfp(double a);           // Appears to return -a
float __negsf2vfp(float a);             // Appears to return -a
float __negsf2vfp(float a);             // Appears to return -a
double __subdf3vfp(double a, double b); // Appears to return a - b
float __subsf3vfp(float a, float b);    // Appears to return a - b
float __truncdfsf2vfp(double a);        // Appears to convert from
                                        //     double to float.
int __unorddf2vfp(double a, double b);  // Appears to return __unorddf2
int __unordsf2vfp(float a, float b);    // Appears to return __unordsf2


Preconditions are listed for each function at the definition when there are any.
Any preconditions reflect the specification at
http://gcc.gnu.org/onlinedocs/gccint/Libgcc.html#Libgcc.

Assumptions are listed in "int_lib.h", and in individual files.  Where possible
assumptions are checked at compile time.

DRM README file


There are two main parts to this package: the DRM client library/interface
(libdrm.so) and kernel/hardware-specific device modules (such as radeon.ko).
The kernel device modules are not shipped with libdrm releases and should only
be built from the git tree by developers and bleeding-edge testers of
non-Intel hardware.  The Intel kernel modules are developed in the Linux
kernel tree.


Compiling
---------

By default, libdrm and the DRM header files will install into /usr/local/.
If you want to install this DRM to replace your system copy, say:

	./configure --prefix=/usr --exec-prefix=/

Then,
	make install


To build the device-specific kernel modules from the git tree:

	cd linux-core/
	make
	cp *.ko /lib/modules/VERSION/kernel/drivers/char/drm/
	   (where VERSION is your kernel version: uname -f)

Or,
	cd bsd-core/
	make
	copy the kernel modules to the appropriate place



Tips & Trouble-shooting
-----------------------

1. You'll need kernel sources.  If using Fedora Core 5, for example, you may
   need to install RPMs such as:

	kernel-smp-devel-2.6.15-1.2054_FC5.i686.rpm
	kernel-devel-2.6.15-1.2054_FC5.i686.rpm
	etc.


2. You may need to make a symlink from /lib/modules/VERSION/build to your
   kernel sources in /usr/src/kernels/VERSION (where version is `uname -r`):

	cd /lib/modules/VERSION
	ln -s /usr/src/kernels/VERSION build


3. If you've build the kernel modules but they won't load because of an
   error like this:

	$ /sbin/modprobe drm
	FATAL: Error inserting drm (/lib/modules/2.6.15-1.2054_FC5smp/kernel/drivers/char/drm/drm.ko): Invalid module format

   And 'dmesg|tail' says:

	drm: disagrees about version of symbol struct_module 

   Try recompiling your drm modules without the Module.symvers file.
   That is rm the /usr/src/kernels/2.6.15-1.2054_FC5-smp-i686/Module.symvers
   file (or rename it).  Then do a 'make clean' before rebuilding your drm
   modules.


************************************************************
* For the very latest on DRI development, please see:      *
*     http://dri.freedesktop.org/                          *
************************************************************

The Direct Rendering Manager (drm) is a device-independent kernel-level
device driver that provides support for the XFree86 Direct Rendering
Infrastructure (DRI).

The DRM supports the Direct Rendering Infrastructure (DRI) in four major
ways:

    1. The DRM provides synchronized access to the graphics hardware via
       the use of an optimized two-tiered lock.

    2. The DRM enforces the DRI security policy for access to the graphics
       hardware by only allowing authenticated X11 clients access to
       restricted regions of memory.

    3. The DRM provides a generic DMA engine, complete with multiple
       queues and the ability to detect the need for an OpenGL context
       switch.

    4. The DRM is extensible via the use of small device-specific modules
       that rely extensively on the API exported by the DRM module.


Documentation on the DRI is available from:
    http://dri.freedesktop.org/wiki/Documentation
    http://sourceforge.net/project/showfiles.php?group_id=387
    http://dri.sourceforge.net/doc/

For specific information about kernel-level support, see:

    The Direct Rendering Manager, Kernel Support for the Direct Rendering
    Infrastructure
    http://dri.sourceforge.net/doc/drm_low_level.html

    Hardware Locking for the Direct Rendering Infrastructure
    http://dri.sourceforge.net/doc/hardware_locking_low_level.html

    A Security Analysis of the Direct Rendering Infrastructure
    http://dri.sourceforge.net/doc/security_low_level.html

             IMPORTANT information related to the gnu-efi package
             ----------------------------------------------------
                                June 2001

As of version 3.0, the gnu-efi package is now split in two different packages:

	-> gnu-efi-X.y: contains the EFI library, include files and crt0.

	-> elilo-X.y  : contains the ELILO bootloader. 
	
Note that X.y don't need to match for both packages. However elilo-3.x
requires at least gnu-efi-3.0. EFI support for x86_64 is provided in
gnu-efi-3.0d.

Both packages can be downloaded from:

	http://www.sf.net/projects/gnu-efi
	http://www.sf.net/projects/elilo
	-------------------------------------------------
	Building EFI Applications Using the GNU Toolchain
	-------------------------------------------------

		David Mosberger <davidm@hpl.hp.com>

			23 September 1999


		Copyright (c) 1999-2007 Hewlett-Packard Co.
		Copyright (c) 2006-2010 Intel Co.

Last update: 04/09/2007

* Introduction

This document has two parts: the first part describes how to develop
EFI applications for IA-64,x86 and x86_64 using the GNU toolchain and the EFI
development environment contained in this directory.  The second part
describes some of the more subtle aspects of how this development
environment works.



* Part 1: Developing EFI Applications


** Prerequisites:

 To develop x86 and x86_64 EFI applications, the following tools are needed:

	- gcc-3.0 or newer (gcc 2.7.2 is NOT sufficient!)
	  As of gnu-efi-3.0b, the Redhat 8.0 toolchain is known to work,
	  but the Redhat 9.0 toolchain is not currently supported.

	- A version of "objcopy" that supports EFI applications.  To
	  check if your version includes EFI support, issue the
	  command:

		objcopy --help

	  and verify that the line "supported targets" contains the
	  string "efi-app-ia32" and "efi-app-x86_64". The binutils release
	  binutils-2.17.50.0.14 supports Intel64 EFI.

	- For debugging purposes, it's useful to have a version of
	  "objdump" that supports EFI applications as well.  This
	  allows inspect and disassemble EFI binaries.

 To develop IA-64 EFI applications, the following tools are needed:

	- A version of gcc newer than July 30th 1999 (older versions
	  had problems with generating position independent code).
	  As of gnu-efi-3.0b, gcc-3.1 is known to work well.

	- A version of "objcopy" that supports EFI applications.  To
	  check if your version includes EFI support, issue the
	  command:

		objcopy --help

	  and verify that the line "supported targets" contains the
	  string "efi-app-ia64".

	- For debugging purposes, it's useful to have a version of
	  "objdump" that supports EFI applications as well.  This
	  allows inspect and disassemble EFI binaries.


** Directory Structure

This EFI development environment contains the following
subdirectories:

 inc:   This directory contains the EFI-related include files.  The
	files are taken from Intel's EFI source distribution, except
	that various fixes were applied to make it compile with the
	GNU toolchain.

 lib:   This directory contains the source code for Intel's EFI library.
	Again, the files are taken from Intel's EFI source
	distribution, with changes to make them compile with the GNU
	toolchain.

 gnuefi: This directory contains the glue necessary to convert ELF64
	binaries to EFI binaries.  Various runtime code bits, such as
	a self-relocator are included as well.  This code has been
	contributed by the Hewlett-Packard Company and is distributed
	under the GNU GPL.

 apps:	This directory contains a few simple EFI test apps.

** Setup

It is necessary to edit the Makefile in the directory containing this
README file before EFI applications can be built.  Specifically, you
should verify that macros CC, AS, LD, AR, RANLIB, and OBJCOPY point to
the appropriate compiler, assembler, linker, ar, and ranlib binaries,
respectively.

If you're working in a cross-development environment, be sure to set
macro ARCH to the desired target architecture ("ia32" for x86, "x86_64" for
x86_64 and "ia64" for IA-64).  For convenience, this can also be done from
the make command line (e.g., "make ARCH=ia64").


** Building

To build the sample EFI applications provided in subdirectory "apps",
simply invoke "make" in the toplevel directory (the directory
containing this README file).  This should build lib/libefi.a and
gnuefi/libgnuefi.a first and then all the EFI applications such as a
apps/t6.efi.


** Running

Just copy the EFI application (e.g., apps/t6.efi) to the EFI
filesystem, boot EFI, and then select "Invoke EFI application" to run
the application you want to test.  Alternatively, you can invoke the
Intel-provided "nshell" application and then invoke your test binary
via the command line interface that "nshell" provides.


** Writing Your Own EFI Application

Suppose you have your own EFI application in a file called
"apps/myefiapp.c".  To get this application built by the GNU EFI build
environment, simply add "myefiapp.efi" to macro TARGETS in
apps/Makefile.  Once this is done, invoke "make" in the top level
directory.  This should result in EFI application apps/myefiapp.efi,
ready for execution.

The GNU EFI build environment allows to write EFI applications as
described in Intel's EFI documentation, except for two differences:

 - The EFI application's entry point is always called "efi_main".  The
   declaration of this routine is:

    EFI_STATUS efi_main (EFI_HANDLE image, EFI_SYSTEM_TABLE *systab);

 - UNICODE string literals must be written as W2U(L"Sample String")
   instead of just L"Sample String".  The W2U() macro is defined in
   <efilib.h>.  This header file also declares the function W2UCpy()
   which allows to convert a wide string into a UNICODE string and
   store the result in a programmer-supplied buffer.

 - Calls to EFI services should be made via uefi_call_wrapper(). This
   ensures appropriate parameter passing for the architecture.


* Part 2: Inner Workings

WARNING: This part contains all the gory detail of how the GNU EFI
toolchain works.  Normal users do not have to worry about such
details.  Reading this part incurs a definite risk of inducing severe
headaches or other maladies.

The basic idea behind the GNU EFI build environment is to use the GNU
toolchain to build a normal ELF binary that, at the end, is converted
to an EFI binary.  EFI binaries are really just PE32+ binaries.  PE
stands for "Portable Executable" and is the object file format
Microsoft is using on its Windows platforms.  PE is basically the COFF
object file format with an MS-DOS2.0 compatible header slapped on in
front of it.  The "32" in PE32+ stands for 32 bits, meaning that PE32
is a 32-bit object file format.  The plus in "PE32+" indicates that
this format has been hacked to allow loading a 4GB binary anywhere in
a 64-bit address space (unlike ELF64, however, this is not a full
64-bit object file format because the entire binary cannot span more
than 4GB of address space).  EFI binaries are plain PE32+ binaries
except that the "subsystem id" differs from normal Windows binaries.
There are two flavors of EFI binaries: "applications" and "drivers"
and each has there own subsystem id and are identical otherwise.  At
present, the GNU EFI build environment supports the building of EFI
applications only, though it would be trivial to generate drivers, as
the only difference is the subsystem id.  For more details on PE32+,
see the spec at

	http://msdn.microsoft.com/library/specs/msdn_pecoff.htm.

In theory, converting a suitable ELF64 binary to PE32+ is easy and
could be accomplished with the "objcopy" utility by specifying option
--target=efi-app-ia32 (x86) or --target=efi-app-ia64 (IA-64).  But
life never is that easy, so here some complicating factors:

 (1) COFF sections are very different from ELF sections.

	ELF binaries distinguish between program headers and sections.
	The program headers describe the memory segments that need to
	be loaded/initialized, whereas the sections describe what
	constitutes those segments.  In COFF (and therefore PE32+) no
	such distinction is made.  Thus, COFF sections need to be page
	aligned and have a size that is a multiple of the page size
	(4KB for EFI), whereas ELF allows sections at arbitrary
	addresses and with arbitrary sizes.

 (2) EFI binaries should be relocatable.

	Since EFI binaries are executed in physical mode, EFI cannot
	guarantee that a given binary can be loaded at its preferred
	address.  EFI does _try_ to load a binary at it's preferred
	address, but if it can't do so, it will load it at another
	address and then relocate the binary using the contents of the
	.reloc section.

 (3) On IA-64, the EFI entry point needs to point to a function
     descriptor, not to the code address of the entry point.

 (4) The EFI specification assumes that wide characters use UNICODE
     encoding.

	ANSI C does not specify the size or encoding that a wide
	character uses.  These choices are "implementation defined".
	On most UNIX systems, the GNU toolchain uses a wchar_t that is
	4 bytes in size.  The encoding used for such characters is
	(mostly) UCS4.

In the following sections, we address how the GNU EFI build
environment addresses each of these issues.


** (1) Accommodating COFF Sections

In order to satisfy the COFF constraint of page-sized and page-aligned
sections, the GNU EFI build environment uses the special linker script
in gnuefi/elf_$(ARCH)_efi.lds where $(ARCH) is the target architecture
("ia32" for x86, "x86_64" for x86_64 and "ia64" for IA-64).
This script is set up to create only eight COFF section, each page aligned
and page sized.These eight sections are used to group together the much
greater number of sections that are typically present in ELF object files.
Specifically:

 .hash
	Collects the ELF .hash info (this section _must_ be the first
	section in order to build a shared object file; the section is
	not actually loaded or used at runtime).

 .text
	Collects all sections containing executable code.

 .data
	Collects read-only and read-write data, literal string data,
	global offset tables, the uninitialized data segment (bss) and
	various other sections containing data.

	The reason read-only data is placed here instead of the in
	.text is to make it possible to disassemble the .text section
	without getting garbage due to read-only data.  Besides, since
	EFI binaries execute in physical mode, differences in page
	protection do not matter.

	The reason the uninitialized data is placed in this section is
	that the EFI loader appears to be unable to handle sections
	that are allocated but not loaded from the binary.

 .dynamic, .dynsym, .rela, .rel, .reloc
	These sections contains the dynamic information necessary to
	self-relocate the binary (see below).

A couple of more points worth noting about the linker script:

 o On IA-64, the global pointer symbol (__gp) needs to be placed such
   that the _entire_ EFI binary can be addressed using the signed
   22-bit offset that the "addl" instruction affords.  Specifically,
   this means that __gp should be placed at ImageBase + 0x200000.
   Strictly speaking, only a couple of symbols need to be addressable
   in this fashion, so with some care it should be possible to build
   binaries much larger than 4MB.  To get a list of symbols that need
   to be addressable in this fashion, grep the assembly files in
   directory gnuefi for the string "@gprel".

 o The link address (ImageBase) of the binary is (arbitrarily) set to
   zero.  This could be set to something larger to increase the chance
   of EFI being able to load the binary without requiring relocation.
   However, a start address of 0 makes debugging a wee bit easier
   (great for those of us who can add, but not subtract... ;-).

 o The relocation related sections (.dynamic, .rel, .rela, .reloc)
   cannot be placed inside .data because some tools in the GNU
   toolchain rely on the existence of these sections.

 o Some sections in the ELF binary intentionally get dropped when
   building the EFI binary.  Particularly noteworthy are the dynamic
   relocation sections for the .plabel and .reloc sections.  It would
   be _wrong_ to include these sections in the EFI binary because it
   would result in .reloc and .plabel being relocated twice (once by
   the EFI loader and once by the self-relocator; see below for a
   description of the latter).  Specifically, only the sections
   mentioned with the -j option in the final "objcopy" command are
   retained in the EFI binary (see apps/Makefile).


** (2) Building Relocatable Binaries

ELF binaries are normally linked for a fixed load address and are thus
not relocatable.  The only kind of ELF object that is relocatable are
shared objects ("shared libraries").  However, even those objects are
usually not completely position independent and therefore require
runtime relocation by the dynamic loader.  For example, IA-64 binaries
normally require relocation of the global offset table.

The approach to building relocatable binaries in the GNU EFI build
environment is to:

 (a) build an ELF shared object

 (b) link it together with a self-relocator that takes care of
     applying the dynamic relocations that may be present in the
     ELF shared object

 (c) convert the resulting image to an EFI binary

The self-relocator is of course architecture dependent.  The x86
version can be found in gnuefi/reloc_ia32.c, the x86_64 version
can be found in gnuefi/reloc_x86_64.c and the IA-64 version can be
found in gnuefi/reloc_ia64.S.

The self-relocator operates as follows: the startup code invokes it
right after EFI has handed off control to the EFI binary at symbol
"_start".  Upon activation, the self-relocator searches the .dynamic
section (whose starting address is given by symbol _DYNAMIC) for the
dynamic relocation information, which can be found in the DT_REL,
DT_RELSZ, and DT_RELENT entries of the dynamic table (DT_RELA,
DT_RELASZ, and DT_RELAENT in the case of rela relocations, as is the
case for IA-64).  The dynamic relocation information points to the ELF
relocation table.  Once this table is found, the self-relocator walks
through it, applying each relocation one by one.  Since the EFI
binaries are fully resolved shared objects, only a subset of all
possible relocations need to be supported.  Specifically, on x86 only
the R_386_RELATIVE relocation is needed.  On IA-64, the relocations
R_IA64_DIR64LSB, R_IA64_REL64LSB, and R_IA64_FPTR64LSB are needed.
Note that the R_IA64_FPTR64LSB relocation requires access to the
dynamic symbol table.  This is why the .dynsym section is included in
the EFI binary.  Another complication is that this relocation requires
memory to hold the function descriptors (aka "procedure labels" or
"plabels").  Each function descriptor uses 16 bytes of memory.  The
IA-64 self-relocator currently reserves a static memory area that can
hold 100 of these descriptors.  If the self-relocator runs out of
space, it causes the EFI binary to fail with error code 5
(EFI_BUFFER_TOO_SMALL).  When this happens, the manifest constant
MAX_FUNCTION_DESCRIPTORS in gnuefi/reloc_ia64.S should be increased
and the application recompiled.  An easy way to count the number of
function descriptors required by an EFI application is to run the
command:

  objdump --dynamic-reloc example.so | fgrep FPTR64 | wc -l

assuming "example" is the name of the desired EFI application.


** (3) Creating the Function Descriptor for the IA-64 EFI Binaries

As mentioned above, the IA-64 PE32+ format assumes that the entry
point of the binary is a function descriptor.  A function descriptors
consists of two double words: the first one is the code entry point
and the second is the global pointer that should be loaded before
calling the entry point.  Since the ELF toolchain doesn't know how to
generate a function descriptor for the entry point, the startup code
in gnuefi/crt0-efi-ia64.S crafts one manually by with the code:

	        .section .plabel, "a"
	_start_plabel:
	        data8   _start
	        data8   __gp

this places the procedure label for entry point _start in a section
called ".plabel".  Now, the only problem is that _start and __gp need
to be relocated _before_ EFI hands control over to the EFI binary.
Fortunately, PE32+ defines a section called ".reloc" that can achieve
this.  Thus, in addition to manually crafting the function descriptor,
the startup code also crafts a ".reloc" section that has will cause
the EFI loader to relocate the function descriptor before handing over
control to the EFI binary (again, see the PECOFF spec mentioned above
for details).

A final question may be why .plabel and .reloc need to go in their own
COFF sections.  The answer is simply: we need to be able to discard
the relocation entries that are generated for these sections.  By
placing them in these sections, the relocations end up in sections
".rela.plabel" and ".rela.reloc" which makes it easy to filter them
out in the filter script.  Also, the ".reloc" section needs to be in
its own section so that the objcopy program can recognize it and can
create the correct directory entries in the PE32+ binary.


** (4) Convenient and Portable Generation of UNICODE String Literals

As of gnu-efi-3.0, we make use (and somewhat abuse) the gcc option
that forces wide characters (WCHAR_T) to use short integers (2 bytes) 
instead of integers (4 bytes). This way we match the Unicode character
size. By abuse, we mean that we rely on the fact that the regular ASCII
characters are encoded the same way between (short) wide characters 
and Unicode and basically only use the first byte. This allows us
to just use them interchangeably.

The gcc option to force short wide characters is : -fshort-wchar

			* * * The End * * *

The files in the "lib" and "inc" subdirectories are using the EFI Application 
Toolkit distributed by Intel at http://developer.intel.com/technology/efi

This code is covered by the following agreement:

Copyright (c) 1998-2000 Intel Corporation

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

Redistributions of source code must retain the above copyright notice, this list of conditions and
the following disclaimer.

Redistributions in binary form must reproduce the above copyright notice, this list of conditions
and the following disclaimer in the documentation and/or other materials provided with the
distribution.

THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE. THE EFI SPECIFICATION AND ALL OTHER INFORMATION
ON THIS WEB SITE ARE PROVIDED "AS IS" WITH NO WARRANTIES, AND ARE SUBJECT
TO CHANGE WITHOUT NOTICE.
There are no CVS/SVN ids in sljit repository but don't forget to
remove .svn before importing the new version.

Current sljit import is @ r313:

svn co https://svn.code.sf.net/p/sljit/code@r313 dist

                     SLJIT - Stack Less JIT Compiler

Purpose:
  A simple, machine independent JIT compiler, which suitable for
  translating interpreted byte code to machine code. The sljitLir.h
  describes the LIR (low-level intermediate representation) of SLJIT.

Compatible:
  Any C (C++) compiler. At least I hope so.

Using sljit:
  Copy the content of sljit_src directory into your project source directory.
  Add sljitLir.c source file to your build environment. All other files are
  included by sljitLir.c (if required). Define the machine by SLJIT_CONFIG_*
  selector. See sljitConfig.h for all possible values. For C++ compilers,
  rename sljitLir.c to sljitLir.cpp.

More info:
  http://sljit.sourceforge.net/

Contact:
  hzmester@freemail.hu

Special thanks:
  Alexander Nasonov
  Daniel Richard G.
  Giuseppe D'Angelo
  Jiong Wang (TileGX support)
  Michael McConville
  Walter Lee
  Wen Xichang
These files were contributed by Wen Xichang.

Copyright 2015 Wen Xichang (wenxichang@163.com). All rights reserved.
$NetBSD: README.port,v 1.4 2006/11/24 22:52:16 wiz Exp $

What to Look for when Porting the IPKDB Interface
===============================================

Try to avoid calling any routine from the rest of the kernel.
(It's OK to call these routines during initialization).
You wouldn't be able to set breakpoints within these routines
during debugging, since this would hang the debugging interface.


Interface between IPKDB and Ethernet Board (sys/dev/yy/if_xx.c)
--------------------------------------------------------------

General Considerations


There is a problem when the debugger uses the same ethernet board as
does the rest of the kernel.  For one thing there might arrive packets
destined for the kernel during debugging sessions.  These packets
are currently lost.  For packets on their way out the driver has
to leave the interrupt pending condition alone, so the interrupt
handler gets a chance to send more packets via the interface.


Configuration Files


For any interface that may be used for debugging there should be
an option, put into opt_ipkdb.h, that is enabled if the kernel
shall actually use this interface.  The relevant part of the
"files" file for interface "xx" would look like this:

	defopt	opt_ipkdb.h	IPKDB_XX	: IPKDB
	device	xx: ether, ifnet, arp
	file	dev/zz/if_xx.c		xx | ipkdb_xx

The file dev/zz/if_xx.c contains both the code of the kernel
driver and the IPKDB driver for this interface.  You should
#include "opt_ipkdb.h" in there and conditionalize the
compilation of the IPKDB driver with
#ifdef IPKDB_XX

The appropriate part of the machine configuration would read like
this:

	options 	IPKDBKEY="\"IPKDB key for remote debugging\""
	options 	IPKDB_XX


Driver Code


In order to be able to find the debugging interface, the driver
has to provide an attach routine that the machine dependent code
can call at an appropriate time (see below).  The attach routine
should take as its first argument a pointer to a struct ipkdb_if
plus some additional parameters that allow it to access the
devices registers, hopefully using bus_space_* methods.
In the ipkdb_if structure, the attach routine must initialize
the following fields:

	myenetaddr	fill this with the own ethernet address of
			the device/machine.
	flags		mark at least IPKDB_MYHW here.
	name		Name of the device, only used for a message.
	start		routine called everytime IPKDB is entered.
	leave		routine called everytime IPKDB is left.
	receive		routine called to receive a packet.
	send		routine called to send a packet.

Additional fields that may be set are:

	myinetaddr	fill this with the own internet address,
			and mark IPKDB_MYIP in flags.
	port		may be used as a pointer to some device
			dependent data.  Unused by other code.

The routine should check for existence of the ethernet board.
This routine should also note enough information so it is able
to later find the system driver instance for the same board in
order to coordinate its action with the system driver.

The routine should return 0 on success and -1 on failure.

The remainder of the routines are called via function pointers
in the ipkdb_if structure.  The probe routine needs to fill in
these function pointers with proper values.

void start(struct ipkdb_if *kip)

This routine gets called every time the debugger is entered.
kip is a pointer to the ipkdb_if structure used for debugging.

It should initialize the hardware and software interface.

This routine should also note the current state of the board
(as far as it can) so a later call to leave can reinstantiate
this state.

void leave(struct ipkdb_if *kip)

This routine is called whenever the debugger is left.  It should
restore the ethernet hardware to the state prior to the last call to
start.

int receive(struct ipkdb_if *kip, u_char *buf, int poll)

This routine should return an ethernet packet to the buffer pointed to
by buf and return its length.  The packet should be complete with the
ethernet header, i.e. it starts with the recipient address, but does not
contain the ethernet checksum.

If poll is set, it should return immediately, if no packet is available.
Otherwise it should wait for the next packet.

This routine shall return the number of bytes transferred to buf.

void send(struct ipkdb_if *kip, u_char *buf, int l)

This routine should send an ethernet packet out of the debugging
interface.  The packet is already complete with the ethernet header,
but does not contain the ethernet checksum.


Interface between IPKDB and Machine (sys/arch/xxx/xxx/ipkdb_glue.c)
-----------------------------------------------------------------


void ipkdbinit(void)

This routine gets called when the debugger should be entered for the
first time.

int ipkdb_poll(void)

This routine gets called after a panic to check for a keypress by the user.
If implemented it allows the user to press any key on the console to do
the automatic reboot after a panic.  Otherwise the debugging interface
will wait forever for some remote debugger to attach in case of a panic.

int ipkdbcmds(void)

There should be call to this routine from somewhere in locore when the
trap mechanism determines that the debugger should be entered, i.e. on
a single step or breakpoint interrupt from kernel code.  The trapping
mechanism should already have stored the registers into the global area
ipkdbregs.  The layout of this area must be the same as that expected
by GDB.  The return value of this routine is 0, if the user wants to
continue, 1 if the user wants to do single stepping, and 2 if the user
has detached from debugging.

int ipkdbfbyte(u_char *p)

This routine should fetch a byte from address p. It must not enter any
trap handling code, but instead return -1 on inability to access the data.

void ipkdbsbyte(u_char *p,u_char c)

This routine should set the byte pointed to by p to the value given as c.
The routine must not enter any trap handling code.  Furthermore it should
reset the modification bit in the relevant page table entry to the value
before the store.


sys/arch/xxx/include/ipkdb.h


Machine dependent definitions and protoypes should be in
sys/arch/xxx/include/ipkdb.h, i.e. in <machine/ipkdb.h>.  This includes
the size of the array ipkdbregs, that holds the contents of the registers
of the debuggee at the time IPKDB is entered.
$NetBSD: README,v 1.62 2014/03/31 11:25:48 martin Exp $

acorn26	arm	20000509	Acorn ARM2- and ARM3-based machines
acorn32	arm	20011118	Acorn computers Ltd. ARM 6/7/SA based machines
algor	mipsel,mips64el	20010528	Algorithmics, Ltd. MIPS evaluation boards
alpha	alpha	19950213	Compaq (formerly Digital Equipment Corp.) Alpha
amd64	x86_64	20010619	AMD's x86 64 bit architecture
amiga	m68k	19930902	Commodore et al. Amiga
amigappc	powerpc	20000525	Phase 5 Amiga
arc	mipsel,mips64el	20000123	MIPS Advanced Risc Computing spec machines
atari	m68k	19950326	Atari TT30, Falcon, and Hades
bebox	powerpc	19971014	Be Inc. BeBox
cats	arm	19981005	Chalice Technologies, CATS motherboard
cesfic	m68k	20010514	FIC8234 VME processor board
cobalt	mipsel,mips64el	20000319	Cobalt Networks Microservers
dreamcast	sh3el	20010107	SEGA Dreamcast
emips	mipseb	20110126	Machines based on Extensible MIPS
epoc32	arm	20130502	32bit EPOC OS machines
evbarm	armeb	20010905	ARM-based eval boards
evbmips	mipseb,mipsel,mips64eb,mips64el	20020307	MIPS-based eval boards
evbppc	powerpc,powerpc64	20021209	PowerPC-based eval boards
evbsh3	sh3eb,sh3el	20010206	Hitachi SuperH(TM) sh3 and sh4 eval boards
ews4800mips	mipseb	20051229	NEC's MIPS based EWS4800 workstations
hp300	m68k	19930512	Hewlett Packard 300- and 400-series machines
hppa	hppa	20020606	Hewlett Packard 700-series machines
hpcarm	arm	20010220	ARM based handheld PCs
hpcmips	mipsel	19990925	MIPS based handheld PCs
hpcsh	sh3el	20010117	Hitachi SuperH(TM) based handheld PCs
i386	i386	19930321	Intel/AMD etc. x86 processor line
ia64	ia64	00000000	Intel Itanium/Itanium2 processor based workstations
ibmnws	powerpc	00000000	IBM Network Station Thin Clients
iyonix	arm	20040713	Castle Technology xscale based workstations
landisk	sh3el	20060901	SH4 processor based NAS appliances by I-O DATA
luna68k	m68k	20000105	LUNA product line of OMRON Tateisi Electronics
mac68k	m68k	19930929	Apple Macintosh
macppc	powerpc,powerpc64	19980515	Apple Power Macintosh and clones
mipsco	mipseb	20000812	MIPS Corp Magnum 3000 computers
mmeye	sh3eb	19990913	Hitachi SuperH(TM) computer controlled camera
mvme68k	m68k	19950725	Motorola's VMEbus 68K based single board computers
mvmeppc	powerpc	20020227	Motorola's PowerPC machines running older PPCBUG
netwinder	arm	20010609	StrongARM based Netwinder machines
news68k	m68k	19991209	Sony's m68k based NET WORK STATION
newsmips	mipseb	19980218	Sony's MIPS based NET WORK STATION
next68k	m68k	19980609	NeXT Computer's cubes and slabs
ofppc	powerpc,powerpc64	19980528	Open Firmware based PowerPC machines
playstation2	mipsel	20011016	Sony PlayStation 2
pmax	mipsel,mips64el	19931012	Digital Equipment Corp. MIPS based machines
prep	powerpc	20000229	PowerPC Reference Platform machines
rs6000	powerpc	20071217	MCA-based IBM RS/6000 wokstations
sandpoint	powerpc	20010211	Motorola Sandpoint based NAS systems
sbmips	mipseb,mipsel,mips64eb,mips64el	20020306	Broadcom's SiByte processor evaluation boards
sgimips	mipseb,mips64eb	20000614	MIPS based Silicon Graphics machines
shark	arm	19960131	Digital Network Appliance Reference Design ("Shark")
sparc	sparc	19931002	Sun Microsystems SPARC (sun4, sun4c, sun4m) 32 bit machines
sparc64	sparc64/sparc	19980620	Sun Microsystems UltraSPARC 64 bit machines
sun2	m68000	20010328	Sun m68010 based machines
sun3	m68k	19930625	Sun m680[23]0 based machines
vax	vax	19940802	Digital Equipment Corp. VAX machines
x68k	m68k	19960505	Sharp X68000, X68030
xen	xen	20040311	Xen virtual machine monitor
zaurus	arm	20061217	Sharp Zaurus C7x0/860/1000/3x00 PDAs

Generic cpu features shared among multiple ports
arm:		ARM CPU based platform files
hppa:		Hewlett Packard PA-RISC CPU based platform files
m68k:		Motorola 680x0 CPU based platform files
mips:		MIPS CPU based platform files
powerpc:	PowerPC CPU based platform files
sh3:		Hitachi SuperH(TM) sh3 and sh4 CPU based platform files
sparc:		Sun Microsystems SPARC(TM) CPU based platform files
x86:		Intel x86 based platform files.

Generic architectural features shared among multiple ports
hpc:		Handheld PC reference platform files
sun68k:		Sun Microsystems Motorola 680x0 CPU based platform files

Single port cpu families
alpha:		Digital Equipment Alpha processor
ia64:		Intel Itanium/Itanium2 processor
$NetBSD: README,v 1.3 2005/12/11 12:17:04 christos Exp $

This is a port of NetBSD to the FIC8234 VME processor board, made by the
swiss company CES (Geneve). These boards are (or have been) popular in
high energy physics data acquisition (think of CERN!). See
http://www.ces.ch/Products/CPUs/FIC8234/FIC8234.html
for some technical data.

The highlights:
- MC68040 processor at 25 MHz (optional dual-processor)
- 8 or 32 MByte RAM
- 2 serial ports on Z85c30
- 79c900 (ILACC) ethernet
- 53c710 SCSI

The port is quite rudimentary at the moment. The kernel is started out of
a running OS-9 system. SCSI support is not present yet, so it only works
diskless with NFS (or ramdisk - not tested) root.
It is good enough for multiuser, self-hosting etc. however.

To start it:
- make OS image by "objcopy --output-target=binary netbsd <imagename>"
- load image to physical address 0x20100000 (RAM start + 1M)
- jump to 0x20100400

For questions and contributions, contact Matthias Drochner
(drochner@NetBSD.org).
$NetBSD: README.evbarm,v 1.16 2017/10/08 18:07:09 jmcneill Exp $

config		date		boards
-------------------------------------------------------------------------------
ADI_BRH		2003/01/25	ADI Eng. Big Read Head i80200 eval board
ARMADILLO210	2006/02/06	Atmark Techno Armadillo-210
ARMADILLO9	2005/11/13	Atmark Techno Armadillo-9
BCM5301X	2012/08/31	Broadcom BCM95301X evaluation/reference board
BEAGLEBOARD	2008/10/22	TI OMAP3530 BeagleBoard
BEAGLEBOARDXM	2008/08/20	TI DM37xx BeagleBoard-XM
BEAGLEBONE	2012/08/20	TI AM335x BeagleBone
BPI		2014/09/07	Banana Pi
CUBIEBOARD	2013/09/03	Cubietech Cubieboard 1 & 2
CUBIETRUCK	2014/04/11	Cubietech Cubietruck
CUBOX		2017/01/07	SolidRun Cubox
CP3100		2006/11/08	Certance IOP321 CP-3100
DEVKIT8000	2010/09/08	Embest OMAP3530 DevKit8000 eval Kit 
DNS323		2010/10/02	D-Link DNS-323 Marvell SoC based NAS
DUOVERO		2016/10/15	Gumstix Inc. DuoVero COMS boards
GEMINI		2008/10/24	Cortina Systems SL3516 eval board
GUMSTIX		2006/10/16	Gumstix Inc. PXA255/270 based boards
HDL_G		2006/04/16	I-O DATA HDL-G Giga LANDISK
HPT5325		2012/03/31	HP t5325 Thin Client
HUMMINGBIRD_A31	2014/10/10	Merrii Hummingbird A31
IGEPV2		2010/06/16	IGEPv2 OMAP3530 eval board
IMX31LITE	2008/04/27	Freescale i.M31 DEV LITE KIT
INTEGRATOR	2001/10/27	ARM Integrator board
IQ31244		2003/05/14	Intel IQ31244 reference board
IQ80310		2001/09/05	Intel IQ80310 eval board
IQ80321		2002/03/27	Intel IQ321 eval board
IXDP425		2003/04/08	Intel IXDP425/IXCDP1100 development platform
IXM1200		2002/07/15	Intel IMX1200 eval board
KOBO		2014/07/25	Kobo touch (eReaders)
KUROBOX_PRO	2010/10/02	Kuroutoshikou KURO-BOX/PRO
KURONAS_X4	2016/07/11	Kuroutoshikou KURO-NAS/x4
LUBBOCK		2003/06/18	Intel Lubbock DBPXA250 board
MARVELL_NAS	2010/10/02	Generic Marvell SoC based NAS
MINI2440	2012/01/30	FrendlyARM Mini2440 S3C2440 SoC board
MMNET_GENERIC	2011/11/04	Propox MMnet1002 board
MPCSA_GENERIC	2008/07/03	MPCSA Atmel AT91RM9200 based board
MV2120		2011/07/20	HP Media Vault MV2011 Marvell Orion board
N900		2012/12/07	Nokia N900 smartphone
NAPPI		2002/07/15	Netwise APlication Platform Board
NETWALKER	2010/11/13	Sharp NetWalker
NSLU2		2006/02/28	Linksys NSLU2 (a.k.a. "Slug")
ODROID-C1	2015/02/07	Hardkernel ODROID-C1
OPENBLOCKS_A6	2012/08/01	Plat'Home. OpenBlockS A6
OPENBLOCKS_AX3	2013/09/30	Plat'Home. OpenBlockS AX3
OPENRD		2012/08/10	open-rd.org Marvell Orion board
OSK5912		2007/01/06	TI OMAP 5912 OSK board
OVERO		2010/07/10	Gumstix Inc. Overo COMS boards
PANDABOARD	2012/08/20	TI OMAP4430 PandaBoard
PARALLELLA	2015/01/23	Xilinx Zynq and Epiphany multi-core chips
PEPPER		2016/10/15	Gumstix Inc. Pepper SBC(Single Board Computer)
ROCKCHIP	2014/12/26	Radxa Rock/Lite/Pro, MINIX NEO X7, Rayeager PX2
SHEEVAPLUG	2010/10/02	Marvell SheevaPlug
SMDK2410	2003/07/31	Samsung SMDK2410 S3C2410 eval board
SMDK2800	2002/11/20	Samsung SMDK2800 S3C2800 eval board
RPI		2012/07/26	Raspberry Pi
RPI2		2015/03/04	Raspberry Pi 2
SUNXI		2017/06/28	Allwinner family SoCs (FDT)
TEAMASA_NPWR	2002/02/07	Team ASA Npwr IOP310 based server appliance
TEAMASA_NPWR_FC	2003/12/24	Team ASA NPWR-FC i80321 server appliance
TEGRA		2015/05/29	NVIDIA Tegra family SoCs (FDT)
TISDP2420	2008/04/27	TI OMAP 2420 eval board
TISDP2430	2008/04/27	TI OMAP 2430 eval board
TOASTER		2005/08/14	NetBSD/toaster based on TS-7200
TS7200		2004/12/23	Technologic Systems TS-7200 board
TWINTAIL	2005/02/26	Genetec corp. "Twintail" PXA255 eval board
VIPER		2005/06/06	Arcom Viper PXA255 ARM board
VTC100		2016/07/04	NEXCOM VTC100
ZAO425		2003/05/23	NOVATEC NTNP425B "ZAO425" IXP425 eval board
ZEDBOARD	2015/01/23	Xilinx Zynq-7000
$NetBSD: README,v 1.1 2006/11/08 23:49:02 scw Exp $

"The Certance CP3100 product family provides high-end disk-to-disk-to-tape
(D2D2T) functionality for small-to-medium businesses."

Physically, the CP3100 is designed to fit in a single 5.25" half-height
drive bay. Storage is provided by a removable SATA disk integral to the unit.

To software, the hardware is very similar to the IQ80321 and IQ31244 eval
boards from Intel. As such, we share almost all of their code.

Onboard hardware:

 - IOP321 XScale CPU. Core clock is 600MHz.
 - 256MB SDRAM (not sure if that's true for all)
 - Four-port Intel i31244 SATA controller. One port is connected to the
   internal disk. The remaining three are available on the back-panel.
 - Dual GigE ports on the back panel, using an Intel i82546EB controller.
 - Two Symbios Logic 53c1010 SCSI controllers, one in host mode the other
   in target mode. Both SCSI busses are available on the back panel.
   Note that NetBSD does not support SCSI target mode.
 - 8MB of NOR Flash, containing a fairly vanilla Redboot together with
   a minimal compressed Linux image.
 - Some front-panel LEDS (not yet supported).
 - Serial console.

Power is provided via a standard 4-pin Molex connector (5v/Gnd/Gnd/12v).

The serial console is available on the back-panel "MISC" pins. Looking
at the back-panel, with the Molex power connector at the bottom left,
the MISC connector pin-out is:

             M I S C
        1  3  5  7  9  11
        .  .  .  .  .  .
                       
        .  .  .  .  .  .
        2  4  6  8  10 12

There should be a jumper between pins 11-12. The serial console is
available on pins 8, 9, and 10:

 8  - Gnd
 9  - Tx
 10 - Rx

No TTL-RS232 level conversion is needed. Serial parameters are 115200 8N1.


Getting NetBSD onto the CP3100 is a breeze. Simply break into Redboot by
sending ^C before it loads the Linux kernel. Run "fconfig" to set the
network parameters to suit your set up (and set "Run script" to "false"
while you're in there). Drop "netbsd.bin" onto your TFTP server, and
load it onto the CP3100 using "load -r -b 0x200000 netbsd.bin" followed
by "go".

The IQ80321 version of gzboot works fine with the CP3100, so you can use
Redboot to program a gzboot/kernel image into onboard Flash and have it
boot NetBSD on power-up.
$NetBSD: README,v 1.1 2003/05/14 21:41:34 thorpej Exp $

The IQ31244 ("Red Canyon") is a reference board based around the
i80321 I/O processor and i31244 S-ATA controller.

To software, it is very similar to the IQ80321 eval board.  As such, we
share almost all of the IQ80321 code, and name our functions in a way
compatible with the IQ80321 code.
$NetBSD: README,v 1.7 2009/11/22 19:09:15 mbalmer Exp $

NetBSD for the Linksys NSLU2 (a.k.a. "Slug")
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The NSLU2 (Network Storage Link for USB 2.0 Disk Drives) is a small, cheap
NAS device consisting of an Intel IXP420 (Xscale) CPU, a 10/100mbit Ethernet
port, and two USB 2.0 ports. It has 32MB of SDRAM and 8MB of Flash memory,
and runs RedBoot/Linux out of the box.

It is eminently hackable.

The guys over at http://www.nslu2-linux.org/ have done a good job of
documenting just about every aspect of the hardware and original firmware.
They also provide a custom "Unslung" Linux distribution to replace the
original hobbled kernel/userland.

Because of the amount of documentation available, and the fact that Slugs
are available so cheaply (I paid just over UKP 50 for mine, brand new) I
decided to buy one and port NetBSD to it.

This is the result of that effort.

Note: The Slug's IXP420 CPU runs in big-endian mode, so when building a
cross toolchain you must pass "-m evbarm -a armeb" to build.sh.



Current status
==============

The following bits of Slug hardware are not (yet?) supported:

 - Flash ROM
   You can write gzboot kernels (when support is added) to Flash using
   RedBoot, so all is not lost.

 - Buzzer
   In the absence of a decent API to expose the onboard buzzer to userland,
   this is not yet supported. I envisage using timer1 to generate an
   interrupt at the required rate (1-2 kHz). The handler will toggle the
   buzzer GPIO pin. Obviously timer1 will be configured only when necessary
   as a 1-2 kHz interrupt rate will sap a fair bit of CPU horsepower.

Everything else is fully supported, including the power/reset buttons and
disk activity/status LEDs.

Non-hardware items on the TODO list include:

 - gzboot support.
   The Slug's 8MB of Flash is split into 5 segments:

    1 0x50000000-0x5003ffff: RedBoot (with some additional bits at the end).
    2 0x50040000-0x5005ffff: Sysconf (used by the Linksys firmware).
    3 0x50060000-0x5015ffff: Self-extracting compressed kernel image.
    4 0x50160000-0x507dffff: Compressed ramdisk image.
    5 0x507e0000-0x507fffff: SerComm Flash trailer.

   Segments 1, 2, and 5 should be considered immutable. Segments 3 and 4
   have a 16-byte header, the first 4 bytes of which describe the length
   of the image contained in that segment (not including the header).

   On power-up, RedBoot copies the image in segment 3 into SDRAM at 0x01d00000,
   and the image in segment 4 into SDRAM at 0x01000000. RedBoot then jumps to
   0x01d00000. This is just a regular ARM Linux compressed kernel bootloader.

   So, we need to create a version of gzboot linked not at Flash address
   0x50060000, but at 0x01d00000 instead. The only downside is that it looks
   like the combined size of gzboot plus compressed kernel cannot exceed 1MB.

   To support an md(4) root filesystem, we will need to modify gzboot to
   decompress the ramdisk image from segment 4 and copy it to the correct
   place in the decompressed kernel image.

 - Move the kernel link address closer to the start of SDRAM. We waste a
   little under 2MB with the current setup.



Getting NetBSD onto the NSLU2
=============================

Thanks to the efforts of the guys over at www.nslu2-linux.org, hacking the
Slug is a pretty easy proposition, but some soldering skills are essential.
For a first-time install of NetBSD (at least until someone comes up with a
nice easy binary install image) you will almost certainly require access to
the serial console. This means firing up your trusty soldering iron and
hooking up a MAX3232 chip to your Slug. While your soldering iron is hot,
you should seriously consider de-restricting your Slug's CPU core clock
speed (133MHz stock, 266MHz de-restricted) by removing a single surface-
mount resistor. Full instructions for both these mods are on the above
website.

Once you have console access you can interrupt RedBoot's auto-boot process
using CTRL-C. You are now in a position to download a NetBSD kernel into
SDRAM.

You will have to configure a TFTP server on a machine hooked up to the same
Ethernet segment as the Slug. This machine's Ethernet interface must also
be configured to have an address in the 192.168.0.0/24 subnet since the
Slug's Ethernet *always* defaults to 192.168.0.1 when running RedBoot.
There seems to be no way to alter this, so the best course of action will
probably be to set up an alias on the server's interface. 192.168.0.2 is
a good choice.

Assuming you've done all that and have dropped a suitable kernel image
into the TFTP directory, the following commands will load and run the
kernel.

redboot> ip_address -h 192.168.0.2
redboot> load -r -b 0x200000 netbsd.bin
redboot> go

At this point you should mount a root filesystem from a USB disk device.
The ethernet is now supported, so you may also be able to use a NFS root.
USB Ethernet devices can also be used for a NFS root.

Note that the kernel will always report the CPU core clock speed as 266MHz
even if your Slug's CPU clock is running at a stock 133MHz.


Burning a NetBSD kernel into Flash
==================================

TBD (waiting for gzboot support).

$NetBSD: README,v 1.2 2008/04/30 21:22:19 garbled Exp $

Contents.
	boot        NetBSD/rs6000 boot-loader
	mkbootimage Make bootable netboot image.

mkbootimage is located in ../../powerpc/stand/mkbootimage

How to make bootable floppy disk image.

	 $ cd /sys/arch/rs6000/stand
	 $ make
	    For cross compile environment:
	        $ for i in common boot; do (cd $i; ppc-make); done
	        $ (cd ../../powerpc/stand/mkbootimage; make)
	 $ mkbootimage boot/boot /tmp/boot.fs
	    To make kernel attached bootable network image:
		    $ mkbootimage -m rs6000 -b boot/boot -k ../compile/YOUR_KERNEL/netbsd /tmp/boot.fs
$NetBSD: README,v 1.3 1996/11/19 20:58:32 gwr Exp $

NetBSD/sun3 supports the following busses:

Bus:
mainbus -  An imaginary bus on which the other busses all reside.

obio	-  Devices on the motherboard, accessed by having their "registers"
	   mapped into the kernel's virtual address space
obmem	-  Devices on the motherboard that are mapped into main memory
	   by the hardware.  Only true of some framebuffers.
vmes	-  VME D16 space
vmel	-  VME D32 space

Devices supported:

'obio' Bus: 
Device	Type    Description
---------------------------------------------------------------------------
zs	CHAR	zilog 8530 serial ports; used for serial ports, keybd, mouse
le	IFNET	lance ethernet driver

XXX - very much incomplete...

$NetBSD: README,v 1.1.1.1 1999/11/19 00:43:20 lonhyn Exp $

Routines for converting back and forth between CHRP boot icons
in an SGML type ASCII format and PPM files.

These two programs depend on PPM support being installed. This is
most easily accomplished by installing the graphics/netpbm package.

The makefiles as written expect to find PBM under /usr/pkg
#	$NetBSD: README,v 1.3 2015/11/11 16:08:52 phx Exp $

BUILD INSTRUCTIONS

Building LoadBSD isn't easy since several sources from the NetBSD repository
are required. Compiling these sources under AmigaOS without clashes with the
native GCC headers requires some knowledge. This document tries to describe
the steps necessary to rebuild LoadBSD with an AmigaOS gcc. These instructions
do only apply for LoadBSD versions using the loadfile() interface. Previous
version do only require getopt.c and reboot.h.

Note: Its not possible to build LoadBSD with the native NetBSD compiler!
      LoadBSD is an *AmigaOS* program and must be built with an AmigaOS
      compiler. Of course, a properly setup cross-compiler does work.

Required sources from NetBSD (either HEAD or from a release branch)

   From src/sys/lib/libsa: loadfile.h,loadfile.c,loadfile_elf32.c,loadfile_aout.c
   From src/lib/libc/stdlib: getopt.c

      place these files in the directory where you have loadbsd.c

   From src/sys/arch/m68k/include: aout_machdep.h,elf_machdep.h

      place these files in: <loadbsd directory>/include/m68k

   From src/sys/arch/amiga/include: aout_machdep.h,elf_machdep.h,loadfile_machdep.h

      place these files in: <loadbsd directory>/include/machine

   From src/sys/sys: exec.h,exec_elf.h,exec_aout.h,reboot.h

      place these files in: <loadbsd directory>/include/sys

   Additional headers (see below): inttypes.h,namespace.h,lib/libsa/stand.h,lib/libkern/libkern.h

      place these files in: <loadbsd directory>/include

If all the mentioned files are placed at the correct place, loadfile_machdep.h
must be modfied. The patch is included below. Another small patch to
loadfile_aout.c must be applied to fix an incompatibility for LoadBSD.
However, that patch breaks loadfile() for other architectures using a.out!
Note: This patch is required to be able to suppress loaded symbols when
      booting ancient a.out kernels that don't support them. Without the
      patch symbol suppressing doesn't work! That also means ELF isn't
      affected and LoadBSD could handle it differently but then it could
      probably break in other unpredictable ways...

Then it should be possible to recompile LoadBSD by typing "make". If make
fails, fix the problem and try again :-P

Good luck!

--- Missing files/patches ---

      loadfile_aout.c modification:
--cut--
--- loadfile_aout.c~	Mon Feb 11 21:25:56 2002
+++ loadfile_aout.c	Thu Jan 23 10:43:27 2003
@@ -217,8 +217,8 @@ loadfile_aout(fd, x, marks, flags)
 		BCOPY(&x->a_syms, maxp, sizeof(x->a_syms));
 
 	if (flags & (LOAD_SYM|COUNT_SYM)) {
-		maxp += sizeof(x->a_syms);
 		aoutp = maxp;
+		maxp += sizeof(x->a_syms);
 	}
 
 	if (x->a_syms > 0) {
--cut--

      loadfile_machdep.h modification:
--cut--
--- loadfile_machdep.h~	Wed Oct 31 18:20:45 2001
+++ loadfile_machdep.h	Thu Jan 16 14:02:39 2003
@@ -42,6 +42,21 @@
 #define	BOOT_AOUT
 #define	BOOT_ELF32
 
+#if 1
+
+#define LOADADDR(a)		(((u_long)(a)) + offset)
+#define ALIGNENTRY(a)		0
+#define READ(f, b, c)		read((f), (void *)LOADADDR(b), (c))
+#define BCOPY(s, d, c)		memcpy((void *)LOADADDR(d), (void *)(s), (c))
+#define BZERO(d, c)		memset((void *)LOADADDR(d), 0, (c))
+#define WARN(a)			warn a
+#define PROGRESS(a)		/* nothing */
+#define ALLOC(a)		malloc(a)
+#define FREE(a, b)		free(a)
+#define OKMAGIC(a)		((a) == NMAGIC)
+
+#else /* ! true, false */
+
 #define	LOAD_KERNEL		LOAD_ALL
 #define	COUNT_KERNEL		COUNT_ALL
 
@@ -83,4 +98,7 @@ void vcopy __P((u_long, u_long, u_long *
 void vzero __P((u_long, u_long *, size_t));
 
 #endif
+
+#endif /* ! false */
+
 #endif /* ! _AMIGA_LOADFILE_MACHDEP_H_ */
--cut--

      Makefile:
--cut--
TARGET	= loadbsd

CC	= gcc -m68020 -Wa,-m68030 -fbaserel
CFLAGS	= -D_STANDALONE -I./include -O -fomit-frame-pointer -msmall-code
LDFLAGS	= -noixemul
LDLIBS	=

OBJS	= loadbsd.o loadfile.o loadfile_aout.o loadfile_elf32.o getopt.o

$(TARGET): $(OBJS)
	$(CC) $(LDFLAGS) -o $@ $(OBJS) $(LDLIBS)
--cut--

      include/inttypes.h:
--cut--
#ifndef _INTTYPES_H
#define _INTTYPES_H

#include <sys/types.h>

typedef	unsigned char      uint8_t;
typedef	unsigned short     uint16_t;
typedef	unsigned int       uint32_t;
typedef	unsigned long long uint64_t;
/*
typedef	         int       int32_t;
typedef	         long long int64_t;
*/
typedef unsigned long vaddr_t;
typedef unsigned long paddr_t;

#endif /* !_INTTYPES_H */
--cut--

    include/namespace.h
--cut--
#define _DIAGASSERT(x) /**/

extern char *program_name;
#define getprogname() program_name
--cut--

      include/lib/libsa/stand.h
--cut--
#include <stdio.h>
#include <string.h>
#include <errno.h>
#include <stdlib.h>
#include <unistd.h>
#include <fcntl.h>
#include <err.h>
#include "inttypes.h"
--cut--

      include/lib/libkern/libkern.h
--cut--
/* nothing, must only exist! */
--cut--
$NetBSD: README.IPn,v 1.10 2009/02/10 06:12:27 rumble Exp $

Arch (kernel)	Models				Codename
-------------	------				--------
IP2		IRIS 3000
IP4		4D/50, 4D/70
IP4.5		4D/60, 4D/80, 4D/85
IP5		4D/1x0
IP6		4D/20				Eclipse
IP10 (IP6)	4D/25				Eclipse
IP7		4D/2x0
IP9		4D/210
IP13 (IP7)	4D/3x0
IP15 (IP7)	4D/4x0
IP12		4D/30, 4D/35, Indigo R3K	Magnum (/3x), Hollywood (Indigo)
IP17		Crimson
IP19		Challenge L/XL, Onyx		Everest
IP20		Indigo R4K			Blackjack
IP21		Power Challenge R8K, Power Onyx	Everest
IP22		Indigo2, Challenge M		Fullhouse
IP24 (IP22)	Indy, Challenge S 		Guinness
IP25		Power Challenge R10K		Everest
IP26		Power Indigo2 R8K		Teuton
IP27		Origin 200, Origin 2000, Onyx2
IP28		Power Indigo2 R10K		Pacecar
IP30		Octane				Speedracer
IP32		O2				Moosehead
IP33		SN1 (?)
IP35		SN1 (?)

Architecture	ABI	Bootstrap Load Address	Kernel Load Address
------------	---	----------------------	-------------------
IP19 IP21 IP25	32	0x80004000		0x80100000
IP19 IP21 IP25	64	0xa800000000004000	0xa800000000180000
IP20 IP22 IP24	32	0x88002000		0x88069000
IP22 IP24 IP26	64	0xa800000008002000	0xa800000008069000
IP28		64	0xa800000020003000	0xa800000020080000
IP30		64	0xa800000020004000	0xa800000020080000
IP32		32	0x80002000		0x80069000
IP32		64	0xa800000000002000	0xa800000000078000
IP33		64	0xa800000000019000	0xa800000000300000
$NetBSD: README,v 1.1 2000/06/14 15:49:07 soren Exp $

By reading this file you are agreeing to one or more of the following:

* Write XIO bus support.
* Send a machine with XIO to Soren.
* Send documentation for a machine with XIO to Soren.
What is Windermere?

I don't know...  Open Psion guys says it.

  http://linux-7110.sourceforge.net/

It is SoC for Psion.
Alos I have not found this datasheet. This written with reference to

  http://svn.exactcode.de/linux24-psionw/
  svn co http://svn.exactcode.de/linux24-psionw/ linux24-psion
#	$NetBSD: README.ipl,v 1.1 2014/02/24 07:23:43 skrll Exp $

Coding note:

	In order to make this relocatable, you must follow following
	restrictions:

	1. Do not place any objects in text segment
	1.1. For compiler,
		(1) do not declare or define any objects to be placed in
		    text segment, that is, do not use ``const'' keyword
		    (but declaring a pointer to const is probably OK),

		(2) make sure string literals, if any, are placed in data
		    segment (use traditional compiler),

		(3) avoid initialization of automatic objects (non-static
		    function-local variables) of aggregate types (arrays,
		    structs and unions), which may implicitly emits
		    constant data.

	     In summary, do not use ANSI extension.  Use traditional C. :-)

	1.2. For linker, do not actually place objects in text segment.

	2. Do not use function pointers.


On-disk layout:

	We have 6.5KB for the primary boot.

	disk address
	start	 size
	000000	0000FC	LIF header
	0000FC	000104	unused
	000200	000194	disklabel (404 bytes for 16-partition label)
	000394	00006C	unused
	000400	000400	ipl part 2 (1KB)
	000800	000100	optional LIF directory
	000900	000100	unused
	000A00	000600	ipl part 3 (1.5KB)
	001000	001000	ipl part 1 (4KB)
	002000		(file system starts here)


On-memory layout on IPL startup:

	The firmware loads ipl part 1 on the memory, and executes it.

	address offset
	 start	 size
	000000	001000	ipl part 1
	001000	000A00	(not loaded yet)
			(bss section etc)
	x	001000	temporary disk buffer
	x+1000		stack


Then the IPL will load the rest of itself:

	ipl part 1 loads parts 2 and 3, then continues execution.

	address offset
	 start	 size
	000000	001000	ipl part 1
	001000	000400	ipl part 2
	001400	000600	ipl part 3
	001A00	xxxxxx	(bss section etc)
$NetBSD: README,v 1.4 2011/02/25 10:06:24 kiyohara Exp $

contents
	boot:	     NetBSD/bebox booter.
	elf2pef:     Convert ELF image to PEF image.  Integrated to
		     powerpc's mkbootimage.  Obsolete.
	mkbootimage: Make bootable BeOS DR8 filesystem(a.k.a. obfs) image.
		     Integrated to powerpc's mkbootimage.  Obsolete.


boot:
	enable one line and comment out other lines,
	to select one console device from vga,frame buffer,serial.
		vga:		CPPFLAGS+= -DCONS_VGA
			e.g. S3 Trio64, ...

		frame buffer:	CPPFLAGS+= -DCONS_BE
			e.g. Trio64v+, Millennium I/II, Mystique 220, ...

		serial:		CPPFLAGS+= -DCONS_SERIAL ...

	if change CPPFLAGS after make, use 'make cleandir' before 'make'.

	boot can read/exec kernel on ffs(floppy/ide/scsi) or attached
	in-kernel image by powerpc's mkbootimage.


Example of making bootable floppy disk:
	% cd sys/arch/bebox/stand
	% $(TOOLDIR)/bin/nbmake-bebox

	% nbpowerpc-mkbootimage -I -m bebox -b boot/boot /tmp/fd.img

	Or 

	% nbpowerpc-mkbootimage -m bebox -b boot/boot \
	      -k ../compile/INSTALL/netbsd /tmp/fd.img

	[insert formatted floppy disk]
	% dd if=/tmp/fd.img of=/dev/rfd0a


                PocketBSD boot loader for NetBSD/hpcmips

                            Sun May 23 1999


                                     Shin Takemura<takemura@ca2.so-net.ne.jp>
                                                            PocketBSD project

0. What is this? 

  This program load the NetBSD/hpcmips kernel and execute it.

  - All data on your PDA will be lost if the kernel boot successfully.
  - NetBSD/hpcmips kernel is ELF format binary executable.
  - This program supports only MIPS (especially NEC VR41X1 or VR4102 CPU)
    platforms -- NEC MC-R series, CASIO E- series, Everex Freestyle and 
    others.

1. How to use

  - Copy pbsdboot.exe and the kernel 'netbsd into your PDA and start
    pbsdboot.exe.
  - Select 'kernel name' and 'Frame buffer' appropriately.
  - Push [boot] button.

2. How to compile

  Use Microsoft Visual C++ 5.0 and Windows CE toolkit for VC++ 5.0. It also
  needs Embedded toolkit for accessing raw block device to support native
  file system, FFS.

  VC++ 6.0 and toolkit 6.0 may works. But I've never tried.

3. About version string

  If you modify this program and update pbsdboot.exe.uue, change version string
  which is coded in main.c appropriately.

  The version string is in format:

    Version A.B.C YYYY.MM.DD

  in where:

    A: Don't change this.
    B: Increment this number if you change program's behavior, fix some bugs
       or add new features.
    C: Increment this number if you change/add some parameters, constants or
       windows' resources
    YYYY.MM.DD: date
#	$NetBSD: README.jp,v 1.2 1998/01/05 20:52:31 perry Exp $

(This file is written in JIS code.)

palette - $B%F%-%9%H$N%Q%l%C%HA`:n$r9T$J$&(B

FC2 $B$r@^$C$F!"L5;vF0:n3NG'$7$?$"$H$G!"K\Ev$K(B VRAM $B%"%/%;%9$G$-(B
$B$k$h$&$K$J$C$?$N$+$N3NG'$K$*;H$$$/$@$5$$!#C1$J$k%F%9%H%W%m%0%i(B
$B%`$G$9$N$G5!G=$OIO<e!"%P%0$F$s$3$b$j$G$9!#<+:n%W%m%0%i%`$r:n$k(B
$B:]$N;29M$K$7$F2<$5$$!#(B

palette 31 31 31 1 $B$H$9$k$H%+!<%=%k$,Gr$/$J$j$^$9!#(B
palette 0 0 31 0 $B$H$9$k$HGX7J$,@D$/$J$j$^$9!#(B
palette 31 31 0 ($B$b$7$/$O(B palette 31 31 0 7) $B$H$9$k$HJ8;z$N?'$,(B
$B2+?'$K$J$j$^$9!#(B
$BH?E>B0@-$NItJ,$N?'$rJQ$($k$K$O(B +8 $B$7$?(B code $B$r;H$C$F2<$5$$!#(B
(Emacs $B$N%b!<%I%i%$%s$G$"$l$PJ8;z$,(B 8 $B$GGX7J$O(B 15)

make $B$r<B9T$7$F!"<B9T%W%m%0%i%`$r@8@.$7$F$+$i$*;H$$2<$5$$!#(B
#	$NetBSD: README.loadkmap,v 1.2 1998/01/05 20:52:27 perry Exp $

loadkmap - $B%-!<%^%C%W$NFI$_9~$_(B

ITE $B$G;HMQ$9$k%-!<%^%C%W$rFI$_9~$_:F@_Dj$9$k%3%^%s%I$G$9(B.

$B;H$$J}(B: loadkmap $B%-!<%^%C%W%U%!%$%kL>(B

$B;HMQ$G$-$k%-!<%^%C%W%U%!%$%kNc$H$7$F(B ascii_kmap $B$*$h$S(B jis_kmap $B$r(B
$BMQ0U$7$^$7$?(B. ascii_kmap $B$O%"%9%-!<G[Ns(B (SHIFT+2 $B$,(B @ $B$K$J$k$d$D(B)
jis_kmap $B$O85!9$NG[Ns$G$b$"$k5l(B JIS $BG[Ns$G$9(B.
/*	$NetBSD: README,v 1.3 2007/10/17 19:56:54 garbled Exp $	*/

Overview

This is a port to the Motorola "SandPoint" evaluation system.  The
SandPoint is the successor to the "Yellowknife" system.  The system
can be fitted with different PMCs (Processor Mezzanine Cards).  This
port is specifically for the rev X2 motherboard system with the PPC
8240 PMC rev X4 installed.  It also works with the Altimus X2 PMC
(MPC7400 with MPC107).

All references (cf) listed here are for the MPC8240 Integrated Processor
User's Manual.

Information on the Sandpoint can be found on Motorola's web site:
http://www.mot.com/SPS/PowerPC/teksupport/refdesigns/sandpoint.html


SandPoint Hardware Configuration

This port was developed on a Sandpoint X2 motherboard with a Unity X4 PMC.

This port assumes that the jumpers are set as follows:
	S3/S4	- Mode 1: PMC w/o IDE (switches opposite, one nearest PCI
		  slot toward near edge)
	S5	- Interrupt to PMC normal (switch toward near edge)
	S6	- Local I/O shared with slot 2 (switch toward near edge)

Mode 0 (PMC w/ IDE) does not appear to work right with ISA interrupts.  The
interrupts from the Winbond chip do not appear at the PMC.

On the PPMC, we assume a 100MHz clock.
on PPMC: (C == closed, or "on")
    SW2:
	C	ROM on PCI bus (DINK32 on mainboard)
	-	Map "B": CHRP
	C	Motorola PPMC
	C	Wait for initialization (peripheral mode)
	-	Program mode: Normal mode
	-	Select normal ROM
	-	33 MHz only
	-	COP only resets local CPU/MPC107
    SW3:
	-C--C	PCI 33, Mem 66, PPC 266
	--	0.5 - 0.9 ns PCI hold time
	C	25 ohm PCI drive strength


Address Map

For this port, we choose the "Address Map B" (CHRP-compatible) for the
system (see SW2, #2, above):

 (Processor View)
0000 0000   0009 FFFF	System Memory
000A 0000   000F FFFF	Compatibility Hole (programmable to go to PCI space
			or system memory--programmed for system memory--cf 5.8)
0010 0000   3FFF FFFF	System memory
4000 0000   7FFF FFFF	Reserved (programmed to give a memory select
			error if accessed--cf 5.7.2)
8000 0000   FCFF FFFF	PCI memory space
FD00 0000   FDFF FFFF	PCI/ISA memory space (see 5.8, CPU_FD_ALIAS_EN)
FE00 0000   FE7F FFFF	PCI/ISA I/O space (Forwarded to PCI address space
			with high byte zeroed, but FE01 0000 and up are
			reserved)
FE80 0000   FEBF FFFF	PCI I/O space (Forwarded to PCI I/O space with high
			byte zeroed)
FEC0 0000   FEDF FFFF	PCI configuration address register (Each word in this
			range is aliased to the PCI CONFIG_ADDR register)
FEE0 0000   FEEF FFFF	PCI configuration data register (Each word in this
			range is aliased to the PCI CONFIG_DATA register)
FEF0 0000   FEFF FFFF	PCI interrupt acknowledge
FF00 0000   FF7F FFFF	32- or 64-bit Flash/ROM space (Can hit either local
			memory or PCI bus -- cf. 5.6)
FF80 0000   FFFF FFFF	8-, 32- or 64-bit Flash/ROM space (Can hit either
			local memory or PCI bus -- cf. 5.6)

This is a host-mode port, so the inbound and output translation windows
are unused.

The Embedded Utilities Memory Block (EUMB) is set to be 1M below the end
of the PCI memory space: FC00 0000, so EUMBBAR is FC00 0000, giving us

Message unit (I2O) base	: FC00 0000	(cf. 10.2, 10.2.3, 10.3)
DMA base		: FC00 1000	(cf. 9.2)
ATU base		: FC00 2000	(cf. 4.3.3)
I2C base		: FC00 3000	(cf. 11.3)
EPIC base		: FC04 0000	(cf. 12.2)



Boot Information

The SandPoint ships with the Motorola DINK32 ROM.  This is a rather
basic ROM with only serial-download (S-Record) capability for
loading the kernel.  Basically, the kernel is loaded to a specified
address and you jump to it.  The ROM takes care of initializing
the MICRs and MCCRs.  There is really no boot information to pass.

It would be nice to have a much more complete ROM interface, allowing
settings for, say, bootp/tftp boot, automatic boot, and persistent
settings (for console rate, auto boot, bootp, etc), and that might
be provided at some point, but that's not available as of this
writing.

So, the kernel is hard-coded to boot w/ 32MB for now.



Interrupt Configuration

The 8240 has the internal EPIC.  For the SandPoint, the EPIC is programmed
in mixed-mode (GCR) with direct interrupts (EICR).  With this configuration,
there are 13 available interrupts:
	4 global timers
	5 direct IRQs
		IRQ0 - PCI Slot #0 INTA#
		IRQ1 - PCI Slot #1 INTA# / shared with WinBond I/O
		IRQ2 - PCI Slot #2 INTA#
		IRQ3 - PCI Slot #3 INTA#
		IRQ4 - On-PPMC 16552 interrupt (Unity X2)
		IRQ4 - pulled down w/ resistor (Unity X4)
	4 internal interrupts
		I2C
		DMA Ch0
		DMA Ch1
		I2O message unit

The SandPoint can run in one of 4 interrupt modes:
  0 - PMC host with IDE (3.3v PCI slots are unavailable)
  1 - PMC host w/o IDE (all PCI slots are available)
  2 - PMC agent, Winbond providing arbitration & interrupt to INTA# on PMC
  3 - Yellowknife mode--just like #2, except drives INTA# on 4th PCI slot

We choose to run in mode 1 as Motorola recommends modes 0 or 1 for
all new development.  Unfortunately, mode 0 does not appear to
work--"ISA" interrupts are lost.  In this mode, with interrupts
routed to PCI slot 3, we have to check for both a Winbond (ISA)
interrupt, and a PCI slot interrupt.  So basically, we have a
two-level interrupt configuration for Winbond interrupts.  The ISA
bus attachment registers an interrupt for PCI slot 3 with its own
interrupt handler.  Drivers for ISA devices on the Winbond will
register interrupts with the ISA interrupt handler.  The sticky
part of this is how to deal with one global interrupt priority.



SandPoint III "SP3" Interrupt Configuration

With a help of additional logic circuit SP3 organizes external
interrupt sources as EPIC serial mode interrupts.
	16 serial IRQs
		IRQ0 - WinBond South bridge i8259 PIC, polarity inverted
		IRQ1 - reserved
		IRQ2 - PCI Slot #1, INTA#
		IRQ3 - PCI Slot #2, INTA#
		IRQ4 - PCI Slot #3, INTA#
		IRQ5 - PCI Slot #4, INTA#
		IRQ6 - WinBond INTA#
		IRQ7 - WinBond INTB#
		IRQ8 - WinBond INTC#
		IRQ9 - WinBond INTD#
		IRQ10 thru 15 - reserved
SP3 provides switch selections to emulate SP1/2 compatible EPIC
direct mode interrupt assignments.
$NetBSD: README.NAS,v 1.23 2013/05/25 15:07:49 phx Exp $

//// MPC8241/8245 NAS products ////

The following NAS products are known by NetBSD/sandpoint.

NIC	IDE		machine description
----	----		--------------------------
			>> Board type BRD_KUROBOX <<
tlp.11	cmdide.12	classic KuroBox, LinkStation HD-HLAN(LS1)
re.11	cmdide.12	KuroBox HG
re.11	iteide.12	Gigabit LinkStation HD-HGLAN (also with cmdide)
re.11	iteide.12/13	classic TeraStation HD-HTGL
re.11	satalink.12/13	TeraStation Pro TS-TGL v1

			>> Board type BRD_SYNOLOGY <<
sk.15	iteide.13	Synology DS-106j, LinkStation LANxxxG
sk.15	satalink.13	Synology DS-101g+/106e/106/106x/107e/107/207
			DS-108j/109j/209j
sk.15	satalink.12/13	Synology CS-406/RS-406/CS-406e/CS-407e

			>> Board type BRD_QNAPTS <<
wm.15	satalink.13 	QNAP TS-100/TS-101(V1.02)
re.15	satalink.13	QNAP TS-101(V200), QNAP TS-201, LevelOne FNS-5000B

			>> Board type BRD_STORCENTER <<
re.15	viaide.13	IOMEGA StorCenter 250/500/1T

			>> Board type BRD_DLINKDSM <<
stge.15	acardide.16	D-Link DSM-G600 (Rev.B only!), Conceptronic CH3WNAS

			>> Board type BRD_NH230NAS <<
re.15	iteide.16	Netronix NH-230, Allnet ALL6250, Encore ENNHD-1000,
			Planex NAS-01G, Lindy NAS Personal Server Premium - IDE
re.15	satalink.16	Netronix NH-231, Allnet ALL6260, Longshine LCS-8311,
			Fujitsu-Siemens AMS150, Fujitsu-Siemens SBLAN2,
			Vibe NS-349-S, SinanPower GigaBit LAN NAS-349,
			Lindy NAS Personal Server Premium - SATA


PCI line/pin and EPIC IRQ assignments

		PCI IDSEL	   EPIC IRQ
Kurobox		11, 12, 13, 14	-> 0, 1, 4, 3
Synology	12, 13, 14, 15	-> 4, 0, 1, 2
QNAP		13, 14, 15, 16	-> 0, 1, 2, 3
StorCenter	13, 14, 15      -> 1, *, 0
DLink		13, 14, 15, 16	-> 0, *, 3, 4
NH230		13, 14, 15, 16	-> 0, 3, 1, 2

- USB is a multiple function PCI device which has
  pin assignment A (OHCI), B (OHCI) and C (EHCI).
  Special cases (*):
		IDSEL	Pin	   EPIC IRQ
  DLink		14	A, B, C	-> 1, 1, 2
  StorCenter	14	A, B, C	-> 2, 3, 4


//// e300 NAS products ////

- Freescale MPC8349E-mITXE

  PCI		bus:IDSEL	IPIC IRQ
satalink	0:16		22
miniPCI		1:14		21
3.3vPCI		1:15		20

  I2C		bus:addr
ds1339 RTC	1:0x68
sat MCU		1:0x0a

- Thecus N1200/N3200 (MPC8347)

  PCI		IDSEL		IPIC IRQ
satalink	16		20

  I2C		bus:addr
f75375 fanctl	0:0x2e
rs5c372a RTC	0:0x32
sat MCU		0:0x36


[ research still in progress ]

---
/// notes about altboot ///

$NetBSD: README.altboot,v 1.11 2012/04/26 19:59:36 phx Exp $

Altboot is a functional bridge to fill the gap between a NAS product
custom bootloader and the NetBSD kernel startup environment.  Altboot
irons out and rectifies erroneously configured HW by product
bootloaders and prepares a sane runtime, better suited for booting
NetBSD kernels.

- provides the foundation of a fast NetBSD porting cycle with functionalities
  product bootloaders don't have.
- facilitates a flexible and clean NetBSD implementation tailoured
  to target HW in detail, minimizing bumpy adjustments and hacks in
  locore asm and machdeps in very early kernel startup stage.
- levels out differences among similar-but-not-the-same porting
  targets to make it possible having common NetBSD kernels for them.
- builds and hands a bootinfo list to the NetBSD kernel.

Altboot is known working on at least these models:

- KuroBox or LinkStation with a popular U-Boot as replacement of
  the vendor's proprietary one

   U-Boot 1.1.4 LiSt 2.1.0 (Sep 21 2006 - 00:22:56) LinkStation / KuroBox

- Synology 101g+ with vendor custom PPCboot

   PPCBoot 2.0.0 (Mar  1 2005 - 15:31:41)

- Synology 106j, 207, 407e with vendor custom PPCboot

   PPCBoot 2.0.0 (Jan 30 2007 - xx:xx:xx)

- D-Link DSM-G600 with heavily restricted vendor custom U-Boot

   U-Boot 0.2.0 (May 26 2005 - 19:38:32)

- QNAP TS-101 (V200) with vendor custom U-Boot

   U-Boot 1.1.2 (Aug 28 2005 - 13:37:25) QNAP System, Inc.

- Iomega StorCenter with vendor custom U-Boot

   U-Boot 1.0.0 (Sep  2 2005 - 14:49:11)

- Allnet 6250 and compatible with restricted vendor custom PPCboot

   PPCBoot 2.0.0-A9 (Feb 13 2006 - 14:56:11)

- KURO-BOX/T4 vendor custom U-Boot

   U-Boot 2009.06-BUFFALO-svn1376 (Jul 11 2009 - 04:11:01) KURO-NAS/T4

The standard use of altboot is to invoke it with a short script from
U-Boot/PPCboot, where the altboot.bin image is stored in an unoccupied 128KB
section of the target's HW NOR flash.  Combined with standard
U-Boot/PPCboot functions, it is possible to boot a NetBSD kernel off
it right after power-on, without the help of manual intervention.  Note
that the original U-Boot/PPCboot still remains useful and altboot works
as a functional extension for them.

In case the firmware was crippled by the vendor so that it only boots
Linux U-Boot images (D-Link, Synology 2007), you can still use altboot by
overwriting the Linux kernel with altboot.img.

Altboot passes the following bootinfo records to the NetBSD/sandpoint
kernel:
- processor clock tick value driving MPC8241/8245.
- serial console selection.
- booted kernel filename and which device it was fetched from.
- Ethernet MAC address, if target HW lacks SEEPROM to store a unit unique
  value.
- product family indication.
- preloaded kernel module names (under development).

When no arguments are given, altboot defaults to boot a kernel called
"netbsd" from the root partition of the first disk in multiuser mode.

Boot arguments may be passed in three ways:
- On the command line, directly after the "go 0x1000000" command.
- From the U-Boot "bootargs" environment variable, when started by "bootm".
- By entering the interactive mode.

The following boot arguments are recognized:
- multi			boot into multiuser
- auto			boot into multiuser
- single		boot into singleuser
- ask			ask for boot device
- ddb			drop into the kernel debugger
- userconf		change configured devices

The following boot flags are recognized:
- norm			boot normally
- quiet			boot quietly
- verb			boot verbosely
- silent		boot silently
- debug			boot with debug output

Additionally the special argument "altboot" is recognized, which replaces
the actually running altboot program with the loaded binary file and
restarts itself. Mainly useful for altboot testing.

Multiple arguments may be specified at once, although not all combinations
make sense. The format of an altboot command line is:

  [[<bootargs> ...] <devicename>:[<bootfile>] ...]

Multiple boot devices and/or paths may be specified, which are booted one
after another until success. When no boot device is specified altboot tries
to boot from all disk devices with a valid NetBSD disklabel, starting with
unit 0.

The following device names are supported:
- tftp			boot from TFTP (address retrieved by DHCP)
- nfs			boot from NFS (address retrieved by DHCP)
- wd[N[P]]		boot from disk N, partition P, defaults to wd0a
- mem			boot from memory

For tftp and nfs the bootfile is determined by DHCP, when missing.
For wd it defaults to "netbsd".
For mem the bootfile is actually a hexadecimal address to load from and
is mandatory.

                             ### ### ###
The config files in this directory are generated by the 'makeconf' script
by combining a '<CONFIG>.in' file with 'GENERIC.in' and asserting a some
preprocessor defines.
If something needs to be changed in 'GENERIC.in' or some of the other '.in'
files, please regenerate the config files and commit them too...

Please commit changes to 'GENERIC.in' and other source files *first*, then
run 'makeconf' and commit the generated files to ensure the 'Created from:'
tags are correct.

Leo.
/*	$NetBSD: README,v 1.1 2001/06/14 12:57:12 fredette Exp $ */

The sun bootblocks are split into two parts: a small 1st-level program that
gets written right after the superblock in a partition (and is hence limited
in size to SBSIZE - DEV_BSIZE bytes), and a 2nd-level program that resides
in the filesystem proper.

The 1st-level program is loaded into memory by the PROM. It loads the second
stage program from a set of filesystem block numbers that are hard-coded
into it by the `installboot' program. The prototype code for the 1st-level
bootblocks are in `bootxx'.

The 2nd-level program (`ufsboot') is normally installed in the root FS
as `/ufsboot'. It uses the device drivers in the PROM and the stand-alone
filesystem code in `libsa.a' to locate and load the kernel.

Use the following command to install the 1st-level bootblocks in the
root filesystem (on `sd0a') using the file `/boot' as the second level
boot program:

	mount /dev/sd0a /mnt
	cd /usr/mdec
	cp -p ufsboot /mnt/ufsboot
	sync ; sleep 1 ; sync
	./installboot -v /mnt/ufsboot bootxx /dev/rsd0a

The above only works with securelevel <= 0 (see init.8 manual).

Status:

netboot works.

bootxx, installboot are tested and working.  It would be nice if
installboot would find the inumber for the 2nd stage boot program
without having the filesystem mounted so this command can work
with securelevel==1 (the default).  Doing this requies adding
code to read and do a directory lookup in the root...

$NetBSD: README,v 1.6 2008/02/26 21:46:38 rafal Exp $

How to use
	'make all' uudecode all binaries.
	hpcboot.exe were compiled for H/PC Pro 2.11 by eMbedded Visual C++ 3.0 
	executable are located in
		binary/ARM/hpcboot.exe
		binary/MIPS/hpcboot.exe *
		binary/SH3/hpcboot.exe
		binary/SH4/hpcboot.exe
	* for MIPS, use pbsdboot.exe (sys/arch/hpcmips/stand/pbsdboot)

How to compile

	1. Generate work space and project files.
		make vc5	# WindowsCE Embedded Toolkit for Visual C++ 5.0
		make vc6	# WindowsCE Toolkit for Visual C++ 6.0
		make evc3	# eMbedded Visual C++ 3.0
		make evc4	# eMbedded Visual C++ 4.0 (only tested for
				# ARM, not promoted to binary directory).

	2. Open hpc_stand.dsw or hpc_stand.vcw from Visual Studio.

How to commit (for developers)
	"make install" copies hpcboot.exe to binary/ directory from compile/
	directory.
	"make uuencode" uuencode binary/*/hpcboot.exe and
	increment build number. (this number is used for next build.)
	commit uuencoded binary and binary/build_number.h.
* $NetBSD: README,v 1.6 2013/04/20 03:26:11 isaki Exp $
* NetBSD/m68k FPE (floating point emulation) README file
* Created Oct/??/95 by kenn@remus.rutgers.edu (Ken Nakata)
* Last updated Oct/15/2011 by tsutsui

1. INSTALLATION AND COMPILATION

To compile a kernel with FPE built-in, do the following:

1) Add a line "options FPU_EMULATE" to your config file.  If you are
going to use the resulted kernel on a machine with an FPU for
debugging purpose, add "options DEBUG_WITH_FPU" as well.

2) Follow the usual procedure to build a new kernel.

NOTE:  If you add "options DEBUG_WITH_FPU", FPE will accept cpID=6 as
emulated FPU.  You will need a modified gas that generates cpID=6 for
floating point instructions, instead of normal cpID=1.  Mount unionfs
or copy the gas source directory and apply the following patch:

*** /usr/src/gnu/usr.bin/gas/config/tc-m68k.c   Mon Nov 21 16:30:41 1994
--- gas/config/tc-m68k.c    Fri Sep 29 07:59:06 1995
***************
*** 1275,1281 ****
                /* memcpy((char *)(&the_ins.operands[1]), (char *)(&the_ins.operands[0]), opsfound*sizeof(the_ins.operands[0])); */
                memset((char *)(&the_ins.operands[0]), '\0', sizeof(the_ins.operands[0]));
                the_ins.operands[0].mode=MSCR;
!               the_ins.operands[0].reg=COPNUM;         /* COP #1 */
                opsfound++;
        }
  
--- 1275,1281 ----
                /* memcpy((char *)(&the_ins.operands[1]), (char *)(&the_ins.operands[0]), opsfound*sizeof(the_ins.operands[0])); */
                memset((char *)(&the_ins.operands[0]), '\0', sizeof(the_ins.operands[0]));
                the_ins.operands[0].mode=MSCR;
!               the_ins.operands[0].reg=COP5;           /* COP #6 */
                opsfound++;
        }
  

Also, with the DEBUG_WITH_FPU option, you will be able to run only ONE
process that uses FPE at once to get correct results.


2. MISSING PARTS

For missing instructions, refer to the Section 3.  Other than that,
there is one thing that is missing from this version of FPE: packed
BCD support.

I have no plan to support it since it's rarely used.  However, all we
need to support it is explosion/implosion functions between the
internal FP representation and the m68k PBCD format, so you are more
than welcome to write such functions if you wish to.


3. IMPLEMENTED INSTRUCTIONS

This is the list of implemented and unimplemented FPU instructions.
All 040's directly supported type 0 instructions are already
implemented except FSGLDIV and FSGLMUL.

Type field = bit 8-6 of opcode word

* Implemented Instructions

Type=0: FMOVE (mem->FPr), FINT, FINTRZ, FSQRT, FABS, FNEG, FGETEXP,
	FGETMAN, FDIV, FADD, FMUL, FSGLDIV(*), FSCALE, FSGLMUL(*), FSUB,
	FCMP, FTST, FMOVE (FPr->mem), FMOVEM (FPr), FMOVEM (FPcr),
	FMOVECR, FLOGNP1, FLOGN, FLOG10, FLOG2, FMOD, FREM,
	FCOSH, FSINH, FTANH, FCOS, FSIN, FTAN, FSINCOS,
	FETOX, FETOXM1, FTENTOX, FTWOTOX, FATANH, FACOS, FASIN, FATAN

Type=1: FDBcc, FScc, FTRAPcc,

Type=2: FBcc (word, incl. FNOP)

Type=3: FBcc (long)

Type=4: none

Type=5: none

	*: currently FSGLMUL and FSGLDIV are just aliases of
	   FMUL and FDIV, respectively

* Unimplemented Instructions

Type=0: none

Type=1: none

Type=2: none

Type=3: none

Type=4: FSAVE

Type=5: FRESTORE


4. HOW TO ADD A NEW INSTRUCTION SUPPORT

Since we need not support FSAVE and FRESTORE operations, all
instructions we have to implement are type 0, all of which are
arithmetic operations.  It is particularly easy to add a new
arithmetic instruction to the existing ones (not that it is easy to
write a "stable" function to perform floating point operation. That's
entirely another matter).  In "fpu_emulate.c", there's a function
fpu_emul_arith() which calls emulation functions for all arithmetic
operations.  In it, there's a large switch() { case ... } which
dispatches each instruction emulator.  An emulation function of any
type 0 arithmetic instruction follows this prototype:

	struct fpn *fpu_op(struct fpemu *fe);

Where fe is a pointer to a struct fpemu in which frame, fpframe, and
fetched operands are accessible.  That's right, you don't have to
fetch the operands by yourself in your emulation funtion.  For
instance, the parts calling FSQRT, FSUB, FADD and FTST look like:

	switch(word1 & 0x3F) {
[...]
	case 0x04:	/* fsqrt */
		res = fpu_sqrt(fe);
		break;
[...]
	case 0x28:	/* fsub */
		fe->fe_f2.fp_sign = !fe->fe_f2.fp_sign; /* f2 = -f2 */
	case 0x22:	/* fadd */
		res = fpu_add(fe);
		break;
[...]
	case 0x3A:	/* ftst */
		res = &fe->fe_f2;
		no_store = 1;
		break;
[...]
	default:
		sig = SIGILL;
	} /* switch */

Here, fe->fe_f1 and fe->fe_f2 are fetched operands.  You can use
fe->fe_f3 for storing the result, or you can return a pointer to
either operand if you want to.  At any rate, you have to follow
the following rules:

	1) A dyadic instruction takes two operands fe->fe_f1 and fe->fe_f2.
	2) A monadic instruction takes one operands fe->fe_f2 (NOT fe_f1).
	3) Must return a pointer to struct fpn where the result is stored,
	and assign the pointer to the variable "res".
	4) If exceptions are detected, set corresponding bits in fe->fe_fpsr.
	The rest is taken care of in fpu_emul_arith().
	5) Condition code need not be calculated.  It's taken care of in
	fpu_emul_arith().

Actually, after above was written, stubs for the missing functions were
added to the source, so you do not have to change fpu_emul_arith() at
all.  Function names and prototypes are in fpu_arith_proto.h, and all
except fpu_sincos() follow the rules above.  fpu_sincos() is declared
as

	struct fpn *fpu_sincos(struct fpemu *fe, int cosreg);

where cosreg is the FP register number to which cosine of the argument
is calculated and assigned.  Sine of the argument is stored into the
destination register in the same manner as the other arithmetic
functions.
$Id: README,v 1.1 2007/04/08 09:36:08 scw Exp $

NetBSD/sh5 was removed from the tree on April 8 2007.

The source is still preserved in the CVS attic, for anyone who cares to look.
$NetBSD: README,v 1.3 2000/03/13 23:52:33 soren Exp $

The smallnet network loader is meant to be a short term solution for
machines that can't netboot full size kernels until a proper two stage
netboot is available.  It isn't the prettiest code around, but it gets
us out of a bind.


To use, "make" in this directory and then in the setnetimage directory:

	make KERNEL=/path/to/nfs/install/kernel kernel

and the resultant smallnet.ecoff is suitable for netbooting.


simonb, March-May 1999.
# $NetBSD: README.models,v 1.5 2016/06/19 10:49:34 rkujawa Exp $

MIPS models and architecture levels
-----------------------------------

Since this is a complex and confusing topic and there's a shortage of
information (especially, a shortage of reliable information), I'm
creating this document as a reference for people doing MIPS stuff on
NetBSD (and elsewhere).

Citations appear in []. With luck all important facts have citations.


------------------------------------------------------------
1. Architecture levels.

These architecture levels exist:

        32-bit     64-bit

        MIPS-I
        MIPS-II
                   MIPS-III
                   MIPS-IV
                   MIPS-V
        MIPS32     MIPS64
        MIPS32r2   MIPS64r2
        MIPS32r3   MIPS64r3
        MIPS32r4   MIPS64r4
        MIPS32r5   MIPS64r5
        MIPS32r6   MIPS64r6

Note that while MIPS32 is a 32-bit subset of MIPS64, each
corresponding pair of MIPS32rN and MIPS64rN are comparable in age and
properties. Later revisions (further down the list) are mostly supersets
of earlier revisions, although some exceptions exist.


------------------------------------------------------------
2. CPU models.

For vintage MIPS these are the standard models as of fall 1996 [idt96
A-198] and the corresponding architecture levels. (There were many
additional models put out by licensees or by the MIPS company itself,
which have model numbers with fewer zeros.)

	R2000		MIPS-I (32-bit)		[idt96 1-5]
	R3000		MIPS-I (32-bit)		[idt96 1-5]
	R4000		MIPS-III (64-bit)	[idt96 A-197]
	R4200		MIPS-III (64-bit)	[idt96 A-197]
	R4300		MIPS-III (64-bit)	[idt96 A-197]
	R4400		MIPS-III (64-bit)	[idt96 A-197]
	R4600		MIPS-III (64-bit)	[idt96 A-197]
	R5000		MIPS-IV (64-bit)	[idt96 1-5]
	R6000		MIPS-II ??
	R8000		MIPS-IV (64-bit)	[idt96 1-5]
	R10000		MIPS-IV (64-bit)	[idt96 1-5]
	R1x000		MIPS-IV (64-bit)

For later models than this I currently have no information.


------------------------------------------------------------
3. CPU models present in various systems.

These are the CPU models found in various systems NetBSD does and
doesn't support. This table also notes endianness; MIPS chips are
bi-endian but are wired up one way or the other on motherboards.

   algor (little-endian [buildsh])
	Algorithmics P-4000i			??
	Algorithmics P-4032			??
	Algorithmics P-5064			??
	Algorithmics P-6032			??
   arc (little-endian [buildsh])
	Acer PICA				??
	MIPS Magnum 4000			??
	NEC Image RISCstation			??
	NEC Express RISCserver			??
	NEC RISCserver 2200			??
	NEC RISCstation 2200			??
	NEC RISCstation 2250			??
	NEC Express5800/230 R4400 PCI		presumably R4400 (MIPS-III)
	NEC Express5800/240 R4400 EISA		presumably R4400 (MIPS-III)
	DeskStation Tyne			??
   cobalt (little-endian [buildsh])
	Qube ... ??				??
	RaQ ... ??				??
   emips (big-endian [buildsh])
	... ??					??
	(see http://research.microsoft.com/en-us/projects/emips/ )
   evbmips
   	Loongson 2F ( gdium, lemote etc. )	more or less LE MIPS-III with
   						some extensions
   	xburst ( as in, jz4780, found on CI20 )	LE MIPS32R2 with extensions
	... ??					?? (various-endian)
   ews4800mips (big-endian [buildsh])
	EWS4800/350				??
	EWS4800/350F				??
	EWS4800/360AD				??
	EWS4800/360ADII				??
	EWS4800/360SX				??
	EWS4800/360EX				??
	EWS4800/360				??
	... ??					??
   hpcmips (big-endian [buildsh])
	see http://wiki.netbsd.org/ports/hpcmips/processor_comparison/
   mipsco (big-endian [buildsh])
	Various MIPS Computer Systems, Inc.	see [mipscolist]	
	Bull DPX/Prostation M-20		??
   newsmips (big-endian [buildsh])
	NWS-3470D				R3000 (MIPS-I) [portpage]
	NWS-3410				R3000 (MIPS-I) [portpage]
	NWS-3460				R3000 (MIPS-I) [portpage]
	NWS-3710				R3000 (MIPS-I) [portpage]
	NWS-3720				R3000 (MIPS-I) [portpage]
	NWS-3800 series				??
	NWS-4000 series				R4600 (MIPS-III) [portpage]
	NWS-5000				R4[04]00 (MIPS-III) [portpage]
   playstation2 (little-endian [buildsh])
	playstation2				R5900
	(almost all of MIPS-III + movn/movz) [linux-mips wiki R5900]
   pmax (little-endian [buildsh])
	DECstation/system 2100 and 3100		R2000 (MIPS-I) [portpage]
	DECsystem 5100				R3000 (MIPS-I) [portpage]
	Personal DECstation 5000/20, /25, /33	R3000 (MIPS-I) [portpage]
	Personal DECstation 5000/50		R4000 (MIPS-III) [portpage]
	DECstation/system 5000/120, /125, /133	R3000 (MIPS-I) [portpage]
	DECstation/system 5000/150		R4000 (MIPS-III) [portpage]
        DECstation/system 5000/200		R3000 (MIPS-I) [portpage]
	DECstation/system 5000/240		R3000 (MIPS-I) [portpage]
	DECstation/system 5000/260		R4400 (MIPS-III) [portpage]
	DECsystem 5900				R3000 (MIPS-I) [portpage]
	DECsystem 5900-260			R4400 (MIPS-III) [portpage]
	DECsystem 5500				R3000 (MIPS-I) [portpage]
   sbmips
	BCM91250A (Swarm) evaluation board	Broadcomm BCM1250 [portpage]
   sgimips (big-endian [buildsh])
	4D/20					??
	4D/25					??
	Indigo					R3000 (MIPS-I) [portpage]
	Indigo (R4x00)				R4?00 (MIPS-III) [portpage]
        4D/30					??
	4D/35					??
        Indigo2 (R4x00)				R4?00 (MIPS-III) [portpage]
        Challenge M				??
        Indy (R4x00)				R4?00 (MIPS-III) [portpage]
        Indy (R5000)				R5000 (MIPS-IV) [portpage]
        Challenge S (R4x00)			R4?00 (MIPS-III) [portpage]
	Challenge S (R5000)			R5000 (MIPS-IV) [portpage]
	R10000 Power Indigo2			R10000 (MIPS-IV) [portpage]
	Octane					R1x000 (MIPS-IV) [portpage]
        O2 (R5000)				R5000 (MIPS-IV) [portpage]
	O2 (RM5200)				RM5200 (MIPS-IV) [portpage]
	O2 (R7000)				R7000  (MIPS-IV) [portpage]
	O2 (R10000)				R10000 (MIPS-IV) [portpage]
	O2 (R12000)				R12000 (MIPS-IV) [portpage]
	O2 (R14000)				R14000 (MIPS-IV) [portpage]
	Fuel					R1x000 (MIPS-IV) [portpage]
	Tezro					R1x000 (MIPS-IV) [portpage]
	... ??


------------------------------------------------------------
4. FPU properties

TBD... this is complex and messy (XXX / ??)


------------------------------------------------------------
5. Exception handling properties

TBD... (XXX / ??)


------------------------------------------------------------
6. MMU properties

TBD... (XXX / ??)


------------------------------------------------------------
7. Cache properties

TBD... (XXX / ??)


------------------------------------------------------------
8. Instruction ordering properties and hazards

TBD... (XXX / ??)

In the absence of the SYNC instruction before MIPS-II [idt96 A-172,
mips32insn 215], apparently on a R3000 you can force pending memory
writes to complete by doing an uncached read. [idt96 11-13]

Apparently also on some models but not others the state of the write
buffer is wired to the coprocessor 0 condition bit and you can also do
this by using the otherwise useless bc0f instruction (branch on
coprocessor 0 false) to loop. [no citation, I found this in passing
the other day with Google while looking for something else]


------------------------------------------------------------
9. Pipeline hazards

TBD... (XXX / ??)

On MIPS-I a load instruction requires an explicit one-cycle wait
before using the result. This restriction was lifted in MIPS-II,
with the addition of an interlock in the pipeline. [idt96 A-2]

A multiply should not be started within two cycles of a MFHI or MFLO
instruction, as an interrupt that requires restarting the MFHI or MFLO
might (will) get the result from the subsequent multiply. At least on
MIPS-I. [idt96 2-4]  I guess this is true for divides as well.


------------------------------------------------------------
10. Coprocessor 0 hazards

TBD... (XXX / ??)


----------------------------------------------------------------------
11. Deprecated/removed material.

When the exception handling model was changed for MIPS-III
(introducing the ERET instruction) the RFE instruction supporting the
old model was removed.  [idt96 A-134]

Coprocessor 3 (that is, the ability to have a third coprocessor, which
had never been used for anything) was removed in MIPS-III [idt96 A-197].

The branch likely instructions (e.g. BEQL) were added in MIPS-II
[mips32insn 56] and deprecated not long after, at least by MIPS32
[mips32insn 57] and were removed in release MIPS32 Release 6
[mips32newinsn2a 71].

SSNOP as a special NOP was deprecated in MIPS32/64 Release 6
[mips32newinsn2a 375] and sequences using SSNOP should include the
new EHB which counts as an SSNOP on older implementations
[mips32newinsn2a 174] ... and possibly SYNC/SYNCI ???
[mips32newinsn2a 394-401]

... ??

------------------------------------------------------------
12. Conditional compilation in NetBSD

TBD... (XXX / ??)


------------------------------------------------------------
References:

[buildsh] The MACHINE/MACHINE_ARCH architecture table in NetBSD
	build.sh.

[idt96] "IDT MIPS Microprocessor Family Software Reference Manual",
	Integrated Device Technology, Version 2.0, dated October 1996.

[linux-mips wiki] https://www.linux-mips.org/wiki/

[mips32intro] "MIPS32(TM) Architecture For Programmers Volume I:
	Introduction to the MIPS32(TM) Architecture", MIPS
	Technologies, Document Number MD00082, Revision 0.95, dated
	March 2001. This was apparently an external review version or
	something and has been available on the Internet; the final
	version, and later revisions, never were. (AFAIK)

[mips32insn] "MIPS32(TM) Architecture For Programmers Volume II: The
	MIPS32(TM) Instruction Set", MIPS Technologies, Document Number
	MD00086, Revision 0.95, March 12, 2001. Ditto.

[mips32newinsn2a] "MIPS32(R) Architecture For Programmers Volume II-A:
	The MIPS32(R) Instruction Set Manual", Imagination, Document
	Number MD00086, Revision 6.02, dated December 10, 2014.

[mips32priv] "MIPS32(TM) Architecture For Programmers Volume III: The
	MIPS32(TM) Privileged Resource Architecture", Document Number
	MD00090, Revision 0.95, dated March 2001. Ditto.

[portpage] The port page for this NetBSD port on wiki.netbsd.org, or a
	subpage. Ideally these references should be replaced with
	something less self-referential.

[mipscolist] List of MIPS Computer Systems, Inc. machines hosted on the
	NetBSD web server: https://www.netbsd.org/ports/mipsco/models.html

$NetBSD: README,v 1.1.1.1 1998/06/09 07:53:06 dbj Exp $

NeXT standalone bootblocks.
Rolf Grossmann, Dec 1994

Started work based on files from hp300/stand. boot.c was from post-1.0 
sparc/stand/boot.c, modified to work for the needs of the NeXT PROM,
i.e. it wants to call the kernel, so the bootblock has to return the
entry point.

The code does not try multiple names for te kernel, as I've seen it in
some other architectures' boot code. (The copied code simply didn't do
that ;)) It also doesn't prompt if the argument to boot ends with a
questionmark '?', like the NeXT bootblock does. Do we need this? (Why
should the bootblock as again when you can specify everything on the
boot command line?)

Most files have nothing to do with their original version anymore. The whole
code is a mixture of my own ideas, various other netbsd code I've looked at
(like the sparc scsi code, the independent scsi code, and the needs of the
standalone library).

In contrast to NeXT's bootblocks, mine keep the PROM's idea of what the
boot parameters are, i.e. logical disk number (the number the disk would
get as sd*), the lun and the partition.

TODO
 Make some additional improvements
bootloader can be found at sys/arch/hpc/stand
bootloader can be found at sys/arch/hpc/stand
$NetBSD: README,v 1.3 2008/04/30 21:20:37 garbled Exp $

Contents.
	boot        NetBSD/prep boot-loader
	boot_com0   NetBSD/prep boot-loader(serial console) 
	mkbootimage Make bootable floppy image.

mkbootimage is located in arch/powerpc/stand/mkbootimage.

How to make bootable floppy disk image.

	 $ cd /sys/arch/prep/stand
	 $ make
	    For cross compile environment:
	        $ for i in common boot_com0 boot; do (cd $i; ppc-make); done
	        $ (cd ../../powerpc/stand/mkbootimage; make)
	 $ mkbootimage boot/boot /tmp/boot.fs
	    To make kernel attached bootable floppy image:
		    $ mkbootimage -m prep -b boot/boot -k ../compile/YOUR_KERNEL/netbsd /tmp/boot.fs
$Id: README,v 1.1 2007/04/08 09:35:52 scw Exp $

NetBSD/evbsh5 was removed from the tree on April 8 2007.

The source is still preserved in the CVS attic, for anyone who cares to look.
$NetBSD: README,v 1.3 2001/12/08 04:26:10 gmcgarry Exp $

This directory contains random snippets related to various hp300 issues.

Debug.tips	Some tips for debugging kernel problems.  Out of date but
		may contains some useful info.

HPMMU.notes	Most of this information was collected by David Davis in
		1987 or so while he was working on hp200/300 BSD.

Options		Kernel configuration options that are either defined in
		the prototype makefile or that can be specified in a config
		file.

README		This.

TODO.dev	A fairly ancient list of projects related to IO devices.

TODO.hp300	A much more up do date list of general hp300 projects.

Mike Hibler (mike@cs.utah.edu)
Center for Software Science
University of Utah
February, 1993.
$NetBSD: README,v 1.1 2008/11/13 10:06:46 ad Exp $

Do not automatically install modules that would fundamentally alter system
behaviour or create a security hole, as the system may automatically load
modules.
	$NetBSD: README,v 1.4 2017/04/23 23:39:42 sevan Exp $

                           Kernel Developer's Manual

DESCRIPTION
     The kernel example dynamic modules.

     This directory contains the following example modules:
     * hello           - the simplest `hello world' module
     * properties      - handle incoming properties during the module load
     * readhappy       - basic implementation of read(9) with happy numbers
     * ping            - basic ioctl(9)
     * luahello        - the simplest `hello world' Lua module
     * luareadhappy    - demonstrates calling Lua code from C

     To build the examples you need a local copy of NetBSD sources. You also
     need the comp set with toolchain. To build the module just enter a
     directory with example modules and use make(1):

         # make

     To load, unload, and stat the module use modload(8), modunload(8) and
     modstat(8).

     The S parameter in the Makefile files points to src/sys and it can be
     overloaded in this way:

         # make S=/data/netbsd/src/sys

     The code of a module does not need to be in src/sys unless you use
     the autoconf(9) framework.

     A cross-built of a module for a target platform is possible with the
     build.sh framework. You need to generate the toolchain and set
     appropriately PATH to point bin/ in the TOOLDIR path. An example command
     to cross-build a module with the amd64 toolchain is as follows:

        # nbmake-amd64 S=/data/netbsd/src/sys


     The example modules should not be used on a production machine.

     All modules that create a cdevsw should be verified that the major number
     should not conflict with a real device.

SEE ALSO
     modctl(2), module(7), modload(8), modstat(8), modunload(8), module(9),
     intro(9lua)

HISTORY
     An example of handling incoming properties first appeared in NetBSD 5.0
     and was written by Julio Merino with further modifications by Martin
     Husemann, Adam Hamsik, John Nemeth and Mindaugas Rasiukevicius.

     This document and additional modules (hello, readhappy, properties,
     ping, luahello and luareadhappy) first appeared in NetBSD 8.0; they were
     written by Kamil Rytarowski.

AUTHORS
     This document was written by Kamil Rytarowski.
$NetBSD: README.syscalls,v 1.3 2000/03/13 23:52:35 soren Exp $

XXX this file should be gutted.  functions' comments should go with
XXX the functions.  Further, this file is ... very out of date.

Once the new syscall argument-handling method was implemented, most
OSF/1 syscalls boiled down to their NetBSD equivalents.  The
differences are detailed in this file.

Note that some OSF/1 syscalls, including some of those that map
directly to equivalent NetBSD syscalls, are not implemented; they
were not needed, so the effort to implement and check them was not
expended.

Finally, there are some OSF/1 syscalls which were left unimplemented,
but which seem strange enough to merit a bit more explanation.

OSF/1 compatibility is helped by the fact that the sigcontext
structure was created for NetBSD/Alpha to be the same as the OSF/1
sigcontext structure.  Because of this, only one sendsig() function is
needed, and then the NetBSD sigreturn() function can be used for OSF/1
sigreturn(), as well.

The system calls are split out among the three files:
	osf1_ioctl.c
	osf1_misc.c
	osf1_mount.c
as follows:
	osf1_ioctl.c contains all osf1_ioctl() handling code.
	osf1_mount.c contains all code dealing with mounting and
		unmounting file systems, and with mount points in
		general (e.g. osf1_getfsstat()).
	osf1_misc.c contains the rest of the emulation functions.

The emulation functions as follows:

osf1_mknod()
	dev_t's are different between OSF/1 and NetBSD.  In OSF/1 a
	dev_t has 12 bits of major number and 20 bits of minor number.
	Under NetBSD, it's 24 bits of major, 8 bits of minor (but the
	top 16 bits of the major number are unused, and may be
	rearranged later).  In any case, it was decided that the
	primary use for OSF/1 binaries would be to complement native
	NetBSD binaries, so file system dev_t's are assumed to be in
	the NetBSD format, and osf1_mknod() translates from the OSF/1
	format to the NetBSD format.

osf1_getfsstat()
	The statfs structure is different between NetBSD and OSF/1,
	and the way file system types are denoted is different, as
	well.  This routine is the same as getfsstat(), except it
	converts the statfs structures before returning them to the
	OSF/1 process.

osf1_lseek()
	To compensate for quad alignment on 32-bit machines, the
	NetBSD lseek() needs an extra argument of padding, before the
	off_t 'offset' argument.  This wrapper inserts the padding,
	and calls the NetBSD routine.

osf1_mount()
	The file system type specification and the way you specify
	mount options differs substantially between NetBSD and OSF/1.
	This routine (and its callees) fakes up NetBSD arguments, and
	calls the NetBSD routine.

osf1_unmount()
	Probably not necessary, but safe; translates flags, in case
	the NetBSD unmount flags ever change.

osf1_exec_with_loader() [UNIMPLEMENTED]
	From the description in the OSF/1 manual page, this executes a
	file with a named loader, or "/sbin/loader" if none is named.
	It appears to be used in some way, when executing dynamically
	linked binaries, but is _not_ called directly from user space
	in the normal case.  The interface by which it passes the name
	of the file to be executed, its arguments, etc., to the loader
	is unspecified, and, from experimental evidence, doesn't seem
	to be the normal UN*X argument-passing convention (i.e.
	argc/argv).  For proper dynamically linked binary support,
	this function will probably have to be implemented, but it's
	unclear how that can be done (short of disassembling a lot of
	code).

osf1_open()
	Translates OSF/1 flags to NetBSD flags.

osf1_ioctl()
	Screens out ioctl requests that aren't known to work, and
	translates those that differ between NetBSD and OSF/1.

osf1_reboot()
	Translates OSF/1 flags to NetBSD flags.

osf1_stat()
	The stat structure differs between NetBSD and OSF/1, both in
	terms of field sizes, and in the dev_t representation.
	This does a NetBSD stat(), translates the results, and returns
	them to the OSF/1 process.

osf1_lstat()
	Same as osf1_stat(), but for lstat().

osf1_mmap()
	The NetBSD version needs 4 bytes of padding before the off_t
	'pos' argument, and also uses different flags than the OSF/1
	version.  This wrapper translates the flags and deals with the
	argument struct padding differences, then calls the NetBSD
	routine.

osf1_fstat()
	Same as osf1_stat(), but for fstat().

osf1_fcntl()
	Translates OSF/1 fcntl() requests into their NetBSD
	counterparts, then calls the NetBSD fcntl() to do the
	operations.

osf1_socket()
	Makes sure that the socket type is valid for NetBSD, and if
	so, calls NetBSD's socket().

osf1_sendto()
	Makes sure that the 'flags' argument doesn't use flags that
	NetBSD can't handle, and calls NetBSD's sendto().

osf1_getrlimit()
	Makes sure that the 'which' selector is one that NetBSD can
	deal with, and calls NetBSD's getrlimit().

osf1_setrlimit()
	Same as osf1_getrlimit(), but for setrlimit().

osf1_sigaction()
	Deals with the differences in the NetBSD and OSF/1 sigaction
	structures, and calls NetBSD's sigaction with the appropriate
	arguments.  If the call requests that the old sigaction be
	passed back, osf1_sigaction() translates it back to the OSF/1
	form, and returns it appropriately.

osf1_statfs()
	Does that statfs() on the given pathname, then translates the
	NetBSD statfs structure into the one that OSF/1 uses and
	returns it.  Makes a best effort on the mount type, because
	there's not a one-to-one mapping between NetBSD and OSF/1
	mount types.

osf1_fstatfs()
	Same as osf1_statfs(), but for fstatfs().

osf1_usleep_thread()
	This function is how sleep() and usleep() (and possibly other
	functions) are implemented in OSF/1.  Its usage was discovered
	by disassembling the library routines that use it.  It takes
	two pointers to timeval structures as arguments.  The first
	contains the amount of time (in seconds and microseconds) to
	sleep.  If the second pointer is non-null, if the process
	wakes up early, the amount of time "unslept" is returned.  If
	the process doesn't wake up early, zero is returned.

osf1_setsysinfo()
	A null-op; used early on, but nothing cares that it actually
	does anything.
$NetBSD: README.dynamic,v 1.3 1999/04/27 16:08:40 cgd Exp $

Dynamically linked programs are supported by NetBSD's Digital UNIX
(formerly DEC OSF/1) emulation.  The OSF/1 dynamic linker scheme is
described in fair detail in:

	%A Larry W. Allen
	%A Harminder G. Singh
	%A Kevin G. Wallace
	%A Melanie B. Weaver
	%T Program Loading in OSF/1
	%P 145-160
	%I USENIX
	%B USENIX Conference Proceedings
	%D January 21-25, 1991
	%C Dallas, TX
	%W Open Software Foundation

Additionally, the object file formats in use are described in the
Digital UNIX _Assembly Language Programmer's Guide_ which can be
found (among other places) on a version-specific page off of:

	http://www.unix.digital.com/faqs/publications/pub_page/doc_list.html

Finally, the actual representation of Auxiliary Vectors came from information
in the Digital UNIX auxv.h header file.

There are at least two potential issues with the implementation as it
currently exists:

1. exec_with_loader() is not emulated.

	Most uses of dynamically linked programs come through execve()
	and use the default loader (/sbin/loader).  In Digital UNIX
	(and OSF/1) you can also force a specific loader to always be
	invoked to load an executable by using the exec_with_loader()
	system call.  Few, if any, programs use this feature.

2. It is not immediately obvious that the right values are used for
   text and data locations when invoking a dynamically linked executable.

	The text and data sections, and the break, are set up as if
	/sbin/loader itself had been executed.  It's not clear that this
	is correct, but /sbin/loader seems to expect that at least the
	break will be set up this way.

	This has certain implications for the way mmap() behaves.  See
	the comment in the osf1_mmap() function in osf1_misc.c.

3. The stack location is used is the normal NetBSD/alpha stack location.

	No attempt is made to put the stack in the place where
	Digital UNIX would normally put it.  This may confuse some
	programs.
$NetBSD: README.mach-traps,v 1.3 2001/06/04 20:06:41 nathanw Exp $

Some Alpha AXP OSF/1 binaries directly use the facilities provided by
the Mach kernel that is the basis for OSF/1.  These include (but are
surely not limited to) 'dd', 'ps', and 'w'.

Invariably, the symptom that these binaries display is that they crash
with an "unimplemented system call" trap (SIGSYS signal) for a syscall
that has a negative number.  In general, binaries that use the Mach
syscalls appear to invoke task_self() as their first syscall.

Note that system call numbers are now range-bounded by masking off the
high bits, so negative system call numbers will appear in the high
end of the system call number range, thanks to the wonders of two's
compliment arithmetic. System call -10, for example, will appear as
"CALL  [502]" in kdump output.

The name, number, and number of arguments for each Mach syscall is
given below; this information was gleaned by looking through the OSF/1
libmach.a's object files with dbx, then double-checked against the
contents of OSF/1's <mach/syscall_sw.h>.

These calls would be very difficult to implement properly in the
OSF/1 emulation code; by its very nature, NetBSD is not Mach, and we
don't and can't provide the underlying facilities that it does.

-- cgd

trap name			number	nargs	notes
---- ----			------	-----	-----
task_self			-10	0
thread_reply			-11	0
task_notify			-12	0
thread_self			-13	0
msg_send_old			-14	3
msg_receive_old			-15	3
msg_rpc_old			-16	5
msg_send_trap			-20	4
msg_receive_trap		-21	5
msg_rpc_trap			-22	6
lw_wire				-30	3
lw_unwire			-31	1
nxm_task_init			-33	2
nxm_sched_thread		-34	1
nxm_idle			-35	1
nxm_wakeup_idle			-36	1
nxm_set_pthid			-37	2
nxm_thread_kill			-38	2
nxm_thread_block		-39	1
nxm_thread_wakeup		-40	1
inode_swap_preference		-40	3	old call?
init_process			-41	0
map_fd				-43	5
nxm_resched			-44	2
htg_unix_syscall		-52	3
host_self			-55	1
host_priv_self			-56	1
swtch_pri			-59	1
swtch				-60	0
thread_switch			-61	3
semop_fast			-62	4
mach_sctimes_0			-70	0	only if MACH_SCTIMES defined
mach_sctimes_1			-71	1	only if MACH_SCTIMES defined
mach_sctimes_2			-72	2	only if MACH_SCTIMES defined
mach_sctimes_3			-73	3	only if MACH_SCTIMES defined
mach_sctimes_4			-74	4	only if MACH_SCTIMES defined
mach_sctimes_5			-75	5	only if MACH_SCTIMES defined
mach_sctimes_6			-76	6	only if MACH_SCTIMES defined
mach_sctimes_7			-77	0	only if MACH_SCTIMES defined
mach_sctimes_8			-78	6	only if MACH_SCTIMES defined
mach_sctimes_9			-79	1	only if MACH_SCTIMES defined
mach_sctimes_10			-80	2	only if MACH_SCTIMES defined
mach_sctimes_11			-81	2	only if MACH_SCTIMES defined
mach_sctimes_port_alloc_dealloc	-82	1	only if MACH_SCTIMES defined
$NetBSD: README,v 1.4 2012/08/04 12:47:00 christos Exp $

Coda is a distributed filesystem.  It is derived from AFS, but
supports disconnected operation, both reading and writing.  This
directory contains the interface between the VFS layer and a
user-space program ("venus") that implements the client part of Coda.
The interface is similar to puffs(9) in many respects.

Coda servers do not need kernel support.

For information on Coda, see
  http://www.coda.cs.cmu.edu

As of Sat Aug  4 15:45:27 EEST 2012
	- The coda/vcoda modules have been tested for read/write
	  operations and load/unload on amd64 with a DEBUG/DIAGNOSTIC
	  kernel and there are no locking errors.
	- If you find issues with coda, please file a bug report.
	  Also help can be obtained via the list codalist@coda.cs.cmu.edu.
	- Pkgsrc/net/coda has the latest source available from cmu and
	  has been fixed to co-exist with openafs (openafs has been modified)

To test:
- Install pkgsrc/net/coda

# rm -fr /var/lib/coda			# remove junk if there was a crash
# /usr/pkg/sbin/venus-setup xxxx	# xxxx the domain name
# /usr/pkg/sbin/venus
# clog guest@testserver.coda.cs.cmu.edu	# password is guest
# echo foo > /coda/testserver.coda.cs.cmu.edu/playground/nb.test0 

You should be able to access things in /coda
This is new nfs code (including nfsv4) imported from FreeBSD. It is
not even experimental yet - you don't want to be mucking with it.

The nfs code (both nfs and nfsd) that NetBSD currently uses is in
sys/nfs.


The following things are here:

   client/	- nfs client code, from sys/fs/nfsclient in freebsd
   server/	- nfsd server code, from sys/fs/nfsserver in freebsd
   nlm/		- kernel-side nfs lock manager, from sys/nlm in freebsd
   common/	- shared common code, from
			* sys/fs/nfs in freebsd
			* sys/nfs in freebsd
   files.newnfs - config goo
   nfs2netbsd.sh - script for preparing a cvs import from a freebsd tree

The FreeBSD shared common code contained two (different) nfsproto.h
and xdr_subs.h files. This is how they've been imported:

   sys/fs/nfs/nfsproto.h	->	common/nfsproto.h
   sys/fs/nfs/xdr_subs.h	->	common/xdr_subs.h
   sys/nfs/nfsproto.h		->	common/oldnfsproto.h
   sys/nfs/xdr_subs.h		->	common/old_xdr_subs.h
#	$NetBSD: README,v 1.3 1999/03/15 00:46:47 perseant Exp $

#	@(#)README	8.1 (Berkeley) 6/11/93

The file system is reasonably stable...I think.

For details on the implementation, performance and why garbage
collection always wins, see Dr. Margo Seltzer's thesis available for
anonymous ftp from toe.cs.berkeley.edu, in the directory
pub/personal/margo/thesis.ps.Z, or the January 1993 USENIX paper.

----------
The disk is laid out in segments.  The first segment starts 8K into the
disk (the first 8K is used for boot information).  Each segment is composed
of the following:

	An optional super block
	One or more groups of:
		segment summary
		0 or more data blocks
		0 or more inode blocks

The segment summary and inode/data blocks start after the super block (if
present), and grow toward the end of the segment.

	_______________________________________________
	|         |            |         |            |
	| summary | data/inode | summary | data/inode |
	|  block  |   blocks   |  block  |   blocks   | ...
	|_________|____________|_________|____________|

The data/inode blocks following a summary block are described by the
summary block.  In order to permit the segment to be written in any order
and in a forward direction only, a checksum is calculated across the
blocks described by the summary.  Additionally, the summary is checksummed
and timestamped.  Both of these are intended for recovery; the former is
to make it easy to determine that it *is* a summary block and the latter
is to make it easy to determine when recovery is finished for partially
written segments.  These checksums are also used by the cleaner.

	Summary block (detail)
	________________
	| sum cksum    |
	| data cksum   |
	| next segment |
	| timestamp    |
	| FINFO count  |
	| inode count  |
	| flags        |
	|______________|
	|   FINFO-1    | 0 or more file info structures, identifying the
	|     .        | blocks in the segment.
	|     .        |
	|     .        |
	|   FINFO-N    |
	|   inode-N    |
	|     .        |
	|     .        |
	|     .        | 0 or more inode daddr_t's, identifying the inode
	|   inode-1    | blocks in the segment.
	|______________|

Inode blocks are blocks of on-disk inodes in the same format as those in
the FFS.  However, spare[0] contains the inode number of the inode so we
can find a particular inode on a page.  They are packed page_size /
sizeof(inode) to a block.  Data blocks are exactly as in the FFS.  Both
inodes and data blocks move around the file system at will.

The file system is described by a super-block which is replicated and
occurs as the first block of the first and other segments.  (The maximum
number of super-blocks is MAXNUMSB).  Each super-block maintains a list
of the disk addresses of all the super-blocks.  The super-block maintains
a small amount of checkpoint information, essentially just enough to find
the inode for the IFILE (fs->lfs_idaddr).

The IFILE is visible in the file system, as inode number IFILE_INUM.  It
contains information shared between the kernel and various user processes.

	Ifile (detail)
	________________
	| cleaner info | Cleaner information per file system.  (Page
	|              | granularity.)
	|______________|
	| segment      | Space available and last modified times per
	| usage table  | segment.  (Page granularity.)
	|______________|
	|   IFILE-1    | Per inode status information: current version #,
	|     .        | if currently allocated, last access time and
	|     .        | current disk address of containing inode block.
	|     .        | If current disk address is LFS_UNUSED_DADDR, the
	|   IFILE-N    | inode is not in use, and it's on the free list.
	|______________|


First Segment at Creation Time:
_____________________________________________________________
|        |       |         |       |       |       |       |
| 8K pad | Super | summary | inode | ifile | root  | l + f |
|        | block |         | block |       | dir   | dir   |
|________|_______|_________|_______|_______|_______|_______|
	  ^
           Segment starts here.

Some differences from the Sprite LFS implementation.

1. The LFS implementation placed the ifile metadata and the super block
   at fixed locations.  This implementation replicates the super block
   and puts each at a fixed location.  The checkpoint data is divided into
   two parts -- just enough information to find the IFILE is stored in
   two of the super blocks, although it is not toggled between them as in
   the Sprite implementation.  (This was deliberate, to avoid a single
   point of failure.)  The remaining checkpoint information is treated as
   a regular file, which means that the cleaner info, the segment usage
   table and the ifile meta-data are stored in normal log segments.
   (Tastes great, less filling...)

2. The segment layout is radically different in Sprite; this implementation
   uses something a lot like network framing, where data/inode blocks are
   written asynchronously, and a checksum is used to validate any set of
   summary and data/inode blocks.  Sprite writes summary blocks synchronously
   after the data/inode blocks have been written and the existence of the
   summary block validates the data/inode blocks.  This permits us to write
   everything contiguously, even partial segments and their summaries, whereas
   Sprite is forced to seek (from the end of the data inode to the summary
   which lives at the end of the segment).  Additionally, writing the summary
   synchronously should cost about 1/2 a rotation per summary.

3. Sprite LFS distinguishes between different types of blocks in the segment.
   Other than inode blocks and data blocks, we don't.

4. Sprite LFS traverses the IFILE looking for free blocks.  We maintain a
   free list threaded through the IFILE entries.

5. The cleaner runs in user space, as opposed to kernel space.  It shares
   information with the kernel by reading/writing the IFILE and through
   cleaner specific system calls.

Line counts.

(Part of the premise of splitting lfs from ufs is that in the long run
the size of a standalone lfs will be substantially smaller than the
size of lfs plus the size of ufs. This file is for keeping track of
this proposition.)

As of 20130604 (before the split):
		.h	.c	total
lfs		1467	13858	15325
ufs		2056	12919	14975

As of 20130605 (copied all ufs files verbatim):

lfs-native	1467	13858	15325
lfs-ulfs	2070	12938	15008
lfs-total	3537	26796	30333

A few extra lines appeared copying ufs because I preserved a copy of
the old rcsids.

As of 20130606 (committed the initial split and made things buildable):

lfs-native	1482	13858	15340
lfs-ulfs	1994	13028	15022
lfs-total	3476	26886	30362
$NetBSD: README,v 1.7 2015/01/12 19:50:47 christos Exp $

makefs - build a file system image from a directory tree

NOTES:

    *   This tool uses modified local copies of source found in other
	parts of the tree.  This is intentional.

    *	makefs is a work in progress, and subject to change.


user overview:
--------------

makefs creates a file system image from a given directory tree.
the following file system types can be built:

	cd9660	ISO 9660 file system
	chfs	"Chip" file system, for flash devices
	ffs	BSD fast file system
	msdos	MS-DOS `FAT' file system (FAT12, FAT16, FAT32)
	udf	Universal Disk Format file system
	v7fs	7th edition(V7) file system

Support for the following file systems maybe be added in the future

	ext2fs	Linux EXT2 file system

Various file system independent parameters and contraints can be
specified, such as:

	- minimum file system size (in KB)
	- maximum file system size (in KB)
	- free inodes
	- free blocks (in KB)
	- mtree(8) specification file containing permissions and ownership
	  to use in image, overridding the settings in the directory tree
	- file containing list of files to specifically exclude or include
	- fnmatch(3) pattern of filenames to exclude or include
	- endianness of target file system

File system specific parameters can be given as well, with a command
line option such as "-o fsspeccific-options,comma-separated".
For example, ffs would allow tuning of:

	- block & fragment size
	- cylinder groups
	- number of blocks per inode
	- minimum free space

Other file systems might have controls on how to "munge" file names to
fit within the constraints of the target file system.

Exit codes:
	0	all ok
	1	fatal error
	2	some files couldn't be added during image creation
		(bad perms, missing file, etc). image will continue
		to be made


Implementation overview:
------------------------

The implementation must allow for easy addition of extra file systems
with minimal changes to the file system independent sections.

The main program will:
	- parse the options, including calling fs-specific routines to
	  validate fs-specific options
	- walk the tree, building up a data structure which represents
	  the tree to stuff into the image. The structure will
	  probably be a similar tree to what mtree(8) uses internally;
	  a linked list of entries per directory with a child pointer
	  to children of directories. ".." won't be stored in the list;
	  the fs-specific tree walker should add this if required by the fs. 
	  this builder have the smarts to handle hard links correctly.
	- (optionally) Change the permissions in the tree according to
	  the mtree(8) specfile
	- Call an fs-specific routine to build the image based on the
	  data structures.

Each fs-specific module should have the following external interfaces:

    prepare_options	optional file system specific defaults that need to be
			setup before parsing fs-specific options.

    parse_options	parse the string for fs-specific options, feeding
			errors back to the user as appropriate

    cleanup_options	optional file system specific data that need to be
			cleaned up when done with this filesystem.

    make_fs		take the data structures representing the
			directory tree and fs parameters,
			validate that the parameters are valid
			(e.g, the requested image will be large enough), 
			create the image, and
			populate the image

prepare_options and cleanup_options are optional and can be NULL.

NOTE: All file system specific options are referenced via the fs_specific
pointer from the fsinfo_t strucutre. It is up to the filesystem to allocate
and free any data needed for this via the prepare and cleanup callbacks.

Each fs-specific module will need to add its routines to the dispatch array
in makefs.c and add prototypes for these to makefs.h

All other implementation details should not need to change any of the
generic code.

ffs implementation
------------------

In the ffs case, we can leverage off sbin/newfs/mkfs.c to actually build
the image. When building and populating the image, the implementation
can be greatly simplified if some assumptions are made:
	- the total required size (in blocks and inodes) is determined
	  as part of the validation phase
	- a "file" (including a directory) has a known size, so
	  support for growing a file is not necessary

Two underlying primitives are provided:
	make_inode	create an inode, returning the inode number

	write_file	write file (from memory if DIR, file descriptor
			if FILE or SYMLINK), referencing given inode.
			it is smart enough to know if a short symlink
			can be stuffed into the inode, etc.

When creating a directory, the directory entries in the previously
built tree data structure is scanned and built in memory so it can
be written entirely as a single write_file() operation.

This is an enhanced version of the CMU BOOTP server which was derived
from the original BOOTP server created by Bill Croft at Stanford.
This version merges all the enhancements and bug-fixes from the
NetBSD, Columbia, and other versions.

Please direct questions, comments, and bug reports to the list:
	<bootp@andrew.cmu.edu>

You can subscribe to this mailing list by sending mail to:
	bootp-request@andrew.cmu.edu
(The body of the message should contain: "Add <your-address>")

[ From the NetBSD README file: ]

BOOTPD is a useful adjunct to the nfs diskless boot EPROM code.

The alternatives for initiating a boot of a kernel across a network
are to use RARP protocol, or BOOTP protocol. BOOTP is more flexible;
it allows additional items of information to be returned to the
booting client; it also supports booting across gateways.

[ From the CMU README file: ]

Notes:
1) BOOTP was originally designed and implemented by Bill Croft at Stanford.
   Much of the credit for the ideas and the code goes to him.  We've added
   code to support the vendor specific area of the packet as specified in
   RFC1048.  We've also improved the host lookup algorithm and added some
   extra logging.

2) The server now uses syslog to do logging.  Specifically it uses the 4.3bsd
   version.  I've #ifdef'd all of these calls.  If you are running 4.2 you
   should compile without the -DSYSLOG switch.

3) You must update your /etc/services file to contain the following two lines:
	bootps		67/udp		bootp		# BOOTP Server
	bootpc		68/udp				# BOOTP Client

4) Edit the bootptab.  It has some explanitory comments, and there
   is a manual entry describing its format (bootptab.5)
   If you have any questions, just let us know.

Construction:
    [ See the file Installation which is more up-to-date. -gwr ]

    Make sure all of the files exist first.  If anything is missing,
    please contact either Walt Wimer or Drew Perkins by E-mail or phone.
    Addresses and phone numbers are listed below.

    Type 'make'.  The options at present are: -DSYSLOG which enables logging
    code, -DDEBUG which enables table dumping via signals, and -DVEND_CMU
    which enables the CMU extensions for CMU PC/IP.

    Edit the bootptab.  The man page and the comments in the file should
    explain how to go about doing so.  If you have any problems, let me know.

    Type 'make install'.  This should put all of the files in the right place.

    Edit your /etc/rc.local or /etc/inetd.conf file to start up bootpd upon
    reboot.  The following is a sample /etc/inetd.conf entry:
	# BOOTP server
	bootps dgram udp wait root /usr/etc/bootpd bootpd -i

Care and feeding:
    If you change the interface cards on your host or add new hosts you will
    need to update /etc/bootptab.  Just edit it as before.  Once you write
    it back out, bootpd will notice that there is a new copy and will
    reread it the next time it gets a request.

    If your bootp clients don't get a response then several things might be
    wrong.  Most often, the entry for that host is not in the database.
    Check the hardware address and then check the entry and make sure
    everything is right.  Other problems include the server machine crashing,
    bad cables, and the like.  If your network is very congested you should
    try making your bootp clients send additional requests before giving up.


November 7, 1988


Walter L. Wimer			Drew D. Perkins
ww0n@andrew.cmu.edu		ddp@andrew.cmu.edu
(412) 268-6252			(412) 268-8576

4910 Forbes Ave
Pittsburgh, PA  15213

[ Contents description by file: ]

Announce*	Text of release announcements
Changes  	Change history, reverse chronological
Installation	Instructions for building and installing
Makefile*	for "make"
README		This file
ToDo		Things not yet done
bootp.h		The protocol header file
bootpd.8	Manual page for bootpd, boopgw
bootpd.c	BOOTP server main module
bootpd.h	 header for above (and others)
bootpef.8	Manual page for bootpef
bootpef.c	BOOTP extension file compiler
bootpgw.c	BOOTP gateway main module
bootptab.5	A manual describing the bootptab format
bootptab.cmu	A sample database file for the server
bootptab.mcs	Another sample from <gwr@mc.com>
bootptest.8	Manual page for bootptest
bootptest.c	BOOTP test program (fake client)
bootptest.h	 header for above
dovend.c	Vendor Option builder (for bootpd, bootpef)
dovend.h	 header for above
dumptab.c	Implements debugging dump for bootpd
getether.c	For bootptest (not used yet)
getif.c		Get network interface info.
getif.h		 header for above
hash.c		The hash table module
hash.h		 header for above
hwaddr.c	Hardware address support
hwaddr.h	 header for above
lookup.c	Internet Protocol address lookup
lookup.h	 header for above
patchlevel.h	Holds version numbers
print-bootp.c	Prints BOOTP packets (taken from BSD tcpdump)
readfile.c	The configuration file-reading routines
readfile.h	 header for above
report.c	Does syslog-style messages
report.h	 header for above
strerror.c	Library errno-to-string (for systems lacking it)
syslog.conf	Sample config file for syslogd(8)
syslog.h	For systems that lack syslog(3)
try*.c		Test programs (for debugging)
tzone.c		Get timezone offset
tzone.h		 header for above
# $NetBSD: README.hardlinks,v 1.1 2016/05/29 22:32:03 dholland Exp $
#

catman.8 notes that this code doesn't handle hard links.

To fix this, one might proceed as follows:

(1) Add an additional data structure mapping (fsid_t, ino_t) pairs
from stat to filenames.

(2) In scanmandir(), in the readdir loop, next to the code that checks
for a symlink, check if the page's linkcount > 1 (manstat.st_nlink > 1)
and if so:
   a. Check the new data structure to see if this file's been seen before.
      If so, use the saved name of the page to construct a hard link in the
      cat directory, and continue to the next directory entry.
   b. If not, add to the new data structure to remember this page and
      process it as normal.

One might clear the data structure for each mandir or not; probably
for each man tree is best. The size of the structure isn't
prohibitive; but the chances of ever seeing the same hardlinked page
in two different man trees (e.g. in both /usr/share/man and
/usr/pkg/man) are pretty well zero.

All of this seems like a SMOP, but it doesn't really seem worth doing
at the moment given that we don't build catpages at all by default and
they aren't particularly useful to have any more except on the slowest
of slow hardware. I've left this note so that someone else can take it
up if they see fit.

 - dholland 20160529
Configuring FAITH IPv6-to-IPv4 TCP relay

Kazu Yamamoto and Jun-ichiro itojun Hagino
$KAME: README,v 1.9 2002/05/09 14:10:06 itojun Exp $


Introduction
============

FAITH is a IPv6-to-IPv4 TCP relay.  It performs tcp relay just as some of
firewall-oriented gateway does, but between IPv6 and IPv4 with address
translation.
TCP connections has to be made from IPv6 node to IPv4 node.  FAITH will
not relay connections for the opposite direction.
To perform relays, FAITH daemon needs to be executed on a router between
your local IPv6 site and outside IPv4 network.  The daemon needs to be
invoked per each TCP services (TCP port number).

	IPv4 node "dest" = 123.4.5.6
		|
	[[[[ outside IPv4 ocean ]]]]
		|
	node that runs FAITH-daemon (usually a router)
		|
	==+=====+===+==== IPv6, or IPv4/v6 network in your site ^
	  |	    |						| connection
	clients	  IPv6 node "src"				|

You will have to allocate an IPv6 address prefix to map IPv4 addresses into.
The following description uses 3ffe:0501:ffff:0000:: as example.
Please use a prefix which belongs to your site.
FAITH will make it possible to make a IPv6 TCP connection From IPv6 node
"src", toward IPv4 node "dest", by specifying FAITH-mapped address
3ffe:0501:ffff:0000::123.4.5.6
(which is, 3ffe:0501:ffff:0000:0000:0000:7b04:0506).
The address mapping can be performed by hand:-), by special nameserver on
the network, or by special resolver on the source node.


Setup
=====

The following example assumes:
- You have assigned 3ffe:0501:ffff:0000:: as FAITH adderss prefix.
- You are willing to provide IPv6-to IPv4 TCP relay for telnet.

<<On the translating router on which faithd runs>>

(1) If you have IPv6 TCP server for the "telnet" service, i.e. telnetd via
    inet6d, disable that daemon.  Comment out the line from "inet6d.conf"
    and send the HUP signal to "inet6d".

(2) Execute sysctl as root to enable FAITH support in the kernel.

        # sysctl -w net.inet6.ip6.keepfaith=1

(3) Route packets toward FAITH prefix into "faith0" interface.

	# ifconfig faith0 up
	# route add -inet6 3ffe:0501:ffff:0000:: -prefixlen 64 ::1
	# route change -inet6 3ffe:0501:ffff:0000:: -prefixlen 64 -ifp faith0

(4) Execute "faithd" by root as follows:

	# faithd telnet /usr/libexec/telnetd telnetd

    1st argument is a service name you are willing to provide TCP relay.
	(it can be specified either by number "23" or by string "telnet")
    2nd argument is a path name for local IPv6 TCP server.  If there is a
    connection toward the router itself, this program will be invoked.
    3rd and the following arguments are arguments for the local IPv6 TCP
    server.  (3rd argument is typically the program name without its path.)

    More examples:

	# faithd ftpd /usr/libexec/ftpd ftpd -l
	# faithd sshd

If inetd(8) on your platform have special support for faithd, it is possible
to setup faithd services via inetd(8).  Consult manpage for details.


<<Routing>>

(4) Make sure that packets whose destinations match the prefix can
reach from the IPv6 host to the translating router.

<<On the IPv6 host>>

There are two ways to translate IPv4 address to IPv6 address:
	(a) Faked by DNS
	(b) Faked by /etc/hosts.

(5.a) Install "newbie" and set up FAITH mode. See kit/ports/newbie.

(5.b) Add an entry into /etc/hosts so that you can resolve hostname into
faked IPv6 address.  For example, add the following line for
www.NetBSD.org:

	3ffe:0501:ffff:0000::140.160.140.252	www.NetBSD.org

<<On the translating router on which faithd runs.>>

(6) To see if "faithd" works, watch "/var/log/daemon". Note: please
setup "/etc/syslog.conf" so that LOG_DAEMON messages are to be stored
in "/var/log/daemon".

	<e.g.>
	daemon.*   /var/log/daemon


Access control
==============

Since faithd implements TCP relaying service, it is critical to implement
proper access control to cope with malicious use.  Bad guy may try to
use your relay router to circumvent access controls, or may try to
abuse your network (like sending SPAMs from IPv4 address that belong to you).
Install IPv6 packet filter directives that would reject traffic from
unwanted source.  If you are using inetd-based setup, you may be able to
use access control mechanisms in inetd.


Advanced configuration
======================

If you would like to restrict IPv4 destination for translation, you may
want to do the following:

	# route add -inet6 3ffe:0501:ffff:0000::123.0.0.0 -prefixlen 104 ::1
	# route change -inet6 3ffe:0501:ffff:0000::123.0.0.0 -prefixlen 104 \
		-ifp faith0

By this way, you can restrict IPv4 destination to 123.0.0.0/8.
You may also want to reject packets toward 3ffe:0501:ffff:0000::/64 which
is not in 3ffe:0501:ffff:0000::123.0.0.0/104.  This will be left as excerside
for the reader.

By doing this, you will be able to provide your IPv4 web server to outside
IPv6 customers, without risks of unwanted open relays.

	[[[[ IPv6 network outside ]]]]			|
		|					| connection
	node that runs FAITH-daemon (usually a router)	v
		|
	========+======== IPv4/v6 network in your site
		|			(123.0.0.0/8)
	IPv4 web server
$NetBSD: README,v 1.9 2008/01/30 14:16:42 ad Exp $

NOTE:
	- tprof driver currently only supports pentium4 (netburst) processors.
	- it samples program counters on every PMIs.
	- it's currently hardcoded to use global_power_events events.
	  for details, see x86/x86/tprof_pmi.c and intel's processor manuals.

usage:

0. set SIZEOF_PTR environment variable, which is used by tpfmt.sh and tpann.sh.
   if not set, SIZEOF_PTR=4 is assumed.

1. add a line to your kernel config.

	pseudo-device	tprof

2. create a device special file.

	# mknod /dev/tprof c 191 0

3. run the tprof command.

	# tprof -o /tmp/foo sleep 1

	tprof statistics:
		sample 57
		overflow 0
		buf 3
		emptybuf 3
		dropbuf 0
		dropbuf_sample 0

4. format the result.
   the first line in the following example means that 11 samples have been
   taken at 0xc0396c36, whose symbolic name is lapic_gettick+0x6.

	# sh ./tpfmt.sh < /tmp/foo
	11      c0396c36        lapic_gettick+0x6
	5       c039b98a        x86_pause+0x2
	4       c010cf9d        __cpu_simple_lock+0xd
	2       c010cfcd        __cpu_simple_lock_try+0xd
	2       c039b571        bus_space_read_4+0x11
	1       c01005c8        sse2_zero_page+0x18
	1       c0100624        sse2_copy_page+0x34
	1       c010ceeb        mutex_spin_enter+0x2b
	1       c010cef5        mutex_spin_enter+0x35
	1       c010cf32        mutex_spin_exit+0x32
	1       c0119ed0        in_localaddr+0x30
	1       c012d0fd        tcp_output+0x1fbd
	1       c02980c2        amap_copy+0x42
	1       c02a0100        uvm_map_lookup_entry_bytree+0x20
	1       c02a27fe        uvm_tree_RB_REMOVE+0xee
	1       c02a8914        uvm_pagelookup+0x4
	1       c02a9d5c        uvm_pagefree+0xfc
	1       c02a9e36        uvm_pagefree+0x1d6
	1       c02dd9d1        _kernel_unlock+0xa1
	1       c02e0285        mutex_vector_enter+0x15
	1       c02eb83a        sleepq_wake+0x5a
	1       c0303467        pool_cache_get_paddr+0x97
	1       c030368b        pool_cache_put_slow+0x6b
	1       c0321ed3        pffasttimo+0x33
	1       c034547a        VOP_LOCK+0xa
	1       c0346235        VOP_ACCESS+0x45
	1       c034a749        genfs_unlock+0x29
	1       c038f251        cpu_idle+0x31
	1       c03938da        pmap_write_protect+0xaa
	1       c0394305        pmap_do_remove+0x2e5
	1       c03944b3        pmap_do_remove+0x493
	1       c0396cdf        lapic_delay+0x5f
	1       c0396d19        lapic_delay+0x99
	1       c0396d1d        lapic_delay+0x9d
	1       c0397429        lapic_clockintr+0x19
	1       c039b984        x86_mwait+0xc
	1       c042f66a        _atomic_swap_32+0xa

5. tpann.sh is another formatter.  it outputs "objdump -d" with numbers of
   samples for each addresses.

	# tprof -o /tmp/bar sleep 100
	# sh ./tpann.sh < /tmp/bar

		:
		snip
		:

	c01005e0 <sse2_zero_page>:
	       4  c01005e0:     55                      push   %ebp
	      11  c01005e1:     89 e5                   mov    %esp,%ebp
	       1  c01005e3:     8b 54 24 08             mov    0x8(%esp),%edx
	       3  c01005e7:     b9 00 10 00 00          mov    $0x1000,%ecx
	       1  c01005ec:     31 c0                   xor    %eax,%eax
	       1  c01005ee:     89 f6                   mov    %esi,%esi
	    7936  c01005f0:     0f c3 42 00             movnti %eax,0x0(%edx)
	    6371  c01005f4:     0f c3 42 04             movnti %eax,0x4(%edx)
	    1220  c01005f8:     0f c3 42 08             movnti %eax,0x8(%edx)
	     741  c01005fc:     0f c3 42 0c             movnti %eax,0xc(%edx)
	    1178  c0100600:     0f c3 42 10             movnti %eax,0x10(%edx)
	    1334  c0100604:     0f c3 42 14             movnti %eax,0x14(%edx)
	     976  c0100608:     0f c3 42 18             movnti %eax,0x18(%edx)
	    1299  c010060c:     0f c3 42 1c             movnti %eax,0x1c(%edx)
	     954  c0100610:     83 e9 20                sub    $0x20,%ecx
	      45  c0100613:     8d 52 20                lea    0x20(%edx),%edx
	     238  c0100616:     75 d8                   jne    c01005f0 <sse2_zero_page+0x10>
	      71  c0100618:     0f ae f8                sfence 
	     297  c010061b:     5d                      pop    %ebp
	      19  c010061c:     c3                      ret    
	       0  c010061d:     8d 76 00                lea    0x0(%esi),%esi

		:
		snip
		:
#	$NetBSD: README,v 1.3 2001/06/13 21:38:30 fredette Exp $

README for ndbootd-0.5

Copyright (c) 2001 Matthew Fredette.  All rights reserved.

See the file COPYING for no-warranty and distribution terms.

ndbootd is a daemon that serves Sun's old Network Disk (ND) protocol.
This protocol was designed by Sun before they designed NFS.  ND simply
makes the raw blocks of a disk available to network clients.  Contrast
this with the true namespace and file abstractions that NFS provides.

The only reason you're likely to encounter ND nowadays is if you have
an old Sun-2 machine, like the 2/120 or 2/50.  The Sun-2 PROMs can
only use ND to boot over the network.  (Later, the Sun-3 PROMs would
use RARP and TFTP to boot over the network.)

ndbootd is a very simple ND server that only supports client reads.

usage: ndbootd [OPTIONS] BOOT1-BIN
where OPTIONS are:
  -s, --boot2 { BOOT2-BIN | DIR }
                          find a second-stage boot program in the file
                          BOOT2-BIN or in the directory DIR
  -i, --interface NAME    use interface NAME
  -w, --window-size COUNT 
                          send at most COUNT unacknowledged packets [default=6]
  -d, --debug             set debug mode

ndbootd exports a disk that the clients consider to be /dev/ndp0 (ND
public unit zero).  The disk is available to any client listed in
/etc/ethers (Sun-2 PROMs don't do RARP, but they do learn their IP
address from the first ND response they receive from the server.)

BOOT1-BIN is a file containing the mandatory first-stage network boot
program.  The layout of the exported disk is:

block 0: normally a Sun disklabel (but ignored by the PROM)
blocks 1-15: the first-stage network boot program

With the --boot2 option, ndbootd will also make a second-stage network
boot program available to clients.  When --boot2 is used with a
filename BOOT2-BIN, that file is the second-stage network boot program
to be served to all clients.

When --boot2 is used with a directory name DIR, ndbootd finds a
client's second-stage network boot program by turning its IP address
into a filename in that directory, in the same manner later Sun-3
PROMs do when TFTPing (i.e., if a client has IP address 192.168.1.10,
ndbootd expects to find DIR/C0A8010A.SUN2).  The expected use of
--boot2 is with the /tftpboot directory, making ndbootd a functional
replacement for tftp when used with an ND-aware first-stage boot
program.

Any second-stage network boot program always begins at block 16 of the
exported disk, regardless of the length of the first-stage network
boot program.

Whether or not there is a second-stage network boot program, the
exported disk appears to have infinite length.  The content of all
blocks not used by the first- or second-stage network boot programs is
undefined.

All first- and second-stage network boot programs must have had their
exec headers stripped off.

Normally, ndbootd listens on the first up and running IP interface it
finds.  Use the --interface option to give a specific interface.

One parameter of the ND protocol is a sort of window size.  This is
the number of 1-kilobyte packets that can be transmitted before
waiting for an acknowledgement.  To change this from the default 6,
use the --window-size option.

When debug support is compiled in (it is by default), the --debug
option turns on debugging.

ndbootd has only been compiled and tested under NetBSD with BPF
support, although there is a fair autoconf framework, and the raw
interface support is broken out, which should allow for reasonable
porting.

Note that ndbootd was developed specifically to help me to boot my
experimental NetBSD port on my Sun-2/120.  In this scenario, the
first-stage network boot (bootyy) continues to use ND to load in the
second-stage boot program (netboot), which can do a full
RARP/bootparams/NFS boot.  (If the program netboot ever fit in 16
blocks, we could eliminate bootyy, but this is unlikely.)

(Aside: it is unusual for a network boot to have two stages of boot
programs before the kernel; this is normally only done on real disks.
But to the Sun-2 PROMs, /dev/ndp0 is just like a real disk in that it
provides no EOF condition (like a tape boot gets, or like the Sun-3
TFTP method gets), so it only loads a fixed number of blocks.)

Whether ndbootd can be used to netboot SunOS on a Sun-2 is unknown, but
the hope is that you can use the SunOS-provided sun2.bb file as the
first-stage boot program, not use any --boot2 option, and ndbootd will
perform as the SunOS ndbootd did.

To configure ndbootd for compiling, run the 'configure' script,
followed by make.  To report bugs in compiling or using ndbootd, email
fredette@alum.mit.edu, and please include as much information as you
can about what you're trying to do and what goes wrong.  I don't have
much time to do support, but I'll try.
#	$NetBSD: README,v 1.3 2003/11/12 13:31:07 grant Exp $


July 1995 -  Ported to NetBSD by Gordon Ross <gwr@NetBSD.org>

Note: to build clnt.pcnfsd use the command:
	make -f Makefile.clnt

Text of original README file from Sun follows:
----------------------------------------------------------------------

This is version @(#)README	1.6 1/28/92 of the pcnfsd README file.
----------------------------------------------------------------------
This is the second cut at PCNFSD v2. All printing now uses the same
SVR4 based printing model: lp, lpstat, cancel. This implies SunOS 4.1
or later on a Sun. The printing hasn't been properly tested on SVR4,
since my SVR4 system is a bit quirky, but authentication via shadow
passwords works fine.

The Makefile supplied will build either SunOS 4.1 or SVR4 versions;
however you will need to tweak a couple of definitions. See the
Makefile for details.

For this round, I have frozen the rpcgen'd code and modified it by hand
to get around a few rpcgen deficiencies. If in doubt, you can take the
pcnfsd.x file included here off to a separate directory, rpcgen everything
up and diff against the versions given here.

You should also be sure to read the man page. I haven't tried [nt]roff'ing
this anywhere outside SunOS, so let me know if there are any quirks.
Note especially the /etc/pcnfsd.conf configuration model.

Standard disclaimers. Read the source for more of the same.

Geoff Arnold (Geoff.Arnold@Sun.COM)
May 2, 1991

----------------------------------------------------------------------
This is the first major spin since the version I posted in May.
Changes are described in the SCCS deltas, but anyone analysing
or porting this should watch for the following:

- Lots of SVR4 stuff. Look at the Makefile and common.h; you will
  need to make sure that SVR4 is defined to build a version for SVR4.
- Fixed a major bug whereby aliased printers (a.k.a. virtual printers)
  didn't show up in the printer list.
- Since some commands (especially lpq) can hang up forever if, say,
  the host of a remote printer is down, the su_popen mechanism now
  includes a watchdog timer. I HAVE NOT HAD A CHANCE TO TEST THIS
  LOGIC ON SVR4 YET! The code was derived from the System V compatiility
  section of the AnswerBook, so it _should_ be OK, but I can't wait to
  test it before posting this.
- I've mades lots of lint-suggested improvements. It doesn't lint
  100% cleanly yet, but it's a lot better.
- The client testbed has been tightened up a bit and the reporting
  is somewhat clearer. Please run it.

Feedback is solicited.

Geoff Arnold (Geoff.Arnold@Sun.COM)
July 24, 1991

----------------------------------------------------------------------------

This version includes support for TCP as well as UDP, both in the
server and in the test client.  Some of the calls can have long
responses, in which case TCP is the preferred transport.

Jon Dreyer
January 28, 1992

----------------------------------------------------------------------------

This release incorporates minor bug fixes occasioned by internal testing
against preliminary versions of PC-NFS subsystems which actually use the
V2 protocol features. It also addresses a security hole which was identified:
see the comment in pcnfsd_print.c for details. This is the version
of the daemon which we expect to send out for beta testing.

Geoff Arnold
November 16, 1991

----------------------------------------------------------------------------

Several people pointed out that if I was going to fix the security
hole, I should *REALLY* fix it. So this revision checks every argument
for every shell metacharacter. This means that if your print subsystem
allows such characters in printer names, print job id's, user names,
etc. you're going to have to relax the restrictions or tighten up
elsewhere. The routine "suspicious()" in pcnfsd_print.c is the starting
point for this.

Geoff Arnold
November 21, 1991

----------------------------------------------------------------------------

This version includes one more tweak on the previous security fix, plus
a fix for the "user shell" security hole.

Geoff Arnold
December 17, 1991

----------------------------------------------------------------------------

Fix a stupid bug - the secondary groups array wasn't static, so it
got overwritten, resulting in the corruption of some GIDs.

Geoff Arnold
December 18, 1991
$NetBSD: README,v 1.6 2014/06/25 00:21:42 rmind Exp $

npftest - a tool for regression testing and debugging NPF.
It uses RUMP framework to run NPF kernel module in the userspace.

---

Test:

npfctl debug npftest.conf /tmp/npf.plist
npftest -c /tmp/npf.plist -t

Stream:

tcpdump -w stream.pcap -i $interface "host $host and tcp"
npfctl debug npftest.conf /tmp/npf.plist
npftest -c /tmp/npf.plist -s stream.pcap > stream_npf_data.txt

Preferably, use MALLOC_OPTIONS="AJ" and/or other facilities.

Benchmark:

npftest -b rule -c /tmp/npf.plist -p $ncpu

---

Update RUMP libraries once the kernel side has been changed.  Hint:

cd src/sys/net/npf
sudo make includes

cd src/sys/rump/net/lib/libnpf
make distclean
MKDEBUG=yes MKDEBUGLIB=yes DBG="-g -O2" make -j8
sudo MKDEBUG=yes MKDEBUGLIB=yes DBG="-g -O2" make install
$NetBSD: README,v 1.2 2003/11/12 13:31:08 grant Exp $

This is the CMU sup system, converted to use a NetBSD Makefile
infrastructure.  To build the sup system for another operating
system, go into the "source" subdirectory, and use the Makefile
provided there.

	-- Jason R. Thorpe <thorpej@NetBSD.org>
	   October 6, 1997

$NetBSD: README,v 1.3 2012/01/28 01:30:42 christos Exp $

Organization of Sources:

This directory hierarchy is using an organization that separates
crypto source for programs that we have obtained from external third
parties (where NetBSD is not the primary maintainer) from the system
source.

This README file is derived from the README file in src/external.

The hierarchy is grouped by license, and then package per license,
and is organized as follows:

	crypto/external/

	    Makefile
			Descend into the license sub-directories.

	    <license>/
			Per-license sub-directories.

		Makefile
			Descend into the package sub-directories.

		<package>/
			Per-package sub-directories.

		    Makefile
			Build the package.
			
		    dist/
			The third-party source for a given package.

		    bin/
		    lib/
		    sbin/
			BSD makefiles "reach over" from these into
			"../dist/".

This arrangement allows for packages to be easily disabled or
excised as necessary, either on a per-license or per-package basis.

The licenses currently used are:

	bsd		BSD (or equivalent) licensed software, possibly with
			the "advertising clause".
	cpl		Common Public License
			http://www.opensource.org/licenses/cpl1.0

If a package has components covered by different licenses
(for example, GPL2 and the LGPL), use the <license> subdirectory
for the more restrictive license.

If a package allows the choice of a license to use, we'll
generally use the less restrictive license.

If in doubt about where a package should be located, please
contact <core@NetBSD.org> for advice.


Migration Strategy:


Eventually src/dist (and associated framework in other base source
directories) and src/gnu will be migrated to this hierarchy.


Maintenance Strategy:

The sources under src/crypto/external/<license>/<package>/dist/ are
generally a combination of a published distribution plus changes
that we submit to the maintainers and that are not yet published
by them.

Make sure all changes made to the external sources are submitted
to the appropriate maintainer, but only after coordinating with
the NetBSD maintainers.

 tpm-tools

 Copyright (C) 2005, 2006 International Business Machines Corporation
 All Rights Reserved.


 DESCRIPTION
 -----------
 tpm-tools is an open-source package designed to enable user and application
 enablement of Trusted Computing using a Trusted Platform Module (TPM),
 similar to a smart card environment.  Trusted Computing is a set of
 specifications set forth by the Trusted Computing Group (TCG).  For more
 information on the TCG and Trusted Computing please visit the Trusted
 Computing Group website at:
   http://www.trustedcomputinggroup.org.

 The tpm-tools package contains commands to allow the platform administrator
 the ability to manage and diagnose the platform's TPM.  Additionally, the
 package contains commands to utilize some of the capabilities available
 in the TPM PKCS#11 interface implemented in the openCryptoki project.

 The tpm-tools package is released under the Common Public License.


 BUILD REQUIREMENTS
 ----- ------------
 Packages needed to build:

  automake
  autoconf
  libtool
  gettext
  gettext-devel
  trousers
  trousers-devel

  optional (for PKCS#11 support - tpmtoken commands)
    openssl-0.9.7 or newer
    openssl-devel-0.9.7 or newer
    opencryptoki-2.2.0 or newer


 BUILDING tpm-tools
 -------- ---------
 $ sh ./bootstrap.sh
 $ ./configure
 $ make
 # make install

 By default the build will place everything in /usr/local. Issue
   ./configure --help
 to see how to install to a different location.


How to get TrouSerS up and running with an SELinux policy.
Kent Yoder <kyoder@users.sf.net>

This howto assumes a Fedora Core 4 install.

1. Install and load the device driver
 # wget http://download.fedora.redhat.com/pub/fedora/linux/core/4/SRPMS/kernel-2.6.11-1.1369_FC4.src.rpm
 # rpm -ivh kernel-2.6.11-1.1369_FC4.src.rpm
 # cd /usr/src/redhat/SPECS
 # rpmbuild -bp ./kernel-2.6.spec
 # cd /usr/src/redhat/BUILD/kernel-2.6.11/linux-2.6.11
 # make menuconfig
   - Goto Device Drivers > Character Devices > TPM Devices
   - enable the drivers
 # make
 # make modules_install
 # make install
 # reboot
 # modprobe tpm_atmel (or others...)

2. Build and install trousers in the system location. The SELinux policy assumes
   that trousers is installed in the system location. To change these, edit
   the trousers.fc file.

 # tar zxvf trousers-0.2.1.tar.gz
 # cd trousers-0.2.1
 # ./configure --prefix=/usr
 # make
 # make install

3. Install the SELinux policy sources

 # yum install selinux-policy-targeted-sources.noarch

4. Install the trousers te and fc files and load the policy

 # cp ./dist/fedora/trousers.te /etc/selinux/targeted/src/policy/domains/program
 # cp ./dist/fedora/trousers.fc /etc/selinux/targeted/src/policy/file_contexts/program
 # cd /etc/selinux/targeted/src/policy
 # make clean
 # make reload
 # make install
 # make relabel

 At this point, there should be a trousers-specific type for /dev/tpm0:

 # ls -Z /dev/tpm*
 crw-rw----  root     root     system_u:object_r:tcsd_device_t  /dev/tpm0

 Also, checking the security context of the running tcsd should show it running
with the tcsd_t type:

 # ps -Zef |grep tcsd
 root:system_r:tcsd_t            root     16362     1  0 15:10 ?        00:00:00 /usr/sbin/tcsd

5. That should be it!  Send bugs and questions to trousers-users@lists.sf.net.

trousers README

  Trousers is an open-source TCG Software Stack (TSS), released under
the Common Public License. Trousers aims to be compliant with the
1.1b and 1.2 TSS specifications available from the Trusted Computing
Group website:

http://www.trustedcomputinggroup.org


CONTACT

  For information on the TrouSerS project, please send mail to the
following lists:

Use of the TSS API and TrouSerS:
  trousers-users@lists.sf.net

Discussion of the internals of the TrouSerS implementation:
  trousers-tech@lists.sf.net

Possibly sensitive security related bugs:
  Debora Velarde <dvelarde@us.ibm.com>

Run-of-the-mill bug reports should use the TrouSerS bug tracker:
  http://sourceforge.net/tracker/?group_id=126012&atid=704358


BUILD REQUIREMENTS

  Packages needed to build:

  automake > 1.4
  autoconf > 1.4
  pkgconfig
  libtool
  gtk2-devel
  openssl-devel >= 0.9.7
  pthreads library (glibc-devel)


BUILDING the TSS 32-bit

  Build and install the latest TPM device driver from
sf.net/projects/tpmdd either compiled in or loaded as a
module. UPDATE: This driver is now included in the vanilla 2.6.12
kernel!  If you are doing this, trousers should just work after a
vanilla build. Follow the build instructions below and read
RUNNING the TSS, below.

  To build trousers after you have the device driver installed:

  $ sh bootstrap.sh
  $ ./configure [--enable-debug] [--enable-gprof] [--enable-gcov]
  $ make
  # make install

  Here are the default locations of files that trousers installs:

  /usr/local/sbin/tcsd
  /usr/local/etc/tcsd.conf
  /usr/local/lib/libtspi.so.0.0.X
  /usr/local/lib/libtspi.so.0 -> libtspi.so.0.0.X
  /usr/local/lib/libtspi.so -> libtspi.so.0.0.X
  /usr/local/lib/libtspi.la
  /usr/local/lib/libtddl.a
  /usr/local/var/lib/tpm

  By default the build will place everything in /usr/local. To install
in a slightly more predictable place, use `./configure --prefix=/usr`.

  'make install' will run ldconfig, but if /usr/local/lib is not in
your /etc/ld.so.conf, this won't make a difference. You may need to
manually add it and run ldconfig as root to allow your apps to link at
run time to libtspi.so.


BUILDING the TSS 64-bit

  TrouSerS has been built and tested on ppc64 and x86_64, so please
don't hesitate to report bugs on these platforms.  Building everything
64-bit will require a few more flags than are necessary for a 32-bit
platform.  Here are some example instructions for ppc64:

  $ sh bootstrap.sh
  $ export PKG_CONFIG_PATH=/usr/lib64/pkgconfig
  $ CFLAGS="-L/usr/lib64 -L/opt/gnome/lib64" LDFLAGS="-L/usr/lib64 \
           -L/opt/gnome/lib64" ./configure --libdir="/usr/local/lib64"
  $ make
  # make install

  Hopefully the above example will get you going on building in your
64-bit environment.  If you need to do anything special, please send
your build steps to trousers-users@lists.sf.net and I'll include it
here.


USING TROUSERS ON AN ALREADY OWNED TPM

  If you've already taken ownership of your TPM using a TSS under another
operating system, there are a few issues you should be aware of.

  Auth vs No-Auth SRK:  In order to trick trousers into thinking it has taken
ownership of the TPM it's running on, you will need to create a persistent
storage file for trousers to use.  Normally trousers would create this file
itself at the time ownership is taken.  If your SRK has been given an
authorization password by the non-Linux OS, you will need to move the file
dist/system.data.auth to /usr/local/var/lib/tpm/system.data.  If you've
taken ownership of your TPM without issuing a password, move
dist/system.data.noauth to /usr/local/var/lib/tpm/system.data.

  Passwords:  When entering passwords for keys you'd like to use in both
Linux and other OS's, you'll need to take note of how you entered those
passwords.  The TSS spec states that when a password is entered through a
GUI popup dialog box provided by the TSS library, the password should be
converted to the UTF-16 encoding and then hashed using SHA-1, including
the UTF-16 null terminator in the hash calculation.

  In order to work around this problem, specify the -u option to the
tpm-tools command line to convert the password to UTF-16 before hashing.
This, however, unfolds yet another problem...

  Some TSS stacks aren't compliant with the TSS spec, in that they hash
their passwords without including the terminating null character.  This
means that there are effectively two versions of any password set through
a popup dialog box.  Trousers will include the terminating null character
in its hashes of UTF-16 data.

  We'll do our best to track other TSS software and how it behaves.  Please
see the trousers FAQ at http://trousers.sf.net for more information.


ARCHITECTURE

  This TSS implementation has several components.

  A) The TCS Daemon - A user space daemon that should be (according to
     the TSS spec) the only portal to the TPM device driver. At boot
     time, the TCS Daemon should be started, it should open the TPM
     device driver and from that point on, all requests to the TPM
     should go through the TSS stack. The TCSD manages TPM resources
     and handles requests from TSP's both local and remote.

  B) The TSP shared library - The TSP (TCG Service Provider) is a
     shared library that enables applications to talk to TCSD's both
     locally and remotely. The TSP also manages resources used in
     commicating with the application and the TCSD and transparently
     contacts the TCSD whenever necessary.

  C) Persistent Storage (PS) files - TSS's have 2 different kinds of
     PS for keys.  PS can be thought of as a database for keys, with
     each key in the database indexed by a UUID.

     'User' persistent storage is maintained by the application's TSP
     library.  Upon writing the first key to User PS, the TSP library
     creates a new file at ~/.trousers/user.data, using the effective
     user id of the process executing the call to find ~. An environment
     variable, TSS_USER_PS_FILE, can also be set to point the TSP library
     to a different location for the User PS. This environment variable
     has the lifetime of the TSP context, so to store 2 keys in 2
     different files, you will need to call Tspi_Context_Close, set the
     new location, and open the context again.

     'System' persistent storage is controlled by the TCS and stays
     valid across all application lifetimes, TCSD restarts and system
     resets. Data registered in system PS stays valid until an application
     requests that it be removed. The System PS file by default is
     /usr/local/var/lib/tpm/system.data. The system PS file is initially
     created when ownership of the TPM is first taken.

  D) A config file. By default located in /usr/local/etc/tcsd.conf.


RUNNING the TSS

  By default, the TCS daemon is not reachable over the internet, so if
you just plan to access it locally, running it as root with a root owned
device node is probably ok.  Just make sure your device driver is loaded
and start the tcsd as root.

  If you would like to run the TCS daemon as an unprivleged user,
please follow these instructions:

  If you're using the device driver from a linux 2.6.12+ kernel and have
udev enabled, you need to add the following line to your
udev.permissions file (usually in /etc/udev somewhere):

  tpm[0-9]:tss:tss:0600

  and then just load the device driver with:
  # modprobe tpm_atmel
  or,
  # modprobe tpm_natl

  start the TCS Core Services daemon, by default /usr/local/sbin/tcsd.
  # /usr/local/sbin/tcsd

  If you're attempting to make the TCS Core Services daemon communicate with a
softwware TPM through TCP, you must call it using the -e option. 

  # /usr/local/sbin/tcsd -e

  The default values for hostname, port and UN socket device path are  "localhost", 
"6545" and "/var/run/tpm/tpmd_socket:0". It will search for the IN socket device,
then for an UN socket one, and then for the real TPM in this order.
The default values match with the current open source project required values, if
for instance case you need to set values of your choice, the environment variables 
for them are TCSD_TCP_DEVICE_HOSTNAME, TCSD_TCP_DEVICE_PORT if using an IN socket 
and TCSD_UN_SOCKET_DEVICE_PATH if running an UN socket.


DEBUGGING

  If you've compiled trousers with './configure --enable-debug' and would like
to turn debugging output off at run-time, set the environment variable
TSS_DEBUG_OFF to any value.


BUILDING a TSS RPM

 # sh bootstrap.sh
 # ./configure
 # cd ..
 # mv trousers trousers-${version}
 # tar zcvf /usr/src/packages/SOURCES/trousers-${version}.tar.gz \
            trousers-${version}
 # rpmbuild -bb trousers-${version}/dist/trousers.spec

EOF
$NetBSD: README,v 1.1 2011/02/20 02:12:31 christos Exp $

These are example configuration files that are supposed to be installed
in /etc/saslc.d/ and are used to configure saslc globally as well as its
different authentication mechanisms.

The tree hierarchy looks like:

Default and global configuration files:

    /etc/saslc.d/saslc/saslc.conf
    /etc/saslc.d/saslc/mechs/{ANONYMOUS,CRAM-MD5,DIGEST-MD5}.conf
    /etc/saslc.d/saslc/mechs/{EXTERNAL,GSSAPI,LOGIN,PLAIN}.conf

Custom configuration files for <program> (for example postfix):

    /etc/saslc.d/<program>/saslc.conf
    /etc/saslc.d/<program>/mechs/{ANONYMOUS,CRAM-MD5,DIGEST-MD5}.conf
    /etc/saslc.d/<program>/mechs/{EXTERNAL,GSSAPI,LOGIN,PLAIN}.conf

Remember that some of the files contain sensitive information and should
be installed with the proper permissions (0600).

-- in order of preference

- client: support KRB5_PADATA_ENCRYPTED_CHALLENGE in lib/krb5/init_creds_pw.c
- client: don't support ENC-TS in FAST

- client: plugin support for fast plugins

- kdc: plugin support for fast plugins
	partly done with "struct kdc_patypes"

- kcm: support FAST armor ticket
-- using PK-INIT anonymous
-- using host key

- client: tgs-req fast support
- kdc: tgs-req fast support

Heimdal is a Kerberos 5 implementation.

For information how to install see <http://www.h5l.org/compile.html>.

There are briefer man pages for most of the commands.

Bug reports and bugs are appreciated, see more under Bug reports in
the manual on how we prefer them: <heimdal-bugs@h5l.org>.

For more information see the web-page at
<http://www.h5l.org/> or the mailing lists:

heimdal-announce@sics.se	low-volume announcement
heimdal-discuss@sics.se		high-volume discussion

send a mail to heimdal-announce-request@sics.se and
heimdal-discuss-request@sics.se respectively to subscribe.
To use the pretty graphs you have to first build/run the ltmtest from the root directory of the package.  
Todo this type 

make timing ; ltmtest

in the root.  It will run for a while [about ten minutes on most PCs] and produce a series of .log files in logs/.

After doing that run "gnuplot graphs.dem" to make the PNGs.  If you managed todo that all so far just open index.html to view
them all :-)

Have fun

Tom
#!/bin/sh

size .libs/libasn1.dylib
size .libs/libasn1base.a | awk '{sum += $1} END {print sum}' | sed 's/^/TEXT baselib: /'
size .libs/asn1_*.o | awk '{sum += $1} END {print sum}' | sed 's/^/generated code stubs: /'
size *_asn1-template.o | awk '{sum += $1} END {print sum}' | sed 's/^/TEXT stubs: /'

exit 0

Notes about the template parser:

- assumption: code is large, tables smaller

- how to generate template based stubs:

	make check asn1_compile_FLAGS=--template > log

- pretty much the same as the generate code, except uses tables instead of code

TODO:
	- Make hdb work

	- Fuzzing tests

	- Performance testing

	- ASN1_MALLOC_ENCODE() as a function, replaces encode_ and length_

	- Fix SIZE constraits

	- Compact types that only contain on entry to not having a header.


SIZE - Futher down is later generations of the template parser

	code:
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	462848	12288	0	323584	798720	c3000 (O2)

	trivial types:
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	446464	12288	0	323584	782336	bf000 (O2)

	OPTIONAL
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	425984	16384	0	323584	765952	bb000 (O2)

	SEQ OF
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	368640	32768	0	327680	729088	b2000 (O2)
	348160	32768	0	327680	708608	ad000 (Os)

	BOOLEAN
	==================
	339968	32768	0	327680	700416	ab000 (Os)

	TYPE_EXTERNAL:
	==================
	331776	32768	0	327680	692224	a9000 (Os)

	SET OF
	==================
	327680	32768	0	327680	688128	a8000 (Os)

	TYPE_EXTERNAL everywhere
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	167936	69632	0	327680	565248	8a000 (Os)

	TAG uses ->ptr (header and trailer)
	==================
	229376	102400	0	421888	753664	b8000 (O0)

	TAG uses ->ptr (header only)
	==================
	221184	77824	0	421888	720896	b0000 (O0)

	BER support for octet string (not working)
	==================
	180224	73728	0	417792	671744	a4000 (O2)

	CHOICE and BIT STRING missign
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	172032	73728	0	417792	663552	a2000 (Os)

	No accessor functions to global variable
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	159744	73728	0	393216	626688	99000 (Os)

	All types tables (except choice) (id still objects)
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	167936	77824	0	421888	667648	a3000
	base lib: 22820

	__TEXT	__DATA	__OBJC	others	dec	hex
	==================
	167936	77824	0	421888	667648	a3000 (Os)
	baselib: 22820
	generated code stubs: 41472
	TEXT stubs: 112560

	All types, id still objects
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	155648	81920	0	430080	667648	a3000 (Os)
	TEXT baselib: 23166
	generated code stubs: 20796
	TEXT stubs: 119891

	All types, id still objects, dup compression
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	143360	65536	0	376832	585728	8f000 (Os)
	TEXT baselib: 23166
	generated code stubs: 20796
	TEXT stubs: 107147

	All types, dup compression, id vars
	==================
	__TEXT	__DATA	__OBJC	others	dec	hex
	131072	65536	0	352256	548864	86000
	TEXT baselib: 23166
	generated code stubs: 7536
	TEXT stubs: 107147
Building Heimdal for Windows
===================

1. Introduction
---------------

Heimdal can be built and run on Windows XP or later.  Older OSs may
work, but have not been tested.

2. Prerequisites
----------------

* __Microsoft Visual C++ Compiler__: Heimdal has been tested with
  Microsoft Visual C/C++ compiler version 15.x.  This corresponds to
  Microsoft Visual Studio version 2008.  The compiler and tools that
  are included with Microsoft Windows SDK versions 6.1 and later can
  also be used for building Heimdal.  If you have a recent Windows
  SDK, then you already have a compatible compiler.

* __Microsoft Windows SDK__: Heimdal has been tested with Microsoft
  Windows SDK version 6.1 and 7.0.

* __Microsoft HTML Help Compiler__: Needed for building documentation.

* __Perl__: A recent version of Perl.  Tested with ActiveState
  ActivePerl.

* __Python__: Tested with Python 2.5 and 2.6.

* __WiX__: The Windows [Installer XML toolkit (WiX)][1] Version 3.x is
  used to build the installers.

* __Cygwin__: The Heimdal build system requires a number of additional
  tools: `awk`, `yacc`, `lex`, `cmp`, `sed`, `makeinfo`, `sh`
  (Required for running tests).  These can be found in the Cygwin
  distribution.  MinGW or GnuWin32 may also be used instead of Cygwin.
  However, a recent build of `makeinfo` is required for building the
  documentation. Cygwin makeinfo 4.7 is known to work.

* __Certificate for code-signing__: The Heimdal build produces a
  number of Assemblies that should be signed if they are to be
  installed via Windows Installer.  In addition, all executable
  binaries produced by the build including installers can be signed
  and timestamped if a code-signing certificate is available.
  As of 1 January 2016 Windows 7 and above require the use of sha256
  signatures.  The signtool.exe provided with Windows SDK 8.1 or
  later must be used.

[1]: http://wix.sourceforge.net/

3. Setting up the build environment
-----------------------------------

* Start with a Windows SDK or Visual Studio build environment.  The
  target platform, OS and build type (debug / release) is determined
  by the build environment.

  E.g.: If you are using the Windows SDK, you can use the `SetEnv.Cmd`
  script to set up a build environment targetting 64-bit Windows XP or
  later with:

      SetEnv.Cmd /xp /x64 /Debug

  The build will produce debug binaries.  If you specify

      SetEnv.Cmd /xp /x64 /Release

  the build will produce release binaries.

* Add any directories to `PATH` as necessary for tools required by
  the build to be found.  The build scripts will check for build
  tools at the start of the build and will indicate which ones are
  missing.  In general, adding Perl, Python, WiX, HTML Help Compiler and
  Cygwin binary directories to the path should be sufficient.

* Set up environment variables for code signing.  This can be done in
  one of two ways.  By specifying options for `signtool` or by
  specifying the code-signing command directly.  To use `signtool`,
  define `SIGNTOOL_C` and optionally, `SIGNTOOL_O` and `SIGNTOOL_T`.

  - `SIGNTOOL_C`: Certificate selection and private key selection
    options for `signtool`.

    E.g.:

        set SIGNTOOL_C=/f c:\mycerts\codesign.pfx

	set SIGNTOOL_C=/n "Certificate Subject Name" /a

  - `SIGNTOOL_O`: Signing parameter options for `signtool`. Optional.

    E.g.:

        set SIGNTOOL_O=/du http://example.com/myheimdal

  - `SIGNTOOL_T`: SHA1 Timestamp URL for `signtool`.  If not specified,
    defaults to `http://timestamp.verisign.com/scripts/timstamp.dll`.

  - `SIGNTOOL_T_SHA256`: SHA256 Timestamp URL for `signtool`.  If not
    specified, defaults to `http://timestamp.geotrust.com/tsa`.

  - `CODESIGN`: SHA1 Code signer command.  This environment variable, if
    defined, overrides the `SIGNTOOL_*` variables.  It should be
    defined to be a command that takes one parameter: the binary to be
    signed.

  - `CODESIGN_SHA256`: SHA256 Code signer command.  This environment variable, if
    defined, applies a second SHA256 signature to the parameter.  It should be
    defined to be a command that takes one parameter: the binary to be
    signed.

    E.g.:

        set CODESIGN=c:\scripts\mycodesigner.cmd
	set CODESIGN_SHA256=c:\scripts\mycodesigner256.cmd

* Define the code sign public key token.  This is contained in the
  environment variable `CODESIGN_PKT` and is needed to build the
  Heimdal assemblies.  If you are not using a code-sign certificate,
  set this to `0000000000000000`.

  You can use the `pktextract` tool to determine the public key token
  corresponding to your code signing certificate as follows (assuming
  your code signing certificate is in `c:\mycerts\codesign.cer`:

      pktextract c:\mycerts\codesign.cer

  The above command will output the certificate name, key size and the
  public key token.  Set the `CODESIGN_PKT` variable to the
  `publicKeyToken` value (excluding quotes).

  E.g.:

      set CODESIGN_PKT=abcdef0123456789

4. Running the build
--------------------

Change the current directory to the root of the Heimdal source tree
and run:

    nmake /f NTMakefile

This should build the binaries, assemblies and the installers.

The build can also be invoked from any subdirectory that contains an
`NTMakefile` using the same command.  Keep in mind that there are
inter-dependencies between directories and therefore it is recommended
that a full build be invoked from the root of the source tree.

Tests can be invoked, after a full build, by executing:

    nmake /f NTMakefile test

The build tree can be cleaned with:

    nmake /f NTMakefile clean

It is recommended that both AMD64 and X86 builds take place on the
same machine.  This permits a multi-platform installer package to
be built.  First build for X86 and then build AMD64

    nmake /f NTMakefile MULTIPLATFORM_INSTALLER=1

The build must be executed under cmd.exe.
This release of OpenSSH is for OpenBSD systems only.

Please read
	http://www.openssh.com/portable.html
if you want to install OpenSSH on other operating systems.

To extract and install this release on your OpenBSD system use:

      # cd /usr/src/usr.bin
      # tar xvfz .../openssh-x.y.tgz
      # cd ssh
      # make obj
      # make cleandir
      # make depend
      # make
      # make install
      # cp ssh_config sshd_config /etc/ssh

OpenSSH is a derivative of the original and free ssh 1.2.12 release
by Tatu Ylonen.  Aaron Campbell, Bob Beck, Markus Friedl, Niels
Provos, Theo de Raadt and Dug Song removed many bugs, re-added newer
features and created OpenSSH.  Markus Friedl contributed the support
for SSH protocol versions 1.5 and 2.0.

See http://www.openssh.com/ for more information.

$OpenBSD: README,v 1.7 2006/04/01 05:37:46 djm Exp $
$NetBSD: README,v 1.5 2016/12/25 00:07:46 christos Exp $

		libdes, Version 4.01 10-Jan-97

		Copyright (c) 1997, Eric Young
			  All rights reserved.

    This program is free software; you can redistribute it and/or modify
    it under the terms specified in COPYRIGHT.
    
--
The primary ftp site for this library is
ftp://ftp.psy.uq.oz.au/pub/Crypto/DES/libdes-x.xx.tar.gz
libdes is now also shipped with SSLeay.  Primary ftp site of
ftp://ftp.psy.uq.oz.au/pub/Crypto/SSL/SSLeay-x.x.x.tar.gz

The best way to build this library is to build it as part of SSLeay.

This kit builds a DES encryption library and a DES encryption program.
It supports ecb, cbc, ofb, cfb, triple ecb, triple cbc, triple ofb,
triple cfb, desx, and MIT's pcbc encryption modes and also has a fast
implementation of crypt(3).
It contains support routines to read keys from a terminal,
generate a random key, generate a key from an arbitrary length string,
read/write encrypted data from/to a file descriptor.

The implementation was written so as to conform with the manual entry
for the des_crypt(3) library routines from MIT's project Athena.

destest should be run after compilation to test the des routines.
rpw should be run after compilation to test the read password routines.
The des program is a replacement for the sun des command.  I believe it
conforms to the sun version.

The Imakefile is setup for use in the kerberos distribution.

These routines are best compiled with gcc or any other good
optimising compiler.
Just turn you optimiser up to the highest settings and run destest
after the build to make sure everything works.

I believe these routines are close to the fastest and most portable DES
routines that use small lookup tables (4.5k) that are publicly available.
The fcrypt routine is faster than ufc's fcrypt (when compiling with
gcc2 -O2) on the sparc 2 (1410 vs 1270) but is not so good on other machines
(on a sun3/260 168 vs 336).  It is a function of CPU on chip cache size.
[ 10-Jan-97 and a function of an incorrect speed testing program in
  ufc which gave much better test figures that reality ].

It is worth noting that on sparc and Alpha CPUs, performance of the DES
library can vary by upto %10 due to the positioning of files after application
linkage.

Eric Young (eay@cryptsoft.com)


OpenSSL ASN1 Revision
=====================

This document describes some of the issues relating to the new ASN1 code.

Previous OpenSSL ASN1 problems
=============================

OK why did the OpenSSL ASN1 code need revising in the first place? Well
there are lots of reasons some of which are included below...

1. The code is difficult to read and write. For every single ASN1 structure
(e.g. SEQUENCE) four functions need to be written for new, free, encode and
decode operations. This is a very painful and error prone operation. Very few
people have ever written any OpenSSL ASN1 and those that have usually wish
they hadn't.

2. Partly because of 1. the code is bloated and takes up a disproportionate
amount of space. The SEQUENCE encoder is particularly bad: it essentially
contains two copies of the same operation, one to compute the SEQUENCE length
and the other to encode it.

3. The code is memory based: that is it expects to be able to read the whole
structure from memory. This is fine for small structures but if you have a
(say) 1Gb PKCS#7 signedData structure it isn't such a good idea...

4. The code for the ASN1 IMPLICIT tag is evil. It is handled by temporarily
changing the tag to the expected one, attempting to read it, then changing it
back again. This means that decode buffers have to be writable even though they
are ultimately unchanged. This gets in the way of constification.

5. The handling of EXPLICIT isn't much better. It adds a chunk of code into 
the decoder and encoder for every EXPLICIT tag.

6. APPLICATION and PRIVATE tags aren't even supported at all.

7. Even IMPLICIT isn't complete: there is no support for implicitly tagged
types that are not OPTIONAL.

8. Much of the code assumes that a tag will fit in a single octet. This is
only true if the tag is 30 or less (mercifully tags over 30 are rare).

9. The ASN1 CHOICE type has to be largely handled manually, there aren't any
macros that properly support it.

10. Encoders have no concept of OPTIONAL and have no error checking. If the
passed structure contains a NULL in a mandatory field it will not be encoded,
resulting in an invalid structure.

11. It is tricky to add ASN1 encoders and decoders to external applications.

Template model
==============

One of the major problems with revision is the sheer volume of the ASN1 code.
Attempts to change (for example) the IMPLICIT behaviour would result in a
modification of *every* single decode function. 

I decided to adopt a template based approach. I'm using the term 'template'
in a manner similar to SNACC templates: it has nothing to do with C++
templates.

A template is a description of an ASN1 module as several constant C structures.
It describes in a machine readable way exactly how the ASN1 structure should
behave. If this template contains enough detail then it is possible to write
versions of new, free, encode, decode (and possibly others operations) that
operate on templates.

Instead of having to write code to handle each operation only a single
template needs to be written. If new operations are needed (such as a 'print'
operation) only a single new template based function needs to be written 
which will then automatically handle all existing templates.

Plans for revision
==================

The revision will consist of the following steps. Other than the first two
these can be handled in any order.
 
o Design and write template new, free, encode and decode operations, initially
memory based. *DONE*

o Convert existing ASN1 code to template form. *IN PROGRESS*

o Convert an existing ASN1 compiler (probably SNACC) to output templates
in OpenSSL form.

o Add support for BIO based ASN1 encoders and decoders to handle large
structures, initially blocking I/O.

o Add support for non blocking I/O: this is quite a bit harder than blocking
I/O.

o Add new ASN1 structures, such as OCSP, CRMF, S/MIME v3 (CMS), attribute
certificates etc etc.

Description of major changes
============================

The BOOLEAN type now takes three values. 0xff is TRUE, 0 is FALSE and -1 is
absent. The meaning of absent depends on the context. If for example the
boolean type is DEFAULT FALSE (as in the case of the critical flag for
certificate extensions) then -1 is FALSE, if DEFAULT TRUE then -1 is TRUE.
Usually the value will only ever be read via an API which will hide this from
an application.

There is an evil bug in the old ASN1 code that mishandles OPTIONAL with
SEQUENCE OF or SET OF. These are both implemented as a STACK structure. The
old code would omit the structure if the STACK was NULL (which is fine) or if
it had zero elements (which is NOT OK). This causes problems because an empty
SEQUENCE OF or SET OF will result in an empty STACK when it is decoded but when
it is encoded it will be omitted resulting in different encodings. The new code
only omits the encoding if the STACK is NULL, if it contains zero elements it
is encoded and empty. There is an additional problem though: because an empty
STACK was omitted, sometimes the corresponding *_new() function would
initialize the STACK to empty so an application could immediately use it, if
this is done with the new code (i.e. a NULL) it wont work. Therefore a new
STACK should be allocated first. One instance of this is the X509_CRL list of
revoked certificates: a helper function X509_CRL_add0_revoked() has been added
for this purpose.

The X509_ATTRIBUTE structure used to have an element called 'set' which took
the value 1 if the attribute value was a SET OF or 0 if it was a single. Due
to the behaviour of CHOICE in the new code this has been changed to a field
called 'single' which is 0 for a SET OF and 1 for single. The old field has
been deleted to deliberately break source compatibility. Since this structure
is normally accessed via higher level functions this shouldn't break too much.

The X509_REQ_INFO certificate request info structure no longer has a field
called 'req_kludge'. This used to be set to 1 if the attributes field was
(incorrectly) omitted. You can check to see if the field is omitted now by
checking if the attributes field is NULL. Similarly if you need to omit
the field then free attributes and set it to NULL.

The top level 'detached' field in the PKCS7 structure is no longer set when
a PKCS#7 structure is read in. PKCS7_is_detached() should be called instead.
The behaviour of PKCS7_get_detached() is unaffected.

The values of 'type' in the GENERAL_NAME structure have changed. This is
because the old code use the ASN1 initial octet as the selector. The new
code uses the index in the ASN1_CHOICE template.

The DIST_POINT_NAME structure has changed to be a true CHOICE type.

typedef struct DIST_POINT_NAME_st {
int type;
union {
	STACK_OF(GENERAL_NAME) *fullname;
	STACK_OF(X509_NAME_ENTRY) *relativename;
} name;
} DIST_POINT_NAME;

This means that name.fullname or name.relativename should be set
and type reflects the option. That is if name.fullname is set then
type is 0 and if name.relativename is set type is 1.

With the old code using the i2d functions would typically involve:

unsigned char *buf, *p;
int len;
/* Find length of encoding */
len = i2d_SOMETHING(x, NULL);
/* Allocate buffer */
buf = OPENSSL_malloc(len);
if(buf == NULL) {
	/* Malloc error */
}
/* Use temp variable because &p gets updated to point to end of
 * encoding.
 */
p = buf;
i2d_SOMETHING(x, &p);


Using the new i2d you can also do:

unsigned char *buf = NULL;
int len;
len = i2d_SOMETHING(x, &buf);
if(len < 0) {
	/* Malloc error */
}

and it will automatically allocate and populate a buffer with the
encoding. After this call 'buf' will point to the start of the
encoding which is len bytes long.
  ENGINE
  ======

  With OpenSSL 0.9.6, a new component was added to support alternative
  cryptography implementations, most commonly for interfacing with external
  crypto devices (eg. accelerator cards). This component is called ENGINE,
  and its presence in OpenSSL 0.9.6 (and subsequent bug-fix releases)
  caused a little confusion as 0.9.6** releases were rolled in two
  versions, a "standard" and an "engine" version. In development for 0.9.7,
  the ENGINE code has been merged into the main branch and will be present
  in the standard releases from 0.9.7 forwards.

  There are currently built-in ENGINE implementations for the following
  crypto devices:

      o CryptoSwift
      o Compaq Atalla
      o nCipher CHIL
      o Nuron
      o Broadcom uBSec

  In addition, dynamic binding to external ENGINE implementations is now
  provided by a special ENGINE called "dynamic". See the "DYNAMIC ENGINE"
  section below for details.

  At this stage, a number of things are still needed and are being worked on:

      1 Integration of EVP support.
      2 Configuration support.
      3 Documentation!

1 With respect to EVP, this relates to support for ciphers and digests in
  the ENGINE model so that alternative implementations of existing
  algorithms/modes (or previously unimplemented ones) can be provided by
  ENGINE implementations.

2 Configuration support currently exists in the ENGINE API itself, in the
  form of "control commands". These allow an application to expose to the
  user/admin the set of commands and parameter types a given ENGINE
  implementation supports, and for an application to directly feed string
  based input to those ENGINEs, in the form of name-value pairs. This is an
  extensible way for ENGINEs to define their own "configuration" mechanisms
  that are specific to a given ENGINE (eg. for a particular hardware
  device) but that should be consistent across *all* OpenSSL-based
  applications when they use that ENGINE. Work is in progress (or at least
  in planning) for supporting these control commands from the CONF (or
  NCONF) code so that applications using OpenSSL's existing configuration
  file format can have ENGINE settings specified in much the same way.
  Presently however, applications must use the ENGINE API itself to provide
  such functionality. To see first hand the types of commands available
  with the various compiled-in ENGINEs (see further down for dynamic
  ENGINEs), use the "engine" openssl utility with full verbosity, ie;
       openssl engine -vvvv

3 Documentation? Volunteers welcome! The source code is reasonably well
  self-documenting, but some summaries and usage instructions are needed -
  moreover, they are needed in the same POD format the existing OpenSSL
  documentation is provided in. Any complete or incomplete contributions
  would help make this happen.

  STABILITY & BUG-REPORTS
  =======================

  What already exists is fairly stable as far as it has been tested, but
  the test base has been a bit small most of the time. For the most part,
  the vendors of the devices these ENGINEs support have contributed to the
  development and/or testing of the implementations, and *usually* (with no
  guarantees) have experience in using the ENGINE support to drive their
  devices from common OpenSSL-based applications. Bugs and/or inexplicable
  behaviour in using a specific ENGINE implementation should be sent to the
  author of that implementation (if it is mentioned in the corresponding C
  file), and in the case of implementations for commercial hardware
  devices, also through whatever vendor support channels are available.  If
  none of this is possible, or the problem seems to be something about the
  ENGINE API itself (ie. not necessarily specific to a particular ENGINE
  implementation) then you should mail complete details to the relevant
  OpenSSL mailing list. For a definition of "complete details", refer to
  the OpenSSL "README" file. As for which list to send it to;

     openssl-users: if you are *using* the ENGINE abstraction, either in an
          pre-compiled application or in your own application code.

     openssl-dev: if you are discussing problems with OpenSSL source code.

  USAGE
  =====

  The default "openssl" ENGINE is always chosen when performing crypto
  operations unless you specify otherwise. You must actively tell the
  openssl utility commands to use anything else through a new command line
  switch called "-engine". Also, if you want to use the ENGINE support in
  your own code to do something similar, you must likewise explicitly
  select the ENGINE implementation you want.

  Depending on the type of hardware, system, and configuration, "settings"
  may need to be applied to an ENGINE for it to function as expected/hoped.
  The recommended way of doing this is for the application to support
  ENGINE "control commands" so that each ENGINE implementation can provide
  whatever configuration primitives it might require and the application
  can allow the user/admin (and thus the hardware vendor's support desk
  also) to provide any such input directly to the ENGINE implementation.
  This way, applications do not need to know anything specific to any
  device, they only need to provide the means to carry such user/admin
  input through to the ENGINE in question. Ie. this connects *you* (and
  your helpdesk) to the specific ENGINE implementation (and device), and
  allows application authors to not get buried in hassle supporting
  arbitrary devices they know (and care) nothing about.

  A new "openssl" utility, "openssl engine", has been added in that allows
  for testing and examination of ENGINE implementations. Basic usage
  instructions are available by specifying the "-?" command line switch.

  DYNAMIC ENGINES
  ===============

  The new "dynamic" ENGINE provides a low-overhead way to support ENGINE
  implementations that aren't pre-compiled and linked into OpenSSL-based
  applications. This could be because existing compiled-in implementations
  have known problems and you wish to use a newer version with an existing
  application. It could equally be because the application (or OpenSSL
  library) you are using simply doesn't have support for the ENGINE you
  wish to use, and the ENGINE provider (eg. hardware vendor) is providing
  you with a self-contained implementation in the form of a shared-library.
  The other use-case for "dynamic" is with applications that wish to
  maintain the smallest foot-print possible and so do not link in various
  ENGINE implementations from OpenSSL, but instead leaves you to provide
  them, if you want them, in the form of "dynamic"-loadable
  shared-libraries. It should be possible for hardware vendors to provide
  their own shared-libraries to support arbitrary hardware to work with
  applications based on OpenSSL 0.9.7 or later. If you're using an
  application based on 0.9.7 (or later) and the support you desire is only
  announced for versions later than the one you need, ask the vendor to
  backport their ENGINE to the version you need.

  How does "dynamic" work?
  ------------------------
    The dynamic ENGINE has a special flag in its implementation such that
    every time application code asks for the 'dynamic' ENGINE, it in fact
    gets its own copy of it. As such, multi-threaded code (or code that
    multiplexes multiple uses of 'dynamic' in a single application in any
    way at all) does not get confused by 'dynamic' being used to do many
    independent things. Other ENGINEs typically don't do this so there is
    only ever 1 ENGINE structure of its type (and reference counts are used
    to keep order). The dynamic ENGINE itself provides absolutely no
    cryptographic functionality, and any attempt to "initialise" the ENGINE
    automatically fails. All it does provide are a few "control commands"
    that can be used to control how it will load an external ENGINE
    implementation from a shared-library. To see these control commands,
    use the command-line;

       openssl engine -vvvv dynamic

    The "SO_PATH" control command should be used to identify the
    shared-library that contains the ENGINE implementation, and "NO_VCHECK"
    might possibly be useful if there is a minor version conflict and you
    (or a vendor helpdesk) is convinced you can safely ignore it.
    "ID" is probably only needed if a shared-library implements
    multiple ENGINEs, but if you know the engine id you expect to be using,
    it doesn't hurt to specify it (and this provides a sanity check if
    nothing else). "LIST_ADD" is only required if you actually wish the
    loaded ENGINE to be discoverable by application code later on using the
    ENGINE's "id". For most applications, this isn't necessary - but some
    application authors may have nifty reasons for using it. The "LOAD"
    command is the only one that takes no parameters and is the command
    that uses the settings from any previous commands to actually *load*
    the shared-library ENGINE implementation. If this command succeeds, the
    (copy of the) 'dynamic' ENGINE will magically morph into the ENGINE
    that has been loaded from the shared-library. As such, any control
    commands supported by the loaded ENGINE could then be executed as per
    normal. Eg. if ENGINE "foo" is implemented in the shared-library
    "libfoo.so" and it supports some special control command "CMD_FOO", the
    following code would load and use it (NB: obviously this code has no
    error checking);

       ENGINE *e = ENGINE_by_id("dynamic");
       ENGINE_ctrl_cmd_string(e, "SO_PATH", "/lib/libfoo.so", 0);
       ENGINE_ctrl_cmd_string(e, "ID", "foo", 0);
       ENGINE_ctrl_cmd_string(e, "LOAD", NULL, 0);
       ENGINE_ctrl_cmd_string(e, "CMD_FOO", "some input data", 0);

    For testing, the "openssl engine" utility can be useful for this sort
    of thing. For example the above code excerpt would achieve much the
    same result as;

       openssl engine dynamic \
                 -pre SO_PATH:/lib/libfoo.so \
                 -pre ID:foo \
                 -pre LOAD \
                 -pre "CMD_FOO:some input data"

    Or to simply see the list of commands supported by the "foo" ENGINE;

       openssl engine -vvvv dynamic \
                 -pre SO_PATH:/lib/libfoo.so \
                 -pre ID:foo \
                 -pre LOAD

    Applications that support the ENGINE API and more specifically, the
    "control commands" mechanism, will provide some way for you to pass
    such commands through to ENGINEs. As such, you would select "dynamic"
    as the ENGINE to use, and the parameters/commands you pass would
    control the *actual* ENGINE used. Each command is actually a name-value
    pair and the value can sometimes be omitted (eg. the "LOAD" command).
    Whilst the syntax demonstrated in "openssl engine" uses a colon to
    separate the command name from the value, applications may provide
    their own syntax for making that separation (eg. a win32 registry
    key-value pair may be used by some applications). The reason for the
    "-pre" syntax in the "openssl engine" utility is that some commands
    might be issued to an ENGINE *after* it has been initialised for use.
    Eg. if an ENGINE implementation requires a smart-card to be inserted
    during initialisation (or a PIN to be typed, or whatever), there may be
    a control command you can issue afterwards to "forget" the smart-card
    so that additional initialisation is no longer possible. In
    applications such as web-servers, where potentially volatile code may
    run on the same host system, this may provide some arguable security
    value. In such a case, the command would be passed to the ENGINE after
    it has been initialised for use, and so the "-post" switch would be
    used instead. Applications may provide a different syntax for
    supporting this distinction, and some may simply not provide it at all
    ("-pre" is almost always what you're after, in reality).

  How do I build a "dynamic" ENGINE?
  ----------------------------------
    This question is trickier - currently OpenSSL bundles various ENGINE
    implementations that are statically built in, and any application that
    calls the "ENGINE_load_builtin_engines()" function will automatically
    have all such ENGINEs available (and occupying memory). Applications
    that don't call that function have no ENGINEs available like that and
    would have to use "dynamic" to load any such ENGINE - but on the other
    hand such applications would only have the memory footprint of any
    ENGINEs explicitly loaded using user/admin provided control commands.
    The main advantage of not statically linking ENGINEs and only using
    "dynamic" for hardware support is that any installation using no
    "external" ENGINE suffers no unnecessary memory footprint from unused
    ENGINEs. Likewise, installations that do require an ENGINE incur the
    overheads from only *that* ENGINE once it has been loaded.

    Sounds good? Maybe, but currently building an ENGINE implementation as
    a shared-library that can be loaded by "dynamic" isn't automated in
    OpenSSL's build process. It can be done manually quite easily however.
    Such a shared-library can either be built with any OpenSSL code it
    needs statically linked in, or it can link dynamically against OpenSSL
    if OpenSSL itself is built as a shared library. The instructions are
    the same in each case, but in the former (statically linked any
    dependencies on OpenSSL) you must ensure OpenSSL is built with
    position-independent code ("PIC"). The default OpenSSL compilation may
    already specify the relevant flags to do this, but you should consult
    with your compiler documentation if you are in any doubt.

    This example will show building the "atalla" ENGINE in the
    crypto/engine/ directory as a shared-library for use via the "dynamic"
    ENGINE.
    1) "cd" to the crypto/engine/ directory of a pre-compiled OpenSSL
       source tree.
    2) Recompile at least one source file so you can see all the compiler
       flags (and syntax) being used to build normally. Eg;
           touch hw_atalla.c ; make
       will rebuild "hw_atalla.o" using all such flags.
    3) Manually enter the same compilation line to compile the
       "hw_atalla.c" file but with the following two changes;
         (a) add "-DENGINE_DYNAMIC_SUPPORT" to the command line switches,
	 (b) change the output file from "hw_atalla.o" to something new,
             eg. "tmp_atalla.o"
    4) Link "tmp_atalla.o" into a shared-library using the top-level
       OpenSSL libraries to resolve any dependencies. The syntax for doing
       this depends heavily on your system/compiler and is a nightmare
       known well to anyone who has worked with shared-library portability
       before. 'gcc' on Linux, for example, would use the following syntax;
          gcc -shared -o dyn_atalla.so tmp_atalla.o -L../.. -lcrypto
    5) Test your shared library using "openssl engine" as explained in the
       previous section. Eg. from the top-level directory, you might try;
          apps/openssl engine -vvvv dynamic \
              -pre SO_PATH:./crypto/engine/dyn_atalla.so -pre LOAD
       If the shared-library loads successfully, you will see both "-pre"
       commands marked as "SUCCESS" and the list of control commands
       displayed (because of "-vvvv") will be the control commands for the
       *atalla* ENGINE (ie. *not* the 'dynamic' ENGINE). You can also add
       the "-t" switch to the utility if you want it to try and initialise
       the atalla ENGINE for use to test any possible hardware/driver
       issues.

  PROBLEMS
  ========

  It seems like the ENGINE part doesn't work too well with CryptoSwift on Win32.
  A quick test done right before the release showed that trying "openssl speed
  -engine cswift" generated errors. If the DSO gets enabled, an attempt is made
  to write at memory address 0x00000002.


 OpenSSL 1.0.2k 26 Jan 2017

 Copyright (c) 1998-2015 The OpenSSL Project
 Copyright (c) 1995-1998 Eric A. Young, Tim J. Hudson
 All rights reserved.

 DESCRIPTION
 -----------

 The OpenSSL Project is a collaborative effort to develop a robust,
 commercial-grade, fully featured, and Open Source toolkit implementing the
 Secure Sockets Layer (SSLv3) and Transport Layer Security (TLS) protocols as
 well as a full-strength general purpose cryptograpic library. The project is
 managed by a worldwide community of volunteers that use the Internet to
 communicate, plan, and develop the OpenSSL toolkit and its related
 documentation.

 OpenSSL is descended from the SSLeay library developed by Eric A. Young
 and Tim J. Hudson.  The OpenSSL toolkit is licensed under a dual-license (the
 OpenSSL license plus the SSLeay license), which means that you are free to
 get and use it for commercial and non-commercial purposes as long as you
 fulfill the conditions of both licenses.

 OVERVIEW
 --------

 The OpenSSL toolkit includes:

 libssl.a:
     Provides the client and server-side implementations for SSLv3 and TLS.

 libcrypto.a:
     Provides general cryptographic and X.509 support needed by SSL/TLS but
     not logically part of it.

 openssl:
     A command line tool that can be used for:
        Creation of key parameters
        Creation of X.509 certificates, CSRs and CRLs
        Calculation of message digests
        Encryption and decryption
        SSL/TLS client and server tests
        Handling of S/MIME signed or encrypted mail
        And more...

 INSTALLATION
 ------------

 See the appropriate file:
        INSTALL         Linux, Unix, etc.
        INSTALL.DJGPP   DOS platform with DJGPP
        INSTALL.NW      Netware
        INSTALL.OS2     OS/2
        INSTALL.VMS     VMS
        INSTALL.W32     Windows (32bit)
        INSTALL.W64     Windows (64bit)
        INSTALL.WCE     Windows CE

 SUPPORT
 -------

 See the OpenSSL website www.openssl.org for details on how to obtain
 commercial technical support.

 If you have any problems with OpenSSL then please take the following steps
 first:

    - Download the latest version from the repository
      to see if the problem has already been addressed
    - Configure with no-asm
    - Remove compiler optimisation flags

 If you wish to report a bug then please include the following information
 and create an issue on GitHub:

    - On Unix systems:
        Self-test report generated by 'make report'
    - On other systems:
        OpenSSL version: output of 'openssl version -a'
        OS Name, Version, Hardware platform
        Compiler Details (name, version)
    - Application Details (name, version)
    - Problem Description (steps that will reproduce the problem, if known)
    - Stack Traceback (if the application dumps core)

 Just because something doesn't work the way you expect does not mean it
 is necessarily a bug in OpenSSL.

 HOW TO CONTRIBUTE TO OpenSSL
 ----------------------------

 See CONTRIBUTING

 LEGALITIES
 ----------

 A number of nations restrict the use or export of cryptography. If you
 are potentially subject to such restrictions you should seek competent
 professional legal advice before attempting to develop or distribute
 cryptographic code.
Run these makefiles from the top level as in
nmake -f ms\makefilename
to build with visual C++ 4.[01].

The results will be in the out directory.

These makefiles and def files were generated by typing

perl util\mk1mf.pl VC-NT >ms/nt.mak
perl util\mk1mf.pl VC-NT dll >ms/ntdll.mak

perl util\mkdef.pl 32 crypto > ms/crypto32.def
perl util\mkdef.pl 32 ssl > ms/ssl32.def
NOTES
-----

I've checked out HPUX (well, version 11 at least) and shl_t is
a pointer type so it's safe to use in the way it has been in
dso_dl.c. On the other hand, HPUX11 support dlfcn too and
according to their man page, prefer developers to move to that.
I'll leave Richard's changes there as I guess dso_dl is needed
for HPUX10.20.

There is now a callback scheme in place where filename conversion can
(a) be turned off altogether through the use of the
    DSO_FLAG_NO_NAME_TRANSLATION flag,
(b) be handled by default using the default DSO_METHOD's converter
(c) overriden per-DSO by setting the override callback
(d) a mix of (b) and (c) - eg. implement an override callback that;
    (i) checks if we're win32 (if(strstr(dso->meth->name, "win32")....)
        and if so, convert "blah" into "blah32.dll" (the default is
	otherwise to make it "blah.dll").
    (ii) default to the normal behaviour - we're not on win32, eg.
         finish with (return dso->meth->dso_name_converter(dso,NULL)).

This is an OpenSSL-compatible version of AES (also called Rijndael).
aes_core.c is basically the same as rijndael-alg-fst.c but with an
API that looks like the rest of the OpenSSL symmetric cipher suite.
Configuration modules. These are a set of modules which can perform
various configuration functions.

Currently the routines should be called at most once when an application
starts up: that is before it starts any threads.

The routines read a configuration file set up like this:

-----
#default section
openssl_conf=init_section

[init_section]

module1=value1
#Second instance of module1
module1.1=valueX
module2=value2
module3=dso_literal
module4=dso_section

[dso_section]

path=/some/path/to/some/dso.so
other_stuff=other_value
----

When this file is loaded a configuration module with the specified string
(module* in the above example) is looked up and its init function called as:

int conf_init_func(CONF_IMODULE *md, CONF *cnf);

The function can then take whatever action is appropriate, for example further
lookups based on the value. Multiple instances of the same config module can be
loaded.

When the application closes down the modules are cleaned up by calling an
optional finish function:

void conf_finish_func(CONF_IMODULE *md);

The finish functions are called in reverse order: that is the last module
loaded is the first one cleaned up.

If no module exists with a given name then an attempt is made to load a DSO
with the supplied name. This might mean that "module3" attempts to load a DSO
called libmodule3.so or module3.dll for example. An explicit DSO name can be
given by including a separate section as in the module4 example above.

The DSO is expected to at least contain an initialization function:

int OPENSSL_init(CONF_IMODULE *md, CONF *cnf);

and may also include a finish function:

void OPENSSL_finish(CONF_IMODULE *md);

Static modules can also be added using,

int CONF_module_add(char *name, dso_mod_init_func *ifunc, dso_mod_finish_func
*ffunc);

where "name" is the name in the configuration file this function corresponds
to.

A set of builtin modules (currently only an ASN1 non functional test module)
can be added by calling OPENSSL_load_builtin_modules(). 

The function OPENSSL_config() is intended as a simple configuration function
that any application can call to perform various default configuration tasks.
It uses the file openssl.cnf in the usual locations.


Mutithreading testing area.

Since this stuff is very very platorm specific, this is not part of the
normal build.  Have a read of doc/threads.doc.

mttest will do some testing and will currently build under Windows NT/95,
Solaris and Linux.  The IRIX stuff is not finished.

I have tested this program on a 12 CPU ultra sparc box (solaris 2.5.1)
and things seem to work ok.

The Linux pthreads package can be retrieved from 
http://www.mit.edu:8001/people/proven/pthreads.html

This is a quick packaging up of my blowfish code into a library.
It has been lifted from SSLeay.
The copyright notices seem a little harsh because I have not spent the
time to rewrite the conditions from the normal SSLeay ones.

Basically if you just want to play with the library, not a problem.

eric 15-Apr-1997
C2.pl works

		libdes, Version 4.01 10-Jan-97

		Copyright (c) 1997, Eric Young
			  All rights reserved.

    This program is free software; you can redistribute it and/or modify
    it under the terms specified in COPYRIGHT.
    
--
The primary ftp site for this library is
ftp://ftp.psy.uq.oz.au/pub/Crypto/DES/libdes-x.xx.tar.gz
libdes is now also shipped with SSLeay.  Primary ftp site of
ftp://ftp.psy.uq.oz.au/pub/Crypto/SSL/SSLeay-x.x.x.tar.gz

The best way to build this library is to build it as part of SSLeay.

This kit builds a DES encryption library and a DES encryption program.
It supports ecb, cbc, ofb, cfb, triple ecb, triple cbc, triple ofb,
triple cfb, desx, and MIT's pcbc encryption modes and also has a fast
implementation of crypt(3).
It contains support routines to read keys from a terminal,
generate a random key, generate a key from an arbitrary length string,
read/write encrypted data from/to a file descriptor.

The implementation was written so as to conform with the manual entry
for the des_crypt(3) library routines from MIT's project Athena.

destest should be run after compilation to test the des routines.
rpw should be run after compilation to test the read password routines.
The des program is a replacement for the sun des command.  I believe it
conforms to the sun version.

The Imakefile is setup for use in the kerberos distribution.

These routines are best compiled with gcc or any other good
optimising compiler.
Just turn you optimiser up to the highest settings and run destest
after the build to make sure everything works.

I believe these routines are close to the fastest and most portable DES
routines that use small lookup tables (4.5k) that are publicly available.
The fcrypt routine is faster than ufc's fcrypt (when compiling with
gcc2 -O2) on the sparc 2 (1410 vs 1270) but is not so good on other machines
(on a sun3/260 168 vs 336).  It is a function of CPU on chip cache size.
[ 10-Jan-97 and a function of an incorrect speed testing program in
  ufc which gave much better test figures that reality ].

It is worth noting that on sparc and Alpha CPUs, performance of the DES
library can vary by upto %10 due to the positioning of files after application
linkage.

Eric Young (eay@cryptsoft.com)

The stuff in here is based on patches supplied to me by
Steven Schoch <schoch@sheba.arc.nasa.gov> to do DSS.
I have since modified a them a little but a debt of gratitude
is due for doing the initial work.
The STORE type
==============

A STORE, as defined in this code section, is really a rather simple
thing which stores objects and per-object associations to a number
of attributes.  What attributes are supported entirely depends on
the particular implementation of a STORE.  It has some support for
generation of certain objects (for example, keys and CRLs).


Supported object types
----------------------

For now, the objects that are supported are the following:

X.509 certificate
X.509 CRL
private key
public key
number
arbitrary (application) data

The intention is that a STORE should be able to store everything
needed by an application that wants a cert/key store, as well as
the data a CA might need to store (this includes the serial number
counter, which explains the support for numbers).


Supported attribute types
-------------------------

For now, the following attributes are supported:

Friendly Name		- the value is a normal C string
Key ID			- the value is a 160 bit SHA1 hash
Issuer Key ID		- the value is a 160 bit SHA1 hash
Subject Key ID		- the value is a 160 bit SHA1 hash
Issuer/Serial Hash	- the value is a 160 bit SHA1 hash
Issuer			- the value is a X509_NAME
Serial			- the value is a BIGNUM
Subject			- the value is a X509_NAME
Certificate Hash	- the value is a 160 bit SHA1 hash
Email			- the value is a normal C string
Filename		- the value is a normal C string

It is expected that these attributes should be enough to support
the need from most, if not all, current applications.  Applications
that need to do certificate verification would typically use Subject
Key ID, Issuer/Serial Hash or Subject to look up issuer certificates.
S/MIME applications would typically use Email to look up recipient
and signer certificates.

There's added support for combined sets of attributes to search for,
with the special OR attribute.


Supported basic functionality
-----------------------------

The functions that are supported through the STORE type are these:

generate_object		- for example to generate keys and CRLs
get_object		- to look up one object
			  NOTE: this function is really rather
			  redundant and probably of lesser usage
			  than the list functions
store_object		- store an object and the attributes
			  associated with it
modify_object		- modify the attributes associated with
			  a specific object
revoke_object		- revoke an object
			  NOTE: this only marks an object as
			  invalid, it doesn't remove the object
			  from the database
delete_object		- remove an object from the database
list_object		- list objects associated with a given
			  set of attributes
			  NOTE: this is really four functions:
			  list_start, list_next, list_end and
			  list_endp
update_store		- update the internal data of the store
lock_store		- lock the store
unlock_store		- unlock the store

The list functions need some extra explanation: list_start is
used to set up a lookup.  That's where the attributes to use in
the search are set up.  It returns a search context.  list_next
returns the next object searched for.  list_end closes the search.
list_endp is used to check if we have reached the end.

A few words on the store functions as well: update_store is
typically used by a CA application to update the internal
structure of a database.  This may for example involve automatic
removal of expired certificates.  lock_store and unlock_store
are used for locking a store to allow exclusive writes.
Notes: 2001-09-24
-----------------

This "description" (if one chooses to call it that) needed some major updating
so here goes. This update addresses a change being made at the same time to
OpenSSL, and it pretty much completely restructures the underlying mechanics of
the "ENGINE" code. So it serves a double purpose of being a "ENGINE internals
for masochists" document *and* a rather extensive commit log message. (I'd get
lynched for sticking all this in CHANGES or the commit mails :-).

ENGINE_TABLE underlies this restructuring, as described in the internal header
"eng_int.h", implemented in eng_table.c, and used in each of the "class" files;
tb_rsa.c, tb_dsa.c, etc.

However, "EVP_CIPHER" underlies the motivation and design of ENGINE_TABLE so
I'll mention a bit about that first. EVP_CIPHER (and most of this applies
equally to EVP_MD for digests) is both a "method" and a algorithm/mode
identifier that, in the current API, "lingers". These cipher description +
implementation structures can be defined or obtained directly by applications,
or can be loaded "en masse" into EVP storage so that they can be catalogued and
searched in various ways, ie. two ways of encrypting with the "des_cbc"
algorithm/mode pair are;

(i) directly;
     const EVP_CIPHER *cipher = EVP_des_cbc();
     EVP_EncryptInit(&ctx, cipher, key, iv);
     [ ... use EVP_EncryptUpdate() and EVP_EncryptFinal() ...]

(ii) indirectly; 
     OpenSSL_add_all_ciphers();
     cipher = EVP_get_cipherbyname("des_cbc");
     EVP_EncryptInit(&ctx, cipher, key, iv);
     [ ... etc ... ]

The latter is more generally used because it also allows ciphers/digests to be
looked up based on other identifiers which can be useful for automatic cipher
selection, eg. in SSL/TLS, or by user-controllable configuration.

The important point about this is that EVP_CIPHER definitions and structures are
passed around with impunity and there is no safe way, without requiring massive
rewrites of many applications, to assume that EVP_CIPHERs can be reference
counted. One an EVP_CIPHER is exposed to the caller, neither it nor anything it
comes from can "safely" be destroyed. Unless of course the way of getting to
such ciphers is via entirely distinct API calls that didn't exist before.
However existing API usage cannot be made to understand when an EVP_CIPHER
pointer, that has been passed to the caller, is no longer being used.

The other problem with the existing API w.r.t. to hooking EVP_CIPHER support
into ENGINE is storage - the OBJ_NAME-based storage used by EVP to register
ciphers simultaneously registers cipher *types* and cipher *implementations* -
they are effectively the same thing, an "EVP_CIPHER" pointer. The problem with
hooking in ENGINEs is that multiple ENGINEs may implement the same ciphers. The
solution is necessarily that ENGINE-provided ciphers simply are not registered,
stored, or exposed to the caller in the same manner as existing ciphers. This is
especially necessary considering the fact ENGINE uses reference counts to allow
for cleanup, modularity, and DSO support - yet EVP_CIPHERs, as exposed to
callers in the current API, support no such controls.

Another sticking point for integrating cipher support into ENGINE is linkage.
Already there is a problem with the way ENGINE supports RSA, DSA, etc whereby
they are available *because* they're part of a giant ENGINE called "openssl".
Ie. all implementations *have* to come from an ENGINE, but we get round that by
having a giant ENGINE with all the software support encapsulated. This creates
linker hassles if nothing else - linking a 1-line application that calls 2 basic
RSA functions (eg. "RSA_free(RSA_new());") will result in large quantities of
ENGINE code being linked in *and* because of that DSA, DH, and RAND also. If we
continue with this approach for EVP_CIPHER support (even if it *was* possible)
we would lose our ability to link selectively by selectively loading certain
implementations of certain functionality. Touching any part of any kind of
crypto would result in massive static linkage of everything else. So the
solution is to change the way ENGINE feeds existing "classes", ie. how the
hooking to ENGINE works from RSA, DSA, DH, RAND, as well as adding new hooking
for EVP_CIPHER, and EVP_MD.

The way this is now being done is by mostly reverting back to how things used to
work prior to ENGINE :-). Ie. RSA now has a "RSA_METHOD" pointer again - this
was previously replaced by an "ENGINE" pointer and all RSA code that required
the RSA_METHOD would call ENGINE_get_RSA() each time on its ENGINE handle to
temporarily get and use the ENGINE's RSA implementation. Apart from being more
efficient, switching back to each RSA having an RSA_METHOD pointer also allows
us to conceivably operate with *no* ENGINE. As we'll see, this removes any need
for a fallback ENGINE that encapsulates default implementations - we can simply
have our RSA structure pointing its RSA_METHOD pointer to the software
implementation and have its ENGINE pointer set to NULL.

A look at the EVP_CIPHER hooking is most explanatory, the RSA, DSA (etc) cases
turn out to be degenerate forms of the same thing. The EVP storage of ciphers,
and the existing EVP API functions that return "software" implementations and
descriptions remain untouched. However, the storage takes more meaning in terms
of "cipher description" and less meaning in terms of "implementation". When an
EVP_CIPHER_CTX is actually initialised with an EVP_CIPHER method and is about to
begin en/decryption, the hooking to ENGINE comes into play. What happens is that
cipher-specific ENGINE code is asked for an ENGINE pointer (a functional
reference) for any ENGINE that is registered to perform the algo/mode that the
provided EVP_CIPHER structure represents. Under normal circumstances, that
ENGINE code will return NULL because no ENGINEs will have had any cipher
implementations *registered*. As such, a NULL ENGINE pointer is stored in the
EVP_CIPHER_CTX context, and the EVP_CIPHER structure is left hooked into the
context and so is used as the implementation. Pretty much how things work now
except we'd have a redundant ENGINE pointer set to NULL and doing nothing.

Conversely, if an ENGINE *has* been registered to perform the algorithm/mode
combination represented by the provided EVP_CIPHER, then a functional reference
to that ENGINE will be returned to the EVP_CIPHER_CTX during initialisation.
That functional reference will be stored in the context (and released on
cleanup) - and having that reference provides a *safe* way to use an EVP_CIPHER
definition that is private to the ENGINE. Ie. the EVP_CIPHER provided by the
application will actually be replaced by an EVP_CIPHER from the registered
ENGINE - it will support the same algorithm/mode as the original but will be a
completely different implementation. Because this EVP_CIPHER isn't stored in the
EVP storage, nor is it returned to applications from traditional API functions,
there is no associated problem with it not having reference counts. And of
course, when one of these "private" cipher implementations is hooked into
EVP_CIPHER_CTX, it is done whilst the EVP_CIPHER_CTX holds a functional
reference to the ENGINE that owns it, thus the use of the ENGINE's EVP_CIPHER is
safe.

The "cipher-specific ENGINE code" I mentioned is implemented in tb_cipher.c but
in essence it is simply an instantiation of "ENGINE_TABLE" code for use by
EVP_CIPHER code. tb_digest.c is virtually identical but, of course, it is for
use by EVP_MD code. Ditto for tb_rsa.c, tb_dsa.c, etc. These instantiations of
ENGINE_TABLE essentially provide linker-separation of the classes so that even
if ENGINEs implement *all* possible algorithms, an application using only
EVP_CIPHER code will link at most code relating to EVP_CIPHER, tb_cipher.c, core
ENGINE code that is independant of class, and of course the ENGINE
implementation that the application loaded. It will *not* however link any
class-specific ENGINE code for digests, RSA, etc nor will it bleed over into
other APIs, such as the RSA/DSA/etc library code.

ENGINE_TABLE is a little more complicated than may seem necessary but this is
mostly to avoid a lot of "init()"-thrashing on ENGINEs (that may have to load
DSOs, and other expensive setup that shouldn't be thrashed unnecessarily) *and*
to duplicate "default" behaviour. Basically an ENGINE_TABLE instantiation, for
example tb_cipher.c, implements a hash-table keyed by integer "nid" values.
These nids provide the uniquenness of an algorithm/mode - and each nid will hash
to a potentially NULL "ENGINE_PILE". An ENGINE_PILE is essentially a list of
pointers to ENGINEs that implement that particular 'nid'. Each "pile" uses some
caching tricks such that requests on that 'nid' will be cached and all future
requests will return immediately (well, at least with minimal operation) unless
a change is made to the pile, eg. perhaps an ENGINE was unloaded. The reason is
that an application could have support for 10 ENGINEs statically linked
in, and the machine in question may not have any of the hardware those 10
ENGINEs support. If each of those ENGINEs has a "des_cbc" implementation, we
want to avoid every EVP_CIPHER_CTX setup from trying (and failing) to initialise
each of those 10 ENGINEs. Instead, the first such request will try to do that
and will either return (and cache) a NULL ENGINE pointer or will return a
functional reference to the first that successfully initialised. In the latter
case it will also cache an extra functional reference to the ENGINE as a
"default" for that 'nid'. The caching is acknowledged by a 'uptodate' variable
that is unset only if un/registration takes place on that pile. Ie. if
implementations of "des_cbc" are added or removed. This behaviour can be
tweaked; the ENGINE_TABLE_FLAG_NOINIT value can be passed to
ENGINE_set_table_flags(), in which case the only ENGINEs that tb_cipher.c will
try to initialise from the "pile" will be those that are already initialised
(ie. it's simply an increment of the functional reference count, and no real
"initialisation" will take place).

RSA, DSA, DH, and RAND all have their own ENGINE_TABLE code as well, and the
difference is that they all use an implicit 'nid' of 1. Whereas EVP_CIPHERs are
actually qualitatively different depending on 'nid' (the "des_cbc" EVP_CIPHER is
not an interoperable implementation of "aes_256_cbc"), RSA_METHODs are
necessarily interoperable and don't have different flavours, only different
implementations. In other words, the ENGINE_TABLE for RSA will either be empty,
or will have a single ENGING_PILE hashed to by the 'nid' 1 and that pile
represents ENGINEs that implement the single "type" of RSA there is.

Cleanup - the registration and unregistration may pose questions about how
cleanup works with the ENGINE_PILE doing all this caching nonsense (ie. when the
application or EVP_CIPHER code releases its last reference to an ENGINE, the
ENGINE_PILE code may still have references and thus those ENGINEs will stay
hooked in forever). The way this is handled is via "unregistration". With these
new ENGINE changes, an abstract ENGINE can be loaded and initialised, but that
is an algorithm-agnostic process. Even if initialised, it will not have
registered any of its implementations (to do so would link all class "table"
code despite the fact the application may use only ciphers, for example). This
is deliberately a distinct step. Moreover, registration and unregistration has
nothing to do with whether an ENGINE is *functional* or not (ie. you can even
register an ENGINE and its implementations without it being operational, you may
not even have the drivers to make it operate). What actually happens with
respect to cleanup is managed inside eng_lib.c with the "engine_cleanup_***"
functions. These functions are internal-only and each part of ENGINE code that
could require cleanup will, upon performing its first allocation, register a
callback with the "engine_cleanup" code. The other part of this that makes it
tick is that the ENGINE_TABLE instantiations (tb_***.c) use NULL as their
initialised state. So if RSA code asks for an ENGINE and no ENGINE has
registered an implementation, the code will simply return NULL and the tb_rsa.c
state will be unchanged. Thus, no cleanup is required unless registration takes
place. ENGINE_cleanup() will simply iterate across a list of registered cleanup
callbacks calling each in turn, and will then internally delete its own storage
(a STACK). When a cleanup callback is next registered (eg. if the cleanup() is
part of a gracefull restart and the application wants to cleanup all state then
start again), the internal STACK storage will be freshly allocated. This is much
the same as the situation in the ENGINE_TABLE instantiations ... NULL is the
initialised state, so only modification operations (not queries) will cause that
code to have to register a cleanup.

What else? The bignum callbacks and associated ENGINE functions have been
removed for two obvious reasons; (i) there was no way to generalise them to the
mechanism now used by RSA/DSA/..., because there's no such thing as a BIGNUM
method, and (ii) because of (i), there was no meaningful way for library or
application code to automatically hook and use ENGINE supplied bignum functions
anyway. Also, ENGINE_cpy() has been removed (although an internal-only version
exists) - the idea of providing an ENGINE_cpy() function probably wasn't a good
one and now certainly doesn't make sense in any generalised way. Some of the
RSA, DSA, DH, and RAND functions that were fiddled during the original ENGINE
changes have now, as a consequence, been reverted back. This is because the
hooking of ENGINE is now automatic (and passive, it can interally use a NULL
ENGINE pointer to simply ignore ENGINE from then on).

Hell, that should be enough for now ... comments welcome: geoff@openssl.org

RIPEMD-160
http://www.esat.kuleuven.ac.be/~bosselae/ripemd160.html

This is my implementation of RIPEMD-160.  The pentium assember is a little
off the pace since I only get 1050 cycles, while the best is 1013.
I have a few ideas for how to get another 20 or so cycles, but at
this point I will not bother right now.  I believe the trick will be
to remove my 'copy X array onto stack' until inside the RIP1() finctions the
first time round.  To do this I need another register and will only have one
temporary one.  A bit tricky....  I can also cleanup the saving of the 5 words
after the first half of the calculation.  I should read the origional
value, add then write.  Currently I just save the new and read the origioal.
I then read both at the end.  Bad.

eric (20-Jan-1998)
<OBSOLETE>

All assember in this directory are just version of the file
crypto/bn/bn_asm.c.

Quite a few of these files are just the assember output from gcc since on 
quite a few machines they are 2 times faster than the system compiler.

For the x86, I have hand written assember because of the bad job all
compilers seem to do on it.  This normally gives a 2 time speed up in the RSA
routines.

For the DEC alpha, I also hand wrote the assember (except the division which
is just the output from the C compiler pasted on the end of the file).
On the 2 alpha C compilers I had access to, it was not possible to do
64b x 64b -> 128b calculations (both long and the long long data types
were 64 bits).  So the hand assember gives access to the 128 bit result and
a 2 times speedup :-).

There are 3 versions of assember for the HP PA-RISC.

pa-risc.s is the origional one which works fine and generated using gcc :-)

pa-risc2W.s and pa-risc2.s are 64 and 32-bit PA-RISC 2.0 implementations
by Chris Ruemmler from HP (with some help from the HP C compiler).

</OBSOLETE>
The OpenSSL project does not (any longer) include root CA certificates.

Please check out the FAQ:
  * How can I set up a bundle of commercial root CA certificates?
GOST ENGINE

This engine provides implementation of Russian cryptography standard.
This is also an example of adding new cryptoalgorithms into OpenSSL
without changing its core. If OpenSSL is compiled with dynamic engine
support, new algorithms can be added even without recompilation of
OpenSSL and applications which use it.

ALGORITHMS SUPPORTED

GOST R 34.10-94 and GOST R 34.10-2001 - digital signature algorithms.
   Also support key exchange based on public keys. See RFC 4357 for
   details of VKO key exchange algorithm. These algorithms use
   256 bit private keys. Public keys are 1024 bit for 94 and 512 bit for
   2001 (which is elliptic-curve based). Key exchange algorithms
   (VKO R 34.10) are supported on these keys too.
   
GOST R 34.11-94  Message digest algorithm. 256-bit hash value

GOST 28147-89 - Symmetric cipher  with 256-bit key. Various modes are
   defined in the standard, but only CFB and CNT modes are implemented
   in the engine. To make statistical analysis more difficult, key
   meshing is supported (see RFC 4357).

GOST 28147-89 MAC mode. Message authentication code. While most MAC
    algorithms  out there are based on hash functions using HMAC
	algorithm, this algoritm is based on symmetric cipher. 
	It has 256-bit symmetric key and only 32 bits of MAC value
	(while HMAC has same key size and value size). 

	It is implemented as combination of EVP_PKEY type and EVP_MD type.

USAGE OF THESE ALGORITHMS

This engine is designed to allow usage of this algorithms in the
high-level openssl functions, such as PKI, S/MIME and TLS.

See RFC 4490 for S/MIME with GOST algorithms and RFC 4491 for PKI.
TLS support is implemented according IETF
draft-chudov-cryptopro-cptls-03.txt and is compatible with
CryptoPro CSP 3.0 and 3.6 as well as with MagPro CSP. 
GOST ciphersuites implemented in CryptoPro CSP 2.0 are not supported
because they use ciphersuite numbers used now by AES ciphersuites.

To use the engine you have to load it via openssl configuration
file. Applications should read openssl configuration file or provide
their own means to load engines. Also, applications which operate with
private keys, should use generic EVP_PKEY API instead of using RSA or
other algorithm-specific API.

CONFIGURATION FILE

Configuration file should include following statement in the global
section, i.e. before first bracketed section header (see config(5) for details)

   openssl_conf = openssl_def

where openssl_def is name of the section in configuration file which
describes global defaults.

This section should contain following statement:

   [openssl_def]
   engines = engine_section

which points to the section which describes list of the engines to be
loaded. This section should contain:

	[engine_section]
	gost = gost_section

And section which describes configuration of the engine should contain

	[gost_section]
	engine_id = gost
	dynamic_path = /usr/lib/ssl/engines/libgost.so
	default_algorithms = ALL
	CRYPT_PARAMS = id-Gost28147-89-CryptoPro-A-ParamSet

Where engine_id parameter specifies name of engine (should be "gost").
dynamic_path is a location of the loadable shared library implementing the
engine. If the engine is compiled statically or is located in the OpenSSL
engines directory, this line can be omitted. 
default_algorithms parameter specifies that all algorithms, provided by
engine, should be used.

The CRYPT_PARAMS parameter is engine-specific. It allows the user to choose
between different parameter sets of symmetric cipher algorithm. RFC 4357
specifies several parameters for the GOST 28147-89 algorithm, but OpenSSL
doesn't provide user interface to choose one when encrypting. So use engine
configuration parameter instead.

Value of this parameter can be either short name, defined in OpenSSL
obj_dat.h header file or numeric representation of OID, defined in RFC
4357. 

USAGE WITH COMMAND LINE openssl UTILITY

1. Generation of private key

	openssl genpkey -algorithm gost2001 -pkeyopt paramset:A -out seckey.pem

  Use -algorithm option to specify algorithm.
  Use -pkeyopt option to pass paramset to algorithm. The following paramsets
  are supported by 
  	gost94: 0,A,B,C,D,XA,XB,XC
	gost2001: 0,A,B,C,XA,XB
  You can also use numeric representation of OID as to destinate
  paramset.

  Paramsets starting with X are intended to use for key exchange keys.
  Paramsets without X are for digital signature keys.

  Paramset for both algorithms 0 is the test paramset which should be used
  only for test purposes.

There are no algorithm-specific things with generation of certificate
request once you have a private key.

2. Generation of certificate request along with private/public keypar

   openssl req -newkey gost2001 -pkeyopt paramset:A

   Syntax of -pkeyopt parameter is identical with genpkey command.

   You can also use oldstyle syntax -newkey gost2001:paramfile, but in
   this case you should create parameter file first. 

   It can be created with

   openssl genpkey -genparam -algorithm gost2001 -pkeyopt paramset:A\
      -out paramfile.

3. S/MIME operations

If you want to send encrypted mail using GOST algorithms, don't forget
to specify -gost89 as encryption algorithm for OpenSSL smime command.
While OpenSSL is clever enough to find out that GOST R 34.11-94 digest
must be used for digital signing with GOST private key, it have no way
to derive symmetric encryption algorithm from key exchange keys.

4. TLS operations

OpenSSL supports all four ciphersuites defined in the IETF draft.
Once you've loaded GOST key and certificate into your TLS server,
ciphersuites which use GOST 28147-89 encryption are enabled.

Ciphersuites with NULL encryption should be enabled explicitely if
needed.

GOST2001-GOST89-GOST89 Uses GOST R 34.10-2001 for auth and key exchange
		GOST 28147-89 for encryption and GOST 28147-89 MAC
GOST94-GOST89-GOST89 Uses GOST R 34.10-94 for auth and key exchange
		GOST 28147-89 for encryption and GOST 28147-89 MAC
GOST2001-NULL-GOST94 Uses GOST R 34.10-2001 for auth and key exchange,
        no encryption and HMAC, based on GOST R 34.11-94
GOST94-NULL-GOST94 Uses GOST R 34.10-94 for auth and key exchange,
        no encryption and HMAC, based on GOST R 34.11-94

Gost 94 and gost 2001 keys can be used simultaneously in the TLS server.
RSA, DSA and EC keys can be used simultaneously with GOST keys, if
server implementation supports loading more than two private
key/certificate pairs. In this case ciphersuites which use any of loaded
keys would be supported and clients can negotiate ones they wish.

This allows creation of TLS servers which use GOST ciphersuites for
Russian clients and RSA/DSA ciphersuites for foreign clients.

5. Calculation of digests and symmetric encryption
 OpenSSL provides specific commands (like sha1, aes etc) for calculation
 of digests and symmetric encryption. Since such commands cannot be
 added dynamically, no such commands are provided for GOST algorithms.
 Use generic commands 'dgst' and 'enc'.

 Calculation of GOST R 34.11-94 message digest

 openssl dgst -md_gost94 datafile

 Note that GOST R 34.11-94 specifies that digest value should be
 interpreted as little-endian number, but OpenSSL outputs just hex dump
 of digest value.

 So, to obtain correct digest value, such as produced by gostsum utility
 included in the engine distribution, bytes of output should be
 reversed.
 
 Calculation of HMAC based on GOST R 34.11-94

 openssl dgst -md_gost94 -mac hmac -macopt key:<32 bytes of key> datafile
  
  (or use hexkey if key contain NUL bytes)
 Calculation of GOST 28147 MAC

 openssl dgst -mac gost-mac -macopt key:<32 bytes of key> datafile

 Note absense of an option that specifies digest algorithm. gost-mac
 algorithm supports only one digest (which is actually part of
 implementation of this mac) and OpenSSL is clever enough to find out
 this.

 Encryption with GOST 28147 CFB mode
 openssl enc -gost89 -out encrypted-file -in plain-text-file -k <passphrase>  
 Encryption with GOST 28147 CNT mode
 openssl enc -gost89-cnt -out encrypted-file -in plain-text-file -k <passphrase>


6. Encrypting private keys and PKCS12

To produce PKCS12 files compatible with MagPro CSP, you need to use
GOST algorithm for encryption of PKCS12 file and also GOST R 34.11-94
hash to derive key from password.

openssl pksc12 -export -inkey gost.pem -in gost_cert.pem -keypbe gost89\
   -certpbe gost89 -macalg md_gost94
 
7. Testing speed of symmetric ciphers.
   
To test performance of GOST symmetric ciphers you should use -evp switch
of the openssl speed command. Engine-provided ciphers couldn't be
accessed by cipher-specific functions, only via generic evp interface

 openssl speed -evp gost89
 openssl speed -evp gost89-cnt


PROGRAMMING INTERFACES DETAILS

Applications never should access engine directly. They only use provided
EVP_PKEY API. But there are some details, which should be taken into
account.

EVP provides two kinds of API for key exchange:

1. EVP_PKEY_encrypt/EVP_PKEY_decrypt functions, intended to use with
	RSA-like public key encryption algorithms

2. EVP_PKEY_derive, intended to use with Diffie-Hellman-like shared key
computing algorithms.

Although VKO R 34.10 algorithms, described in the RFC 4357 are
definitely second case, engine provides BOTH API for GOST R 34.10 keys.

EVP_PKEY_derive just invokes appropriate VKO algorithm and computes
256 bit shared key. VKO R 34.10-2001 requires 64 bits of random user key
material (UKM). This UKM should be transmitted to other party, so it is
not generated inside derive function.

It should be set by EVP_PKEY_CTX_ctrl function using
EVP_PKEY_CTRL_SET_IV command after call of EVP_PKEY_derive_init, but
before EVP_PKEY_derive.
	unsigned char ukm[8];
	RAND_bytes(ukm,8);
   EVP_PKEY_CTX_ctrl(ctx, -1, EVP_PKEY_OP_DERIVE, 8, ukm)

EVP_PKEY_encrypt encrypts provided session key with VKO shared key and
packs it into GOST key transport structure, described in the RFC 4490.

It typically uses ephemeral key pair to compute shared key and packs its
public part along with encrypted key. So, for most cases use of 
EVP_PKEY_encrypt/EVP_PKEY_decrypt with GOST keys is almost same as with
RSA.

However, if peerkey field in the EVP_PKEY_CTX structure is set (using
EVP_PKEY_derive_set_peerkey function) to EVP_PKEY structure which has private
key and uses same parameters as the public key from which this EVP_PKEY_CTX is
created, EVP_PKEY_encrypt will use this private key to compute shared key and
set ephemeral key in the GOST_key_transport structure to NULL. In this case
pkey and peerkey fields in the EVP_PKEY_CTX are used upside-down.

If EVP_PKEY_decrypt encounters GOST_key_transport structure with NULL
public key field, it tries to use peerkey field from the context to
compute shared key. In this case peerkey field should really contain
peer public key.

Encrypt operation supports EVP_PKEY_CTRL_SET_IV operation as well.
It can be used when some specific restriction on UKM are imposed by
higher level protocol. For instance, description of GOST ciphersuites
requires UKM to be derived from shared secret. 

If UKM is not set by this control command, encrypt operation would
generate random UKM.


This sources include implementation of GOST 28147-89 and GOST R 34.11-94
which are completely indepentent from OpenSSL and can be used separately
(files gost89.c, gost89.h, gosthash.c, gosthash.h) Utility gostsum (file
gostsum.c) is provided as example of such separate usage. This is
program, simular to md5sum and sha1sum utilities, but calculates GOST R
34.11-94 hash.

Makefile doesn't include rule for compiling gostsum.
Use command

$(CC) -o gostsum gostsum.c gost89.c gosthash.c
where $(CC) is name of your C compiler.

Implementations of GOST R 34.10-xx, including VKO algorithms heavily
depends on OpenSSL BIGNUM and Elliptic Curve libraries.



README  This file

fingerprints.txt
        PGP fingerprints of authoried release signers

standards.txt
        Pointers to standards, RFC's and IETF Drafts that are
        related to OpenSSL.  Incomplete.

HOWTO/
        A few how-to documents; not necessarily up-to-date
apps/
        The openssl command-line tools; start with openssl.pod
ssl/
        The SSL library; start with ssl.pod
crypto/
        The cryptographic library; start with crypto.pod

Formatted versions of the manpages (apps,ssl,crypto) can be found at
        https://www.openssl.org/docs/manpages.html
NOTE: Don't expect any of these programs to work with current
OpenSSL releases, or even with later SSLeay releases.

Original README:
=============================================================================

Some demo programs sent to me by various people

eric
This directory contains examples of how to contruct
various X509 structures. Certificates, certificate requests
and CRLs.
PKCS#12 demo applications

Written by Steve Henson.
Scripts for using ECC ciphersuites with test/testssl
(these ciphersuites are described in the Internet Draft available at
http://www.ietf.org/internet-drafts/draft-ietf-tls-ecc-03.txt).

Use ECCcertgen.sh, RSAcertgen.sh, ECC-RSAcertgen.sh to generate
root, client and server certs of the following types:

     ECC certs signed with ECDSA
     RSA certs signed with RSA
     ECC certs signed with RSA

Afterwards, you can use ssltest.sh to run the various tests;
specify one of the following options:

     aecdh, ecdh-ecdsa, ecdhe-ecdsa, ecdh-rsa, ecdhe-rsa
This is intended to be an example of a state-machine driven SSL application. It
acts as an SSL tunneler (functioning as either the server or client half,
depending on command-line arguments). *PLEASE* read the comments in tunala.h
before you treat this stuff as anything more than a curiosity - YOU HAVE BEEN
WARNED!! There, that's the draconian bit out of the way ...


Why "tunala"??
--------------

I thought I asked you to read tunala.h?? :-)


Show me
-------

If you want to simply see it running, skip to the end and see some example
command-line arguments to demonstrate with.


Where to look and what to do?
-----------------------------

The code is split up roughly coinciding with the detaching of an "abstract" SSL
state machine (which is the purpose of all this) and its surrounding application
specifics. This is primarily to make it possible for me to know when I could cut
corners and when I needed to be rigorous (or at least maintain the pretense as
such :-).

Network stuff:

Basically, the network part of all this is what is supposed to be abstracted out
of the way. The intention is to illustrate one way to stick OpenSSL's mechanisms
inside a little memory-driven sandbox and operate it like a pure state-machine.
So, the network code is inside both ip.c (general utility functions and gory
IPv4 details) and tunala.c itself, which takes care of application specifics
like the main select() loop. The connectivity between the specifics of this
application (TCP/IP tunneling and the associated network code) and the
underlying abstract SSL state machine stuff is through the use of the "buffer_t"
type, declared in tunala.h and implemented in buffer.c.

State machine:

Which leaves us, generally speaking, with the abstract "state machine" code left
over and this is sitting inside sm.c, with declarations inside tunala.h. As can
be seen by the definition of the state_machine_t structure and the associated
functions to manipulate it, there are the 3 OpenSSL "handles" plus 4 buffer_t
structures dealing with IO on both the encrypted and unencrypted sides ("dirty"
and "clean" respectively). The "SSL" handle is what facilitates the reading and
writing of the unencrypted (tunneled) data. The two "BIO" handles act as the
read and write channels for encrypted tunnel traffic - in other applications
these are often socket BIOs so that the OpenSSL framework operates with the
network layer directly. In this example, those two BIOs are memory BIOs
(BIO_s_mem()) so that the sending and receiving of the tunnel traffic stays
within the state-machine, and we can handle where this gets send to (or read
from) ourselves.


Why?
----

If you take a look at the "state_machine_t" section of tunala.h and the code in
sm.c, you will notice that nothing related to the concept of 'transport' is
involved. The binding to TCP/IP networking occurs in tunala.c, specifically
within the "tunala_item_t" structure that associates a state_machine_t object
with 4 file-descriptors. The way to best see where the bridge between the
outside world (TCP/IP reads, writes, select()s, file-descriptors, etc) and the
state machine is, is to examine the "tunala_item_io()" function in tunala.c.
This is currently around lines 641-732 but of course could be subject to change.


And...?
-------

Well, although that function is around 90 lines of code, it could easily have
been a lot less only I was trying to address an easily missed "gotcha" (item (2)
below). The main() code that drives the select/accept/IO loop initialises new
tunala_item_t structures when connections arrive, and works out which
file-descriptors go where depending on whether we're an SSL client or server
(client --> accepted connection is clean and proxied is dirty, server -->
accepted connection is dirty and proxied is clean). What that tunala_item_io()
function is attempting to do is 2 things;

  (1) Perform all reads and writes on the network directly into the
      state_machine_t's buffers (based on a previous select() result), and only
      then allow the abstact state_machine_t to "churn()" using those buffers.
      This will cause the SSL machine to consume as much input data from the two
      "IN" buffers as possible, and generate as much output data into the two
      "OUT" buffers as possible. Back up in the main() function, the next main
      loop loop will examine these output buffers and select() for writability
      on the corresponding sockets if the buffers are non-empty.

  (2) Handle the complicated tunneling-specific issue of cascading "close"s.
      This is the reason for most of the complexity in the logic - if one side
      of the tunnel is closed, you can't simply close the other side and throw
      away the whole thing - (a) there may still be outgoing data on the other
      side of the tunnel that hasn't been sent yet, (b) the close (or things
      happening during the close) may cause more data to be generated that needs
      sending on the other side. Of course, this logic is complicated yet futher
      by the fact that it's different depending on which side closes first :-)
      state_machine_close_clean() will indicate to the state machine that the
      unencrypted side of the tunnel has closed, so any existing outgoing data
      needs to be flushed, and the SSL stream needs to be closed down using the
      appropriate shutdown sequence. state_machine_close_dirty() is simpler
      because it indicates that the SSL stream has been disconnected, so all
      that remains before closing the other side is to flush out anything that
      remains and wait for it to all be sent.

Anyway, with those things in mind, the code should be a little easier to follow
in terms of "what is *this* bit supposed to achieve??!!".


How might this help?
--------------------

Well, the reason I wrote this is that there seemed to be rather a flood of
questions of late on the openssl-dev and openssl-users lists about getting this
whole IO logic thing sorted out, particularly by those who were trying to either
use non-blocking IO, or wanted SSL in an environment where "something else" was
handling the network already and they needed to operate in memory only. This
code is loosely based on some other stuff I've been working on, although that
stuff is far more complete, far more dependant on a whole slew of other
network/framework code I don't want to incorporate here, and far harder to look
at for 5 minutes and follow where everything is going. I will be trying over
time to suck in a few things from that into this demo in the hopes it might be
more useful, and maybe to even make this demo usable as a utility of its own.
Possible things include:

  * controlling multiple processes/threads - this can be used to combat
    latencies and get passed file-descriptor limits on some systems, and it uses
    a "controller" process/thread that maintains IPC links with the
    processes/threads doing the real work.

  * cert verification rules - having some say over which certs get in or out :-)

  * control over SSL protocols and cipher suites

  * A few other things you can already do in s_client and s_server :-)

  * Support (and control over) session resuming, particularly when functioning
    as an SSL client.

If you have a particular environment where this model might work to let you "do
SSL" without having OpenSSL be aware of the transport, then you should find you
could use the state_machine_t structure (or your own variant thereof) and hook
it up to your transport stuff in much the way tunala.c matches it up with those
4 file-descriptors. The state_machine_churn(), state_machine_close_clean(), and
state_machine_close_dirty() functions are the main things to understand - after
that's done, you just have to ensure you're feeding and bleeding the 4
state_machine buffers in a logical fashion. This state_machine loop handles not
only handshakes and normal streaming, but also renegotiates - there's no special
handling required beyond keeping an eye on those 4 buffers and keeping them in
sync with your outer "loop" logic. Ie. if one of the OUT buffers is not empty,
you need to find an opportunity to try and forward its data on. If one of the IN
buffers is not full, you should keep an eye out for data arriving that should be
placed there.

This approach could hopefully also allow you to run the SSL protocol in very
different environments. As an example, you could support encrypted event-driven
IPC where threads/processes pass messages to each other inside an SSL layer;
each IPC-message's payload would be in fact the "dirty" content, and the "clean"
payload coming out of the tunnel at each end would be the real intended message.
Likewise, this could *easily* be made to work across unix domain sockets, or
even entirely different network/comms protocols.

This is also a quick and easy way to do VPN if you (and the remote network's
gateway) support virtual network devices that are encapsulted in a single
network connection, perhaps PPP going through an SSL tunnel?


Suggestions
-----------

Please let me know if you find this useful, or if there's anything wrong or
simply too confusing about it. Patches are also welcome, but please attach a
description of what it changes and why, and "diff -urN" format is preferred.
Mail to geoff@openssl.org should do the trick.


Example
-------

Here is an example of how to use "tunala" ...

First, it's assumed that OpenSSL has already built, and that you are building
inside the ./demos/tunala/ directory. If not - please correct the paths and
flags inside the Makefile. Likewise, if you want to tweak the building, it's
best to try and do so in the makefile (eg. removing the debug flags and adding
optimisation flags).

Secondly, this code has mostly only been tested on Linux. However, some
autoconf/etc support has been added and the code has been compiled on openbsd
and solaris using that.

Thirdly, if you are Win32, you probably need to do some *major* rewriting of
ip.c to stand a hope in hell. Good luck, and please mail me the diff if you do
this, otherwise I will take a look at another time. It can certainly be done,
but it's very non-POSIXy.

See the INSTALL document for details on building.

Now, if you don't have an executable "tunala" compiled, go back to "First,...".
Rinse and repeat.

Inside one console, try typing;

(i)  ./tunala -listen localhost:8080 -proxy localhost:8081 -cacert CA.pem \
              -cert A-client.pem -out_totals -v_peer -v_strict

In another console, type;

(ii) ./tunala -listen localhost:8081 -proxy localhost:23 -cacert CA.pem \
              -cert A-server.pem -server 1 -out_totals -v_peer -v_strict

Now if you open another console and "telnet localhost 8080", you should be
tunneled through to the telnet service on your local machine (if it's running -
you could change it to port "22" and tunnel ssh instead if you so desired). When
you logout of the telnet session, the tunnel should cleanly shutdown and show
you some traffic stats in both consoles. Feel free to experiment. :-)

Notes:

 - the format for the "-listen" argument can skip the host part (eg. "-listen
   8080" is fine). If you do, the listening socket will listen on all interfaces
   so you can connect from other machines for example. Using the "localhost"
   form listens only on 127.0.0.1 so you can only connect locally (unless, of
   course, you've set up weird stuff with your networking in which case probably
   none of the above applies).

 - ./tunala -? gives you a list of other command-line options, but tunala.c is
   also a good place to look :-)


librsaref.so is a demonstration dynamic engine that does RSA
operations using the old RSAref 2.0 implementation.

To make proper use of this engine, you must download RSAref 2.0
(search the web for rsaref.tar.Z for example) and unpack it in this
directory, so you'll end up having the subdirectories "install" and
"source" among others.

To build, do the following:

	make

This will list a number of available targets to choose from.  Most of
them are architecture-specific.  The exception is "gnu" which is to be
used on systems where GNU ld and gcc have been installed in such a way
that gcc uses GNU ld to link together programs and shared libraries.

The make file assumes you use gcc.  To change that, just reassign CC:

	make CC=cc

The result is librsaref.so, which you can copy to any place you wish.
This directory contains some simple examples of the use of BIO's
to simplify socket programming.

The client-conf, server-conf, client-arg and client-conf include examples
of how to use the SSL_CONF API for configuration file or command line
processing.

easy_tls - generic SSL/TLS proxy
========

(... and example for non-blocking SSL/TLS I/O multiplexing.)


  easy_tls.c, easy_tls.h:

     Small generic SSL/TLS proxy library: With a few function calls,
     an application socket will be replaced by a pipe handled by a
     separate SSL/TLS proxy process.  This allows easily adding
     SSL/TLS support to many programs not originally designed for it.

     [Actually easy_tls.c is not a proper library: Customization
     requires defining preprocessor macros while compiling it.
     This is quite confusing, so I'll probably change it.]

     These files may be used under the OpenSSL license.



  test.c, test.h, Makefile, cert.pem, cacerts.pem:

     Rudimentary example program using the easy_tls library, and
     example key and certificates for it.  Usage examples:

       $ ./test 8443     # create server listening at port 8443
       $ ./test 127.0.0.1 8443  # create client, connect to port 8443
                                # at IP address 127.0.0.1

     'test' will not automatically do SSL/TLS, or even read or write
     data -- it must be told to do so on input lines starting
     with a command letter.  'W' means write a line, 'R' means
     read a line, 'C' means close the connection, 'T' means
     start an SSL/TLS proxy.  E.g. (user input tagged with '*'):

     * R
       <<< 220 mail.example.net
     * WSTARTTLS
       >>> STARTTLS
     * R
       <<< 220 Ready to start TLS
     * T
       test_process_init(fd = 3, client_p = 1, apparg = (nil))
       +++ `E:self signed certificate in certificate chain'
       +++ `<... certificate info ...>'
     * WHELO localhost
       >>> HELO localhost
       R
       <<< 250 mail.example.net

     You can even do SSL/TLS over SSL/TLS over SSL/TLS ... by using
     'T' multiple times.  I have no idea why you would want to though.


This code is rather old.  When I find time I will update anything that
should be changed, and improve code comments.  To compile the sample
program 'test' on platforms other then Linux or Solaris, you will have
to edit the Makefile.

As noted above, easy_tls.c will be changed to become a library one
day, which means that future revisions will not be fully compatible to
the current version.

Bodo Möller <bodo@openssl.org>
This is a demo of the new ASN1 code. Its an OCSP ASN1 module. Doesn't
do much yet other than demonstrate what the new ASN1 modules might look
like.

It wont even compile yet: the new code isn't in place.


Only the windows NT and, linux builds have been tested for SSLeay 0.8.0
IPsec-tools
===========

This package provides a way to use the native IPsec functionality 
in the Linux 2.6+ kernel. It works as well on NetBSD and FreeBSD.

	- libipsec, a PF_KEYv2 library
	- setkey, a tool to directly manipulate policies and SAs
	- racoon, an IKEv1 keying daemon

IPsec-tools were ported to Linux from the KAME project 
(http://www.kame.net) by Derek Atkins  <derek@ihtfp.com>.

Currently the package is actively maintained and developed by: 
	Emmanuel Dreyfus <manu@netbsd.org>
	VANHULLEBUS Yvan <vanhu@free.fr>
	Matthew Grooms <mgrooms@shrew.net>
	Timo Teräs <timo.teras@iki.fi>

Sources can be found at the IPsec-Tools home page at:
	http://ipsec-tools.sourceforge.net/

And CVS repository is hosted at NetBSD tree:
	cvs -danoncvs@anoncvs.netbsd.org:/cvsroot co ipsec-tools

Bug reports and project wiki is located at:
	https://trac.ipsec-tools.net/

Please report any problems to the mailing list:
	ipsec-tools-devel@lists.sourceforge.net
	ipsec-tools-users@lists.sourceforge.net

You can also browse the list archive:
	http://sf.net/mailarchive/forum.php?forum_name=ipsec-tools-devel

Credits:
	IHTFP Consulting, see http://www.ihtfp.com/
	SUSE Linux AG, see http://www.suse.com/
This directory contains sample configurations files used for roadwarrior
remote access using hybrid authentication. In this setup, the VPN 
gateway authenticates to the client using a certificate, and the client
authenticates to the VPN gateway using a login and a password.

Moreover, this setup makes use of ISAKMP mode config to autoconfigure 
the client. After a successful login, the client will receive an 
internal address, netmask and DNS from the VPN gateway.


Server setups
=============
The server setups need racoon built with the following options:
configure --enable-natt --enable-frag --enable-hybrid --enable-dpd \
	  --with-libradius --sysconfdir=/etc/racoon

The first server setup, in server/racoon.conf, is for a VPN gateway 
using authentication against the system password database, and using 
a locally configured pool of addresses. 

The second setup, server/racoon.conf-radius, uses a RADIUS server for 
authentication, IP allocation and accounting. The address and secret
to be used for the RADIUS server are configured in /etc/radius.conf, 
see radius.conf(5).

Both configurations can be used with the Cisco VPN client if it
is set up to use hybrid authentication (aka mutual group authentication,
available in Cisco VPN client version 4.0.5 and above). The group 
password configured in the Cisco VPN client is not used by racoon.

After you have installed /etc/racoon/racoon.conf, you will also have 
to install a server certificate and key in /etc/openssl/certs/server.crt
and /etc/openssl/certs/server.key


Client setup
============
The client setup needs racoon built with the following options:
configure --enable-natt --enable-frag --enable-hybrid --enable-dpd \
	  --enable-adminport --sysconfdir=/etc/racoon --localstatedir=/var

You need to copy client/racoon.conf, client/phase1-up.sh and
client/phase1-down.sh to /etc/racoon, and you need to copy the 
certificate authority that signed the VPN gateway certificate in
/etc/openssl/certs/root-ca.crt

Once this is done, you can run racoon, and then you can start
the VPN using racoonctl:
racoonctl vc -u username vpn-gateway.example.net

Where username is your login, and vpn-gateway.example.net is
the DNS or IP address of the VPN gateway. racoonctl will prompt 
you for the password.

The password can be stored in the psk.txt file. In that situation, 
add this directive to the remote section of racoon.conf:
	 xauth_login "username";
where username is your login.

Note that for now there is no feedback in racoonctl if the authentication
fails. Peek at the racoon logs to discover what goes wrong.

In order to disconnect from the VPN, do this:
racoonctl vd vpn-gateway.example.net

This configuration should be compatible with the Cisco VPN 3000 using 
hybrid authentication, though this has not been tested.
		Using Racoon with Privilege Separation
		     Tue Mar 25 16:37:09 MDT 2008


Racoon can run in a chroot'd environment.  When so instructed, it runs as two
processes, one of which handles a small number of simple requests and runs as
root in the full native filesystem, and another which runs as a less
privileged user in a chroot'd environment and which handles all the other and
very complex business of racoon.

Because racoon does many complex things there are many opportunities for
coding errors to lead to compromises and so this separation is important.  If
someone breaks into your system using racoon and you have enabled privilege
separation, they will find themselves in a very limited environment and unable
to do much damage.  They may be able to alter the host's security associations
or obtain the private keys stored on that system using file descriptors
available to the unprivileged instance of racoon, and from there they will be
able to alter security associations on other hosts in disruptive or dangerous
ways if you have generate_policy enabled on those hosts.  But that's because
in its current form generate_policy is itself dangerous and requires that you
trust anyone with the credentials to use it.

They will also be able to execute any scripts you have placed in the scripts
directory, although racoon will prevent them from mis-using the traditional
environment variables PATH, LD_LIBRARY_PATH, and IFS.  But if you have
introduced vulnerabilities into your scripts you may want to re-visit them.
The thing to watch for is blindly trusting the environment variables passed
in by racoon - assume they could be set to anything by a malicious entity and
check them for suitability before using them.

All these possibilities are present when privilege separation is not enabled,
and they are greatly reduced when it is enabled because the resources
available to the attacker are less.

*****

The basic concept with racoon's privilege separation is that a minimal
environment containing all the files racoon needs to operate - with the
exception of private keys, scripts, and system-wide authentication services -
is placed in a stripped-down copy of the original environment.  The private
keys and scripts are left in the original environment where only the
privileged instance of racoon will have access to them.

Here are basic instructions for setting up racoon to run with privilege
separation:


First, create a user/group for racoon to run under.  For example, user:group
ike:ike.  The account should not have a usable password or real home
directory, so copy the general format of another system-services type account
such as 'daemon'.

You already have files in, e.g. /usr/local/etc/racoon - perhaps racoon.conf, a
certs directory containing certificates, a scripts directory, and other
miscellaneous files such as welcome messages.  Perform the following steps:

cd /usr/local/etc/racoon
mkdir root
mv certs root
mkdir certs
mv root/certs/*.key certs

If you want to be able to switch back and forth between using and not using
privsep, do this too:

cd /usr/local/etc/racoon/certs
for i in ../root/certs/*
do
	ln -s $i .
done

Now root/certs contains certificates and certs contains the keys.  The idea is
that the public certificates are in the chroot'd area
(/usr/local/etc/racoon/root) and the keys are available only to the privileged
instance of racoon.

Move any other racoon configuration data into /usr/local/etc/racoon/root,
with the exception of the scripts directory and racoon.conf.

All the files in /usr/local/etc/racoon/root should be owned by root and the
ike:ike user you created should not have write access to any directories or
files (unless you are using something like 'path backupsa', but you get the
idea).

Create the device nodes:

mkdir root/dev

Do whatever your OS requires to populate the new dev directory with a
minimal set of devices, e.g. mknod, MAKEDEV, or mount devfs...  In freebsd
this is done by adding a line to /etc/fstab:

devfs	/usr/local/etc/racoon/root/dev	devfs	rw		0	0

and then adding a line like this to /etc/rc.conf:

devfs_set_rulesets="/usr/local/etc/racoon/root/dev=devfsrules_basic"

and then adding the following lines to /etc/devfs.rules:

[devfsrules_basic=10]
add include $devfsrules_hide_all
add include $devfsrules_unhide_basic

and then either rebooting or entering "mount -a && /etc/rc.d/devfs start".

When done with that:

mkdir -p root/usr/local/etc
ln -s ../../../ root/usr/local/etc/racoon

This dummy hierarchy keeps the config file consistent between both copies of
racoon. Of course, you could actually put the certs directory and any other
configuration data down in the hierarchy but I prefer to leave it at the root
and link to it as shown.  You may end up with something like this:

root# ls -FC /usr/local/etc/racoon/root
certs/	dev/	usr/

root# ls -l /usr/local/etc/racoon/root/usr/local/etc
lrwxr-xr-x  1 root  wheel  9 Mar  7 22:13 racoon -> ../../../

root# ls -FC /usr/local/etc/racoon/root/usr/local/etc/racoon/
certs/	dev/	usr/

Presumably your racoon.conf already contains something like:

path certificate "/usr/local/etc/racoon/certs";
path script "/usr/local/etc/racoon/scripts";

If so, great. If not, add them. Then, finally, add the privsep section:

privsep {
	user "ike";
	group "ike";
	chroot "/usr/local/etc/racoon/root";
}

Apply the patches posted to the list and rebuild racoon (the patches will be
incorporated into the release subsequent to the date of this memo, so if you
use that or a later release you can skip this step).

Restart racoon and hopefully things will work.  As of the date of this memo,
re-loading the configuration file with racoonctl will not work with privsep
enabled.  However, the problem is not insurmountable and if you figure it out
let us know.

I have not tested privsep with many of racoon's features such as XAUTH or
scripts, so if you have trouble with them and work anything out please reply
to the list so that your discoveries may be incorporated into this document.

Last modified: $Date: 2008/03/28 04:18:52 $
The gss-api authentication mechanism implementation for racoon was
based on the ietf draft draft-ietf-ipsec-isakmp-gss-auth-06.txt.

The implementation uses the Heimdal gss-api library, i.e. gss-api
on top of Kerberos 5. The Heimdal gss-api library had to be modified
to meet the requirements of using gss-api in a daemon. More specifically,
the gss_acquire_cred() call did not work for other cases than
GSS_C_NO_CREDENTIAL ("use default creds"). Daemons are often started
as root, and have no Kerberos 5 credentials, so racoon explicitly
needs to acquire its credentials. The usual method (already used
by login authentication daemons) in these situations is to add
a set of special credentials to be used. For example, authentication
by daemons concerned with login credentials, uses 'host/fqdn' as
its credential, where fqdn is the hostname on the interface that
is being used. These special credentials need to be extracted into
a local keytab from the kdc. The default value used in racoon
is 'ike/fqdn', but it can be overridden in the racoon config file.

The modification to the Heimdal gss-api library implements the
mechanism above. If a credential other than GSS_C_NO_CREDENTIAL
is specified to gss_acquire_cred(), it first looks in the default
credential cache if it its principal matches the desired credential.
If not, it extracts it from the default keytab file, and stores
it in a memory-based credential cache, part of the gss credential
structure.



The modifcations to racoon itself are as follows:

	* The racoon.conf config file accepts a new keyword, "gssapi_id",
	  to be used inside a proposal specification. It specifies
	  a string (a Kerberos 5 principal in this case), specifying the
	  credential that racoon will try to acquire. The default value
	  is 'ike/fqdn', where fqdn is the hostname for the interface
	  being used for the exchange. If the id is not specified, no
	  GSS endpoint attribute will be specified in the first SA sent.
	  However, if the initiator does specify a GSS endpoint attribute,
	  racoon will always respond with its own GSS endpoint name
	  in the SA (the default one if not specified by this option).

	* The racoon.conf file accepts "gssapi_krb" as authentication
	  method inside a proposal specification. The number used
	  for this method is 65001, which is a temporary number as
	  specified in the draft.

	* The cftoken.l and cfparse.y source files were modified to
	  pick up the configuration options. The original sources
	  stored algorithms in bitmask, which unfortunately meant
	  that the maximum value was 32, clearly not enough for 65001.
	  After consulting with the author (sakane@kame.net), it turned
	  out that method was a leftover, and no longer needed. I replaced
	  it with plain integers.

	* The gss-api specific code was concentrated as much as possible
	  in gssapi.c and gssapi.h. The code to call functions defined
	  in these files is conditional on HAVE_GSSAPI, except for the
	  config scan code. Specifying this flag on the compiler commandline
	  is conditional on the --enable-gssapi option to the configure
	  script.

	* Racoon seems to want to send accepted SA proposals back to
	  the initiator in a verbatim fashion, leaving no room to
	  insert the (variable-length) GSS endpoint name attribute.
	  I worked around this by re-assembling the extracted SA
	  into a new SA if the gssapi_krb method is used, and the
	  initiator sent the name attribute. This scheme should
	  possibly be re-examined by the racoon maintainers, storing
	  the SAs (the transformations, to be more precise) in a different
	  fashion to allow for variable-length attributes to be
	  re-inserted would be a good change, but I considered it to be
	  beyond the scope of this project.

	* The various state functions for aggressive and main mode
	  (in isakmp_agg.c and isakmp_ident.c respectively) were
	  changed to conditionally change their behavior if the
	  gssapi_krb method is specified.


This implementation tried to follow the specification in the ietf draft
as close as possible. However, it has not been tested against other
IKE daemon implementations. The only other one I know of is Windows 2000,
and it has some caveats. I attempted to be Windows 2000 compatible.
Should racoon be tried against Windows 2000, the gssapi_id option in
the config file must be used, as Windows 2000 expects the GSS endpoint
name to be sent at all times. I have my doubts as to the W2K compatibility,
because the spec describes the GSS endpoint name sent by W2K as
an unicode string 'xxx@domain', which doesn't seem to match the
required standard for gss-api + kerberos 5 (i.e. I am fairly certain
that such a string will be rejected by the Heimdal gss-api library, as it
is not a valid Kerberos 5 principal).

With the Heimdal gss-api implementation, the gssapi_krb authentication
method will only work in main mode. Aggressive mode does not allow
for the extra round-trips needed by gss_init_sec_context and
gss_accept_sec_context when mutual authentication is requested.
The draft specifies that the a fallback should be done to main mode,
through the return of INVALID-EXCHANGE-TYPE if it turns out that
the gss-api mechanisms needs more roundtrips. This is implemented.
Unfortunately, racoon does not seem to properly fall back to
its next mode, and this is not specific to the gssapi_krb method.
So, to avoid problems, only specify main mode in the config file.


	-- Frank van der Linden <fvdl@wasabisystems.com>

HOW-TO use plainrsa auth, contributed by Simon Chang <simonychang@gmail.com>

Before you begin, you should understand that the RSA authentication
mechanism hinges upon the idea of a split cryptographic key:  one used
by the public, the other readable only to you.  Any data that is
encrypted by a public key can be decrypted only by the corresponding
private key, so that the private key user can be assured that the
content of the transmission has not been examined by unauthorized
parties.  Similarly, any data encrypted by the private key can be
decrypted by the public key so that the public knows that this
transmission came from this user and nobody else (this idea is called
non-repudiation).  Also, the longer the key length, the more difficult
it would be for potential attacker to conduct brute-force discovery of
the keys.  So, what all this means for the security administrator is
that the setup needs a pair of reasonably long keys for each host that
wishes to authenticate in this manner.

With this in mind, it should be relatively straightforward to set up
RSA authentication.  For the purpose of this document, we assume that
we are setting up RSA authentication between two networked hosts
called Boston and Chicago.  Unless otherwise noted, all steps should
be performed on both hosts with corresponding key names.  Here are the
steps:

1)  Included in each default installation of ipsec-tools is a binary
called plainrsa-gen.  This executable is used to generate a pair of
RSA keys for the host.  There are only two parameters that you should
be concerned about: -b, which sets the number of bits for the keys,
and -f, which specifies the output file for plainrsa-gen to send the
results.  On an ordinary Pentium-II with 128 MB of RAM, it takes only
seconds to generate keys that are 2048 bits long, and only slightly
longer to generate 4096-bit keys.  Either key length should be
sufficient; any longer key length actually reduces performance and
does not increase security significantly.  You should therefore run it
as:

	plainrsa-gen -b 2048 -f /var/tmp/boston.keys

2)  When the process completes, you should have a text file that
includes both public and private keys.  GUARD THIS FILE CAREFULLY,
because once a private key is compromised it is no longer any good,
and you must generate a new pair from scratch.  Reading the file
itself, you should see several very long lines of alphanumeric data.
The only line you should be concerned with is the line towards the top
of the output file that begins with "# pubkey=0sAQPAmBdT/" or
something to that effect.  This line is your public key, which should
be made available to the other host that you are setting up.  Copy
this line to a separate file called "boston.pub" and change the
beginning of the line so that it reads ": PUB 0sAQPAmBdT/".
Alternatively, you can also grab the first line of the boston.keys
file and uncomment the line so that it reads the same as above.  Now
rename the file you generated initially to "boston.priv".

3)  You should now have two files, boston.priv and boston.pub
(chicago.priv and chicago.pub on Chicago).  The first file contains
your private key and the second file your public key.  Next you should
find a way to get the public key over to the other host involved.
Boston should have (1) its own key pair, and (2) Chicago's public key
ONLY.  Do not copy Chicago's private key over to Boston, because (a)
it is not necessary, and (b) you would now have two potential places
for losing control of your private key.

4)  You should now configure the racoon.conf configuration file for
each host to (a) turn on RSA authentication, and (b) designate each
host's private key and the remote host(s)'s public key(s).  Take all
your keys and place it in one directory and use the global directive
"path certificate" to specify the location of the keys.  This step is
especially important if you are running racoon with privilege
separation, because if racoon cannot find the keys inside the
directory you have just specified it will fail the authentication
process.  So, write the directive like the following:

	path certificate "/etc/racoon";

Next, you need to specify the host's own private key and the public
keys of all the remote peers involved. For your local private key and 
remote public key(s), you should use the following directives:

	certificate_type plain_rsa "/etc/racoon/boston.priv";
	peers_certfile plain_rsa "/etc/racoon/chicago.pub";

Notice the option "plain_rsa" for both directives.

Finally, under the "proposal" statement section, you should specify
the "rsasig" option for "authentication_method".

5)  You have finished configuring the host for RSA authentication.
Now use racoonctl to reload the configuration or simply restart the
machine and you should be all set.

TROUBLESHOOTING

In the event that the hosts fail to communicate, first go back to the
instructions above and make sure that:

1)  You have placed all the keys in the directory that is specified by
the "path certificate" directive.  Keep in mind that privilege
separation will force racoon to look into that directory and nowhere
else.
2)  You have specified correctly the host's own private key and the
remote peer's public key.
3)  You have specified the "rsasig" method for authentication in the
proposal statement.

If you run into any further problems, you should try to use "racoon
-v" to debug the setup, and send a copy of the debug messages to the
mailing list so that we can help you determine what the problem is.

Last modified: $Date: 2006/12/10 05:51:14 $
See http://www.kame.net/newsletter/20001119b/
$NetBSD: README,v 1.8 2012/03/29 18:35:11 wiz Exp $


Building multi-ABI libraries for NetBSD platforms.


src/compat has a framework to (re)build the libraries shipped with
NetBSD for a different ABI than the default for that platform.  This
allows 32-bit libraries for the amd64 and sparc64 ports, and enables
the mips64 port to support all three of old-style 32-bit ("o32"), the
new 32-bit (default, "n32", 64-bit CPU required) or the 64-bit ABI.


The basic premise is to re-set $MAKEOBJDIR to fresh subdirectory
underneath src/compat and rebuild the libraries with a different set
of options.  Each platform wanting support should create their port
subdirectory directly in src/compat, and then one subdirectory in here
for each ABI required, e.g., src/compat/amd64/i386 is where we build
the 32-bit compat libraries for the amd64 port.  In each of these
subdirectories, a small Makefile and makefile fragment should exist.  The
Makefile should set BSD_MK_COMPAT_FILE to equal the fragment, and then
include "../../compatsubdir.mk".  E.g., amd64/i386/Makefile has:

	BSD_MK_COMPAT_FILE=${.CURDIR}/bsd.i386.mk

	.include "../../compatsubdir.mk"

In the makefile fragment any changes to ABI flags are passed here
and the MLIBDIR variable must be set to the subdirectory in /usr/lib
where libraries for the ABI will be installed.  There are a couple of
helper Makefiles around.  amd64/i386/bsd.i386.mk looks like:

	LD+=			-m elf_i386
	MLIBDIR=		i386
	LIBC_MACHINE_ARCH=	${MLIBDIR}
	COMMON_MACHINE_ARCH=	${MLIBDIR}
	KVM_MACHINE_ARCH=	${MLIBDIR}
	PTHREAD_MACHINE_ARCH=	${MLIBDIR}
	BFD_MACHINE_ARCH=	${MLIBDIR}
	CSU_MACHINE_ARCH=	${MLIBDIR}
	CRYPTO_MACHINE_CPU=	${MLIBDIR}
	LDELFSO_MACHINE_CPU=	${MLIBDIR}

	.include "${NETBSDSRCDIR}/compat/m32.mk"

and the referenced m32.mk looks like:

	COPTS+=			-m32
	CPUFLAGS+=		-m32
	LDADD+=			-m32
	LDFLAGS+=		-m32
	MKDEPFLAGS+=		-m32

	.include "Makefile.compat"


compatsubdir.mk holds the list of subdirectories (the libraries and
ld.elf_so) to build with this ABI.

archdirs.mk holds the list of subdirectories for each port.

Makefile.compat has the basic framework to force the right paths for
library and ld.elf_so linkage.  It contains a hack to create subdirectories
in the build that should be fixed.

dirshack/Makefile is a hack to get objdirs created timely, and should
be fixed in a better way.



mrg@eterna.com.au
december 2009
	$NetBSD: README,v 1.1 2010/11/11 23:04:24 pooka Exp $


This directory contains various examples on how to interface with
a rump kernel.  Another place which contains even more examples
is src/tests and its subdirectories.  Some programs which were
located here have in fact been moved to src/tests to serve as
automated tests.
	$NetBSD: README.txt,v 1.2 2010/03/29 02:11:14 pooka Exp $

Using rump it is possible to build a router test setup consisting
of thousands of NetBSD IP stacks within a single host OS, one
networking stack per application process.  Each IP stack instance
has its own set of interfaces, addresses and routing tables.  These
instances may or may not share the same code, i.e. it is possible
to do compatibility testing of new features.  The advantage over
using full-fledged virtual OS setups (qemu, Xen, etc.) is scalability:
the rump IP router base runtime takes less than 500kB of memory
per instance.

The code is _ONLY AN EXAMPLE_ as opposed a fully featured test kit.
Some code tweaking is probably required to make this do what you
want.  Usage examples follow.

To use one single rump networking stack instance with access to
two real networks, you need tap and bridge on the host system (yes,
this involves some memory copies.  the resulting router setup can
still saturate a GigE, though.  it should not be difficult to bring
performance to be ~the same as an in-kernel stack, but haven't
managed to implement that yet).

Anyway, the following can be done with the current code:

/*
 * Usage:
 *
 * # ifconfig yourrealif0 up
 * # ifconfig tap0 create
 * # ifconfig tap0 up
 * # ifconfig bridge0 create
 * # brconfig bridge0 add tap0 add yourrealif0
 * # brconfig bridge0 up
 * #
 * # ifconfig yourrealif1 up
 * # ifconfig tap1 create
 * # ifconfig tap1 up
 * # ifconfig bridge1 create
 * # brconfig bridge1 add tap1 add yourrealif1
 * # brconfig bridge1 up
 * #
 * # ./router virt0 192.168.1.1 255.255.255.0 192.168.1.255 \
 * #          virt1 192.168.2.1 255.255.255.0 192.168.2.255
 *
 * This will bind virtN to tapN and act as a router.
 */

As brilliant ascii art, it would look something like this:

           network                                 network
              ^                                       ^
              |                                       |
         /----v-------------\            /------------v----\
 kernel  | realif0 <-> tap0 |            | tap1 -> realif1 |
         \---------------^--/            \---^-------------/
-------------------------|-------------------|--------------------
                    /----v-------------------v----\
   user             | virt0 <-> rump IP <-> virt1 |
		    \-----------------------------/

(ok, no more drawing)

The addresses configured to the rump virt0 and virt1 interfaces
will be visible on the physical network, and their traffic can be
examined with e.g. wireshark.   You can also use wireshark on
tap0/tap1.

The alternate approach is to use purely internal simulation.  The
shmif rump driver uses a memory-mapped file as an ethernet "bus"
between multiple rump networking stack instances.  Just use
rump_pub_shmif_create() in the code.  This can also of course be
combined with the tap setup, and you can have setups where border
nodes talk to an internal mesh of shmif's.  Semi-drawn, it looks
like this:

net1 <-> virt0, shm0 <-> shm1, shm2 <-> .... <-> shmN, virt1 <-> net1
           (rump0)         (rump1)      ....      (rumpN)

Linear setups (where router n talks to exactly router n-1 and n+1)
can be easily autogenerated.  Here's a snippet of executed commands
I used to start a few hundred routers (NOTE! the usage of the
example code is different!):

./a.out 10.0.0.1 10.0.0.255 /tmp/rumpshm_0 0 10.0.1.2 10.0.1.255 /tmp/rumpshm_1 10.0.1.1
./a.out 10.0.1.1 10.0.1.255 /tmp/rumpshm_1 10.0.1.2 10.0.2.2 10.0.2.255 /tmp/rumpshm_2 10.0.2.1
./a.out 10.0.2.1 10.0.2.255 /tmp/rumpshm_2 10.0.2.2 10.0.3.2 10.0.3.255 /tmp/rumpshm_3 10.0.3.1
./a.out 10.0.3.1 10.0.3.255 /tmp/rumpshm_3 10.0.3.2 10.0.4.2 10.0.4.255 /tmp/rumpshm_4 10.0.4.1
....
./a.out 10.0.252.1 10.0.252.255 /tmp/rumpshm_252 10.0.252.2 10.0.253.2 10.0.253.
255 /tmp/rumpshm_253 10.0.253.1
./a.out 10.0.253.1 10.0.253.255 /tmp/rumpshm_253 10.0.253.2 10.0.255.1 10.0.255.
255 /tmp/rumpshm_255 0

(see startrouters.sh for a script to produce that output)

Easy but slightly more interesting setups, such as a M^N matrix
(hyper-matrix?) are also possible, but left as an exercise to the
reader.

Compiling the router depends a little on what networking domain
and what interface you want to use for testing.  The very basic
setup with IP+virtif will get you quite far:

cc rumprouter.c -lrumpnet_virtif -lrumpnet_netinet -lrumpnet_net -lrumpnet \
    -lrump -lrumpuser -lpthread
	$NetBSD: README,v 1.1 2011/10/15 12:58:43 mbalmer Exp $

This directory contains example code that illustrates how to use Lua modules.
$NetBSD: README,v 1.2 2011/11/27 09:07:11 skrll Exp $

This directory contains example programs written in assembly language
for a variety of platforms.  They are intended to illustrate the
specific details of how to write assembly code on a given platform;
they are not supposed to teach assembly (although they might have this
side-effect).

If you want to build one of these example programs, you can "cp -rf"
the corresponding directory anywhere else where you have write
permissions and then issue a "make" within the directory.
$NetBSD: README,v 1.1 2006/09/15 15:49:29 elad Exp $

These files can be used by developers interested in writing security models
for NetBSD from scratch.

They address both developing security models for in-tree integration or
distribution as LKMs.

Placed in the public domain.
$NetBSD: README,v 1.1 2011/10/12 01:05:00 yamt Exp $

pgfs - a puffs file system server backed by a PostgreSQL database

install:
	install postgresql (tested with 9.2devel)
	# make
	# make install

newfs:
	# createdb hoge
	# psql -f newfs.sql hoge

mount:
	# mount_pgfs -o dbname=hoge,dbuser=takashi,nconn=16 a /mnt
	$NetBSD: README,v 1.1 2010/07/06 14:16:45 pooka Exp $

dtfs moved to src/tests/fs/puffs/h_dtfs
#	$NetBSD: README,v 1.2 1997/03/26 07:14:32 mikel Exp $
#	@(#)README	8.1 (Berkeley) 6/5/93

WEB ---- (introduction provided by jaw@riacs) -------------------------

Welcome to web2 (Webster's Second International) all 234,936 words worth.
The 1934 copyright has elapsed, according to the supplier.  The
supplemental 'web2a' list contains hyphenated terms as well as assorted
noun and adverbial phrases.  The wordlist makes a dandy 'grep' victim.

     -- James A. Woods    {ihnp4,hplabs}!ames!jaw    (or jaw@riacs)

$NetBSD: README,v 1.9 2017/07/04 09:32:20 wiz Exp $

This directory contains a conversion tool from mdoc to html
for use with groff. It is obsolete since NetBSD defaults to using
mandoc, and can be removed when groff is removed.
This directory contains 14 shell procedures designed to carry out
various verification and regeneration tasks on the UNIX User's
Manual.  The outputs of all procedures are left in files in
/_u_s_r/_m_a_n/_t_m_p; `tocrc (see below) also leaves output in
/_u_s_r/_m_a_n/_m_a_n_0.  By default, these procedures operate on all 8
sections of the manual.  The options `-s' and `-f' are available
(except in `mgrep' and `tocrc') to restrict the list of sections
and/or files to be used.  For example:

          ckspell -s 1 2 3 -f a\*

will check spelling in all files whose names begin with `a' in
Sections 1-3.  Two additional options, `-m' and `-t', can be used
to change the shell procedures' idea of where the manual and its
`tmp' directory reside.  For example:

          list -m /usr/aman -t /usr/aman/tmp

might be meaningful if, for instance, an alternate manual is
located in /_u_s_r/_a_m_a_n.  These options are also useful when a new
manual is being built in a secluded place.

Note that some of the shell procedures produce 8 result files,
one for each section of the manual.  In particular, the 4 shell
procedures prefaced with `ck', which perform different types of
verification, produce a unique sorted list for each section, as
opposed to a file-by-file list.  This means that one must search
all the files in a section (using `grep', most likely) for
occurrences of a particular string.

Occasionally, some of these procedures will produce lines of
spurious output.  This happens when, for instance, some text
looks like a cross-reference or a file name, e.g., `array(3)' or
`nroff/troff'.

The following describes these 14 procedures:

1.  ckcrefs
    Locates all cross-references to other manual entries and
    checks to see whether the referenced pages exist.  Produces
    files _b_a_d_c_r_e_f[_1-_8] containing all bad cross-references in
    each section.  Also produces files _l_o_w_e_r._s_u_f[_1-_8], containing
    occurrences of lower-case section suffixes, i.e., 1c, 1m, 3c,
    which should be changed to upper-case (1C, 1M, 3C, etc.).

2.  ckfrefs
    Locates all references in the FILES portion of manual entries
    and checks to see whether the referenced files exist in the
    running system.  Produces files _b_a_d_f_r_e_f[_1-_8] containing
    references to non-existent files.  Note that file references
    under headings other than FILES are _n_o_t checked.  Temporary
    files will, of course, not be found.

3.  cknames
    Performs various checks on the `.TH' line and the NAME
    section of entries.  Note that the files produced by this
    procedure contain the file names of entries that fail the
    corresponding check:

        Checks to see that the entry contains a `.SH NAME'
        section, producing files _n_o._N_A_M_E[_1-_8].

        Checks the NAME section of the entry to ensure that it is
        exactly one line long (multi-line NAMEs will severely
        confuse `tocrc'), producing files _n_o_t._o_n_e._l_i_n_e[_1-_8].

        Checks to see that the entry contains a `.TH' line,
        producing files _n_o._T_H[_1-_8].

        Checks that the entry name and section given on the TH
        line match the file name of that entry.  For example, a
        file containing `.TH GURP 1M' should be called `gurp.1m'.
        Produces files _f_i_l_e._m_a_t_c_h[_1-_8].

        Checks that the first name appearing on the NAME line is
        the same as the entry name on the TH line (`ckso' below
        assumes that this is always true).  Produces files
        _n_a_m_e._o_r_d_e_r[_1-_8].

4.  ckso
    This procedure performs two types of verification of _n_r_o_f_f
    `.so' pointers in /_u_s_r/_m_a_n/_m_a_n[_1-_8].  It first locates files
    that contain only a `.so' reference to a real entry, and
    checks to see whether that file (entry) exists.  Bad
    references are written to the files _b_a_d_s_o[_1-_8].  Secondly,
    `ckso' verifies the reverse; it locates each real entry,
    looks at the NAME portion to see whether more than one name
    appears there, and checks whether a file with a `.so'
    reference exists for all such names other than the first.
    Missing `.so' entries are written to the files _n_e_e_d_s_o[_1-_8].

5.  ckspell
    Uses _s_p_e_l_l to check for spelling errors in manual
    entries.  Produces file _s_p._e_r_r_s containing a section-by-
    section list of errors.  Uses file /_u_s_r/_m_a_n/_t_o_o_l_s/_s_p._i_g_n_o_r_e
    to eliminate strings that appear often in the manual and are
    normally flagged as errors by `spell'.

6.  list
    Produces file _l_i_s_t containing a `long' listing with block
    counts (`ls -ls') for each section of the manual.

7.  mcmp
    Compares two versions of the manual and reports what files
    are unique to each and whether or not the common files have
    changed.  If the `-d' option is given, _d_i_f_f-style listings
    are generated for each common file instead.  The `-o' option
    is used to specify the name of the second manual directory;
    /_u_s_r/_n_m_a_n is the default.  Produces files _c_m_p[_1-_8] or
    _d_i_f_f[_1-_8].

8.  mgrep
    Searches entire manual for the patterns specified as
    arguments (i.e., `mgrep "typewriter"').  Produces file _g_r_e_p_s,
    containing section-by-section list for each pattern.

9.  mklinks
    Creates files containing appropriate `.so' links to major
    entries where necessary.  These links point to their own
    directory; don't run this procedure anywhere else than in
    /_u_s_r/_m_a_n.  Should resolve all errors noted in _n_e_e_d_s_o[_1-_8]
    (see `ckso' above).

10. mroff
    Uses the _m_a_n command to _t_r_o_f_f and typeset manual entries.
    The `-p' (yes, `-p'!) option is used to produce entries in a
    6x9 inch format, as opposed to the default 8.5x11.  Produces
    files _m_l_o_g[_1-_8] containing logs of the files that were
    processed.  _M_r_o_f_f ignores files that contain only a `.so'
    line.

11. pgcnt
    Produces files _p_a_g_e_s[_1-_8] containing page counts for each
    entry.  Also produces _t_o_t_a_l_p_g_s containing totals for each
    section and a grand total.  The `-p' option should be used to
    count pages in the small format (see `mroff' above).  Uses
    the C program _p_a_g_e_s (compiled from _p_a_g_e_s._c).

12. prnames
    Produces files _n_a_m_e_s[_1-_8] containing the NAME portion of each
    entry.

13. prsynops
    Produces files _s_y_n_o_p_s[_1-_8] containing the SYNOPSIS portion of
    each entry.  A question mark means that the entry has no
    SYNOPSIS portion.

14. tocrc
    Regenerates input for Table of Contents and Permuted Index.
    Use `tocrc all' to regenerate both from scratch, `tocrc t' to
    regenerate both from existing input files _t_o_c_x[_1-_8] in
    /_u_s_r/_m_a_n/_t_m_p, or `tocrc [1-8]' to create, in /_u_s_r/_m_a_n/_t_m_p,
    the corresponding input file _t_o_c_x[_1-_8].  The `-p' option
    should be used when preparing the table of contents and/or
    index in the small (6x9 inch) format (this option, if
    present, _m_u_s_t be the first argument to `tocrc').  See
    description in /_u_s_r/_m_a_n/_R_E_A_D._M_E of the files in
    /_u_s_r/_m_a_n/_m_a_n_0.  Uses files _b_r_e_a_k and _i_g_n_o_r_e in
    /_u_s_r/_m_a_n/_t_o_o_l_s.

The file ._p_a_r_a_m is described in /_u_s_r/_m_a_n/_R_E_A_D._M_E.  The files
_M._f_o_l_i_o and _M._t_a_b_s are self-explanatory.




































In July 2014 the layout of /usr/share/doc was drastically rearranged.
Because of the lack of rename support in CVS, the source dirs for the
docs have not yet been rearranged to match. (And also, many of the
source dirs are scattered across the tree and are hard to find
anyhow.)

The following is a hopefully complete list of the /usr/share/doc docs
and where they come from. For things in external, I've listed where
the makefiles that install the docs are; these point to the doc
sources, which are usually somewhere somewhat different.


Reference docs (in /usr/share/doc/reference/ref[1-9])
------------------------------------------------------------

	ref1/atf		src/external/bsd/atf
	ref1/bzip		src/lib/libbz2
	ref1/config		src/share/doc/smm/config
	ref1/csh		src/bin/csh/USD.doc
	ref1/ex			src/external/bsd/nvi/docs/USD.doc/exref
	ref1/gprof		src/usr.bin/gprof/PSD.doc
	ref1/kyua/*		src/external/bsd/kyua-*/share/doc/kyua-*
	ref1/mail		src/usr.bin/mail/USD.doc
	ref1/make		src/usr.bin/make/USD.doc
	ref1/roff/memacros	src/share/doc/usd/19.memacros
	ref1/roff/meref		src/share/doc/usd/20.meref
	ref1/roff/msdiffs	src/share/doc/usd/18.msdiffs
	ref1/roff/msmacros	src/share/doc/usd/17.msmacros
	ref1/roff/mom		src/gnu/usr.bin/groff/contrib/mom/momdoc
	ref1/roff/mom_examples	src/gnu/usr.bin/groff/contrib/mom/examples
	ref1/sh			src/bin/sh/USD.doc
	ref1/vi			src/external/bsd/nvi/docs/USD.doc/vi.ref

	ref3/curses		src/lib/libcurses/PSD.doc
	ref3/sysman		src/share/doc/psd/05.sysman
	ref3/sockets		src/share/doc/psd/20.ipctut
	ref3/sockets-advanced	src/share/doc/psd/21.ipc

	ref6/rogue		src/games/rogue/USD.doc
	ref6/trek		src/games/trek/USD.doc

	ref7/quotas		src/share/doc/smm/04.quotas

	ref8/bind9		src/external/bsd/bind/bin/html
	ref8/lpd		src/usr.sbin/lpr/SMM.doc
	ref8/ntp		src/external/bsd/ntp/html
	ref8/postfix		src/external/ibm-public/postfix/share/html
	ref8/timedop		src/usr.sbin/timed/SMM.doc/timedop
	ref8/timed		src/usr.sbin/timed/SMM.doc/timed

	ref9/net		src/share/doc/smm/18.net
	ref9/nfs		src/share/doc/smm/06.nfs

Papers (in /usr/share/doc/papers)
------------------------------------------------------------

	mckusick84-ffs		src/share/doc/smm/05.fastfs
	mckusick85-fsck		src/sbin/fsck_ffs/SMM.doc
	morris78-password	src/share/doc/smm/17.password

User supplementary documents (in /usr/share/doc/usd)
------------------------------------------------------------

	beginner		src/share/doc/usd/01.begin
	ed			external/bsd/nvi/docs/USD.doc/edit
	vi			external/bsd/nvi/docs/USD.doc/vitut

System manager manual (in /usr/share/doc/smm)
------------------------------------------------------------

	(none)

Programmer supplementary documents (in /usr/share/doc/psd)
------------------------------------------------------------

	(none)



Here is a list of the historic documents as of just before the reorg:
------------------------------------------------------------

	usd/01.begin		kept in USD
	usd/02.learn		missing
	usd/03.shell		-> ref1/sh
	usd/04.csh		-> ref1/csh
	usd/05.dc		missing
	usd/06.bc		missing
	usd/07.mail		-> ref1/mail
	usd/08.mh		missing
	usd/09.edtut		missing
	usd/10.edadv		missing
	usd/11.edit		kept in USD
	usd/12.ex		-> ref1/ex
	usd/12.vi		kept in USD
	usd/13.viref		-> ref1/vi
	usd/14.jove		missing
	usd/15.sed		missing
	usd/16.awk		missing
	usd/17.msmacros		-> ref1/roff/msmacros
	usd/18.msdiffs		-> ref1/roff/msdiffs
	usd/19.memacros		-> ref1/roff/memacros
	usd/20.meref		-> ref1/roff/meref
	usd/21.troff		missing
	usd/22.trofftut		missing
	usd/23.eqn		missing
	usd/24.eqnguide		missing
	usd/25.tbl		missing
	usd/26.refer		missing
	usd/27.invert		missing
	usd/28.bib		missing
	usd/29.diction		missing
	usd/30.rogue		-> ref6/rogue
	usd/31.trek		-> ref6/trek

	smm/01.setup		kept (for now) in SMM
	smm/02.config		-> ref1/config
	smm/03.fsck_ffs		-> papers/mckusick85-fsck
	smm/04.quotas		-> ref7/quotas
	smm/05.fastfs		-> papers/mckusick84-ffs
	smm/06.nfs		-> ref9/nfs
	smm/07.lpd		-> ref8/lpd
	smm/11.timedop		-> ref8/timed
	smm/12.timed		-> ref5/timed
	smm/13.amd		missing
	smm/16.security		missing
	smm/17.password		-> papers/morris78-password
	smm/18.net		-> ref9/net

	(It is not clear what happened to SMM docs 8-10, 14-15, and 19+.)

	psd/01.cacm		missing
	psd/02.implement	missing
	psd/03.iosys		missing
	psd/04.uprog		missing
	psd/05.sysman		-> ref3/sysman
	psd/06.Clang		missing
	psd/07.pascal		missing
	psd/08.f77		missing
	psd/09.f77io		missing
	psd/10.gdb		missing
	psd/11.adb		missing
	psd/12.make		-> ref1/make
	psd/13.rcs		missing
	psd/14.sccs		missing
	psd/15.yacc		missing
	psd/16.lex		missing
	psd/17.m4		missing
	psd/18.gprof		-> ref1/gprof
	psd/19.curses		-> ref3/curses
	psd/20.ipctut		-> ref3/sockets
	psd/21.ipc		-> ref3/sockets-advanced
$NetBSD: README,v 1.2 1995/03/24 03:58:29 cgd Exp $

June 25, 1986


This is a much modified version of Phantasia.  It is intended to fix
all reported bug fixes, enhance the game, and speed up the game.

I have to thank Chris Robertson for many ideas which have made the game
faster, and more user-friendly.  Most of her changes/additions are
incorporated in this latest versions, although perhaps not in the exact
manner of her design.  I left out a few items which were not in keeping
with the spirit of the game.  (For example, I didn't like the extra lives
and the pausing of the game.  I think it's too easy even WITHOUT that stuff.)

CHANGES:

    - Wormholes have been deleted (I never liked them anyway).
    - The source code has been greatly enhanced for speed, size, readability,
      and maintainability.
      fight.c should no longer cause optimizers to run out of space.
    - A few loopholes have been tightened to make the game more enjoyable.
      (Except for those who are in the habit of exercising those loopholes.)
    - Chris' map is enclosed.
    - The "charac" file is not compatible with older versions of Phantasia
      (3.3.1 and 3.3.1+).  A 'convert' program is provided to convert your
      old file to the new format.  See Makefile for details.
    - Movements can be made with HJKL for WSNE, respectively.
    - Players may examine others while playing ('x') option.
    - Monsters are now stored in a binary data base, to speed calling
      monsters, and to ease formatting of monster listings.
    - Taxes are collected on all gold and gems.
    - Dead players can be resurrected by the 'wizard'.
    - 'setup' is smarter, although not as smart as it should be.
    - Players can change their names and passwords

PORTABILTY:

    I have tried to make this as non-machine/system specific as possible.

    All identifiers are unique to 7 characters or less, dual case.

    The code WILL NOT fit on a 16-bit machine without separate I/D.

    Stdio MUST support fopen() with mode "r+".  I think this is true
    for all Version 7 and later.

    'curses' library functions are required.

    All problems/solutions with portability should be reported to me,
    and fixes will be included in subsequent versions of this software.


Please send me any bugs, (of which I am sure there are many), you may find,
but PLEASE be specific.  I cannot correct a bug which is described as:

    "When I choose a character type, it blows up."

    (What blows up?  What exactly was printed at the terminal?
    Which character type was chosen?  Etc. . . ?)

Also, please tell me which version of UN*X you are running, and upon
which type of hardware.

I will also do my best to help anyone with problems just trying to
get the game running.  Again, I need to know which version of UN*X
and what type of CPU.  Also, a copy of the output from 'make'
would be extremely useful.

Any and all ideas/suggestions/additions are more than welcome.  If
you feel strongly enough about it, write the change and send it to me,
and I will do my best to incorporate it in the next version of Phantasia.
Otherwise, I will give serious thought to adding it myself.

Follow the directions in the Makefile CAREFULLY to set up the game.
Read the comments at the beginning of 'main.c', if you haven't already.

Enjoy.

Ted Estes
AT&T Information Systems
Skokie, IL  60077

...!ihnp4!ttrdc!ttrda!estes
$NetBSD: README,v 1.2 1995/03/21 12:14:21 cgd Exp $

Bog is a fairly portable simulation of Parker Brother's game of Boggle and
is similar to the 4.[23] BSD "boggle" and Sun's "boggletool".
Bog has not been derived from any proprietary code.
It has been tested on the Sun 3 under SunOS 3.2 and on the Atari 1040ST (MWC).

What You Need

You will need curses/termcap and a large word list.
The minix word list or /usr/dict/words will do nicely.
The word list must already be sorted (you can use "sort -c" to check).

Contents

	README		- this file
	Makefile
	bog.man		- half-hearted man page (use the game's help command)
	bog.h		- configuration and header info
	bog.c		- machine independent game code
	word.c		- machine independent word list routines
	help.c		- (curses) help routine
	mach.c		- (curses) display code
	prtable.c	- ditto
	timer.c		- machine dependent (os) input polling
	mkdict.c	- convert a word list to a bog dictionary
	mkindex.c	- create an index file for the bog dictionary
	showdict.c	- print a bog dictionary to stdout

Portability

- I've tried to make bog.c (the program logic) independent of the I/O.
  My plan was to make it straightforward to adapt the game to run under a
  windowing system (eg., Suntools, GEM).  I have no plan to actually do this.
  I've stuck to a small subset of the curses routines.
- The program runs with the input in raw mode.
- If you want the running timer you must #define TIMER in bog.h
  and insert the input polling code in timer.c for your system.  There is
  already code there for BSD, SYSV, and ATARI.

Setup

1. Check bog.h and Makefile and edit to fit your environment
2. "make all"
   This will make all the binaries and create the dictionary and index files
3. Move "dict", "dict.ind", and "helpfile" to where you specified in bog.h
4. Play away

Distribution

You may use this software for your enjoyment and you may share it with others.
You may not sell this software or use it for any commercial purposes
whatsoever.  All modified versions of the software that you redistribute must
clearly indicate your changes.

If you come across any bugs or make any changes you'd like to share please
send mail to me rather than posting to the net.

Enjoy. [But beware: boggle can be addictive!]

-----
Barry Brachman           | UUCP:    {alberta,uw-beaver,uunet}!
Dept. of Computer Science|           ubc-vision!ubc-csgrads!brachman
Univ. of British Columbia| Internet: brachman@cs.ubc.ca
Vancouver, B.C. V6T 1W5  |           brachman%ubc.csnet@csnet-relay.arpa
(604) 228-5010           | brachman@ubc.csnet
$NetBSD: README,v 1.2 1995/03/23 08:33:07 cgd Exp $

Larn is a dungeon type game program.  Larn is a adventure/action game similar
in concept to rogue or hack, but with a much different feel. 
Try it, you'll like it!

You will have to edit the Makefile to reflect your configuration.  Define
LARNHOME as the place where the larn auxiliary files will reside, and
BINDIR as the place where the larn executable should be placed.  Type
"make" to compile, or "make all" to compile and install ("make install"
does just the install).

Here's a list of what is in each of the various source files:

Fixed.Bugs		 this is a list of the things that were changed
				since ver 11.0
Makefile		 makefile script to compile the program
Make.lint		 makefile script to run larn sources through lint
README			 this is what you are now reading
bill.c          code for the letters of praise if player wins
config.c        data definitions for the installation dependent data --
                    savefilenames, where the scorefiles are, etc.
create.c        code to create the dungeon and all objects
data.c          data definitions for the game -- no code here
diag.c          code to produce diagnostic data for wizards, & savegame stuff
display.c       code to update the display on the screen
fortune.c		code for the fortune cookies
global.c        code for globally used functions that are specific to larn
header.h        constant and structure definitions
help.c          code for the help screens in the game of larn
.holidays		data file which lists upcoming holidays
io.c    	    code to handle file and terminal i/o
.larn.help.uue	larn help file (UUENCODED)
.larnmaze		data file for pre-made mazes
.larnopts		a sample .larnopts option data file
.lfortune		data file which contains the hints
main.c          code for the main command control and parsing
monster.c       code to handle attack and defense modes with monsters
moreobj.c		code for the fountains, altars, thrones
movem.c         code to move the monsters around the dungeon
nap.c           code to sleep for less than a second
object.c        code to handle objects in the dungeon
regen.c			code to regenerate the player and advance game time
savelev.c	code to get/put a level from level storage into working
		level memory
scores.c        code to process and manage the scoreboard
signal.c        code to handle UNIX signals that are trapped
store.c         code for the larn thrift shoppe, bank, trading post, lrs
tok.c           code for the input front end and options file processing

To find out how to play the game, run it and type in a '?' to get the help
screens.  By the way, the wizards password is "pvnert(x)" and to become wizard
type in an underscore, you are then prompted for the password.  Wizards are
non-scoring characters that get enlightenment, everlasting expanded 
awareness, and one of every object in the game.  They help the author to debug
the game.

Note regarding the wizard id:  If you are using userid's, then WIZID must be
set to the userid of the person who can become wizard.  If you are using
player id's, WIZID must be set to the playerid (edit file .playerids if needed)
of the player who can become wizard.

You may want to clear out the scoreboard.  The command "larn -c" will make a
new scoreboard.  It will prompt you for the wizards password.

BUGS & FIXES:

James McNamara has volunteered to maintain the latest sources, and provide
latest bug fixes to anyone who asks.  Both James and I will field requests for
sources, for those who ask.

			  ___	Prince of Gems (alias Noah Morgan)
			 /.  \	USENET: panda!condor!noah
			 \   /	at GenRad Inc.  Bolton MA
			  \ /
			   v

Below is some additional info about the installation of larn:

Install: Notes on the game LARN installation.
Larn is copyrighted 1986 by Noah Morgan.
This file (below) originally by James D. McNamara, last update 7/27/86 by nm

THIS DISTRIBUTION:

	You should receive six (6) shar files, which are:

	larn.part-1
	larn.part-2
	larn.part-3
	larn.part-4
	larn.part-5
	larn.part-6

I.	Use /bin/sh (or your system equivalent) to "unravel" shar files
	larn.part-1, ..., larn.part-6.  I suggest you do this directly
	into $LARNHOME (See Section III.).  Notable files:

	README	-	The author's how-to.
	MANIFEST -	Files you should have.

III.	Edit a copy of "Makefile" and leave the edited version in $LARNHOME.

All the "configuration" options are tidily near the top of the "Makefile."
Here are the ones you probably will want to edit:

LARNHOME	I specified (literally) the directory, with path from root,
	where "larn" will reside.  This included where I put the *.c files,
	it is where the *.o files ended up, as well as all data and *.h files.
	i suspect the *.c and intallation-documentation files can be moved off,
	but the data and bits must all remain here for execution.

BINDIR		I specified (literally) the directory, with path from root,
	where the executable "larn" will reside.  The "Makefile" will dump
	the "a.out", named "larn", in this directory.  My BINDIR was not
	my LARNHOME, so $BINDIR/larn was the ONLY file dumed here.  You'll
	probably have to chmod it for public execute, etc.


OPTIONS		This is how *I* specified them... they are documented in-line:
	OPTIONS = -DWIZZARD -DWIZID=157 -DEXTRA -DBSD -DSAVEINHOME

IV.	Compile the bugger.  Read "README" before you do.  You have a couple
	of options here:

	make			- will not install, suspect good for updates.
	make all		- compile (and) intall
	make install		- just install

	I did "make" and then "make install" -- seems to work "ok", but
	"make all" probably safer, if I had known.  Note that "Makefile"
	is the default file for "make."

V.	Execute and have fun.  If wizard code "ok", larn -c will refresh the
	scoreboard.  Play and win (or get killed) to put somebody on the
	scoreboard.

VI.	BUGS and FIXES.

	Please forward any bug-fixes in these regards to me (or Noah), so I may
	compile a fix-list for other installers.  Thanks.

Regards,
===============================================================================
James D. McNamara                    CSNET:   jim@bu-cs
                                     ARPANET: jim%bu-cs@csnet-relay
                                     UUCP:    ...harvard!bu-cs!jim
                                     BITNET:  jim%bu-cs%csnet-relay.arpa@wiscvm
===============================================================================

#	$NetBSD: README,v 1.2 1995/03/23 08:28:29 cgd Exp $
#	@(#)README	8.1 (Berkeley) 5/31/93

The potentially offensive fortunes are not installed by default on BSD
systems.  If you're absolutely, *positively*, without-a-shadow-of-a-doubt
sure that your user community wants them installed, whack the Makefile
in the subdirectory datfiles, and do "make all install".

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
	Some years ago, my neighbor Avery said to me: "There has not been an
adequate jokebook published since "Joe_Miller", which came out in 1739 and
which, incidentally, was the most miserable no-good ... jokebook in the
history of the printed word."
	In a subsequent conversation, Avery said: "A funny story is a funny
story, no matter who is in it - whether it's about Catholics or Protestants,
Jews or Gentiles, blacks or whites, browns or yellows.  If a story is genuinely
funny it makes no difference how dirty it is.  Shout it from the rooftops.
Let the chips fall all over the prairie and let the bonehead wowsers yelp.
... on them."
	It is a nice thing to have a neighbor of Avery's grain.  He has
believed in the aforestated principles all his life.  A great many other
people nowadays are casting aside the pietistic attitude that has led them
to plug up their ears against the facts of life.  We of The Brotherhood
believe as Avery believes; we have never been intimidated by the pharisaical
meddlers who have been smelling up the American landscape since the time of
the bundling board.  Neither has any one of our members ever been called a
racist.  Still, we have been in unremitting revolt against the ignorant
propensity which ordains, in effect, that "The Green Pastures" should never
have been written; the idiot attitude which compelled Arthur Kober to abandon
his delightful Bella Gross, and Octavius Roy Cohen to quit writing about the
splendiferous Florian Slappey; the moronic frame of mind which, if carried
to its logical end, would have forbidden Ring Lardner from writing in the
language of the masses.
		-- H. Allen Smith, "Rude Jokes"

	... let us keep in mind the basic governing philosophy of The
Brotherhood, as handsomely summarized in these words: we believe in
healthy, hearty laughter -- at the expense of the whole human race, if
needs be.
	Needs be.
		-- H. Allen Smith, "Rude Jokes"

THE HUNT PROTOCOL
=================

These are some notes on the traditional INET protocol between hunt(6) and 
huntd(6) as divined from the source code.

(In the original hunt, AF_UNIX sockets were used, but they are not 
considered here.)

The game of hunt is played with one server and several clients. The clients
act as dumb 'graphics' clients in that they mostly only ever relay the
user's keystrokes to the server, and the server usually only ever sends
screen-drawing commands to the client. ie, the server does all the work.

The game server (huntd) listens on three different network ports which 
I'll refer to as W, S and P, described as follows:

	W	well known UDP port (26740, or 'udp/hunt' in netdb)
	S	statistics TCP port
	P	game play TCP port

The protocol on each port is different and are described separately in
the following sections.

Lines starting with "C:" and "S:" will indicate messages sent from the 
client (hunt) or server (huntd) respectively.

W - well known port
-------------------
	This server port is used only to query simple information about the 
	game such as the port numbers of the other two ports (S and P),
	and to find out how many players are still in the game.

	All datagrams sent to (and possibly from) this UDP port consist of 
	a single unsigned 16-bit integer, encoded in network byte order.

	Server response datagrams should be sent to the source address
	of the client request datagrams.

	It is not useful to run multiple hunt servers on the one host
	interface, each of which perhaps listen to the well known port and
	respond appropriately. This is because clients will not be able to
	disambiguate which game is which.

	It is reasonable (and expected) to have servers listen to a 
	broadcast or multicast network address and respond, since the
	clients can extract a particular server's network address from
	the reply packet's source field.

    Player port request

	A client requests the game play port P with the C_PLAYER message.
	This is useful for clients broadcasting for any available games. eg:
		
		C: {uint16: 0 (C_PLAYER)}
		S: {uint16: P (TCP port number for the game play port)}

	The TCP address of the game play port should be formed from the
	transmitted port number and the source address as received by
	the client.

    Monitor port request

	A client can request the game play port P with the C_MONITOR message.
	However, the server will NOT reply if there are no players in
	the game. This is useful for broadcasting for 'active' games. eg:

		C: {uint16: 1 (C_MONITOR)}
		S: {uint16: P (TCP port number for the game play port)}

    Message port request

	If the server receives the C_MESSAGE message it will
	respond with the number of players currently in its game, unless
	there are 0 players, in which case it remains silent. This
	is used when a player wishes to send a text message to all other
	players, but doesn't want to connect if the game is over. eg:

		C: {uint16: 2 (C_MESSAGE)}
		S: {uint16: n (positive number of players)}

    Statistics port request

	The server's statistics port is queried with the C_SCORES message.
	eg:

		C: {uint16: 3 (C_SCORES)}
		S: {uint16: S (TCP port number for the statistics port)}


S - statistics port
-------------------
	The statistics port accepts a TCP connection, and keeps
	it alive for long enough to send a text stream to the client.
	This text consists of the game statistics. Lines in the
	text message are terminated with the \n (LF) character. 

		C: <connect>
		S: <accept>
		S: {char[]: lines of text, each terminated with <LF>}
		S: <close>

	The client is not to send any data to the server with this
	connection.

P - game play port
------------------
	This port provides the TCP channel for the main game play between
	the client and the server.

	All integers are unsigned, 32-bit and in network byte order.
	All fixed sized octet strings are ASCII encoded, NUL terminated.

    Initial connection

	The initial setup protocol between the client and server is as follows.
	The client sends some of its own details, and then the server replies
	with the version number of the server (currently (uint32)-1).

		C: <connect>
		S: <accept>
		C: {uint32:   uid}
		C: {char[20]: name}
		C: {char[1]:  team}
		C: {uint32:   'enter status'}
		C: {char[20]: ttyname}
		C: {uint32:   'connect mode'}
		S: {uint32:   server version (-1)}

	If the 'connect mode' is C_MESSAGE (2) then the server will wait
	for a single packet (no longer than 1024 bytes) containing
	a text message to be displayed to all players. (The message is not
	nul-terminated.)

		C: {char[]:	client's witty message of abuse}
		S: <close>

	The only other valid 'connect mode's are C_MONITOR and C_PLAYER.
	The server will attempt to allocate a slot for the client. 
	If allocation fails, the server will reply immediately with 
	"Too many monitors\n" or "Too many players\n', e.g.:

		S: Too many players<LF>
		S: <close>

	The 'enter status' integer is one of the following:

		1 (Q_CLOAK)	the player wishes to enter cloaked
		2 (Q_FLY)	the player wishes to enter flying
		3 (Q_SCAN)	the player wishes to enter scanning

	Any other value indicates that the player wishes to enter in
	'normal' mode.

	A team value of 32 (space character) means no team, otherwise
	it is the ASCII value of a team's symbol.

	On successful allocation, the server will immediately enter the 
	following phase of the protocol.

    Game play protocol

	The client provides a thin 'graphical' client to the server, and
	only ever relays keystrokes typed by the user:

		C: {char[]:	user keystrokes}

	Each character must be sent by the client as soon as it is typed.


	The server only ever sends screen drawing commands to the client.
	The server assumes the initial state of the client is a clear
	80x24 screen with the cursor at the top left (position y=0, x=0)

	    Literal character	225 (ADDCH)

		S: {uint8: 225} {uint8: c}

		The client must draw the character with ASCII value c
		at the cursor position, then advance the cursor to the right.
		If the cursor goes past the rightmost column of the screen,
		it wraps, moving to the first column of the next line down.
		The cursor should never be advanced past the bottom row.

		(ADDCH is provided as an escape prefix.)

	    Cursor motion	237 (MOVE)

		S: {uint8: 237} {uint8: y} {uint8: x}

		The client must move its cursor to the absolute screen
		location y, x, where y=0 is the top of the screen and
		x=0 is the left of the screen.

	    Refresh screen	242 (REFRESH)

		S: {uint8: 242}

		This indicates to the client that a burst of screen
		drawing has ended. Typically the client will flush its
		own drawing output so that the user can see the results.

		Refreshing is the only time that the client must
		ensure that the user can see the current screen. (This
		is intended for use with curses' refresh() function.)

	    Clear to end of line 227 (CLRTOEOL)

		S: {uint8: 227}

		The client must replace all columns underneath and
		to the right of the cursor (on the one row) with 
		space characters. The cursor must not move.

	    End game		229 (ENDWIN)

		S: {uint8: 229} {uint8: 32}
		S,C: <close>

		S: {uint8: 229} {uint8: 236}
		S,C: <close>

		The client and server must immediately close the connection.
		The client should also refresh the screen.
		If the second octet is 236 (LAST_PLAYER), then 
		the client should give the user an opportunity to quickly 
		re-enter the game. Otherwise the client should quit.

	    Clear screen	195 (CLEAR)

		S: {uint8: 195}

		The client must erase all characters from the screen
		and move the cursor to the top left (x=0, y=0).

	    Redraw screen	210 (REDRAW)

		S: {uint8: 210}

		The client should attempt to re-draw its screen.

	    Audible bell	226 (BELL)

		S: {uint8: 226}

		The client should generate a short audible tone for
		the user.

	    Server ready	231 (READY)

		S: {uint8: 231} {uint8: n}

		The client must refresh its screen.

		The server indicates to the client that it has
		processed n of its characters in order, and is ready
		for more commands. This permits the client to 
		synchronise user actions with server responses if need be.

	    Characters other than the above.

		S: {uint8: c}

		The client must draw the character with ASCII value c
		in the same way as if it were preceded with ADDCH
		(see above).


David Leonard, 1999.

$OpenBSD: README.protocol,v 1.1 1999/12/12 14:51:03 d Exp $
What *is* hunt?

	Hunt is a multi-player search-and-destroy game that takes place
	in a maze.  The game may either be slow and strategic or fast
	and tactical, depending on how familiar the players are with the
	keyboard commands.

Distribution Policy:

	Hunt is part of the user-contributed software distributed by
	Berkeley in 4BSD.  The sources are copyrighted by the authors
	and the University of California.  You may redistribute freely
	as long as the copyright notices are retained.

Words of Warning:

	hunt uses the socket mechanism of 4BSD Unix, so if you are on
		System V (my sympathies), you're on your own.
	If your machine does not permit non-setuid-root processes to
		broadcast UDP packets, then hunt uses a *very* inefficient
		method for locating the hunt server: it sends a packet
		to every host on your network.  If your machine falls
		into this category, we strongly recommend that you use
		either standalone or inetd mode *and* start hunt by
		specifying the hunt server host.
	hunt can be configured to use Unix-domain sockets, but that
		code has not been tested in recent memory.  Also, since
		4.2BSD Unix-domain sockets are buggy, running hunt on
		4.2BSD with Unix-domain sockets will probably crash
		your system.  If you want to experiment, feel free to
		do so.  However, don't say I didn't warn you :-).
	hunt uses a fair amount of CPU time, both in user time (for
		computing interactions) and system time (for processing
		terminal interrupts).  We found that a VAX 750 can
		support about three users before the system is
		noticeably impacted.  The number goes up to about 8 or
		10 for a VAX 8650.  On a network of Sun 3/50's with the
		server running on a 3/280, things work much more
		smoothly as the computing load is distributed across
		many machines.
	hunt may be dangerous to your health.  "Arthritic pain" and
		"lack of circulation" in fingers have been reported by
		hunt abusers.  Hunt may also be addictive, and the
		withdrawal symptoms are not pretty :-)

Installation:

	1. Edit file "Makefile" and make sure the options selected are
		reasonable.  There are four "make" variables that you
		should check: GAME_PARAM, SYSCFLAGS, SYSLDFLAGS, and DEFS.
		GAME_PARAM controls what features of the game will be
		compiled in (e.g. reflecting walls).  The optional features
		are listed in comments above where GAME_PARAM is defined.
		If you want to try them, just add the ones you want to the 
		GAME_PARAM definition.

		DEFS is where most system configuration is described.
		If your system is 4.3BSD, Sun, Ultrix, Convex, HPUX
		v6.0.1, or SGI, you're in luck.  We provide the
		appropriate definitions for these systems and you just
		need to select one of them (e.g. if you have an Ultrix
		system, just change the line
			DEFS=	$(GAME_PARAM) $(DEFS_43)
		to
			DEFS=	$(GAME_PARAM) $(DEFS_ULTRIX)
		).  If your system is *not* listed above, then you may
		need to do some experiments.  All of the options are
		documented in the Makefile, be brave.

		SYSCFLAGS and SYSLDFLAGS are used for "unusual" systems
		and you probably won't need to deal with it.  An
		example of an unusual system is the Silicon Graphics
		IRIS, which keeps the network socket code in a BSD
		emulation library that is in -lbsd.  Edit these only if
		you *know* your system is "different."

	2. Edit file "Makefile" and look at the "install:" target.  By
		default, files are installed in /usr/games,
		/usr/games/lib, and /usr/man/man6, which are "standard"
		locations for games.  If your system has a local games
		directory, you'll need to change these.
	3. Edit file "pathname.c" and make sure the file names and port
		numbers are reasonable.  You can ignore the first set
		of variables as they are used only for debugging
		purposes.  The second set is used in the installed
		version of hunt.  The important variables are "Driver"
		(where the server is kept), "Test_port" (the Internet
		UDP port number that new players should use to contact
		the server), and "Stat_file" (where scoring statistics
		and body counts are written).  The only tricky variable
		here is "Test_port".  The default value is chosen so
		that it is unlikely to conflict with other service port
		numbers, but you can change it if you want to.
	4. Type "make install", which will compile and install the
		programs and manual pages.  Now you're almost ready to
		go (see next section).  There may be some warnings during
		compilation.  Ignore them.

Setting up the network:

	Hunt may be set up in one of three modes: standalone, inetd, or
	nothing.  In "standalone" mode, there is always a hunt server
	running on a server machine.  All players who enter the game
	will be talking to this server.  This is the mode we use at
	UCSF.  The cost is one entry in the process table on the server
	machine.  In "inetd" mode, the server is started via inetd.
	Again, only one machine should be set up to answer game
	requests.  The cost is having to edit a few system files.  In
	"nothing" mode, no server is running when there is no one
	playing.  The first person to enter hunt will automatically
	start up a server on his machine.  This, of course, gives him
	an unfair advantage.  Also, there may be race conditions such
	that players end up in different games.  The choice of which
	mode to use depends on site configuration and politics.  We
	recommend using "standalone" mode because it is simple to set
	up and starts up rapidly.

	-----

	FOR STANDALONE MODE, put these lines in /etc/rc.local on the
	server machine.  THERE SHOULD ONLY BE ONE SERVER MACHINE!

	# start up the hunt daemon if present
	if [ -f /usr/games/lib/huntd ]; then
		/usr/games/lib/huntd -s & (echo -n ' huntd')	>/dev/console
	fi

	Also, you should start one up (on the off chance that you will
	want to test this mess :-) by typing "/usr/games/lib/hunt -s".

	-----

	FOR INETD MODE, then things get more complicated.  You need to
	edit both /etc/services and /etc/inetd.conf.  In /etc/services,
	add the line

	hunt		26740/udp

	26740 corresponds to the default "Test_port".  If you changed
	that variable, then you should put whatever value you used here
	as well.  In /etc/inetd.conf, add the line

	hunt	dgram	udp	wait	nobody	/usr/games/lib/huntd	huntd

	This works for 4.3BSD.  I don't remember the configuration file
	format for 4.2BSD inetd.

	See the huntd.6 manual page for more details.

	-----

	FOR NOTHING MODE, do nothing.

Testing:
	Now you are ready to test the code.  Type "/usr/games/hunt" or
	whatever you call the hunt executable.  You should be prompted
	for your name and team.  Then you should get the display of a
	maze.  At this point, you should read the manual page :-).

======

Hunt is not officially supported by anyone anywhere (that I know of);
however, bug reports will be read and bug fixes/enhancements may be
sent out at irregular intervals.  Send no flames, just money.  Happy
hunting.

					Conrad Huang
					conrad@cgl.ucsf.edu
					Greg Couch
					gregc@cgl.ucsf.edu
					October 17, 1988

P.S.  The authors of the game want to emphasize that this version of hunt
was started over eight years ago, and the programming style exhibited here
in no way reflects the current programming practices of the authors.
#	$NetBSD: README.files,v 1.5 2012/09/06 22:20:38 riz Exp $

What's in this directory:

CHANGES		Changes between the XXX.XXX-1 and XXX.XXX releases.

CHANGES.prev	Changes in previous NetBSD releases.

LAST_MINUTE	Last minute changes and notes about the release.

README.files	This file.

images/		Bootable images, including ISOs and USB disk images.

patches/	Post-release binary code patches.

shared/		Binary sets shared between multiple ports.

source/		Source code.

source/sets/	Source distribution sets; see below.

source/patches/	Post-release source code patches.



In addition to the files and directories listed above, there is one
directory per architecture, for each of the architectures for which
NetBSD XXX.XXX has a binary distribution.  The contents of each
architecture's directory are described in an "INSTALL" file found in
that directory.

The most recent list of mirror sites for NetBSD is viewable at the URL:

	http://www.NetBSD.org/mirrors/

If you are receiving this distribution on a CD set, some files and
subdirectories may be on a separate disc; read all README files for more
information.

See http://www.NetBSD.org/about/crypto-export.html for the formal status of
the exportability out of the United States of some pieces of the
distribution tree containing cryptographic software.  If you export these
bits and the above document says you should not do so, it's your fault,
not ours.
= RUMP howto =

Build rump dm library, build libdm with RUMP_ACTION, build dmct with RUMP_ACTION.

cd lib/libdm/
RUMP_ACTION=1 USETOOLS=no make

cd ../../sbin/dmctl
RUMP_ACTION=1 USETOOLS=no make
RUMP_ACTION=1 USETOOLS=no make -f Makefile.server

== Server Startup ==

RUMP_SP_SERVER=unix:///tmp/dmc
env RUMP_VERBOSE=1 ./dmctl_server

Server uses /home/haad/test.img file as disk1 device.

== Client Startup ==

RUMP_SP_CLIENT=unix:///tmp/dmc
./dmctl version
	$NetBSD: README,v 1.3 2010/08/18 21:28:03 pooka Exp $

New tests must use the ATF framework; see atf(7)
and the src/tests directory.

All exceptions need prior approval from core.

Package Overview for TestFloat Release 2a

John R. Hauser
1998 December 16


TestFloat is a program for testing that a floating-point implementation
conforms to the IEC/IEEE Standard for Binary Floating-Point Arithmetic.
TestFloat is distributed in the form of C source code.  The TestFloat
package actually provides two related programs:

-- The `testfloat' program tests a system's floating-point for conformance
   to the IEC/IEEE Standard.  This program uses the SoftFloat software
   floating-point implementation as a basis for comparison.

-- The `testsoftfloat' program tests SoftFloat itself for conformance to
   the IEC/IEEE Standard.  These tests are performed by comparing against a
   separate, slower software floating-point that is included in the TestFloat
   package.

TestFloat depends on SoftFloat, but SoftFloat is not included in the
TestFloat package.  SoftFloat can be obtained through the Web page `http://
HTTP.CS.Berkeley.EDU/~jhauser/arithmetic/SoftFloat.html'.

TestFloat is documented in three text files:

   testfloat.txt          Documentation for using the TestFloat programs
                              (both `testfloat' and `testsoftfloat').
   testfloat-source.txt   Documentation for porting and compiling TestFloat.
   testfloat-history.txt  History of major changes to TestFloat.

The following file is also provided:

   systemBugs.txt         Information about processor bugs found using
                              TestFloat.

Other files in the package comprise the source code for TestFloat.

Please be aware that some work is involved in porting this software to other
targets.  It is not just a matter of getting `make' to complete without
error messages.  I would have written the code that way if I could, but
there are fundamental differences between systems that I can't make go away.
You should not attempt to compile the TestFloat sources without first
reading `testfloat-source.txt'.

At the time of this writing, the most up-to-date information about
TestFloat and the latest release can be found at the Web page `http://
HTTP.CS.Berkeley.EDU/~jhauser/arithmetic/TestFloat.html'.

$NetBSD: README.NetBSD,v 1.3 2003/07/26 19:38:47 salo Exp $


Changes at initial import...ross@NetBSD.org

  o  WARNS=2 fixes
  o  source reorganized with arch/
  o  <sys/endian.h> conversion
  o  <inttypes.h> conversion
  o  <ieeefp.h> conversion
  o  <asm.h> conversion
  o  the previously target-specific softfloat.h and milieu.h made mostly
     generic. Still some work to do with default NaN bitpatterns, endian,
     and arm/fpa-DEMANGLE issues.
  o  arch/i386/systfloat.S extended to handle:
	int32_t syst_floatx80_to_int32_round_to_zero(floatx80)
	int64_t syst_floatx80_to_int64_round_to_zero(floatx80)
  o  LONG_DOUBLE_IS_FLOATX80 was used to enable both the C module system
     access ops and the FLOATX80 tests, all of which also required FLOATX80.
     Besides being rundundant, this made it impossible to use the asm module
     for i386 which the package actually comes with and also made it
     impossible to test FLOATX80 ops without compiler support even if the
     machine actually does them.  While this is arguably OK for a regression
     test, the two cases are obviously different. Now, the tests (which
     don't actually require any compiler-understood extended type) are
     always run just by defining FLOATX80. If LONG_DOUBLE_IS_FLOATX80 is
     also defined, then the C system ops are also enabled. To switch back
     and forth, you modify only the arch/${MACHINE_ARCH}/Makefile.inc to
     do the cpp def and add or remove systfloat.S. For regression testing,
     it's better in C, but for testing the test itself or verifying a
     regression, the .S module is a better reference case.

     A similar change should probably be made for LONG_DOUBLE_IS_FLOAT128,
     but we can't test that yet.

     The basic rules:
	LONG_DOUBLE_IS_FLOAT{X80,128}
		define if you do not have a .S file and do have compiler
		support, or have both and want to use the compiled version
		(but then it may be necessary to remove the .S file or add
		an ifndef to it)
	FLOAT{X80,128}
		define if you have either HW or SW support and want it tested

    And to confuse things further: the .S files for i386 appear to have been
    originally created by compiling C code with -S, at least as a start.

    You can see some of the price of ANSI+IEEE in these files: a simple
    conversion from floatxx to intxx requires storing, modifying, loading,
    and restoring  the modes on each conversion in order to get a specific
    round, thanks to the four IEEE-mandated options. A pipeline/peephole
    pass can no doubt fix this up within one function, but not if other
    arithmetic is done in between.
$NetBSD: README,v 1.1 2006/10/09 12:32:46 yamt Exp $

this directory contains some random test programs for page replacement
policies found in sys/uvm/uvm_pdpolicy*.

TODO:
	- write how to use.
	- write a makefile.
$NetBSD: README,v 1.4 2015/01/03 13:20:11 apb Exp $

Notes for NetBSD src/tools


Background
==========

Several programs that are part of NetBSD are also built as tools.  Such
programs are typically built twice: once as a tool and once as part of
the release build.  Tools are relevant only when the make(1) variable
USETOOLS=yes, which is the default for most NetBSD builds.

Tools are built on the host platform, using the host compiler,
and will run on the host platform during the cross-build of the
remainder of NetBSD.  They are built near the beginning of a NetBSD
build (e.g. "build.sh tools" or "make tools" from the top level src
directory), and installed in ${TOOLDIR}.

Tools are executed during the main part of the build, when several
TOOL_* variables defined in src/share/mk/bsd.*.mk will refer to the
tools installed in ${TOOLDIR}.


Portability
===========

Programs that are built as tools need to be more portable than other
parts of NetBSD, because they will need to run on the host platform.

Most tools should restrict themselves to C language features that are
defined in C89 (ISO 9899-1989); they should avoid using C99 language
features.  There are a few tools, such as compilers, where it is not
practical for the C89 restriction to be maintained.  There are also a
few features, such as the long long data type, that are used by many
tools despite not being defined in C89.

Tools may use library features such as functions, macros, and
types, that are defined in C89 and in POSIX (IEEE Std 1003.1) (XXX
year?), and features that are provided by the compatibility framework
(src/tools/compat) described in a separate section below.  This is
usually not an onerous burden, because many C99 library features, and
NetBSD-specific features, are already provided by src/tools/compat, or
can be added when the need for them becomes apparent.

If a tool attempts to use a feature that is not available on the host
platform, then the tools build will fail.  This can be addressed by
changing the tool to avoid that feature, or by adding the feature to the
src/tools/compat framework.  It is usually easy to add new macros or
functions to src/tools/compat, and that is usually better than adding
compatibility definitions to individual tools.


Compatibility framework
=======================

src/tools/compat provides a compatibility framework for use by tools.
It installs the following components, and more:

${TOOLDIR}/lib/libnbcompat.a

    A library containing functions that are needed by some tools.

${TOOLDIR}/include/nbtool_compat.h

    A header file defining macros that are needed by some tools.

${TOOLDIR}/share/compat/defs.mk

    A makefile fragment, to be included by other makefiles,
    to define make variables appropriate for building tools.

    Among other things, this makefile fragment automatically adds
    the libnbcompat.a library to the LDADD and DPADD variables,
    so that tools will be linked with that library, and adds
    -I${NETBSDSRCDIR}/tools/compat and -DHAVE_NBTOOL_CONFIG_H=1 to the
    HOST_CPPFLAGS variable, so that compiled programs can detect when
    they are being built as tools.


Adapting Makefiles for use with tools
=====================================

Makefiles under src/tools/*/Makefile should define the HOSTPROG
variable.  This is typically done by tools/Makefile.hostprog,
which is directly or indirectly included by all Makefiles in
src/tools/*/Makefile.

Makefiles in the non-tools part of the src tree can test whether or not
the HOSTPROG variable is defined, in order tell the difference between
building a tool and building part of a NetBSD release, and they may
alter their behavior accordingly.

For example, the Makefile may conditionally refrain from compiling and
linking certain files, and the Makefile may conditionally pass macros to
the compiler via constructs like this:

    .if defined(HOSTPROG)
    CPPFLAGS+= -DWITH_FEATURE_X=0 # exclude feature X from tools build
    .else
    CPPFLAGS+= -DWITH_FEATURE_X=1 # include feature X in release build
    .endif

Adapting Programs for use with tools
====================================

When a tool is being built, the C compiler should automatically be
invoked with -DHAVE_NBTOOL_CONFIG_H=1.  This is done as a result of
settings in ${TOOLDIR}/share/compat/defs.mk, which should be included
from src/tools/Makefile.host, which should be included directly or
indirectly from src/tools/*/Makefile.

A C source file can test whether the HAVE_NBTOOL_CONFIG_H macro is
defined, in order to tell whether or not it is being compiled as part of
a tool.

In order to obtain the definitions provided by the tools compatibility
framework, almost every C source file that is built as part of a tool
should have lines like these as the first non-comment lines:

    #if HAVE_NBTOOL_CONFIG_H
    #include "nbtool_config.h"
    #endif

To omit features from the tools version of a program, the program
may test the HAVE_NBTOOL_CONFIG_H macro, like this:

    #if HAVE_NBTOOL_CONFIG_H
       ... code to be used when built as a tool
    #else
       ... code to be used when built as part of a release
    #endif

It is often preferable to use macros whose names refer to the features
that should be included or omitted.  See the section on "Adapting
Makefiles for use with tools" for an example in which the Makefile
passes -DWITH_FEATURE_X=0 or -DWITH_FEATURE_X=1 to the compiler
according to whether or not the program is being built as a tool.  Then
the program can use code like this:

    #if WITH_FEATURE_X 
       ... code to be used when FEATURE X is desired,
       ... e.g. when being built as part of a release.
    #else
       ... code to be used when FEATURE X is not desired,
       ... e.g. when being built as a tool.
    #endif
$NetBSD: README.mknative,v 1.5 2011/09/26 02:36:19 christos Exp $

This file describes how to use the cross-compiler to generate the
native files for GDB on a target platform.

NOTE:  DO NOT RUN "mknative" BY HAND!  It requires the Makefile in this
directory to set up certain environments first.

Since libc's features change over time, the config.h files can change as a
result; thus the instructions below are the same no matter whether
bootstrapping on a cross or native host.  This is important: even on a
"native" host, you should bootstrap the toolchain by building from an
up-to-date source tree to a $DESTDIR using the exact same instructions.

In these notes, MACHINE is the $MACHINE of the target.  These files can be
cross-generated.  Though a $MACHINE_ARCH all uses the same config files, you
must pick a specific $MACHINE so that building the requisite bits below will
work.

1. Set MKMAINTAINERTOOLS=yes in mk.conf.  (Needed so that src/tools/gettext
   gets built, eliciting proper HAVE_*GETTEXT* defns in config.h files.)

2. Build and install a cross toolchain (via "build.sh -m MACHINE tools").

3. At top level, do "nbmake-MACHINE do-distrib-dirs obj includes".

4. In src/gnu/lib/crtstuff4 do "nbmake-MACHINE depend all install"

5. In src/lib/csu, src/gnu/lib/libgcc4, and src/lib, do
   "nbmake-MACHINE all install".

6. In src/tools/gdb, do "nbmake-MACHINE obj native-gdb".

   This will do a full configury in ${.OBJDIR}/.native that is a "Canadian"
   cross toolchain (--build reflects the host platform, but --host and
   --target are the target).  The result is a tree that would build a
   native-to-NetBSD GDB on a cross host, and mknative pulls glue data
   from this.

   NOTE: this step writes files under src/external/gpl3/gdb/bin/gdb, so you
   need to do it in a writable src tree!

7. Try out a full build using "nbmake-MACHINE" in
   src/external/gpl3/bin/gdb; the result should include a native GDB.

8. If all is well, commit the glue files and directories added to
   src/external/gpl3/gdb/bin/gdb.
$NetBSD: README,v 1.12 2005/04/05 00:21:22 jmc Exp $

Special notes for cross-hosting a NetBSD build on certain platforms.  
Only those platforms which have been tested to complete a "build.sh" run
are listed.

All hosts must have a POSIX compatible sh. /bin/sh is assumed unless 
otherwise set. This can be overridden by setting HOST_SH in the environment.

In addition all hosts must provide the following local tools:

gzip

=====

NetBSD:

* _NETBSD_SOURCE is *not* to be defined/pulled in during compat/tools builds.
  compat_defs.h will error out if it finds it defined. 

HP-UX:

* zlib must be available.
  This will be fixed in the future to include zlib in libnbcompat.

=====

LINUX:

* Tested on RedHat Linux 7.1 (i386).
  Tested on RedHat Linux 7.3 (i686) on 16 Sep 2002.  Requires "LANG=C"
  in the environment.

* Tested on Redhat Linux 8.0 (i686) in Fall 2003. Requires no special settings.

* Tested on Redhat ES3 and AS3 in spring of 2004. Requires no special settings.

* The gcc (and libstdc++, if needed) package must be installed, along
  with the typical system development packages (glibc-devel, etc.).

* The ncurses-devel package must be installed (for nbinfo).

* The zlib and zlib-devel packages must be installed.  This will be
  fixed in the future to include zlib in libnbcompat.

=====

MACOS
  Requires a case sensitive filesystem such as UFS

* Tested on 10.2.8 with Dec 2002 Developer Tools
    - may require a fix to /usr/bin/join, netbsd's join should work fine
* Tested on 10.3 with xcode 1.5
    - compiles fine out of the box

=====

NETBSD (earlier releases):

* Tested on NetBSD 1.5.2 (machine-independently).

* Should need no special setup.

=====

SOLARIS:

* Tested on Solaris/x86 8 (5.8) with gcc 2.95.2 and Solaris/sparc 8 (5.8)
  with gcc 3.2 (not yet tested with SUNWspro).

* $HOST_CC needs to be set properly (for gcc, it should be set to "gcc",
  otherwise the improper /usr/ucb/cc may be invoked by accident).

* The SUNWzlib package (or a built version of zlib visible to $HOST_CC,
  such as SMCzlib from sunfreeware.com) must be installed.  This will be
  fixed in the future to include zlib in libnbcompat.

* Needs the following paths, in this order, in $PATH:

      /usr/xpg4/bin
      /usr/ccs/bin
      <path to host C and C++ compilers>
      /usr/bin

  /usr/ucb may optionally be placed before /usr/bin, per your preference,
  but /usr/ucb *MUST NOT* be before /usr/ccs/bin or before the path to
  the host C and C++ compilers.
$NetBSD: README.mknative,v 1.21 2017/05/21 15:28:42 riastradh Exp $

This file describes how to bootstrap the native toolchain on a new NetBSD
platform (and how to update the new toolchain files, if needed).  These
files may be generated on a cross-compile host without problems.

NOTE:  DO NOT RUN "mknative" BY HAND!  It requires the Makefile in this
directory to set up certain environments first.

Since libc's features change over time, the config.h files can change as a
result; thus the instructions below are the same no matter whether
bootstrapping on a cross or native host.  This is important: even on a
"native" host, you should bootstrap the toolchain by building from an
up-to-date source tree to a $DESTDIR using the exact same instructions.

In these notes, MACHINE is the $MACHINE of the target.  These files can be
cross-generated.  Though a $MACHINE_ARCH all uses the same config files, you
must pick a specific $MACHINE so that building the requisite bits below will
work.

0. Note that example paths like src/external/gpl3/gcc/lib/libgcc/arch will
   really be src/external/gpl3/gcc.old/lib/libgcc/arch for the previous GCC.

1. Set MKMAINTAINERTOOLS=yes in mk.conf.  (Needed so that src/tools/gettext
   gets built, eliciting proper HAVE_*GETTEXT* defns in config.h files.)

2. Build and install a cross toolchain (via "build.sh -m MACHINE tools").
   Note that while PR #47353 is not fixed, you can not use the -O option
   to build.sh. Use -M instead. (The differences are in layout and pathname
   prefixes in the object directory pointed to by each option.)

3. In src/tools/gcc, do "nbmake-MACHINE HAVE_GCC=48 bootstrap-libgcc".

   This will create just enough glue in src/external/gpl3/gcc/lib/libgcc/arch
   to make it possible to build, based on the toolchain built in
   ${.OBJDIR}/build.
   Because the files generated in this step contain things like
   -DCROSS_COMPILE, they are not suitable for committing.  Step 8 below
   will regenerate the "proper" libgcc config files.

4. At top level, do
   "nbmake-MACHINE obj do-distrib-dirs MKGCC=no MKBINUTILS=no HAVE_GCC=48", and
   "nbmake-MACHINE includes HAVE_GCC= MKGCC=no MKBINUTILS=no HAVE_GCC=48".
   (Note: replace 48 [for gcc 4.8.x] with the appropriate version you are
   going to mknative-for, the MKGCC=no prevents the standard makefiles from
   picking up any gcc version info automatically)

5. In src/lib/csu, do
   "nbmake-MACHINE dependall". and "nbmake-MACHINE install".

6. In src/external/gpl3/gcc/lib/libgcc, do
   "nbmake-MACHINE obj includes dependall install".

7. In each of src/external/lgpl3/gmp/lib/libgmp,
   src/external/lgpl3/mpfr/lib/libmpfr, src/external/lgpl3/mpc/lib/libmpc
   do "nbmake-MACHINE obj dependall".

8. In src/lib, do
   "nbmake-MACHINE dependall install MKGCC=no HAVE_GCC=48".

   Optionally, all of the following may be set in the environment to reduce
   the amount of code needed to build at this step.  Basically, it must be
   possible for static binaries to build and base system libs to exist so
   that "configure" can do its job for the target--these MK* options omit
   the rest for this stage of the build. 

   MKLINT=no
   MKPROFILE=no
   MKSHARE=no
   MKRUMP=no

9. In src/tools/gcc, do "nbmake-MACHINE native-gcc".

   This will do a full configury in ${.OBJDIR}/.native that is a "Canadian"
   cross toolchain (--build reflects the host platform, but --host and
   --target are the target).  The result is a tree that would build a
   native-to-NetBSD compiler on a cross host, and mknative pulls glue data
   from this.

10. Try out a full build using "nbmake-MACHINE"; the result should include
   a native compiler.

11. If all is well, commit the glue files added to src/gnu/{lib,usr.bin}/*.
$NetBSD: README,v 1.9 1995/03/21 09:04:33 cgd Exp $

ed is an 8-bit-clean, POSIX-compliant line editor.  It should work with
any regular expression package that conforms to the POSIX interface
standard, such as GNU regex(3).

If reliable signals are supported (e.g., POSIX sigaction(2)), it should
compile with little trouble.  Otherwise, the macros SPL1() and SPL0()
should be redefined to disable interrupts.

The following compiler directives are recognized:
DES		- to add encryption support (requires crypt(3))
NO_REALLOC_NULL	- if realloc(3) does not accept a NULL pointer
BACKWARDS	- for backwards compatibility
NEED_INSQUE	- if insque(3) is missing

The file `POSIX' describes extensions to and deviations from the POSIX
standard.

The ./test directory contains regression tests for ed. The README
file in that directory explains how to run these.

For a description of the ed algorithm, see Kernighan and Plauger's book
"Software Tools in Pascal," Addison-Wesley, 1981.
$NetBSD: README,v 1.8 1995/03/21 09:05:18 cgd Exp $

The files in this directory with suffixes `.t', `.d', `.r' and `.err' are
used for testing ed.  To run the tests, set the ED variable in the Makefile
for the path name of the program to be tested (e.g., /bin/ed), and type
`make'.  The tests do not exhaustively verify POSIX compliance nor do
they verify correct 8-bit or long line support.

The test file suffixes have the following meanings:
.t    Template - a list of ed commands from which an ed script is
      constructed
.d    Data - read by an ed script
.r    Result - the expected output after processing data via an ed
      script.
.err  Error - invalid ed commands that should generate an error

The output of the tests is written to the two files err.o and scripts.o.
At the end of the tests, these files are grep'ed for error messages,
which look like:
	*** The script u.ed exited abnormally ***
or:
	*** Output u.o of script u.ed is incorrect ***

The POSIX requirement that an address range not be used where at most
a single address is expected has been relaxed in this version of ed.
Therefore, the  following scripts  which test for compliance with this
POSIX rule exit abnormally:
=-err.ed
a1-err.ed
i1-err.ed
k1-err.ed
r1-err.ed
ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.10 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://tools.ietf.org/html/rfc1950 (zlib format), rfc1951 (deflate format) and
rfc1952 (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  A usage example
of the library is given in the file test/example.c which also tests that
the library is working correctly.  Another example is given in the file
test/minigzip.c.  The compression library itself is composed of all source
files in the root directory.

To compile all files and run the test program, follow the instructions given at
the top of Makefile.in.  In short "./configure; make test", and if that goes
well, "make install" should work for most flavors of Unix.  For Windows, use
one of the special makefiles in win32/ or contrib/vstudio/ .  For VMS, use
make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version.  The zlib home page is
http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read the zlib FAQ http://zlib.net/zlib_faq.html before asking for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan.  1997
issue of Dr.  Dobb's Journal; a copy of the article is available at
http://marknelson.us/1997/01/01/zlib-engine/ .

The changes made in version 1.2.10 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory contrib/ .

zlib is available in Java using the java.util.zip package, documented at
http://java.sun.com/developer/technicalArticles/Programming/compression/ .

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is available
at CPAN (Comprehensive Perl Archive Network) sites, including
http://search.cpan.org/~pmqs/IO-Compress-Zlib/ .

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://docs.python.org/library/zlib.html .

zlib is built into tcl: http://wiki.tcl.tk/4610 .

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS or BEOS.

- For PalmOs, see http://palmzlib.sourceforge.net/


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz.  The deflate and
  zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
  people who reported problems and suggested various improvements in zlib; they
  are too numerous to cite here.

Copyright notice:

 (C) 1995-2017 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
This directory contains examples of the use of zlib and other relevant
programs and documentation.

enough.c
    calculation and justification of ENOUGH parameter in inftrees.h
    - calculates the maximum table space used in inflate tree
      construction over all possible Huffman codes

fitblk.c
    compress just enough input to nearly fill a requested output size
    - zlib isn't designed to do this, but fitblk does it anyway

gun.c
    uncompress a gzip file
    - illustrates the use of inflateBack() for high speed file-to-file
      decompression using call-back functions
    - is approximately twice as fast as gzip -d
    - also provides Unix uncompress functionality, again twice as fast

gzappend.c
    append to a gzip file
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of deflatePrime() to start at any bit

gzjoin.c
    join gzip files without recalculating the crc or recompressing
    - illustrates the use of the Z_BLOCK flush parameter for inflate()
    - illustrates the use of crc32_combine()

gzlog.c
gzlog.h
    efficiently and robustly maintain a message log file in gzip format
    - illustrates use of raw deflate, Z_PARTIAL_FLUSH, deflatePrime(),
      and deflateSetDictionary()
    - illustrates use of a gzip header extra field

zlib_how.html
    painfully comprehensive description of zpipe.c (see below)
    - describes in excruciating detail the use of deflate() and inflate()

zpipe.c
    reads and writes zlib streams from stdin to stdout
    - illustrates the proper use of deflate() and inflate()
    - deeply commented in zlib_how.html (see above)

zran.c
    index a zlib or gzip stream and randomly access it
    - illustrates the use of Z_BLOCK, inflatePrime(), and
      inflateSetDictionary() to provide random access
All files under this contrib directory are UNSUPPORTED. There were
provided by users of zlib and were not tested by the authors of zlib.
Use at your own risk. Please contact the authors of the contributions
for help about these, not the zlib authors. Thanks.


ada/        by Dmitriy Anisimkov <anisimkov@yahoo.com>
        Support for Ada
        See http://zlib-ada.sourceforge.net/

amd64/      by Mikhail Teterin <mi@ALDAN.algebra.com>
        asm code for AMD64
        See patch at http://www.freebsd.org/cgi/query-pr.cgi?pr=bin/96393

asm686/     by Brian Raiter <breadbox@muppetlabs.com>
        asm code for Pentium and PPro/PII, using the AT&T (GNU as) syntax
        See http://www.muppetlabs.com/~breadbox/software/assembly.html

blast/      by Mark Adler <madler@alumni.caltech.edu>
        Decompressor for output of PKWare Data Compression Library (DCL)

delphi/     by Cosmin Truta <cosmint@cs.ubbcluj.ro>
        Support for Delphi and C++ Builder

dotzlib/    by Henrik Ravn <henrik@ravn.com>
        Support for Microsoft .Net and Visual C++ .Net

gcc_gvmat64/by Gilles Vollant <info@winimage.com>
        GCC Version of x86 64-bit (AMD64 and Intel EM64t) code for x64
        assembler to replace longest_match() and inflate_fast()

infback9/   by Mark Adler <madler@alumni.caltech.edu>
        Unsupported diffs to infback to decode the deflate64 format

inflate86/  by Chris Anderson <christop@charm.net>
        Tuned x86 gcc asm code to replace inflate_fast()

iostream/   by Kevin Ruland <kevin@rodin.wustl.edu>
        A C++ I/O streams interface to the zlib gz* functions

iostream2/  by Tyge Løvset <Tyge.Lovset@cmr.no>
        Another C++ I/O streams interface

iostream3/  by Ludwig Schwardt <schwardt@sun.ac.za>
            and Kevin Ruland <kevin@rodin.wustl.edu>
        Yet another C++ I/O streams interface

masmx64/    by Gilles Vollant <info@winimage.com>
        x86 64-bit (AMD64 and Intel EM64t) code for x64 assembler to
        replace longest_match() and inflate_fast(),  also masm x86
        64-bits translation of Chris Anderson inflate_fast()

masmx86/    by Gilles Vollant <info@winimage.com>
        x86 asm code to replace longest_match() and inflate_fast(),
        for Visual C++ and MASM (32 bits).
        Based on Brian Raiter (asm686) and Chris Anderson (inflate86)

minizip/    by Gilles Vollant <info@winimage.com>
        Mini zip and unzip based on zlib
        Includes Zip64 support by Mathias Svensson <mathias@result42.com>
        See http://www.winimage.com/zLibDll/minizip.html

pascal/     by Bob Dellaca <bobdl@xtra.co.nz> et al.
        Support for Pascal

puff/       by Mark Adler <madler@alumni.caltech.edu>
        Small, low memory usage inflate.  Also serves to provide an
        unambiguous description of the deflate format.

testzlib/   by Gilles Vollant <info@winimage.com>
        Example of the use of zlib

untgz/      by Pedro A. Aranda Gutierrez <paag@tid.es>
        A very simple tar.gz file extractor using zlib

vstudio/    by Gilles Vollant <info@winimage.com>
        Building a minizip-enhanced zlib with Microsoft Visual Studio
        Includes vc11 from kreuzerkrieg and vc12 from davispuh
Puff -- A Simple Inflate
3 Mar 2003
Mark Adler
madler@alumni.caltech.edu

What this is --

puff.c provides the routine puff() to decompress the deflate data format.  It
does so more slowly than zlib, but the code is about one-fifth the size of the
inflate code in zlib, and written to be very easy to read.

Why I wrote this --

puff.c was written to document the deflate format unambiguously, by virtue of
being working C code.  It is meant to supplement RFC 1951, which formally
describes the deflate format.  I have received many questions on details of the
deflate format, and I hope that reading this code will answer those questions.
puff.c is heavily commented with details of the deflate format, especially
those little nooks and cranies of the format that might not be obvious from a
specification.

puff.c may also be useful in applications where code size or memory usage is a
very limited resource, and speed is not as important.

How to use it --

Well, most likely you should just be reading puff.c and using zlib for actual
applications, but if you must ...

Include puff.h in your code, which provides this prototype:

int puff(unsigned char *dest,           /* pointer to destination pointer */
         unsigned long *destlen,        /* amount of output space */
         unsigned char *source,         /* pointer to source data pointer */
         unsigned long *sourcelen);     /* amount of input available */

Then you can call puff() to decompress a deflate stream that is in memory in
its entirety at source, to a sufficiently sized block of memory for the
decompressed data at dest.  puff() is the only external symbol in puff.c  The
only C library functions that puff.c needs are setjmp() and longjmp(), which
are used to simplify error checking in the code to improve readabilty.  puff.c
does no memory allocation, and uses less than 2K bytes off of the stack.

If destlen is not enough space for the uncompressed data, then inflate will
return an error without writing more than destlen bytes.  Note that this means
that in order to decompress the deflate data successfully, you need to know
the size of the uncompressed data ahead of time.

If needed, puff() can determine the size of the uncompressed data with no
output space.  This is done by passing dest equal to (unsigned char *)0.  Then
the initial value of *destlen is ignored and *destlen is set to the length of
the uncompressed data.  So if the size of the uncompressed data is not known,
then two passes of puff() can be used--first to determine the size, and second
to do the actual inflation after allocating the appropriate memory.  Not
pretty, but it works.  (This is one of the reasons you should be using zlib.)

The deflate format is self-terminating.  If the deflate stream does not end
in *sourcelen bytes, puff() will return an error without reading at or past
endsource.

On return, *sourcelen is updated to the amount of input data consumed, and
*destlen is updated to the size of the uncompressed data.  See the comments
in puff.c for the possible return codes for puff().
These classes provide a C++ stream interface to the zlib library. It allows you
to do things like:

  gzofstream outf("blah.gz");
  outf << "These go into the gzip file " << 123 << endl;

It does this by deriving a specialized stream buffer for gzipped files, which is
the way Stroustrup would have done it. :->

The gzifstream and gzofstream classes were originally written by Kevin Ruland
and made available in the zlib contrib/iostream directory. The older version still
compiles under gcc 2.xx, but not under gcc 3.xx, which sparked the development of
this version.

The new classes are as standard-compliant as possible, closely following the
approach of the standard library's fstream classes. It compiles under gcc versions
3.2 and 3.3, but not under gcc 2.xx. This is mainly due to changes in the standard
library naming scheme. The new version of gzifstream/gzofstream/gzfilebuf differs
from the previous one in the following respects:
- added showmanyc
- added setbuf, with support for unbuffered output via setbuf(0,0)
- a few bug fixes of stream behavior
- gzipped output file opened with default compression level instead of maximum level
- setcompressionlevel()/strategy() members replaced by single setcompression()

The code is provided "as is", with the permission to use, copy, modify, distribute
and sell it for any purpose without fee.

Ludwig Schwardt
<schwardt@sun.ac.za>

DSP Lab
Electrical & Electronic Engineering Department
University of Stellenbosch
South Africa
This is a patched version of zlib, modified to use
Pentium-Pro-optimized assembly code in the deflation algorithm. The
files changed/added by this patch are:

README.686
match.S

The speedup that this patch provides varies, depending on whether the
compiler used to build the original version of zlib falls afoul of the
PPro's speed traps. My own tests show a speedup of around 10-20% at
the default compression level, and 20-30% using -9, against a version
compiled using gcc 2.7.2.3. Your mileage may vary.

Note that this code has been tailored for the PPro/PII in particular,
and will not perform particuarly well on a Pentium.

If you are using an assembler other than GNU as, you will have to
translate match.S to use your assembler's syntax. (Have fun.)

Brian Raiter
breadbox@muppetlabs.com
April, 1998


Added for zlib 1.1.3:

The patches come from
http://www.muppetlabs.com/~breadbox/software/assembly.html

To compile zlib with this asm file, copy match.S to the zlib directory
then do:

CFLAGS="-O3 -DASMV" ./configure
make OBJA=match.o


Update:

I've been ignoring these assembly routines for years, believing that
gcc's generated code had caught up with it sometime around gcc 2.95
and the major rearchitecting of the Pentium 4. However, I recently
learned that, despite what I believed, this code still has some life
in it. On the Pentium 4 and AMD64 chips, it continues to run about 8%
faster than the code produced by gcc 4.1.

In acknowledgement of its continuing usefulness, I've altered the
license to match that of the rest of zlib. Share and Enjoy!

Brian Raiter
breadbox@muppetlabs.com
April, 2007
Read blast.h for purpose and usage.

Mark Adler
madler@alumni.caltech.edu
See infback9.h for what this is and how to use it.
        ZLIB version 1.2.10 for OS/400 installation instructions

1) Download and unpack the zlib tarball to some IFS directory.
   (i.e.: /path/to/the/zlib/ifs/source/directory)

   If the installed IFS command suppors gzip format, this is straightforward,
else you have to unpack first to some directory on a system supporting it,
then move the whole directory to the IFS via the network (via SMB or FTP).

2) Edit the configuration parameters in the compilation script.

        EDTF STMF('/path/to/the/zlib/ifs/source/directory/os400/make.sh')

Tune the parameters according to your needs if not matching the defaults.
Save the file and exit after edition.

3) Enter qshell, then work in the zlib OS/400 specific directory.

        QSH
        cd /path/to/the/zlib/ifs/source/directory/os400

4) Compile and install

        sh make.sh

The script will:
- create the libraries, objects and IFS directories for the zlib environment,
- compile all modules,
- create a service program,
- create a static and a dynamic binding directory,
- install header files for C/C++ and for ILE/RPG, both for compilation in
  DB2 and IFS environments.

That's all. 


Notes:  For OS/400 ILE RPG programmers, a /copy member defining the ZLIB
                API prototypes for ILE RPG can be found in ZLIB/H(ZLIB.INC).
                In the ILE environment, the same definitions are available from
                file zlib.inc located in the same IFS include directory as the
                C/C++ header files.
                Please read comments in this member for more information.

        Remember that most foreign textual data are ASCII coded: this
                implementation does not handle conversion from/to ASCII, so
                text data code conversions must be done explicitely.

        Mainly for the reason above, always open zipped files in binary mode.
This directory contains files that have not been updated for zlib 1.2.x

(Volunteers are encouraged to help clean this up.  Thanks.)
This Makefile requires devkitARM (http://www.devkitpro.org/category/devkitarm/) and works inside "contrib/nds". It is based on a devkitARM template.

Eduardo Costa <eduardo.m.costa@gmail.com>
January 3, 2009

ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.10 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://www.ietf.org/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate format)
and rfc1952.txt (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  Two compiled
examples are distributed in this package, example and minigzip.  The example_d
and minigzip_d flavors validate that the zlib1.dll file is working correctly.

Questions about zlib should be sent to <zlib@gzip.org>.  The zlib home page
is http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read DLL_FAQ.txt, and the the zlib FAQ http://zlib.net/zlib_faq.html
before asking for help.


Manifest:

The package zlib-1.2.10-win32-x86.zip will contain the following files:

  README-WIN32.txt This document
  ChangeLog        Changes since previous zlib packages
  DLL_FAQ.txt      Frequently asked questions about zlib1.dll
  zlib.3.pdf       Documentation of this library in Adobe Acrobat format

  example.exe      A statically-bound example (using zlib.lib, not the dll)
  example.pdb      Symbolic information for debugging example.exe

  example_d.exe    A zlib1.dll bound example (using zdll.lib)
  example_d.pdb    Symbolic information for debugging example_d.exe

  minigzip.exe     A statically-bound test program (using zlib.lib, not the dll)
  minigzip.pdb     Symbolic information for debugging minigzip.exe

  minigzip_d.exe   A zlib1.dll bound test program (using zdll.lib)
  minigzip_d.pdb   Symbolic information for debugging minigzip_d.exe

  zlib.h           Install these files into the compilers' INCLUDE path to
  zconf.h          compile programs which use zlib.lib or zdll.lib

  zdll.lib         Install these files into the compilers' LIB path if linking
  zdll.exp         a compiled program to the zlib1.dll binary

  zlib.lib         Install these files into the compilers' LIB path to link zlib
  zlib.pdb         into compiled programs, without zlib1.dll runtime dependency
                   (zlib.pdb provides debugging info to the compile time linker)

  zlib1.dll        Install this binary shared library into the system PATH, or
                   the program's runtime directory (where the .exe resides)
  zlib1.pdb        Install in the same directory as zlib1.dll, in order to debug
                   an application crash using WinDbg or similar tools.

All .pdb files above are entirely optional, but are very useful to a developer
attempting to diagnose program misbehavior or a crash.  Many additional
important files for developers can be found in the zlib127.zip source package
available from http://zlib.net/ - review that package's README file for details.


Acknowledgments:

The deflate format used by zlib was defined by Phil Katz.  The deflate and
zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
people who reported problems and suggested various improvements in zlib; they
are too numerous to cite here.


Copyright notice:

  (C) 1995-2012 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.
